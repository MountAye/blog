{"pageProps":{"metadata":{"slug":"qc-hackathon-write-up","filename":"2021-04-22-qc-hackathon-write-up.md","date":"2021-04-22","title":".qs | QC Hack 量子编程马拉松","layout":"post","keywords":["md","phy"],"hasMath":true,"excerpt":"4月初的时候，系秘书转发了一封邮件，耶鲁和斯坦福有两个关于量子计算的学生社团，打算举办为期一周的线上训练营,然后在周末举办一个24小时的编程马拉松","content":"\n\n## 一\n\n4月初的时候，系秘书转发了一封邮件，耶鲁和斯坦福有两个关于量子计算的学生社团，打算举办为期一周的[线上训练营](https://www.quantumcoalition.io/)，然后在周末举办一个24小时的编程马拉松 ([hackathon](https://en.wikipedia.org/wiki/Hackathon)) 的活动。只要年满18岁就可以参加，并不限定本科生。\n\n整个活动由几家从事量子计算的科技公司赞助，前面的线上训练营基本就是各家轮流上来介绍一下自己家的量子计算平台的使用方法，最后的编程马拉松也由他们每家出一套题，所以这个活动也有在学生和公司之间搭桥，给参与者争取实习机会的目的在里面。参与者可以自由组队，但是在项目提交的的时候每个人只能属于一支队伍。虽然参与者可以参加任意数量的题目，但是每一名参与者最终只能成为一家公司的优胜者。如果预感到自己在某一个项目的赢面比较大，可以在提交之前通知自己参加的其他队伍把自己除名。24小时的时间限制还是比较紧迫的，所以基本上认准一家答题就可以了。\n\n女朋友也收到了一样的邮件，所以理所当然地一起组队。我之前在本科阶段上过一门一学期的量子信息和量子通信课程，内容约等于在量子力学之后再上一个学期的习题课，以及在不讲群论的情况下应用 SU(2) 群，并没有接触过这个活动中会用到的编程语言。女朋友没有上过这门课，基本就是物理专业普通研究生的量子力学水平。周中的线上训练营，我只参加了第一天的，是 Microsoft 的 Quantum Development Kit (QDK) 和 Q# 编程语言相关的，顺便介绍了一下量子计算中很有名的 Deutsch 算法。剩下的讲座我基本上都没有参加，一方面是知道前面的规则之后就懒下来了，另一方面是实验室的工作仍然需要继续，再有就是线上活动实在是太容易摸鱼了没有效率。周五的晚上，女朋友看了一晚上我的量子信息笔记，我看了看 Q# 的语法规则，在台式机上安装了开发环境。以上就是我们参加编程马拉松之前的基础和准备。\n\n\n## 二\n\nHackathon 美东时间周六上午10点开始，周日上午10点结束。因为我们只看了 Microsoft 相关的内容，所以直奔[相关题目](https://github.com/quantumcoalition/qchack-microsoft-challenge)。\n\n题目一共分为两部分。\n\n第一部分一共四道题，就像是一般的计算机课程的作业一样，参赛者只需要在举办方写好的主程序文件里的指定区域填入代码，然后运行主办方写好的测试文件检查结果，测试通过即可得分。四道题目要求如下：\n\n1. 判断一个3-5位的2进制数能否被4整除。\n2. 判断一个3比特位当中是否至少有两位不同。\n3. 同第2题，但是要求量子比特门最多只能使用 3-比特，而且 3-比特门最多使用一次。\n4. 给定一个有两种颜色的无向图，判断图当中不含有任何单一颜色的三角形。\n\n第二部分内容比较自由，要求用 Grover's 算法解决一个自己感兴趣的问题，打哪指哪，然后写一篇文章介绍自己的这个项目，并提交相关的代码。根据问题深度(6分)、工具使用(5分)、创新性(4分)、教育价值(5分) 四方面进行评分。\n\n\n## 三\n\n### I.1.\n\n第一道题最简单，但是我们当时约等于0基础，所以做起来也颇费了一些时间。不过由于我听过第一天的课，知道 `oracle` 在 Q# 编程语言中是一个很重要的概念，所以在题目给出的参考教程 [Quantum Kantas](https://github.com/Microsoft/QuantumKatas/) 里找到了[oracle 相关的教程](https://github.com/microsoft/QuantumKatas/tree/main/tutorials/Oracles)。里面有个名为 `ControlledOnBitString` 的 function，可以根据一串量子比特的取值是否等于一个特定的二值串而对另外一个比特做一个特定的操作。前一天晚上又知道了 `Microsoft.Quantum.Convert` 的 namespace 里有各种数据类型转换的函数，搭配 `IntAsBoolArray`，就做出了第一题的初版。后来看到了更简单的 `ControlledOnInt` 函数，就直接用上了。\n\n### I.2.\n\n第二题的初版是女朋友做的。题目要求是找出是否至少两位不同，这一判断的否定就是三位比特全部相同，所以同样用 `ControlledOnBitString` 函数，然后判断一次全 `true` 一次全 `false`，再把最终结果取反就可以了。但是在做第三题的时候，因为两个题目长得太像了，中间不小心把一个能通过第二题测试但是通不过第三题测试的答案直接覆盖在了第二题上面，懒的改回去了，于是就成了最后提交的版本。\n\n### I.3.\n\n第三题和第二题非常不同。第二题的解决思路中，判断全 `true` 和全 `false`有3个控制位1个输出位，这里用了两次 4-量子比特门，所以第三问需要全新的思路。另外我曾经试过在一个 `operation` 里申请一个新的 `Qubit()` 结果测试报错，因为误解了报错信息，所以误认为除了程序的主 operation 之外不能创建新的 qubit，于是被卡住了。这时候已经来到了下午，实在想不出来又很困，于是去床上躺了一会。半睡半醒之间想到，题目虽然要求输入的量子比特不变，但是我们仍然可以直接改动输入，只要在函数结束之前把对输入的改动全部复原就可以了。于是用 CNOT 门分别作用在 1-2, 1-3 对输入的量子比特上，两个门分别以第2、3号比特为输出。然后用一个 3-bit 门判断2、3号比特是否相同，并输出到结果位上。为了复原第2、3号比特，只需要把 CNOT 在两对比特上分别再用一次就行了。\n\n但是这个结果还是无法通过测试（后来成为了第二题的提交版本），报错的提示信息是使用了超过一次 3-量子比特门——这不是开玩笑吗？于是打开了官方提供的测试文件，发现测试代码计算 3-量子比特门的使用次数的时候，会把用户定义的 3-量子比特门的数量，和 `CCNOT` 门的数量做加法，于是看文档，我们定义的那个 “用一个 3-bit 门判断2、3号比特是否相同，并输出到结果位上” 的操作和 `CCNOT` 门是等价的，于是直接换用 `CCNOT` 门，问题解决。\n\n### I.4.\n\n第四题看起来复杂，但是可以分成三个部分：\n\n1. 找出图中所有的三角形，确定每个三角形的三条边，这一步完全可以用经典算法完成；\n2. 创建一个和三角形相同数量的量子比特数列，对每个三角形，把三条边直接带入第二/三题的操作里，结果输入创建的量子比特列中；\n3. 判断量子比特列是否全为 `true`，结果输出到整个程序的结果位上。\n\n第一步由女朋友来想我来写（毕竟只有一台电脑有开发环境），难点在于：\n\n1. Q# 语法改变数列值的语法十分难受\n    <br>`mutable points = [-1,-1,-1,-1,-1,-1];`\n    <br>`set points w/=0..1 <- [0,1];`；\n2. 作为一种强类型语言对元组和数列的区分让我这个 python 选手十分蛋疼\n    <br>`(Int,Int)`/`Int[]`；\n3. 求数列中不重复的值居然不排序不能给出正确结果。\n    <br>`let uniquePoints = Arrays.Unique(EqualI,Arrays.Sorted(LessThanI,points));`\n\n这也是唯一一段用上了 `Message()` 函数来 debug 的部分。\n\n第三步就重新回到了第三题暂时敷衍掉的问题：对于在操作中创建的 `Qubit()`/`Qubit[]`，`Reset()`/`ResetAll()` 函数相当于测量，会破坏操作的 adjoint 性质，不测量则（当时的我）没有办法将这个量子比特列复原。\n\n此时已经午夜，我来解决这个问题，女朋友去看第二部分，后来她看完 Grover‘s 算法的教程去睡了，我还在想这个问题。直接把报错信息复制到 Google，找到了一个[论坛里的问答]()，好像是去年微软在其他地方举办的类似活动的。里面只是提到要“uncompute the qubits”，给出的例子用的是旧版本 Q# 的语法，~~没法直接抄~~ 。最终不抱希望地把之前对那个 `Qubit[]` 做过的循环顺序倒过来重做了一遍，诶，您猜怎么着，还是没通过！绝望了！正序重做一边，诶，通过了！为什么为什么为什么？到现在也没弄清楚。\n\n### II.\n\n然后把女朋友叫醒，让她来讲一讲 Grover's 算法。听完之后我的理解是，对于一个 $$f:(0,1)^N \\rarr (0,1)$$ 的函数，这个算法可以大概率地找到一个解 $$S\\in(0,1)^N$$ 满足 $$f(S)=1$$. \n\n至于这个函数 $$f$$，之前每一道题都是这样一个函数，当时已经夜里两三点了，实在是没时间再想一个新函数了，于是我们直接就拿复杂度最高的第4题来换个皮。换个什么皮呢？为了这个活动翘掉了这周的[《文明6》联机游戏](barrier-forward-keyboard-mouse-to-another-computer)，然后之前看 YouTuber [\"PotatoMcWhiskey\"](https://www.youtube.com/user/PotatoMcWhiskey)介绍过[一个 Mod](https://steamcommunity.com/sharedfiles/filedetails/?id=1753346735&searchtext=diplomacy)，里面可以将文明之间的外交关系可视化为无向图，所以，诶嘿嘿嘿……\n\n女朋友写完文稿就睡了，我把文稿改了改，然后和官方对 Grover's 算法的实现缝合了一下。提交的时候，距离截止时间大约还有一个小时。\n\n\n## 四\n\n之后的周五的时候收到了消息，我们得奖了。优胜者一共6支队伍。从活动结束之后公布的结果看，要想成为优胜，第一部分的4道题必须全部正确，然后第二部分得分在 8-20 分之间。\n\n这个成绩是个什么水平？截止到写这篇文章的此刻，官方题目的 Github 仓库有 80 份 fork，有少数几份 fork 是针对已有的 fork，有可能来自同一队伍，再考虑到可能有些队伍的不同成员分别 fork 了主项目，所以估测 60 支队伍应该是有的，官方给出 6 组优胜者这么一个不零不整的数字，个人猜测是取了前 10%？据主办方在 discord 提供的消息，有一支队伍的第二题成绩高于8分，但是前面没有全对，所以没有得奖；其余队伍的第二题都不超过6分；并不清楚有多少队伍第一题全对，主办方也不打算公布各队的详细成绩。\n\n这大约说明活动的参与者，其成绩基本上符合二八原理——少数人得到的分数，占据了所有参赛者全部得分的大多数。\n\n参加过这个活动之后，我们一下子就从量子计算小白摇身一变，成了优秀人才了？实际上，直到现在，我还是搞不太清楚 oracle 到底是个什么东西，女朋友对量子计算的理解估计比我还差（逃）。美国哲学教授约翰·希尔勒提出过一个叫做[“中文房间”](https://zh.wikipedia.org/wiki/%E4%B8%AD%E6%96%87%E6%88%BF%E9%97%B4)的思想实验，说一个只会说英语的人被关在一间满是汉字字块的房间里，不断从房间外收到写着中文问题的纸条。房间里有一本英文写成的手册，指示如何对输入的汉字进行回复。凭借这个手册，房中人可以在完全不会中文的情况下，与外界进行交流。希尔勒类比外人、房中人、手册，与程序员、计算机、计算机程序，认为房中人不会中文，进而论证计算机不可能通过程序来获得理解力。\n\n希尔勒教授想论证啥是他的事，我倒是对这个类比的本体很感兴趣——如果一个人已经能够熟练运用那个英文写成的汉字使用手册了，我们还能不能，能在多大程度上说他不懂中文呢？就说一般的程序员，工作时间能保证不看 stack overflow 的有几个，所以他们都不会编程？反对中文房间思想实验结论的人，很多都支持用图灵测试超过某一阈值来作为有智能的标志，但是我觉得，智能本身就不是一个非有即无的性质，而是一个连续分布，没有上限的谱。\n\n另一方面，得分名列前茅，和能力名列前茅，又是两回事。本科的时候做建模美赛，我们学校数理金融的一个学神前一年成绩“略有不佳”，没拿到 M 奖，于是我们那年找到了我和风神俩学物理的，准备再次冲击荣誉。巧了这一年的题目正好有一道浴缸放热水的问题，这不就是物理中的扩散方程嘛，那得奖还不是手拿把掐的？结果呢，H 奖，丢人丢到姥姥家去了。合着我们两个成绩还都不错的物理专业学生，在自己的专业里，打不过那么多同龄的非物理专业本科生？\n\n两相对照之下，我想起了很久之前看过的一篇博客文章，文章以一个问题开头——“熟练”的反义词是什么？当然说“生疏”这文章就写不下去了，作者给出的答案是——“应变”。熟练意味着，你对于问题、选项、最优解已经有了充分且完备的了解，只需要重复自己的经验就可以了，但是在自己不了解的战场上，经验至少不能直接派上用场，这时候，脱离具体环境的应变能力就成了生存和取胜的关键，我们当时的专业水平高不成低不就，反而成了掣肘我们的桎梏。\n\n读到这篇文章的时候，我被这种剑走偏锋的观点击中了，从那以后，一直都在注意培养自己的应变能力——如果明天我所研究的这个领域消失了，我还有没有谋生的能力？如果自己正在解决的问题被上帝或者 Matrix 作弊修改成一个新问题，我能不能看到连作弊都改动不了的题眼，然后一击命中？在凌晨两三点的时候，我也没有放弃解决第一题第 4 问的 Qubit 复位问题，虽然当时我并不知道评分标准，但是内心非常确定，这个问题必须解决。\n\n以上两次活动的成绩差别，也可以从得奖难度来看。建模美赛的 M 奖，得奖率应该远小于 10%，即便考虑到二八原理中绝大多数参赛者都只是凑数，而且样本越大凑数者越多，这个差距也还是无法忽略。我们能够得奖，和量子计算领域才刚刚萌芽，连“方兴未艾”都算不上，因此竞争并不激烈也有很大关系，应变能力是切入这些蓝海领域的必要条件，是躲避内卷的利器。我们现在对“内卷”人人喊打，但是培养应变能力是需要牺牲相当多本可以精进专业的时间和精力的。当社会中的大多数人向往着逃离内卷的时候，真的不需要有人咬定一个领域不断深耕？我现在的选择真的正确吗？我不知道。我是打算留在当前的领域继续熟练，还是换个领域应变，抑或是虚掷 PhD 光阴换一张工作签证？我也不知道。\n\n## 五\n\n哦对了，我有女朋友了，而且在 hackathon 的过程中把女朋友惹哭了……问题是我现在已经不记得具体是怎么把人家惹哭的了，连道歉都显得很不诚恳……我确实是一个不擅长合作的人，或者说跟别人说话的我，和想问题的我并不是同一个人，之前本科 CUPT 和建模的时候也一样，需要和人打交道的时候就几乎干不了活儿，严重的时候自己就退化成了鼓励师……总之一切错误在我，希望她不要记仇…… <br>（。・＿・。）ﾉ\n"},"tocStr":"{\"index\":null,\"endIndex\":null,\"map\":{\"type\":\"list\",\"ordered\":false,\"spread\":true,\"children\":[{\"type\":\"listItem\",\"spread\":false,\"children\":[{\"type\":\"paragraph\",\"children\":[{\"type\":\"link\",\"title\":null,\"url\":\"#一\",\"children\":[{\"type\":\"text\",\"value\":\"一\"}]}]}]},{\"type\":\"listItem\",\"spread\":false,\"children\":[{\"type\":\"paragraph\",\"children\":[{\"type\":\"link\",\"title\":null,\"url\":\"#二\",\"children\":[{\"type\":\"text\",\"value\":\"二\"}]}]}]},{\"type\":\"listItem\",\"spread\":true,\"children\":[{\"type\":\"paragraph\",\"children\":[{\"type\":\"link\",\"title\":null,\"url\":\"#三\",\"children\":[{\"type\":\"text\",\"value\":\"三\"}]}]},{\"type\":\"list\",\"ordered\":false,\"spread\":false,\"children\":[{\"type\":\"listItem\",\"spread\":false,\"children\":[{\"type\":\"paragraph\",\"children\":[{\"type\":\"link\",\"title\":null,\"url\":\"#i1\",\"children\":[{\"type\":\"text\",\"value\":\"I.1.\"}]}]}]},{\"type\":\"listItem\",\"spread\":false,\"children\":[{\"type\":\"paragraph\",\"children\":[{\"type\":\"link\",\"title\":null,\"url\":\"#i2\",\"children\":[{\"type\":\"text\",\"value\":\"I.2.\"}]}]}]},{\"type\":\"listItem\",\"spread\":false,\"children\":[{\"type\":\"paragraph\",\"children\":[{\"type\":\"link\",\"title\":null,\"url\":\"#i3\",\"children\":[{\"type\":\"text\",\"value\":\"I.3.\"}]}]}]},{\"type\":\"listItem\",\"spread\":false,\"children\":[{\"type\":\"paragraph\",\"children\":[{\"type\":\"link\",\"title\":null,\"url\":\"#i4\",\"children\":[{\"type\":\"text\",\"value\":\"I.4.\"}]}]}]},{\"type\":\"listItem\",\"spread\":false,\"children\":[{\"type\":\"paragraph\",\"children\":[{\"type\":\"link\",\"title\":null,\"url\":\"#ii\",\"children\":[{\"type\":\"text\",\"value\":\"II.\"}]}]}]}]}]},{\"type\":\"listItem\",\"spread\":false,\"children\":[{\"type\":\"paragraph\",\"children\":[{\"type\":\"link\",\"title\":null,\"url\":\"#四\",\"children\":[{\"type\":\"text\",\"value\":\"四\"}]}]}]},{\"type\":\"listItem\",\"spread\":false,\"children\":[{\"type\":\"paragraph\",\"children\":[{\"type\":\"link\",\"title\":null,\"url\":\"#五\",\"children\":[{\"type\":\"text\",\"value\":\"五\"}]}]}]}]}}","htmlAst":{"type":"root","children":[{"type":"element","tagName":"h2","properties":{"id":"一"},"children":[{"type":"text","value":"一","position":{"start":{"line":3,"column":4,"offset":5},"end":{"line":3,"column":5,"offset":6}}},{"type":"element","tagName":"a","properties":{"ariaHidden":"true","tabIndex":-1,"href":"#一"},"children":[{"type":"element","tagName":"span","properties":{"className":["icon","icon-link"]},"children":[]}]}],"position":{"start":{"line":3,"column":1,"offset":2},"end":{"line":3,"column":5,"offset":6}}},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"4月初的时候，系秘书转发了一封邮件，耶鲁和斯坦福有两个关于量子计算的学生社团，打算举办为期一周的","position":{"start":{"line":5,"column":1,"offset":8},"end":{"line":5,"column":49,"offset":56}}},{"type":"element","tagName":"a","properties":{"href":"https://www.quantumcoalition.io/","rel":["nofollow","noopener","noreferrer"],"target":"_blank"},"children":[{"type":"text","value":"线上训练营","position":{"start":{"line":5,"column":50,"offset":57},"end":{"line":5,"column":55,"offset":62}}}],"position":{"start":{"line":5,"column":49,"offset":56},"end":{"line":5,"column":90,"offset":97}}},{"type":"text","value":"，然后在周末举办一个24小时的编程马拉松 (","position":{"start":{"line":5,"column":90,"offset":97},"end":{"line":5,"column":112,"offset":119}}},{"type":"element","tagName":"a","properties":{"href":"https://en.wikipedia.org/wiki/Hackathon","rel":["nofollow","noopener","noreferrer"],"target":"_blank"},"children":[{"type":"text","value":"hackathon","position":{"start":{"line":5,"column":113,"offset":120},"end":{"line":5,"column":122,"offset":129}}}],"position":{"start":{"line":5,"column":112,"offset":119},"end":{"line":5,"column":164,"offset":171}}},{"type":"text","value":") 的活动。只要年满18岁就可以参加，并不限定本科生。","position":{"start":{"line":5,"column":164,"offset":171},"end":{"line":5,"column":191,"offset":198}}}],"position":{"start":{"line":5,"column":1,"offset":8},"end":{"line":5,"column":191,"offset":198}}},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"整个活动由几家从事量子计算的科技公司赞助，前面的线上训练营基本就是各家轮流上来介绍一下自己家的量子计算平台的使用方法，最后的编程马拉松也由他们每家出一套题，所以这个活动也有在学生和公司之间搭桥，给参与者争取实习机会的目的在里面。参与者可以自由组队，但是在项目提交的的时候每个人只能属于一支队伍。虽然参与者可以参加任意数量的题目，但是每一名参与者最终只能成为一家公司的优胜者。如果预感到自己在某一个项目的赢面比较大，可以在提交之前通知自己参加的其他队伍把自己除名。24小时的时间限制还是比较紧迫的，所以基本上认准一家答题就可以了。","position":{"start":{"line":7,"column":1,"offset":200},"end":{"line":7,"column":265,"offset":464}}}],"position":{"start":{"line":7,"column":1,"offset":200},"end":{"line":7,"column":265,"offset":464}}},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"女朋友也收到了一样的邮件，所以理所当然地一起组队。我之前在本科阶段上过一门一学期的量子信息和量子通信课程，内容约等于在量子力学之后再上一个学期的习题课，以及在不讲群论的情况下应用 SU(2) 群，并没有接触过这个活动中会用到的编程语言。女朋友没有上过这门课，基本就是物理专业普通研究生的量子力学水平。周中的线上训练营，我只参加了第一天的，是 Microsoft 的 Quantum Development Kit (QDK) 和 Q# 编程语言相关的，顺便介绍了一下量子计算中很有名的 Deutsch 算法。剩下的讲座我基本上都没有参加，一方面是知道前面的规则之后就懒下来了，另一方面是实验室的工作仍然需要继续，再有就是线上活动实在是太容易摸鱼了没有效率。周五的晚上，女朋友看了一晚上我的量子信息笔记，我看了看 Q# 的语法规则，在台式机上安装了开发环境。以上就是我们参加编程马拉松之前的基础和准备。","position":{"start":{"line":9,"column":1,"offset":466},"end":{"line":9,"column":401,"offset":866}}}],"position":{"start":{"line":9,"column":1,"offset":466},"end":{"line":9,"column":401,"offset":866}}},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{"id":"二"},"children":[{"type":"text","value":"二","position":{"start":{"line":12,"column":4,"offset":872},"end":{"line":12,"column":5,"offset":873}}},{"type":"element","tagName":"a","properties":{"ariaHidden":"true","tabIndex":-1,"href":"#二"},"children":[{"type":"element","tagName":"span","properties":{"className":["icon","icon-link"]},"children":[]}]}],"position":{"start":{"line":12,"column":1,"offset":869},"end":{"line":12,"column":5,"offset":873}}},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"Hackathon 美东时间周六上午10点开始，周日上午10点结束。因为我们只看了 Microsoft 相关的内容，所以直奔","position":{"start":{"line":14,"column":1,"offset":875},"end":{"line":14,"column":63,"offset":937}}},{"type":"element","tagName":"a","properties":{"href":"https://github.com/quantumcoalition/qchack-microsoft-challenge","rel":["nofollow","noopener","noreferrer"],"target":"_blank"},"children":[{"type":"text","value":"相关题目","position":{"start":{"line":14,"column":64,"offset":938},"end":{"line":14,"column":68,"offset":942}}}],"position":{"start":{"line":14,"column":63,"offset":937},"end":{"line":14,"column":133,"offset":1007}}},{"type":"text","value":"。","position":{"start":{"line":14,"column":133,"offset":1007},"end":{"line":14,"column":134,"offset":1008}}}],"position":{"start":{"line":14,"column":1,"offset":875},"end":{"line":14,"column":134,"offset":1008}}},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"题目一共分为两部分。","position":{"start":{"line":16,"column":1,"offset":1010},"end":{"line":16,"column":11,"offset":1020}}}],"position":{"start":{"line":16,"column":1,"offset":1010},"end":{"line":16,"column":11,"offset":1020}}},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"第一部分一共四道题，就像是一般的计算机课程的作业一样，参赛者只需要在举办方写好的主程序文件里的指定区域填入代码，然后运行主办方写好的测试文件检查结果，测试通过即可得分。四道题目要求如下：","position":{"start":{"line":18,"column":1,"offset":1022},"end":{"line":18,"column":94,"offset":1115}}}],"position":{"start":{"line":18,"column":1,"offset":1022},"end":{"line":18,"column":94,"offset":1115}}},{"type":"text","value":"\n"},{"type":"element","tagName":"ol","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"判断一个3-5位的2进制数能否被4整除。","position":{"start":{"line":20,"column":4,"offset":1120},"end":{"line":20,"column":24,"offset":1140}}}],"position":{"start":{"line":20,"column":1,"offset":1117},"end":{"line":20,"column":24,"offset":1140}}},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"判断一个3比特位当中是否至少有两位不同。","position":{"start":{"line":21,"column":4,"offset":1144},"end":{"line":21,"column":24,"offset":1164}}}],"position":{"start":{"line":21,"column":1,"offset":1141},"end":{"line":21,"column":24,"offset":1164}}},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"同第2题，但是要求量子比特门最多只能使用 3-比特，而且 3-比特门最多使用一次。","position":{"start":{"line":22,"column":4,"offset":1168},"end":{"line":22,"column":45,"offset":1209}}}],"position":{"start":{"line":22,"column":1,"offset":1165},"end":{"line":22,"column":45,"offset":1209}}},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"给定一个有两种颜色的无向图，判断图当中不含有任何单一颜色的三角形。","position":{"start":{"line":23,"column":4,"offset":1213},"end":{"line":23,"column":37,"offset":1246}}}],"position":{"start":{"line":23,"column":1,"offset":1210},"end":{"line":23,"column":37,"offset":1246}}},{"type":"text","value":"\n"}],"position":{"start":{"line":20,"column":1,"offset":1117},"end":{"line":23,"column":37,"offset":1246}}},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"第二部分内容比较自由，要求用 Grover's 算法解决一个自己感兴趣的问题，打哪指哪，然后写一篇文章介绍自己的这个项目，并提交相关的代码。根据问题深度(6分)、工具使用(5分)、创新性(4分)、教育价值(5分) 四方面进行评分。","position":{"start":{"line":25,"column":1,"offset":1248},"end":{"line":25,"column":116,"offset":1363}}}],"position":{"start":{"line":25,"column":1,"offset":1248},"end":{"line":25,"column":116,"offset":1363}}},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{"id":"三"},"children":[{"type":"text","value":"三","position":{"start":{"line":28,"column":4,"offset":1369},"end":{"line":28,"column":5,"offset":1370}}},{"type":"element","tagName":"a","properties":{"ariaHidden":"true","tabIndex":-1,"href":"#三"},"children":[{"type":"element","tagName":"span","properties":{"className":["icon","icon-link"]},"children":[]}]}],"position":{"start":{"line":28,"column":1,"offset":1366},"end":{"line":28,"column":5,"offset":1370}}},{"type":"text","value":"\n"},{"type":"element","tagName":"h3","properties":{"id":"i1"},"children":[{"type":"text","value":"I.1.","position":{"start":{"line":30,"column":5,"offset":1376},"end":{"line":30,"column":9,"offset":1380}}},{"type":"element","tagName":"a","properties":{"ariaHidden":"true","tabIndex":-1,"href":"#i1"},"children":[{"type":"element","tagName":"span","properties":{"className":["icon","icon-link"]},"children":[]}]}],"position":{"start":{"line":30,"column":1,"offset":1372},"end":{"line":30,"column":9,"offset":1380}}},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"第一道题最简单，但是我们当时约等于0基础，所以做起来也颇费了一些时间。不过由于我听过第一天的课，知道 ","position":{"start":{"line":32,"column":1,"offset":1382},"end":{"line":32,"column":52,"offset":1433}}},{"type":"element","tagName":"code","properties":{},"children":[{"type":"text","value":"oracle","position":{"start":{"line":32,"column":52,"offset":1433},"end":{"line":32,"column":60,"offset":1441}}}],"position":{"start":{"line":32,"column":52,"offset":1433},"end":{"line":32,"column":60,"offset":1441}}},{"type":"text","value":" 在 Q# 编程语言中是一个很重要的概念，所以在题目给出的参考教程 ","position":{"start":{"line":32,"column":60,"offset":1441},"end":{"line":32,"column":94,"offset":1475}}},{"type":"element","tagName":"a","properties":{"href":"https://github.com/Microsoft/QuantumKatas/","rel":["nofollow","noopener","noreferrer"],"target":"_blank"},"children":[{"type":"text","value":"Quantum Kantas","position":{"start":{"line":32,"column":95,"offset":1476},"end":{"line":32,"column":109,"offset":1490}}}],"position":{"start":{"line":32,"column":94,"offset":1475},"end":{"line":32,"column":154,"offset":1535}}},{"type":"text","value":" 里找到了","position":{"start":{"line":32,"column":154,"offset":1535},"end":{"line":32,"column":159,"offset":1540}}},{"type":"element","tagName":"a","properties":{"href":"https://github.com/microsoft/QuantumKatas/tree/main/tutorials/Oracles","rel":["nofollow","noopener","noreferrer"],"target":"_blank"},"children":[{"type":"text","value":"oracle 相关的教程","position":{"start":{"line":32,"column":160,"offset":1541},"end":{"line":32,"column":172,"offset":1553}}}],"position":{"start":{"line":32,"column":159,"offset":1540},"end":{"line":32,"column":244,"offset":1625}}},{"type":"text","value":"。里面有个名为 ","position":{"start":{"line":32,"column":244,"offset":1625},"end":{"line":32,"column":252,"offset":1633}}},{"type":"element","tagName":"code","properties":{},"children":[{"type":"text","value":"ControlledOnBitString","position":{"start":{"line":32,"column":252,"offset":1633},"end":{"line":32,"column":275,"offset":1656}}}],"position":{"start":{"line":32,"column":252,"offset":1633},"end":{"line":32,"column":275,"offset":1656}}},{"type":"text","value":" 的 function，可以根据一串量子比特的取值是否等于一个特定的二值串而对另外一个比特做一个特定的操作。前一天晚上又知道了 ","position":{"start":{"line":32,"column":275,"offset":1656},"end":{"line":32,"column":339,"offset":1720}}},{"type":"element","tagName":"code","properties":{},"children":[{"type":"text","value":"Microsoft.Quantum.Convert","position":{"start":{"line":32,"column":339,"offset":1720},"end":{"line":32,"column":366,"offset":1747}}}],"position":{"start":{"line":32,"column":339,"offset":1720},"end":{"line":32,"column":366,"offset":1747}}},{"type":"text","value":" 的 namespace 里有各种数据类型转换的函数，搭配 ","position":{"start":{"line":32,"column":366,"offset":1747},"end":{"line":32,"column":396,"offset":1777}}},{"type":"element","tagName":"code","properties":{},"children":[{"type":"text","value":"IntAsBoolArray","position":{"start":{"line":32,"column":396,"offset":1777},"end":{"line":32,"column":412,"offset":1793}}}],"position":{"start":{"line":32,"column":396,"offset":1777},"end":{"line":32,"column":412,"offset":1793}}},{"type":"text","value":"，就做出了第一题的初版。后来看到了更简单的 ","position":{"start":{"line":32,"column":412,"offset":1793},"end":{"line":32,"column":434,"offset":1815}}},{"type":"element","tagName":"code","properties":{},"children":[{"type":"text","value":"ControlledOnInt","position":{"start":{"line":32,"column":434,"offset":1815},"end":{"line":32,"column":451,"offset":1832}}}],"position":{"start":{"line":32,"column":434,"offset":1815},"end":{"line":32,"column":451,"offset":1832}}},{"type":"text","value":" 函数，就直接用上了。","position":{"start":{"line":32,"column":451,"offset":1832},"end":{"line":32,"column":462,"offset":1843}}}],"position":{"start":{"line":32,"column":1,"offset":1382},"end":{"line":32,"column":462,"offset":1843}}},{"type":"text","value":"\n"},{"type":"element","tagName":"h3","properties":{"id":"i2"},"children":[{"type":"text","value":"I.2.","position":{"start":{"line":34,"column":5,"offset":1849},"end":{"line":34,"column":9,"offset":1853}}},{"type":"element","tagName":"a","properties":{"ariaHidden":"true","tabIndex":-1,"href":"#i2"},"children":[{"type":"element","tagName":"span","properties":{"className":["icon","icon-link"]},"children":[]}]}],"position":{"start":{"line":34,"column":1,"offset":1845},"end":{"line":34,"column":9,"offset":1853}}},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"第二题的初版是女朋友做的。题目要求是找出是否至少两位不同，这一判断的否定就是三位比特全部相同，所以同样用 ","position":{"start":{"line":36,"column":1,"offset":1855},"end":{"line":36,"column":54,"offset":1908}}},{"type":"element","tagName":"code","properties":{},"children":[{"type":"text","value":"ControlledOnBitString","position":{"start":{"line":36,"column":54,"offset":1908},"end":{"line":36,"column":77,"offset":1931}}}],"position":{"start":{"line":36,"column":54,"offset":1908},"end":{"line":36,"column":77,"offset":1931}}},{"type":"text","value":" 函数，然后判断一次全 ","position":{"start":{"line":36,"column":77,"offset":1931},"end":{"line":36,"column":89,"offset":1943}}},{"type":"element","tagName":"code","properties":{},"children":[{"type":"text","value":"true","position":{"start":{"line":36,"column":89,"offset":1943},"end":{"line":36,"column":95,"offset":1949}}}],"position":{"start":{"line":36,"column":89,"offset":1943},"end":{"line":36,"column":95,"offset":1949}}},{"type":"text","value":" 一次全 ","position":{"start":{"line":36,"column":95,"offset":1949},"end":{"line":36,"column":100,"offset":1954}}},{"type":"element","tagName":"code","properties":{},"children":[{"type":"text","value":"false","position":{"start":{"line":36,"column":100,"offset":1954},"end":{"line":36,"column":107,"offset":1961}}}],"position":{"start":{"line":36,"column":100,"offset":1954},"end":{"line":36,"column":107,"offset":1961}}},{"type":"text","value":"，再把最终结果取反就可以了。但是在做第三题的时候，因为两个题目长得太像了，中间不小心把一个能通过第二题测试但是通不过第三题测试的答案直接覆盖在了第二题上面，懒的改回去了，于是就成了最后提交的版本。","position":{"start":{"line":36,"column":107,"offset":1961},"end":{"line":36,"column":205,"offset":2059}}}],"position":{"start":{"line":36,"column":1,"offset":1855},"end":{"line":36,"column":205,"offset":2059}}},{"type":"text","value":"\n"},{"type":"element","tagName":"h3","properties":{"id":"i3"},"children":[{"type":"text","value":"I.3.","position":{"start":{"line":38,"column":5,"offset":2065},"end":{"line":38,"column":9,"offset":2069}}},{"type":"element","tagName":"a","properties":{"ariaHidden":"true","tabIndex":-1,"href":"#i3"},"children":[{"type":"element","tagName":"span","properties":{"className":["icon","icon-link"]},"children":[]}]}],"position":{"start":{"line":38,"column":1,"offset":2061},"end":{"line":38,"column":9,"offset":2069}}},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"第三题和第二题非常不同。第二题的解决思路中，判断全 ","position":{"start":{"line":40,"column":1,"offset":2071},"end":{"line":40,"column":27,"offset":2097}}},{"type":"element","tagName":"code","properties":{},"children":[{"type":"text","value":"true","position":{"start":{"line":40,"column":27,"offset":2097},"end":{"line":40,"column":33,"offset":2103}}}],"position":{"start":{"line":40,"column":27,"offset":2097},"end":{"line":40,"column":33,"offset":2103}}},{"type":"text","value":" 和全 ","position":{"start":{"line":40,"column":33,"offset":2103},"end":{"line":40,"column":37,"offset":2107}}},{"type":"element","tagName":"code","properties":{},"children":[{"type":"text","value":"false","position":{"start":{"line":40,"column":37,"offset":2107},"end":{"line":40,"column":44,"offset":2114}}}],"position":{"start":{"line":40,"column":37,"offset":2107},"end":{"line":40,"column":44,"offset":2114}}},{"type":"text","value":"有3个控制位1个输出位，这里用了两次 4-量子比特门，所以第三问需要全新的思路。另外我曾经试过在一个 ","position":{"start":{"line":40,"column":44,"offset":2114},"end":{"line":40,"column":95,"offset":2165}}},{"type":"element","tagName":"code","properties":{},"children":[{"type":"text","value":"operation","position":{"start":{"line":40,"column":95,"offset":2165},"end":{"line":40,"column":106,"offset":2176}}}],"position":{"start":{"line":40,"column":95,"offset":2165},"end":{"line":40,"column":106,"offset":2176}}},{"type":"text","value":" 里申请一个新的 ","position":{"start":{"line":40,"column":106,"offset":2176},"end":{"line":40,"column":115,"offset":2185}}},{"type":"element","tagName":"code","properties":{},"children":[{"type":"text","value":"Qubit()","position":{"start":{"line":40,"column":115,"offset":2185},"end":{"line":40,"column":124,"offset":2194}}}],"position":{"start":{"line":40,"column":115,"offset":2185},"end":{"line":40,"column":124,"offset":2194}}},{"type":"text","value":" 结果测试报错，因为误解了报错信息，所以误认为除了程序的主 operation 之外不能创建新的 qubit，于是被卡住了。这时候已经来到了下午，实在想不出来又很困，于是去床上躺了一会。半睡半醒之间想到，题目虽然要求输入的量子比特不变，但是我们仍然可以直接改动输入，只要在函数结束之前把对输入的改动全部复原就可以了。于是用 CNOT 门分别作用在 1-2, 1-3 对输入的量子比特上，两个门分别以第2、3号比特为输出。然后用一个 3-bit 门判断2、3号比特是否相同，并输出到结果位上。为了复原第2、3号比特，只需要把 CNOT 在两对比特上分别再用一次就行了。","position":{"start":{"line":40,"column":124,"offset":2194},"end":{"line":40,"column":407,"offset":2477}}}],"position":{"start":{"line":40,"column":1,"offset":2071},"end":{"line":40,"column":407,"offset":2477}}},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"但是这个结果还是无法通过测试（后来成为了第二题的提交版本），报错的提示信息是使用了超过一次 3-量子比特门——这不是开玩笑吗？于是打开了官方提供的测试文件，发现测试代码计算 3-量子比特门的使用次数的时候，会把用户定义的 3-量子比特门的数量，和 ","position":{"start":{"line":42,"column":1,"offset":2479},"end":{"line":42,"column":125,"offset":2603}}},{"type":"element","tagName":"code","properties":{},"children":[{"type":"text","value":"CCNOT","position":{"start":{"line":42,"column":125,"offset":2603},"end":{"line":42,"column":132,"offset":2610}}}],"position":{"start":{"line":42,"column":125,"offset":2603},"end":{"line":42,"column":132,"offset":2610}}},{"type":"text","value":" 门的数量做加法，于是看文档，我们定义的那个 “用一个 3-bit 门判断2、3号比特是否相同，并输出到结果位上” 的操作和 ","position":{"start":{"line":42,"column":132,"offset":2610},"end":{"line":42,"column":195,"offset":2673}}},{"type":"element","tagName":"code","properties":{},"children":[{"type":"text","value":"CCNOT","position":{"start":{"line":42,"column":195,"offset":2673},"end":{"line":42,"column":202,"offset":2680}}}],"position":{"start":{"line":42,"column":195,"offset":2673},"end":{"line":42,"column":202,"offset":2680}}},{"type":"text","value":" 门是等价的，于是直接换用 ","position":{"start":{"line":42,"column":202,"offset":2680},"end":{"line":42,"column":216,"offset":2694}}},{"type":"element","tagName":"code","properties":{},"children":[{"type":"text","value":"CCNOT","position":{"start":{"line":42,"column":216,"offset":2694},"end":{"line":42,"column":223,"offset":2701}}}],"position":{"start":{"line":42,"column":216,"offset":2694},"end":{"line":42,"column":223,"offset":2701}}},{"type":"text","value":" 门，问题解决。","position":{"start":{"line":42,"column":223,"offset":2701},"end":{"line":42,"column":231,"offset":2709}}}],"position":{"start":{"line":42,"column":1,"offset":2479},"end":{"line":42,"column":231,"offset":2709}}},{"type":"text","value":"\n"},{"type":"element","tagName":"h3","properties":{"id":"i4"},"children":[{"type":"text","value":"I.4.","position":{"start":{"line":44,"column":5,"offset":2715},"end":{"line":44,"column":9,"offset":2719}}},{"type":"element","tagName":"a","properties":{"ariaHidden":"true","tabIndex":-1,"href":"#i4"},"children":[{"type":"element","tagName":"span","properties":{"className":["icon","icon-link"]},"children":[]}]}],"position":{"start":{"line":44,"column":1,"offset":2711},"end":{"line":44,"column":9,"offset":2719}}},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"第四题看起来复杂，但是可以分成三个部分：","position":{"start":{"line":46,"column":1,"offset":2721},"end":{"line":46,"column":21,"offset":2741}}}],"position":{"start":{"line":46,"column":1,"offset":2721},"end":{"line":46,"column":21,"offset":2741}}},{"type":"text","value":"\n"},{"type":"element","tagName":"ol","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"找出图中所有的三角形，确定每个三角形的三条边，这一步完全可以用经典算法完成；","position":{"start":{"line":48,"column":4,"offset":2746},"end":{"line":48,"column":42,"offset":2784}}}],"position":{"start":{"line":48,"column":1,"offset":2743},"end":{"line":48,"column":42,"offset":2784}}},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"创建一个和三角形相同数量的量子比特数列，对每个三角形，把三条边直接带入第二/三题的操作里，结果输入创建的量子比特列中；","position":{"start":{"line":49,"column":4,"offset":2788},"end":{"line":49,"column":63,"offset":2847}}}],"position":{"start":{"line":49,"column":1,"offset":2785},"end":{"line":49,"column":63,"offset":2847}}},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"判断量子比特列是否全为 ","position":{"start":{"line":50,"column":4,"offset":2851},"end":{"line":50,"column":16,"offset":2863}}},{"type":"element","tagName":"code","properties":{},"children":[{"type":"text","value":"true","position":{"start":{"line":50,"column":16,"offset":2863},"end":{"line":50,"column":22,"offset":2869}}}],"position":{"start":{"line":50,"column":16,"offset":2863},"end":{"line":50,"column":22,"offset":2869}}},{"type":"text","value":"，结果输出到整个程序的结果位上。","position":{"start":{"line":50,"column":22,"offset":2869},"end":{"line":50,"column":38,"offset":2885}}}],"position":{"start":{"line":50,"column":1,"offset":2848},"end":{"line":50,"column":38,"offset":2885}}},{"type":"text","value":"\n"}],"position":{"start":{"line":48,"column":1,"offset":2743},"end":{"line":50,"column":38,"offset":2885}}},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"第一步由女朋友来想我来写（毕竟只有一台电脑有开发环境），难点在于：","position":{"start":{"line":52,"column":1,"offset":2887},"end":{"line":52,"column":34,"offset":2920}}}],"position":{"start":{"line":52,"column":1,"offset":2887},"end":{"line":52,"column":34,"offset":2920}}},{"type":"text","value":"\n"},{"type":"element","tagName":"ol","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Q# 语法改变数列值的语法十分难受\n","position":{"start":{"line":54,"column":4,"offset":2925},"end":{"line":55,"column":1,"offset":2943}}},{"type":"element","tagName":"code","properties":{},"children":[{"type":"text","value":"mutable points = [-1,-1,-1,-1,-1,-1];","position":{"start":{"line":55,"column":9,"offset":2951},"end":{"line":55,"column":48,"offset":2990}}}],"position":{"start":{"line":55,"column":9,"offset":2951},"end":{"line":55,"column":48,"offset":2990}}},{"type":"text","value":"\n","position":{"start":{"line":55,"column":48,"offset":2990},"end":{"line":56,"column":1,"offset":2991}}},{"type":"element","tagName":"code","properties":{},"children":[{"type":"text","value":"set points w/=0..1 <- [0,1];","position":{"start":{"line":56,"column":9,"offset":2999},"end":{"line":56,"column":39,"offset":3029}}}],"position":{"start":{"line":56,"column":9,"offset":2999},"end":{"line":56,"column":39,"offset":3029}}},{"type":"text","value":"；","position":{"start":{"line":56,"column":39,"offset":3029},"end":{"line":56,"column":40,"offset":3030}}}],"position":{"start":{"line":54,"column":1,"offset":2922},"end":{"line":56,"column":40,"offset":3030}}},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"作为一种强类型语言对元组和数列的区分让我这个 python 选手十分蛋疼\n","position":{"start":{"line":57,"column":4,"offset":3034},"end":{"line":58,"column":1,"offset":3071}}},{"type":"element","tagName":"code","properties":{},"children":[{"type":"text","value":"(Int,Int)","position":{"start":{"line":58,"column":9,"offset":3079},"end":{"line":58,"column":20,"offset":3090}}}],"position":{"start":{"line":58,"column":9,"offset":3079},"end":{"line":58,"column":20,"offset":3090}}},{"type":"text","value":"/","position":{"start":{"line":58,"column":20,"offset":3090},"end":{"line":58,"column":21,"offset":3091}}},{"type":"element","tagName":"code","properties":{},"children":[{"type":"text","value":"Int[]","position":{"start":{"line":58,"column":21,"offset":3091},"end":{"line":58,"column":28,"offset":3098}}}],"position":{"start":{"line":58,"column":21,"offset":3091},"end":{"line":58,"column":28,"offset":3098}}},{"type":"text","value":"；","position":{"start":{"line":58,"column":28,"offset":3098},"end":{"line":58,"column":29,"offset":3099}}}],"position":{"start":{"line":57,"column":1,"offset":3031},"end":{"line":58,"column":29,"offset":3099}}},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"求数列中不重复的值居然不排序不能给出正确结果。\n","position":{"start":{"line":59,"column":4,"offset":3103},"end":{"line":60,"column":1,"offset":3127}}},{"type":"element","tagName":"code","properties":{},"children":[{"type":"text","value":"let uniquePoints = Arrays.Unique(EqualI,Arrays.Sorted(LessThanI,points));","position":{"start":{"line":60,"column":9,"offset":3135},"end":{"line":60,"column":84,"offset":3210}}}],"position":{"start":{"line":60,"column":9,"offset":3135},"end":{"line":60,"column":84,"offset":3210}}}],"position":{"start":{"line":59,"column":1,"offset":3100},"end":{"line":60,"column":84,"offset":3210}}},{"type":"text","value":"\n"}],"position":{"start":{"line":54,"column":1,"offset":2922},"end":{"line":60,"column":84,"offset":3210}}},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"这也是唯一一段用上了 ","position":{"start":{"line":62,"column":1,"offset":3212},"end":{"line":62,"column":12,"offset":3223}}},{"type":"element","tagName":"code","properties":{},"children":[{"type":"text","value":"Message()","position":{"start":{"line":62,"column":12,"offset":3223},"end":{"line":62,"column":23,"offset":3234}}}],"position":{"start":{"line":62,"column":12,"offset":3223},"end":{"line":62,"column":23,"offset":3234}}},{"type":"text","value":" 函数来 debug 的部分。","position":{"start":{"line":62,"column":23,"offset":3234},"end":{"line":62,"column":38,"offset":3249}}}],"position":{"start":{"line":62,"column":1,"offset":3212},"end":{"line":62,"column":38,"offset":3249}}},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"第三步就重新回到了第三题暂时敷衍掉的问题：对于在操作中创建的 ","position":{"start":{"line":64,"column":1,"offset":3251},"end":{"line":64,"column":32,"offset":3282}}},{"type":"element","tagName":"code","properties":{},"children":[{"type":"text","value":"Qubit()","position":{"start":{"line":64,"column":32,"offset":3282},"end":{"line":64,"column":41,"offset":3291}}}],"position":{"start":{"line":64,"column":32,"offset":3282},"end":{"line":64,"column":41,"offset":3291}}},{"type":"text","value":"/","position":{"start":{"line":64,"column":41,"offset":3291},"end":{"line":64,"column":42,"offset":3292}}},{"type":"element","tagName":"code","properties":{},"children":[{"type":"text","value":"Qubit[]","position":{"start":{"line":64,"column":42,"offset":3292},"end":{"line":64,"column":51,"offset":3301}}}],"position":{"start":{"line":64,"column":42,"offset":3292},"end":{"line":64,"column":51,"offset":3301}}},{"type":"text","value":"，","position":{"start":{"line":64,"column":51,"offset":3301},"end":{"line":64,"column":52,"offset":3302}}},{"type":"element","tagName":"code","properties":{},"children":[{"type":"text","value":"Reset()","position":{"start":{"line":64,"column":52,"offset":3302},"end":{"line":64,"column":61,"offset":3311}}}],"position":{"start":{"line":64,"column":52,"offset":3302},"end":{"line":64,"column":61,"offset":3311}}},{"type":"text","value":"/","position":{"start":{"line":64,"column":61,"offset":3311},"end":{"line":64,"column":62,"offset":3312}}},{"type":"element","tagName":"code","properties":{},"children":[{"type":"text","value":"ResetAll()","position":{"start":{"line":64,"column":62,"offset":3312},"end":{"line":64,"column":74,"offset":3324}}}],"position":{"start":{"line":64,"column":62,"offset":3312},"end":{"line":64,"column":74,"offset":3324}}},{"type":"text","value":" 函数相当于测量，会破坏操作的 adjoint 性质，不测量则（当时的我）没有办法将这个量子比特列复原。","position":{"start":{"line":64,"column":74,"offset":3324},"end":{"line":64,"column":126,"offset":3376}}}],"position":{"start":{"line":64,"column":1,"offset":3251},"end":{"line":64,"column":126,"offset":3376}}},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"此时已经午夜，我来解决这个问题，女朋友去看第二部分，后来她看完 Grover‘s 算法的教程去睡了，我还在想这个问题。直接把报错信息复制到 Google，找到了一个","position":{"start":{"line":66,"column":1,"offset":3378},"end":{"line":66,"column":83,"offset":3460}}},{"type":"element","tagName":"a","properties":{"href":""},"children":[{"type":"text","value":"论坛里的问答","position":{"start":{"line":66,"column":84,"offset":3461},"end":{"line":66,"column":90,"offset":3467}}}],"position":{"start":{"line":66,"column":83,"offset":3460},"end":{"line":66,"column":93,"offset":3470}}},{"type":"text","value":"，好像是去年微软在其他地方举办的类似活动的。里面只是提到要“uncompute the qubits”，给出的例子用的是旧版本 Q# 的语法，","position":{"start":{"line":66,"column":93,"offset":3470},"end":{"line":66,"column":164,"offset":3541}}},{"type":"element","tagName":"del","properties":{},"children":[{"type":"text","value":"没法直接抄","position":{"start":{"line":66,"column":166,"offset":3543},"end":{"line":66,"column":171,"offset":3548}}}],"position":{"start":{"line":66,"column":164,"offset":3541},"end":{"line":66,"column":173,"offset":3550}}},{"type":"text","value":" 。最终不抱希望地把之前对那个 ","position":{"start":{"line":66,"column":173,"offset":3550},"end":{"line":66,"column":189,"offset":3566}}},{"type":"element","tagName":"code","properties":{},"children":[{"type":"text","value":"Qubit[]","position":{"start":{"line":66,"column":189,"offset":3566},"end":{"line":66,"column":198,"offset":3575}}}],"position":{"start":{"line":66,"column":189,"offset":3566},"end":{"line":66,"column":198,"offset":3575}}},{"type":"text","value":" 做过的循环顺序倒过来重做了一遍，诶，您猜怎么着，还是没通过！绝望了！正序重做一边，诶，通过了！为什么为什么为什么？到现在也没弄清楚。","position":{"start":{"line":66,"column":198,"offset":3575},"end":{"line":66,"column":265,"offset":3642}}}],"position":{"start":{"line":66,"column":1,"offset":3378},"end":{"line":66,"column":265,"offset":3642}}},{"type":"text","value":"\n"},{"type":"element","tagName":"h3","properties":{"id":"ii"},"children":[{"type":"text","value":"II.","position":{"start":{"line":68,"column":5,"offset":3648},"end":{"line":68,"column":8,"offset":3651}}},{"type":"element","tagName":"a","properties":{"ariaHidden":"true","tabIndex":-1,"href":"#ii"},"children":[{"type":"element","tagName":"span","properties":{"className":["icon","icon-link"]},"children":[]}]}],"position":{"start":{"line":68,"column":1,"offset":3644},"end":{"line":68,"column":8,"offset":3651}}},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"然后把女朋友叫醒，让她来讲一讲 Grover's 算法。听完之后我的理解是，对于一个 ","position":{"start":{"line":70,"column":1,"offset":3653},"end":{"line":70,"column":44,"offset":3696}}},{"type":"element","tagName":"span","properties":{"className":["katex"]},"children":[{"type":"element","tagName":"span","properties":{"className":["katex-mathml"]},"children":[{"type":"element","tagName":"math","properties":{"xmlns":"http://www.w3.org/1998/Math/MathML"},"children":[{"type":"element","tagName":"semantics","properties":{},"children":[{"type":"element","tagName":"mrow","properties":{},"children":[{"type":"element","tagName":"mi","properties":{},"children":[{"type":"text","value":"f"}]},{"type":"element","tagName":"mo","properties":{},"children":[{"type":"text","value":":"}]},{"type":"element","tagName":"mo","properties":{"stretchy":"false"},"children":[{"type":"text","value":"("}]},{"type":"element","tagName":"mn","properties":{},"children":[{"type":"text","value":"0"}]},{"type":"element","tagName":"mo","properties":{"separator":"true"},"children":[{"type":"text","value":","}]},{"type":"element","tagName":"mn","properties":{},"children":[{"type":"text","value":"1"}]},{"type":"element","tagName":"msup","properties":{},"children":[{"type":"element","tagName":"mo","properties":{"stretchy":"false"},"children":[{"type":"text","value":")"}]},{"type":"element","tagName":"mi","properties":{},"children":[{"type":"text","value":"N"}]}]},{"type":"element","tagName":"mo","properties":{},"children":[{"type":"text","value":"→"}]},{"type":"element","tagName":"mo","properties":{"stretchy":"false"},"children":[{"type":"text","value":"("}]},{"type":"element","tagName":"mn","properties":{},"children":[{"type":"text","value":"0"}]},{"type":"element","tagName":"mo","properties":{"separator":"true"},"children":[{"type":"text","value":","}]},{"type":"element","tagName":"mn","properties":{},"children":[{"type":"text","value":"1"}]},{"type":"element","tagName":"mo","properties":{"stretchy":"false"},"children":[{"type":"text","value":")"}]}]},{"type":"element","tagName":"annotation","properties":{"encoding":"application/x-tex"},"children":[{"type":"text","value":"f:(0,1)^N \\rarr (0,1)"}]}]}]}]},{"type":"element","tagName":"span","properties":{"className":["katex-html"],"ariaHidden":"true"},"children":[{"type":"element","tagName":"span","properties":{"className":["base"]},"children":[{"type":"element","tagName":"span","properties":{"className":["strut"],"style":"height:0.8889em;vertical-align:-0.1944em;"},"children":[]},{"type":"element","tagName":"span","properties":{"className":["mord","mathnormal"],"style":"margin-right:0.10764em;"},"children":[{"type":"text","value":"f"}]},{"type":"element","tagName":"span","properties":{"className":["mspace"],"style":"margin-right:0.2778em;"},"children":[]},{"type":"element","tagName":"span","properties":{"className":["mrel"]},"children":[{"type":"text","value":":"}]},{"type":"element","tagName":"span","properties":{"className":["mspace"],"style":"margin-right:0.2778em;"},"children":[]}]},{"type":"element","tagName":"span","properties":{"className":["base"]},"children":[{"type":"element","tagName":"span","properties":{"className":["strut"],"style":"height:1.0913em;vertical-align:-0.25em;"},"children":[]},{"type":"element","tagName":"span","properties":{"className":["mopen"]},"children":[{"type":"text","value":"("}]},{"type":"element","tagName":"span","properties":{"className":["mord"]},"children":[{"type":"text","value":"0"}]},{"type":"element","tagName":"span","properties":{"className":["mpunct"]},"children":[{"type":"text","value":","}]},{"type":"element","tagName":"span","properties":{"className":["mspace"],"style":"margin-right:0.1667em;"},"children":[]},{"type":"element","tagName":"span","properties":{"className":["mord"]},"children":[{"type":"text","value":"1"}]},{"type":"element","tagName":"span","properties":{"className":["mclose"]},"children":[{"type":"element","tagName":"span","properties":{"className":["mclose"]},"children":[{"type":"text","value":")"}]},{"type":"element","tagName":"span","properties":{"className":["msupsub"]},"children":[{"type":"element","tagName":"span","properties":{"className":["vlist-t"]},"children":[{"type":"element","tagName":"span","properties":{"className":["vlist-r"]},"children":[{"type":"element","tagName":"span","properties":{"className":["vlist"],"style":"height:0.8413em;"},"children":[{"type":"element","tagName":"span","properties":{"style":"top:-3.063em;margin-right:0.05em;"},"children":[{"type":"element","tagName":"span","properties":{"className":["pstrut"],"style":"height:2.7em;"},"children":[]},{"type":"element","tagName":"span","properties":{"className":["sizing","reset-size6","size3","mtight"]},"children":[{"type":"element","tagName":"span","properties":{"className":["mord","mathnormal","mtight"],"style":"margin-right:0.10903em;"},"children":[{"type":"text","value":"N"}]}]}]}]}]}]}]}]},{"type":"element","tagName":"span","properties":{"className":["mspace"],"style":"margin-right:0.2778em;"},"children":[]},{"type":"element","tagName":"span","properties":{"className":["mrel"]},"children":[{"type":"text","value":"→"}]},{"type":"element","tagName":"span","properties":{"className":["mspace"],"style":"margin-right:0.2778em;"},"children":[]}]},{"type":"element","tagName":"span","properties":{"className":["base"]},"children":[{"type":"element","tagName":"span","properties":{"className":["strut"],"style":"height:1em;vertical-align:-0.25em;"},"children":[]},{"type":"element","tagName":"span","properties":{"className":["mopen"]},"children":[{"type":"text","value":"("}]},{"type":"element","tagName":"span","properties":{"className":["mord"]},"children":[{"type":"text","value":"0"}]},{"type":"element","tagName":"span","properties":{"className":["mpunct"]},"children":[{"type":"text","value":","}]},{"type":"element","tagName":"span","properties":{"className":["mspace"],"style":"margin-right:0.1667em;"},"children":[]},{"type":"element","tagName":"span","properties":{"className":["mord"]},"children":[{"type":"text","value":"1"}]},{"type":"element","tagName":"span","properties":{"className":["mclose"]},"children":[{"type":"text","value":")"}]}]}]}]},{"type":"text","value":" 的函数，这个算法可以大概率地找到一个解 ","position":{"start":{"line":70,"column":69,"offset":3721},"end":{"line":70,"column":90,"offset":3742}}},{"type":"element","tagName":"span","properties":{"className":["katex"]},"children":[{"type":"element","tagName":"span","properties":{"className":["katex-mathml"]},"children":[{"type":"element","tagName":"math","properties":{"xmlns":"http://www.w3.org/1998/Math/MathML"},"children":[{"type":"element","tagName":"semantics","properties":{},"children":[{"type":"element","tagName":"mrow","properties":{},"children":[{"type":"element","tagName":"mi","properties":{},"children":[{"type":"text","value":"S"}]},{"type":"element","tagName":"mo","properties":{},"children":[{"type":"text","value":"∈"}]},{"type":"element","tagName":"mo","properties":{"stretchy":"false"},"children":[{"type":"text","value":"("}]},{"type":"element","tagName":"mn","properties":{},"children":[{"type":"text","value":"0"}]},{"type":"element","tagName":"mo","properties":{"separator":"true"},"children":[{"type":"text","value":","}]},{"type":"element","tagName":"mn","properties":{},"children":[{"type":"text","value":"1"}]},{"type":"element","tagName":"msup","properties":{},"children":[{"type":"element","tagName":"mo","properties":{"stretchy":"false"},"children":[{"type":"text","value":")"}]},{"type":"element","tagName":"mi","properties":{},"children":[{"type":"text","value":"N"}]}]}]},{"type":"element","tagName":"annotation","properties":{"encoding":"application/x-tex"},"children":[{"type":"text","value":"S\\in(0,1)^N"}]}]}]}]},{"type":"element","tagName":"span","properties":{"className":["katex-html"],"ariaHidden":"true"},"children":[{"type":"element","tagName":"span","properties":{"className":["base"]},"children":[{"type":"element","tagName":"span","properties":{"className":["strut"],"style":"height:0.7224em;vertical-align:-0.0391em;"},"children":[]},{"type":"element","tagName":"span","properties":{"className":["mord","mathnormal"],"style":"margin-right:0.05764em;"},"children":[{"type":"text","value":"S"}]},{"type":"element","tagName":"span","properties":{"className":["mspace"],"style":"margin-right:0.2778em;"},"children":[]},{"type":"element","tagName":"span","properties":{"className":["mrel"]},"children":[{"type":"text","value":"∈"}]},{"type":"element","tagName":"span","properties":{"className":["mspace"],"style":"margin-right:0.2778em;"},"children":[]}]},{"type":"element","tagName":"span","properties":{"className":["base"]},"children":[{"type":"element","tagName":"span","properties":{"className":["strut"],"style":"height:1.0913em;vertical-align:-0.25em;"},"children":[]},{"type":"element","tagName":"span","properties":{"className":["mopen"]},"children":[{"type":"text","value":"("}]},{"type":"element","tagName":"span","properties":{"className":["mord"]},"children":[{"type":"text","value":"0"}]},{"type":"element","tagName":"span","properties":{"className":["mpunct"]},"children":[{"type":"text","value":","}]},{"type":"element","tagName":"span","properties":{"className":["mspace"],"style":"margin-right:0.1667em;"},"children":[]},{"type":"element","tagName":"span","properties":{"className":["mord"]},"children":[{"type":"text","value":"1"}]},{"type":"element","tagName":"span","properties":{"className":["mclose"]},"children":[{"type":"element","tagName":"span","properties":{"className":["mclose"]},"children":[{"type":"text","value":")"}]},{"type":"element","tagName":"span","properties":{"className":["msupsub"]},"children":[{"type":"element","tagName":"span","properties":{"className":["vlist-t"]},"children":[{"type":"element","tagName":"span","properties":{"className":["vlist-r"]},"children":[{"type":"element","tagName":"span","properties":{"className":["vlist"],"style":"height:0.8413em;"},"children":[{"type":"element","tagName":"span","properties":{"style":"top:-3.063em;margin-right:0.05em;"},"children":[{"type":"element","tagName":"span","properties":{"className":["pstrut"],"style":"height:2.7em;"},"children":[]},{"type":"element","tagName":"span","properties":{"className":["sizing","reset-size6","size3","mtight"]},"children":[{"type":"element","tagName":"span","properties":{"className":["mord","mathnormal","mtight"],"style":"margin-right:0.10903em;"},"children":[{"type":"text","value":"N"}]}]}]}]}]}]}]}]}]}]}]},{"type":"text","value":" 满足 ","position":{"start":{"line":70,"column":105,"offset":3757},"end":{"line":70,"column":109,"offset":3761}}},{"type":"element","tagName":"span","properties":{"className":["katex"]},"children":[{"type":"element","tagName":"span","properties":{"className":["katex-mathml"]},"children":[{"type":"element","tagName":"math","properties":{"xmlns":"http://www.w3.org/1998/Math/MathML"},"children":[{"type":"element","tagName":"semantics","properties":{},"children":[{"type":"element","tagName":"mrow","properties":{},"children":[{"type":"element","tagName":"mi","properties":{},"children":[{"type":"text","value":"f"}]},{"type":"element","tagName":"mo","properties":{"stretchy":"false"},"children":[{"type":"text","value":"("}]},{"type":"element","tagName":"mi","properties":{},"children":[{"type":"text","value":"S"}]},{"type":"element","tagName":"mo","properties":{"stretchy":"false"},"children":[{"type":"text","value":")"}]},{"type":"element","tagName":"mo","properties":{},"children":[{"type":"text","value":"="}]},{"type":"element","tagName":"mn","properties":{},"children":[{"type":"text","value":"1"}]}]},{"type":"element","tagName":"annotation","properties":{"encoding":"application/x-tex"},"children":[{"type":"text","value":"f(S)=1"}]}]}]}]},{"type":"element","tagName":"span","properties":{"className":["katex-html"],"ariaHidden":"true"},"children":[{"type":"element","tagName":"span","properties":{"className":["base"]},"children":[{"type":"element","tagName":"span","properties":{"className":["strut"],"style":"height:1em;vertical-align:-0.25em;"},"children":[]},{"type":"element","tagName":"span","properties":{"className":["mord","mathnormal"],"style":"margin-right:0.10764em;"},"children":[{"type":"text","value":"f"}]},{"type":"element","tagName":"span","properties":{"className":["mopen"]},"children":[{"type":"text","value":"("}]},{"type":"element","tagName":"span","properties":{"className":["mord","mathnormal"],"style":"margin-right:0.05764em;"},"children":[{"type":"text","value":"S"}]},{"type":"element","tagName":"span","properties":{"className":["mclose"]},"children":[{"type":"text","value":")"}]},{"type":"element","tagName":"span","properties":{"className":["mspace"],"style":"margin-right:0.2778em;"},"children":[]},{"type":"element","tagName":"span","properties":{"className":["mrel"]},"children":[{"type":"text","value":"="}]},{"type":"element","tagName":"span","properties":{"className":["mspace"],"style":"margin-right:0.2778em;"},"children":[]}]},{"type":"element","tagName":"span","properties":{"className":["base"]},"children":[{"type":"element","tagName":"span","properties":{"className":["strut"],"style":"height:0.6444em;"},"children":[]},{"type":"element","tagName":"span","properties":{"className":["mord"]},"children":[{"type":"text","value":"1"}]}]}]}]},{"type":"text","value":".","position":{"start":{"line":70,"column":119,"offset":3771},"end":{"line":70,"column":120,"offset":3772}}}],"position":{"start":{"line":70,"column":1,"offset":3653},"end":{"line":70,"column":121,"offset":3773}}},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"至于这个函数 ","position":{"start":{"line":72,"column":1,"offset":3775},"end":{"line":72,"column":8,"offset":3782}}},{"type":"element","tagName":"span","properties":{"className":["katex"]},"children":[{"type":"element","tagName":"span","properties":{"className":["katex-mathml"]},"children":[{"type":"element","tagName":"math","properties":{"xmlns":"http://www.w3.org/1998/Math/MathML"},"children":[{"type":"element","tagName":"semantics","properties":{},"children":[{"type":"element","tagName":"mrow","properties":{},"children":[{"type":"element","tagName":"mi","properties":{},"children":[{"type":"text","value":"f"}]}]},{"type":"element","tagName":"annotation","properties":{"encoding":"application/x-tex"},"children":[{"type":"text","value":"f"}]}]}]}]},{"type":"element","tagName":"span","properties":{"className":["katex-html"],"ariaHidden":"true"},"children":[{"type":"element","tagName":"span","properties":{"className":["base"]},"children":[{"type":"element","tagName":"span","properties":{"className":["strut"],"style":"height:0.8889em;vertical-align:-0.1944em;"},"children":[]},{"type":"element","tagName":"span","properties":{"className":["mord","mathnormal"],"style":"margin-right:0.10764em;"},"children":[{"type":"text","value":"f"}]}]}]}]},{"type":"text","value":"，之前每一道题都是这样一个函数，当时已经夜里两三点了，实在是没时间再想一个新函数了，于是我们直接就拿复杂度最高的第4题来换个皮。换个什么皮呢？为了这个活动翘掉了这周的","position":{"start":{"line":72,"column":13,"offset":3787},"end":{"line":72,"column":96,"offset":3870}}},{"type":"element","tagName":"a","properties":{"href":"barrier-forward-keyboard-mouse-to-another-computer"},"children":[{"type":"text","value":"《文明6》联机游戏","position":{"start":{"line":72,"column":97,"offset":3871},"end":{"line":72,"column":106,"offset":3880}}}],"position":{"start":{"line":72,"column":96,"offset":3870},"end":{"line":72,"column":159,"offset":3933}}},{"type":"text","value":"，然后之前看 YouTuber ","position":{"start":{"line":72,"column":159,"offset":3933},"end":{"line":72,"column":175,"offset":3949}}},{"type":"element","tagName":"a","properties":{"href":"https://www.youtube.com/user/PotatoMcWhiskey","rel":["nofollow","noopener","noreferrer"],"target":"_blank"},"children":[{"type":"text","value":"\"PotatoMcWhiskey\"","position":{"start":{"line":72,"column":176,"offset":3950},"end":{"line":72,"column":193,"offset":3967}}}],"position":{"start":{"line":72,"column":175,"offset":3949},"end":{"line":72,"column":240,"offset":4014}}},{"type":"text","value":"介绍过","position":{"start":{"line":72,"column":240,"offset":4014},"end":{"line":72,"column":243,"offset":4017}}},{"type":"element","tagName":"a","properties":{"href":"https://steamcommunity.com/sharedfiles/filedetails/?id=1753346735&searchtext=diplomacy","rel":["nofollow","noopener","noreferrer"],"target":"_blank"},"children":[{"type":"text","value":"一个 Mod","position":{"start":{"line":72,"column":244,"offset":4018},"end":{"line":72,"column":250,"offset":4024}}}],"position":{"start":{"line":72,"column":243,"offset":4017},"end":{"line":72,"column":339,"offset":4113}}},{"type":"text","value":"，里面可以将文明之间的外交关系可视化为无向图，所以，诶嘿嘿嘿……","position":{"start":{"line":72,"column":339,"offset":4113},"end":{"line":72,"column":371,"offset":4145}}}],"position":{"start":{"line":72,"column":1,"offset":3775},"end":{"line":72,"column":371,"offset":4145}}},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"女朋友写完文稿就睡了，我把文稿改了改，然后和官方对 Grover's 算法的实现缝合了一下。提交的时候，距离截止时间大约还有一个小时。","position":{"start":{"line":74,"column":1,"offset":4147},"end":{"line":74,"column":68,"offset":4214}}}],"position":{"start":{"line":74,"column":1,"offset":4147},"end":{"line":74,"column":68,"offset":4214}}},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{"id":"四"},"children":[{"type":"text","value":"四","position":{"start":{"line":77,"column":4,"offset":4220},"end":{"line":77,"column":5,"offset":4221}}},{"type":"element","tagName":"a","properties":{"ariaHidden":"true","tabIndex":-1,"href":"#四"},"children":[{"type":"element","tagName":"span","properties":{"className":["icon","icon-link"]},"children":[]}]}],"position":{"start":{"line":77,"column":1,"offset":4217},"end":{"line":77,"column":5,"offset":4221}}},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"之后的周五的时候收到了消息，我们得奖了。优胜者一共6支队伍。从活动结束之后公布的结果看，要想成为优胜，第一部分的4道题必须全部正确，然后第二部分得分在 8-20 分之间。","position":{"start":{"line":79,"column":1,"offset":4223},"end":{"line":79,"column":86,"offset":4308}}}],"position":{"start":{"line":79,"column":1,"offset":4223},"end":{"line":79,"column":86,"offset":4308}}},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"这个成绩是个什么水平？截止到写这篇文章的此刻，官方题目的 Github 仓库有 80 份 fork，有少数几份 fork 是针对已有的 fork，有可能来自同一队伍，再考虑到可能有些队伍的不同成员分别 fork 了主项目，所以估测 60 支队伍应该是有的，官方给出 6 组优胜者这么一个不零不整的数字，个人猜测是取了前 10%？据主办方在 discord 提供的消息，有一支队伍的第二题成绩高于8分，但是前面没有全对，所以没有得奖；其余队伍的第二题都不超过6分；并不清楚有多少队伍第一题全对，主办方也不打算公布各队的详细成绩。","position":{"start":{"line":81,"column":1,"offset":4310},"end":{"line":81,"column":264,"offset":4573}}}],"position":{"start":{"line":81,"column":1,"offset":4310},"end":{"line":81,"column":264,"offset":4573}}},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"这大约说明活动的参与者，其成绩基本上符合二八原理——少数人得到的分数，占据了所有参赛者全部得分的大多数。","position":{"start":{"line":83,"column":1,"offset":4575},"end":{"line":83,"column":53,"offset":4627}}}],"position":{"start":{"line":83,"column":1,"offset":4575},"end":{"line":83,"column":53,"offset":4627}}},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"参加过这个活动之后，我们一下子就从量子计算小白摇身一变，成了优秀人才了？实际上，直到现在，我还是搞不太清楚 oracle 到底是个什么东西，女朋友对量子计算的理解估计比我还差（逃）。美国哲学教授约翰·希尔勒提出过一个叫做","position":{"start":{"line":85,"column":1,"offset":4629},"end":{"line":85,"column":111,"offset":4739}}},{"type":"element","tagName":"a","properties":{"href":"https://zh.wikipedia.org/wiki/%E4%B8%AD%E6%96%87%E6%88%BF%E9%97%B4","rel":["nofollow","noopener","noreferrer"],"target":"_blank"},"children":[{"type":"text","value":"“中文房间”","position":{"start":{"line":85,"column":112,"offset":4740},"end":{"line":85,"column":118,"offset":4746}}}],"position":{"start":{"line":85,"column":111,"offset":4739},"end":{"line":85,"column":187,"offset":4815}}},{"type":"text","value":"的思想实验，说一个只会说英语的人被关在一间满是汉字字块的房间里，不断从房间外收到写着中文问题的纸条。房间里有一本英文写成的手册，指示如何对输入的汉字进行回复。凭借这个手册，房中人可以在完全不会中文的情况下，与外界进行交流。希尔勒类比外人、房中人、手册，与程序员、计算机、计算机程序，认为房中人不会中文，进而论证计算机不可能通过程序来获得理解力。","position":{"start":{"line":85,"column":187,"offset":4815},"end":{"line":85,"column":359,"offset":4987}}}],"position":{"start":{"line":85,"column":1,"offset":4629},"end":{"line":85,"column":359,"offset":4987}}},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"希尔勒教授想论证啥是他的事，我倒是对这个类比的本体很感兴趣——如果一个人已经能够熟练运用那个英文写成的汉字使用手册了，我们还能不能，能在多大程度上说他不懂中文呢？就说一般的程序员，工作时间能保证不看 stack overflow 的有几个，所以他们都不会编程？反对中文房间思想实验结论的人，很多都支持用图灵测试超过某一阈值来作为有智能的标志，但是我觉得，智能本身就不是一个非有即无的性质，而是一个连续分布，没有上限的谱。","position":{"start":{"line":87,"column":1,"offset":4989},"end":{"line":87,"column":211,"offset":5199}}}],"position":{"start":{"line":87,"column":1,"offset":4989},"end":{"line":87,"column":211,"offset":5199}}},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"另一方面，得分名列前茅，和能力名列前茅，又是两回事。本科的时候做建模美赛，我们学校数理金融的一个学神前一年成绩“略有不佳”，没拿到 M 奖，于是我们那年找到了我和风神俩学物理的，准备再次冲击荣誉。巧了这一年的题目正好有一道浴缸放热水的问题，这不就是物理中的扩散方程嘛，那得奖还不是手拿把掐的？结果呢，H 奖，丢人丢到姥姥家去了。合着我们两个成绩还都不错的物理专业学生，在自己的专业里，打不过那么多同龄的非物理专业本科生？","position":{"start":{"line":89,"column":1,"offset":5201},"end":{"line":89,"column":211,"offset":5411}}}],"position":{"start":{"line":89,"column":1,"offset":5201},"end":{"line":89,"column":211,"offset":5411}}},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"两相对照之下，我想起了很久之前看过的一篇博客文章，文章以一个问题开头——“熟练”的反义词是什么？当然说“生疏”这文章就写不下去了，作者给出的答案是——“应变”。熟练意味着，你对于问题、选项、最优解已经有了充分且完备的了解，只需要重复自己的经验就可以了，但是在自己不了解的战场上，经验至少不能直接派上用场，这时候，脱离具体环境的应变能力就成了生存和取胜的关键，我们当时的专业水平高不成低不就，反而成了掣肘我们的桎梏。","position":{"start":{"line":91,"column":1,"offset":5413},"end":{"line":91,"column":208,"offset":5620}}}],"position":{"start":{"line":91,"column":1,"offset":5413},"end":{"line":91,"column":208,"offset":5620}}},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"读到这篇文章的时候，我被这种剑走偏锋的观点击中了，从那以后，一直都在注意培养自己的应变能力——如果明天我所研究的这个领域消失了，我还有没有谋生的能力？如果自己正在解决的问题被上帝或者 Matrix 作弊修改成一个新问题，我能不能看到连作弊都改动不了的题眼，然后一击命中？在凌晨两三点的时候，我也没有放弃解决第一题第 4 问的 Qubit 复位问题，虽然当时我并不知道评分标准，但是内心非常确定，这个问题必须解决。","position":{"start":{"line":93,"column":1,"offset":5622},"end":{"line":93,"column":207,"offset":5828}}}],"position":{"start":{"line":93,"column":1,"offset":5622},"end":{"line":93,"column":207,"offset":5828}}},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"以上两次活动的成绩差别，也可以从得奖难度来看。建模美赛的 M 奖，得奖率应该远小于 10%，即便考虑到二八原理中绝大多数参赛者都只是凑数，而且样本越大凑数者越多，这个差距也还是无法忽略。我们能够得奖，和量子计算领域才刚刚萌芽，连“方兴未艾”都算不上，因此竞争并不激烈也有很大关系，应变能力是切入这些蓝海领域的必要条件，是躲避内卷的利器。我们现在对“内卷”人人喊打，但是培养应变能力是需要牺牲相当多本可以精进专业的时间和精力的。当社会中的大多数人向往着逃离内卷的时候，真的不需要有人咬定一个领域不断深耕？我现在的选择真的正确吗？我不知道。我是打算留在当前的领域继续熟练，还是换个领域应变，抑或是虚掷 PhD 光阴换一张工作签证？我也不知道。","position":{"start":{"line":95,"column":1,"offset":5830},"end":{"line":95,"column":320,"offset":6149}}}],"position":{"start":{"line":95,"column":1,"offset":5830},"end":{"line":95,"column":320,"offset":6149}}},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{"id":"五"},"children":[{"type":"text","value":"五","position":{"start":{"line":97,"column":4,"offset":6154},"end":{"line":97,"column":5,"offset":6155}}},{"type":"element","tagName":"a","properties":{"ariaHidden":"true","tabIndex":-1,"href":"#五"},"children":[{"type":"element","tagName":"span","properties":{"className":["icon","icon-link"]},"children":[]}]}],"position":{"start":{"line":97,"column":1,"offset":6151},"end":{"line":97,"column":5,"offset":6155}}},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"哦对了，我有女朋友了，而且在 hackathon 的过程中把女朋友惹哭了……问题是我现在已经不记得具体是怎么把人家惹哭的了，连道歉都显得很不诚恳……我确实是一个不擅长合作的人，或者说跟别人说话的我，和想问题的我并不是同一个人，之前本科 CUPT 和建模的时候也一样，需要和人打交道的时候就几乎干不了活儿，严重的时候自己就退化成了鼓励师……总之一切错误在我，希望她不要记仇…… ","position":{"start":{"line":99,"column":1,"offset":6157},"end":{"line":99,"column":189,"offset":6345}}},{"type":"text","value":"（。・＿・。）ﾉ","position":{"start":{"line":99,"column":193,"offset":6349},"end":{"line":99,"column":201,"offset":6357}}}],"position":{"start":{"line":99,"column":1,"offset":6157},"end":{"line":99,"column":201,"offset":6357}}}],"position":{"start":{"line":1,"column":1,"offset":0},"end":{"line":100,"column":1,"offset":6358}}},"collectedBy":[["md",[{"slug":"notes-on-pytorch-autograd","filename":"2024-11-10-notes-on-pytorch-autograd.md","date":"2024-11-10","title":".ai | PyTorch 中的自动微分 autograd","layout":"post","keywords":["md","py","ai"],"excerpt":"我对 PyTorch 中的自动微分还是一知半解，停留在知道计算图和反向传播的概念，以及能看懂和小改别人写好的代码的水平。之所以存在这样的认知断层，是因为介绍自动微分的时候一般是手动计算张量，然后给读者画个计算图；而真正干活的时候一般是写一个神经网络的类，这个类继承自 `torch.nn.Module`。","cover":"2024-11-10-computation-graph.png","content":"\n> 装了这么久的 ML/AI 爱好者，我发现我对 PyTorch 中的自动微分还是一知半解，停留在知道计算图和反向传播的概念，以及能看懂和小改别人写好的代码的水平。所以自觉还得再补补课。\n> \n> ~~真实动机是前女友找到了国内的研发工作，马上要回国赚大钱了，我破防了~~\n> \n> 之所以存在这样的认知断层，是因为介绍自动微分的时候一般是手动计算张量，然后给读者画个计算图；而真正干活的时候一般是写一个神经网络的类，这个类继承自 `torch.nn.Module`。\n> \n> 这样一包装，承载微分的张量很少被点名取用；而计算过程被一刀两段，绝大多数计算包装在了这个类的 `forward()` 方法中，但是最后一步计算模型预测和训练目标的差异 (loss)，往往留在了训练的控制流中，让人以为两者之间有什么本质区别。\n> \n> 其实不然——自动微分只针对 PyTorch 张量，`torch.nn.Module` 的实例是一个包含若干参数的函数，其参数要么是 `torch.nn.Parameter`，要么是其他含参的 `torch.nn.Module` 实例的参数，（用 `.parameters()` 方法取得，常见于 optimizer 的首个参数）。每次把训练输入张量带入这个函数，中间的数学计算就会被自动微分机制记录。对于自动微分来说，这些计算和最后对 loss 的计算没什么不同。\n> \n> 这个道理其实在做《[**一个 PyTorch 机器学习项目长什么样**](what-a-PyTorch-project-looks-like)》这篇笔记的时候就应该理解，而实际上没有理解，说明——\n> \n> - 一来信息和知识不是同一层面的概念，相同的信息嵌入在不同元信息中，可以给出不同深度的知识；\n> - 再者费曼学习法虽然像实验记录一样有让人快速恢复学习进度的好处，但也有让人沉溺于流量，忘记学习本心的危险，戒之在躁。\n\n一个机器学习项目的过程包括：准备数据、定义模型、训练、评估效果。先来个简单例子：\n\n```python\nimport torch\nfrom torch import data,nn,optim\n# Data Preparation\ndataset = data.Dataset(...)\ntrainloader = data.DataLoader(dataset,...)\n# Model Definition\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        # ...\n    def forward(self, x):\n        # forward calculation ...\n        return x\nnet = Net()\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n# Training\nfor epoch in range(2):\n    running_loss = 0.0\n    for i, data in enumerate(trainloader, 0):\n        # get the inputs\n        inputs, labels = data\n        # zero the parameter gradients\n        optimizer.zero_grad()\n        # forward + backward + optimize\n        outputs = net(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        # print statistics\n        # ...\nprint('Finished Training')\n# Validation\ncorrect = 0\ntotal = 0\nwith torch.no_grad():\n    for data in testloader:\n        images, labels = data\n        outputs = net(images)\n```\n\n## [张量——自动微分的对象](https://pytorch.org/tutorials/beginner/introyt/tensors_deeper_tutorial.html)\n\n对于基本数据结构——张量，~~不应该~~ 也可以用大写的 `torch.Tensor(data)` 来创建；官方推荐的方法是小写的 `torch.tensor(data)`，产生的张量是 `data` 的拷贝，对一方的改动不反映到另一方。\n\n如果数据已经是 `numpy.ndarray` 了，可以用 `x_tensor = torch.from_numpy(x_array)` 和 `x_array = x_tensor.numpy()` 互相转化，他们使用同一片内存，对一方的改动会反映到另一方。\n\n更推荐的方法是用各种工厂函数，比如 \n\n- `torch.empty()`\n- `torch.zeros()`, `torch.zeros_like()`\n- `torch.ones()`, `torch.ones_like()`\n- `torch.eye()`\n- `torch.rand()`, `torch.rand_like()`\n\n其中在声明随机张量之前使用 `torch.manual_seed(seed)`，可以让距离该函数相同执行距离的随机数代码总是产生相同的数值：\n\n> [https://pytorch.org/docs/stable/notes/randomness.html](https://pytorch.org/docs/stable/notes/randomness.html)\n> \n\n### 尺寸相关\n\n张量的尺寸是一个它本身的一个性质 `x.shape`, 属于 `torch.Size` 类。\n\n张量之间的计算，在尺寸不同时，遵循类似 NumPy 的 broadcasting 规则。\n\n`x.item()` 从一个单元素张量（不一定是标量）里取出它的值，返回值的数据类型不再是 PyTorch 自己的类型，而是 python 原生类型。\n\n`x.unsqueeze(dim)` 在第 dim 个维度增加一个长度为 1 的维度。\n\n更一般的改变形状需要 `x.reshape(*new_shape)` 和 `torch.reshape(x,new_shape)`\n\n### 物理设备相关\n\n一般数学函数也有 `out` 参数，可以原位写入已存在的张量，不改变原张量的 id。\n\n原位计算是在一般数学函数名字后面加一下划线，如 `torch.sin_(x)`, `x1.add_(x2)`\n\n张量的赋值是传递指针，而不是传递数值。数值拷贝需要 `x2 = x1.clone()`. 克隆产物会保持和之前一样的 `requires_grad` 参数值。如果原张量在计算图里，新张量不需要的话应该 `c = a.detach().clone()`\n\n参与运算的两个张量需要在同一设备 (CPU/GPU). 创建张量的时候可以用 `device` 参数声明设备，已经创建的张量可以用 `x.to(device)` 改变设备。\n\n### 自动微分和神经网络相关\n\nPyTorch 中的张量都有 `requires_grad` 参数，默认为 False。\n\n对于带有 `requires_grad=True` 参数的张量，都受到自动微分机制 autograd 的控制。\n\n前面每个每个创建张量的函数都接受这个参数，也自动传染给对自动微分的张量进行数学计算的结果。\n\n神经网络，也就是继承自 `torch.nn.Module` 的对象（假设实例化为 `net`），内部有两个方法`net.parameters()` 和 `net.buffers()`，它们取到的是张量的 iterator，其中前者里面的张量（绝大多数）受 autograd 控制，后者不然。\n\n要想取得某个神经网络的单个参数，需要知道 `__init__()` 方法中的变量名，以及官网文档中的参数名（一般在 Variables 小节，比如[这个例子](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html)）\n\n```python\nclass MyNetwork(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.param1 = torch.nn.Parameter(torch.zeros((5,)),requires_grad=True)\n        self.linear = torch.nn.Linear(in_features=5,out_features=1,bias=True)\n    def forward(self,x):\n        return self.linear(x * self.param1)\nnet = MyNetwork()\nwith torch.no_grad():\n    print(net.param1)\n    print(net.linear.weight)\n    print(net.linear.bias)\n```\n\n## [自动微分](https://pytorch.org/tutorials/beginner/introyt/autogradyt_tutorial.html)\n\nautograd 自动根据代码动态构建计算图：\n\n```python\na = torch.linspace(0., 2. * math.pi, steps=25, requires_grad=True)\nb = torch.sin(a)\nc = 2 * b\nd = c + 1\nout = d.sum()\n```\n\n![上述代码构成的计算图](/photos/2024-11-10-computation-graph.png)\n\n对于数学函数 `b = f(a,*args)` 计算出来的张量 `b` ——\n\n### `.grad_fn` 是一个函数\n\n- 输入只有一个，尺寸需要和 `a` 的尺寸对齐 ~~（我感觉很奇怪）~~\n- 输出的数量等于计算出这个张量的函数的输入的数量。\n- ~~直接运行这个函数 ，基本就是把输入张量重复输出~~。从[这个问答](https://stackoverflow.com/questions/66402331/in-pytorch-what-exactly-does-the-grad-fn-attribute-store-and-how-is-it-used)来看，直接运行这个函数几乎没有意义，因为自动微分是用其他编程语言实现的，`SinBackward0` 并不是 Python 对象。\n- `t.grad_fn.next_functions` 是一个 tuple，成员是 `x1` 等参数的 `.grad_fn`, 可以继续递推到工厂函数创建的张量，也就是这个计算图的叶节点。叶节点的 `.grad_fn` 为 `None`\n\n```python\nprint(a.grad_fn)   # None\nprint(b.grad_fn)   # <SinBackward0 object at 0x000001C8E240F400>\nprint(c.grad_fn)   # <MulBackward0 object at 0x000001C8E240EB30>\nprint(d.grad_fn)   # <AddBackward0 object at 0x000001C8E240F400>\nprint(out.grad_fn) # <SumBackward0 object at 0x000001C8E240F190>\n```\n\n### `.backward()`\n\n- 默认状态下，计算图的最终节点 `out` 是一个标量，也就是 0 阶张量。\n    - 要想让计算图的最终节点是一个高阶张量，需要指定 `out.backward(gradient)` 参数。`gradient`和 `out` 尺寸相同，两者对应元素相乘后求和，从而成为一个标量。\n    - 这种设定可以让一个张量的梯度 `.grad` 的尺寸只和自身的尺寸相等；如果允许计算从 `out` 到自身的 Jacobian 矩阵，会导致计算规模按多项式（一般是平方）增长。\n- 默认状态下，一个计算图只能有一个`.backward()`，且只能执行一次。多次执行会报 `RuntimeError`: “Trying to backward through the graph a second time”。\n    - 要想对完全相同的计算图做多次 `.backward()`, 需指定 `out.backward(retain_graph-True)` 参数。\n\n### `t.grad`\n\n- 偏导数张量 $$\\partial\\mathrm {out}/\\partial \\mathrm t$$，在最终节点运行 `.backward()` 之后，计算图中各节点的 `.grad` 会自动被 autograd 赋值\n- 默认情况下，只有计算图的叶节点可以访问其 `.grad`\n- 要取得中间节点的 `.grad`，需要在 `.backward()` 之前对相应张量执行 `.retain_grad()`\n\n```python\nb.retain_grad()\nout.backward()\n\nprint(a.grad)   # tensor([...],...), 约等于 2 * torch.cos(a)\nprint(b.grad)   # tensor([2.,2.,2.,...2.],...)\nprint(c.grad)   # None，不是计算图的叶节点，也没有 .retain_grad()\nprint(d.grad)   # None，不是计算图的叶节点，也没有 .retain_grad()\nprint(out.grad) # None，不是计算图的叶节点，也没有 .retain_grad()\n```\n\n以上各个操作基本上符合数学的习惯。\n\n## 张量及其微分的更新\n\n以下设定更多的是为了工程上方便：\n\n`.grad` 是一个张量的性质，不是最终节点和该张量两者之间的性质，也不是计算图整体的性质。\n\n不论是 `.backward(retain_graph=True)`，还是另外构建了新的计算图并运行 `.backward()`（比如训练时加载了训练集的新一轮 batch），当一个已有 `.grad` 的张量被新的 `.backward()` 波及时，`.grad` 的新值会是旧值和本轮偏导数的相加。\n\n如果不想要这种累加，需要主动提前将张量 `.grad` 改成 0 或者 None。\n\n除了梯度张量的置 0，\n\n### 手动\n\n正常情况下，`requires_grad=True` 的张量，对其做的所有操作都会被 autograd 记录，包括赋值操作，这就导致正常情况下，不论是清空梯度，还是根据梯度更新模型的参数，会导致 auto grad 的混乱。\n\n所以需要特殊操作，通知 autograd 某些对受控张量的改动，不需要成为计算图的一部分。\n\n所谓的特殊操作就是 `torch.no_grad()`\n\n```python\na = torch.arange(5.,requires_grad=True)\nb = torch.sin(a)\nb.backward(gradient=torch.ones_like(b))\nwith torch.no_grad():\n    a[1] = 10\n    a.grad.zero_()  # a.grad = 0\nprint(a)  # tensor([0,10,2,3,4], requires_grad=True)\n```\n\n在完全不涉及训练，只需要推理的环境，`with torch.inference():` 比 `with torch.no_grad():` 更节省计算资源。\n\n### 自动\n\nPyTorch 提供了上述操作的接口。\n\n神经网络的参数的更新有一整个 `torch.optim` [模块](https://pytorch.org/docs/stable/optim.html#algorithms) 提供的更新参数的方式（`Optimizer` 的子类）。里面的优化器都实现了两个方法：`.step()` 和 `.zero_grad()`，前者修改参数本身的值，后者将各参数的梯度归零。\n\n```python\nclass MyModel(torch.nn.Module):\n    def __init__():\n        # ...\n    def forward(x):\n        # ...\n        return x\nnet = MyModel()\nopt = torch.optim.SGD(net.parameters(),lr=0.01)\nfor e in range(epoch):\n    # ...\n    opt.step()\n    opt.zero_grad()\n```\n\n如果只需要将梯度归零的话，nn.Module 也有一个 `.zero_grad()` 方法。\n\n### `tensor` vs. `tensor.data`\n\n看 SGD 优化器的源码的时候，看到更新权重的时候不是 `with torch.no_grad():`，而是用到了张量的 `a.data` 和 `a.grad.data`\n\n`a` vs. `a.data`, `b.grad` vs. `b.grad.data` 的区别今天不需要知道，不应该知道。这是旧的 Variable 对象的残留，新的接口将 Variable 合并进了 Tensor，`.data` 只是为了兼容旧代码而存在的，不再推荐这种用法。\n"},{"slug":"switched-domain-name","filename":"2024-09-30-switched-domain-name.md","date":"2024-09-30","title":"通知：本站网址変更","layout":"post","keywords":["md","html"],"excerpt":"未来可能无法自动跳转，请浏览器收藏和 RSS 订阅的读者更新网址。","content":"\n2024 年 9 月 30 日起，本站网址\n- 从 `https://mountaye.github.io/blog/` \n- 变更为 `https://blog.mountaye.com`.\n\n因为网站仍然架设在 GitHub Pages 上，所以旧网址可以自动跳转到新网址。\n\n但是下一步计划把网站迁移到 Cloudflare Pages 上，因为 Next.js 项目不同构建之间文件差异过多，不适合作为 git 仓库的内容进行托管。\n\n所以无法保证将来旧网址依然可以自动跳转。请使用浏览器收藏功能的读者更新收藏夹，使用 RSS 订阅功能的读者更新 RSS 源。\n\n根据初步计划，过渡期将持续到 2024 年 12 月 31 日。\n"},{"slug":"static-blog-with-nextjs-tailwindcss-shadcn","filename":"2024-09-17-static-blog-with-nextjs-tailwindcss-shadcn.md","date":"2024-09-17","title":".js | 博客改用 Next.js + TailwindCSS + Shadcn.UI","layout":"post","keywords":["md","js"],"excerpt":"我的博客本来是用 Jekyll 生成的静态网站，如今改用 Next.js + TailwindCSS + Shadcn.UI","content":"\n## 技术选型\n\n### 不再选择静态网站生成工具\n\n我的博客本来是用 Jekyll 生成的静态网站，因为网站架在 GitHub Pages 上，而 Jekyll 是 GitHub Pages 默认的构建工具。但是 Jekyll 的核心开发者年事已高，有的甚至已经去世，所以我感觉这个项目未来的活力堪忧。\n\nJekyll 是用 Ruby on Rails 写成的，其他编程语言，和 Jekyll 功能类似的工具有：\n\n- Python 的 Pelican\n- JavaScript 的 Hexo\n\n但是我还想做些更复杂的事情，预计需要靠服务端来实现。所以与其花时间移植到一个和 Jekyll 功能类似的工具，不如直接一步到位，学习一个全栈框架。\n\n### Next.js\n\n虽然不同的编程语言也都有自己的全栈框架，比如 Python 有 Django，但是既然浏览器主要支持 JavaScript，所以索性前后端都用 JS 比较方便，而且这样想的人很多，社区规模使得遇到问题更容易找到答案（这个优势在生成式语言模型的时代似乎没那么重要了）。\n\nJavaScript 语言之下，也存在至少 React 和 Vue 两大阵营。之所以选择 Next.js 这样一个基于 React.js 的框架，主要是路径依赖，很久很久以前学过[赫尔辛基大学的全栈公开课](https://fullstackopen.com/zh/)，那里教的就是 React。\n\n对于 Next.js 本身，Youtube@Fireship 有一个很简洁的介绍：[https://www.youtube.com/watch?v=Sklc_fQBmcs](https://www.youtube.com/watch?v=Sklc_fQBmcs)\n\n官网提供的教程在这里：[https://nextjs.org/learn](https://nextjs.org/learn)，和我学习的时候已经不一样了，那时候只有 page router，没有 app router。开发这一框架的 Vercel 公司也提供 Next.js 的云服务。但是鉴于其有过把用户引诱到 app router 架构然后给自家服务提价的黑历史，所以短期内不打算学和用 app router.\n\n### TailwindCSS\n\n[https://tailwindcss.com/](https://tailwindcss.com/)\n\n早就学过了，但是一直没有机会用。这次本来也可以不用的，直到决定使用 Shadcn.ui 组件库，因为 TailwindCSS 是它的一个依赖项。既然已经安装了，那不用白不用。\n\n### Shadcn.UI 而非 HeadlessUI\n\nhttps://ui.shadcn.com/\n\n一套组件库，也就是网页中经常出现的功能单元。这个库相对于竞品的最大优势，是允许直接复制粘贴代码而不用安装，用多少抄多少～\n\nTailwindCSS 自家也有一个组件库 [HeadlessUI](https://headlessui.com/)，但是主要是 `<form/>` 表单及其成员，侧重于向后端传数据的 HTML 元素的封装。而 Shacn 有很多炫酷的交互方面的组件，突出一个现成且好看。\n\n### 没用 Figma\n\n有用的功能都收钱，免费的功能不如直接 `next dev` 实时预览。\n\n## ~~踩过的坑~~ 学到的经验\n\n### 组件化作为一种思路\n\n正常的网页，HTML 负责内容，CSS 负责装饰，JavaScript 负责交互。这种分工，专业上叫做解耦。\n\n工程实践表明，这种解耦方式非常反人类。用户看到和使用的网页，是以功能上的相似、空间上的相邻为组织的，而要对其修改时，则需要到不同源代码的不同位置去；反之，某处代码的改动，无意中可能对远处的另一部分视觉效果和功能造成破坏。\n\n于是较新的全栈框架，其解耦的方式都是以组件为单位的，内容、样式、交互逻辑都写在一起。\n\n### JavaScript/JSX 语法中的 `{}`\n\nJSX 是对 JavaScript 语法的扩展，添加了类似于 HTML 标记的写法 `<MyComponent></MyComponent>`，用来表示 react 组件。\n\n在 jsx 语言的内部写 Javascript 时，需要将 JS 外面包裹一层 `{}`\n\nJavaScript 里类似 Python f-string 的结构写作``${}``，名叫 template literal: ``this is var: ${var}``\n\n——以上规则结合起来，会产生让初学者迷惑的现象：\n\n- `<MyComponent className=’dark’/>`: 一个正常的类名，直接用引号\n- `<MyComponent className={dark}/>:` 类名是一个 JS 字符串变量\n- 类名的一部分根据一个变量取值:\n  ```javascript\n  <MyComponent className={`bg-${dark}`}/>\n  ```\n\n### JavaScript 箭头函数中的 `{}` 和 `()`\n\nJavaScript 的箭头函数类似于 python 的 lambda 纯函数，但是有不同。\n\n`(var) => (expression(var))` 相当于 Python 中的 `lambda x: expression(x)` \n\n但是箭头函数的右侧可以是 `{}` 包裹的若干表达式，此时需要显式 return：\n\n```jsx\n(var) => {\n    expression1(var);\n    expression2(var);\n    return expression3(var);\n}\n```\n\nPython 的 lambda 必须是单一表达式的纯函数，不允许上面第二种写法。\n\n### 对象解包中的 `{}`\n\n当有一个包含若干键值对的对象时\n\n```jsx\nobj = {\n\t\tk1: \"value1\",\n\t\tk2: \"value2\",\n\t\tk3: \"value3\",\n\t\t...\n}\n```\n\n可以用 `const { k2 } = obj;` 的方式拿到 `obj[’k2’]` 的值，赋给 `k2`。\n\n结合上一节，\n\n- `(k1,k2)=>(k1+k2);` 是一个两个自变量的函数；\n- `({k1,k2})=>(k1+k2);` 是以一个 Object 为自变量的函数，这个 Object 的名字无所谓，也不确定一共有多少个属性，但属性中至少包含 `k1` 和 `k2`.\n\n### TailwindCSS 的 arbitrary value、JavaScript 的模板字符串、Shadcn 中的 `cn()` 函数\n\nTailwindCSS 的很多属性都允许在方括号中使用任意值，比如背景色 `bg-[#a4b4c4]`\n\n本以为可以直接在 `className` 里面用 template literal `<div className={`bg-[${myColor}]`}>`，但是并不总是生效。\n\n这个“并不总是”是个大坑，一开始在开发模式用得好好的，结果某次刷新页面之后就挂了，简直莫名其妙。\n\n好在 Shadcn 提供了一个 [`cn()` 函数](https://github.com/shadcn-ui/ui/blob/main/apps/www/lib/utils.ts)，接受一个或多个 TailwindCSS 类名字符串作为输入，就可以正常使用 template literal 了，例如 `<div className={cn(\"block\",\"border-0\",`bg-[${myColor}]`)}></div>`\n\n### HTML + CSS 布局\n\n两类套路：\n\n1. 传统的 `display`, `position`, `float` 属性；\n2. Flex 和 Grid 布局，看阮一峰先生的博客里的教程就挺方便：[Flex](https://www.ruanyifeng.com/blog/2015/07/flex-grammar.html), [Grid](https://www.ruanyifeng.com/blog/2019/03/grid-layout-tutorial.html).\n\n理论上后者新一些，消耗的脑力也更少一些，应该是更好的选择。\n\n但是前者也有一些优势场景。比如现在整个博客页面的上边栏和剩下的部分就是一个上下结构的 flex 布局，这导致屏幕最右侧的滚动条其实是页面一部分的，而不是整个页面的的滚动条。在 iOS 的 Safari 浏览器下，会导致网址栏不能自动隐藏，浪费很大一片屏幕空间。之后可能会换回传统功夫。\n\n而像是目录，文章跳转开头和评论区的按钮等等，需要在页面滚动时相对屏幕静止的元素，就不得不用 position，而且为了锚定在正文上，还需要嵌套好几层。\n\n按照传统功夫——\n\n- `display`可选的取值有 4 个: block | inline | inline-block | none\n    - block: 竖排，哪怕同一行内仍有空间容纳 html 中的下一个元素。\n    - inline: 横排，像文字内容一样，没有盒模型\n    - inline-block: 横排，但是有盒模型\n    - none: 不显示，和 `visibility:hidden` 的区别是，后者依然占有显示时的空间。\n- `position`可选的取值有 5 个: static | relative | fixed | absolute | sticky\n    - static: 默认值，按照 html 文件的顺序排列。\n    - relative: 相对于 static 默认值进行偏移，偏移量由 `top`, `right`, `bottom`, `left` 四个性质决定。所谓 `left: 50px` 的意思是左侧 margin 外多出 50 像素的空间，实际是向右偏移的效果。\n    - fixed: 相对于**视窗**的位置固定，位置由 `top`, `right`, `bottom`, `left` 四个性质决定。`left: 50px` 的意思是该元素的左侧 border **外沿**距离窗口左边 50 像素。\n    - absolute: 相对于最近一层父元素的位置固定，位置由 `top`, `right`, `bottom`, `left` 四个性质决定。`left: 50px` 的意思是该元素的左侧 border **外沿**距离父元素左侧**内沿** 50 像素。设计的时候需要考虑 border 宽度。\n    - sticky: 网页加载时按照 html 文件的顺序排列，直到网页滑动到某一位置，之后该元素固定在视窗，就像 fixed 一样，行为改变的位置由 `top`, `right`, `bottom`, `left` 四个性质决定。\n- `float`:  none | left | right | inherit\n    - none: 默认值，按照 html 文件的顺序排列。\n    - left: 保持在父元素左侧，其他元素环绕之。\n    - right: 保持在父元素右侧，其他元素环绕之。\n    - inherit: 和父元素的 float 的取值一致。\n\n### Unified.js 将 Markdown 文档转换为基于 JSX 的 HTML\n\nJekyll 等静态网站生成器的核心功能，就是把 markdown 文档翻译成 html 网页文档。在 Next.js 框架下，这一工作由以 unified.js 为基础的一群第三方库来完成。\n\n最早看到这个框架是在 DIYGOD 的博文《[如何优雅编译一个 Markdown 文档](https://diygod.cc/unified-markdown)》里，但是直接抄他在 xlog 里面的代码的话，在 next.js 之下好像会报错。所以又去官网仔细读了一下文档，现在可以说是略懂。\n\n这个话题本身值得专门写一篇文章，所以不在这里展开了。\n\n### Giscus 评论区切换黑夜模式\n\n自己写的组件的亮暗切换，是通过在 `<html/>` 元素添加和删除 `dark` 类，然后搭配 TailwindCSS 的 `dark:` 来实现的。\n\nGiscus 官方支持切换黑夜模式：[https://github.com/giscus/giscus/blob/main/ADVANCED-USAGE.md#parent-to-giscus-message-events](https://github.com/giscus/giscus/blob/main/ADVANCED-USAGE.md#parent-to-giscus-message-events)。这套方法的关键，在于服务端返回的 iframe 有一个名为 `giscus-frame` ****的类。\n\nGiscus 为 react 提供了一套组件可以直接使用，但是在这套组件里面并没有这个类。\n\n所以只能弃用官方的组件，自己用 `useEffect` 模拟官网的 `<script/>`\n\n```jsx\nexport function MyGiscus() {\n  useEffect(\n    () => {\n      const onPageLoad = () => {\n        console.log(\"<MyGiscus/>: activated on page load.\")\n        // START real business\n        const script = document.createElement('script');\n        script.src = \"https://giscus.app/client.js\";\n        script.setAttribute('data-repo',              '');\n        script.setAttribute('data-repo-id',           '');\n        script.setAttribute('data-category',          '');\n        script.setAttribute('data-category-id',       '');\n        script.setAttribute('data-mapping',           '');\n        script.setAttribute('data-strict',            '');\n        script.setAttribute('data-reactions-enabled', '');\n        script.setAttribute('data-emit-metadata',     '');\n        script.setAttribute('data-input-position',    '');\n        script.setAttribute('data-theme',             '');\n        script.setAttribute('data-lang',              '');\n        script.crossOrigin = 'anonymous';\n        script.async = true;\n        document.getElementById(\"comments\").appendChild(script);\n        // END real business\n      };\n      // Check if the page is already loaded\n      if (document.readyState==='complete') {\n        onPageLoad();\n      } else {\n        // Add event listener for page load\n        window.addEventListener('load',onPageLoad);\n        // Cleanup the event listener on component unmount\n        return () => { window.removeEventListener('load',onPageLoad); }\n      };\n    },\n    []\n  );\n  return (<div id='comments'></div>);\n}\n```\n\n### Next.js 的构建参数部分\n\n要让 next.js 构建静态网站，需要在 next.config.js 中写：\n\n```jsx\nmodule.exports = {\n  basePath: '/blog',\n  output: 'export',\n  generateBuildId: async () => \"buildID\",\n  // i18n: {\n  //   locales: ['zh-CN', 'en'],\n  //   defaultLocale: 'zh-CN',\n  //   localeDetection: false,\n  // },\n}\n```\n\n`basePath` 是因为博客的 GitHub 仓库 blog 不是默认的个人网站仓库；需要注意的是，next.js 自己的 Link 组件的 `href` 参数不需要包含这个值，但是图片等等的 `src` 参数需要。\n\n`generateBuildId` 函数的返回值是随便写的，不设定的话会导致输出里的 `_next/` 文件夹里有很多哈希值为名的文件夹，在 git 下会被当成不同的 blob 一直留在项目里。\n\n`i18n` 参数被注释掉了，因为静态生成的 next.js 项目不支持自动 i18n.\n\n### RSS 源和 sitemap\n\nRSS 由 feed 这个 npm 包来构建；\n\nsitemap 则是在 ChatGPT 的帮助下手写字符串。\n\n写好的字符串，通过在主页或者历史归档页面的 `getStaticProps()` 函数，写入 next.js 项目的 `public/` 文件夹。\n\n## 还没解决的问题\n\n### 手机端搜索框的汉字输入问题\n\n页面顶端的搜索框在手机触摸屏上，用汉字输入法输入关键词之后，直接按回车键，会导致已经输入的汉字被当成拼音，传递给搜索引擎。\n\n暂时的办法是在输入汉字之后，按回车键之前按一下空格。\n\n### Shadcn.Drawer 组件的上游代码报错；Google Ads\n\n新博客把谷歌广告撤了。赚不到多少钱不说，它还会往网页里动态添加元素，破坏原来的排版。\n\n话虽如此，每篇文章的右下角还是有个[要饭的图标](https://lucide.dev/icons/hand-heart)，计划用 shadcn 的 Drawer，放赞赏二维码，或者交换来的友站链接。\n\n但是目前 Drawer 的上游代码会报错，看起来作者已经在修复了，等更新。\n\n### Google Analytics\n\nnext.js 提供了 Google analytics: [https://nextjs.org/docs/pages/building-your-application/optimizing/third-party-libraries](https://nextjs.org/docs/pages/building-your-application/optimizing/third-party-libraries)\n\n但是加入之后没有反应，google 后台看不到，数据一落千丈。\n\n据说把相应的代码放到 `pages/_app.js` 可以解决问题，还没试。"},{"slug":"python-test-notes","filename":"2023-10-16-python-test-notes.md","date":"2023-10-16","title":".py | Python 测试笔记","layout":"post","keywords":["md","py"],"excerpt":"如题","content":"\n## 一些名词\n\n### 调试和测试\n\n调试一般是由代码的作者进行，用于自行检查程序运行过程，是否存在思路和实现不匹配的错误，调试的代码一般和程序主体写在一起，主要包括错误处理和日志记录。简单的可以用 print 和 assert，复杂的程序可以用 logging。\n\n测试一般由第三方进行，测试代码和程序代码分离，写测试的人甚至不需要理解程序的具体工作原理，只关注给定的输入能否得到程序宣称的输出。\n\n### 单元测试\n\n对一个模块、一个函数或者一个类来进行正确性检验的测试工作。\n\n检验的方法是写一堆测试用例，把测试员拍脑袋想的输入交给相应的模块，看模块的输出是否正确；以及不合理的输入是否被程序识别，抛出异常。\n\n单元测试全通过了不代表程序整体一定就没错误，但是单元测试通不过的程序一定有问题。\n\n### 文档测试\n\n文档是对代码的功能介绍，其中不免要举例子，给出实例代码和相应的输出，这个过程很像是单元测试，只不过是纯嘴炮。\n\n文档测试就是自动寻找文档中的示例代码，运行之后，和文档中的结果进行比对。\n\n### 集成测试\n\n集成测试模拟用户的行为，测试各个模块之间的配合，测试结果应该保证程序可以在生产环境中工作。\n\n## Python 中的测试\n\n### 文件结构\n\n上一篇文章里面提到了 src-layout，在 setuptools 的官方文档里提供了[一篇博客文章](https://blog.ionelmc.ro/2014/05/25/python-packaging/#the-structure)，里面提到，这种文件结构的一个优点就是，测试代码的文件夹一般和 `src/` 而不是 package 平级，这就导致运行测试的时候只能先（在虚拟环境里）安装待测试的包，而不会无意中出现测试的代码和用户下载到的内容不同的问题。\n\n```\n<project_name>\n├── LICENSE\n├── pyproject.toml\n├── README.md\n├── src/\n│   └── <package_name>/\n│       ├── __init__.py\n│       └── example.py\n└── tests/\n```\n\n### 单元测试·`unittest`\n\n`unittest` 是 Python 自带的单元测试库。\n\n测试脚本的内容基本如下：\n\n```\nimport unittest\n\nclass TestName1(unittest.TestCase):\n\n    def test_sum(self):\n        self.assertEqual(sum([1, 2, 3]), 6, \"Should be 6\")\n\n    def test_sum_tuple(self):\n        self.assertEqual(sum((1, 2, 2)), 6, \"Should be 6\")\n\nclass TestName2(unittest.TestCase):\n\n    def test_sum(self):\n        self.assertEqual(sum([1, 2, 3]), 6, \"Should be 6\")\n\n    def test_sum_tuple(self):\n        self.assertEqual(sum((1, 2, 2)), 6, \"Should be 6\")\n\nif __name__ == '__main__':\n    unittest.main()\n```\n\n- 测试用例包装在一个 class 里面，这个 class 继承自 `unittest.TestCase`\n- 所有测试方法名字以 “test” 开头，能测试的性质有限，都是类自带的方法，以 `self.` 开头。支持的方法见下表，感觉 `[assertTrue(x)](https://docs.python.org/3/library/unittest.html#unittest.TestCase.assertTrue)` 和 `assertRaises(Exception)` 包打一切\n- 作为 `'__main__'` 运行，运行的是自带的函数 `unittest.main()`。\n- 运行时需要 python 指明脚本的文件名。安装了 nose2 这个库的话，可以直接运行 `python -m nose2`, 它会自动寻找所有的测试依次运行。（后面发现 `unittest` 好像也有自动发现功能）\n\n| Method | Checks that | New in |\n| --- | --- | --- |\n|         [unittest.TestCase.assertEqual](https://docs.python.org/3/library/unittest.html#unittest.TestCase.assertEqual) | a == b |  |\n|      [unittest.TestCase.assertNotEqual](https://docs.python.org/3/library/unittest.html#unittest.TestCase.assertNotEqual) | a != b |  |\n|          [unittest.TestCase.assertTrue](https://docs.python.org/3/library/unittest.html#unittest.TestCase.assertTrue) | bool(x) is True |  |\n|         [unittest.TestCase.assertFalse](https://docs.python.org/3/library/unittest.html#unittest.TestCase.assertFalse) | bool(x) is False |  |\n|            [unittest.TestCase.assertIs](https://docs.python.org/3/library/unittest.html#unittest.TestCase.assertIs) | a is b | 3.1 |\n|         [unittest.TestCase.assertIsNot](https://docs.python.org/3/library/unittest.html#unittest.TestCase.assertIsNot) | a is not b | 3.1 |\n|        [unittest.TestCase.assertIsNone](https://docs.python.org/3/library/unittest.html#unittest.TestCase.assertIsNone) | x is None | 3.1 |\n|     [unittest.TestCase.assertIsNotNone](https://docs.python.org/3/library/unittest.html#unittest.TestCase.assertIsNotNone) | x is not None | 3.1 |\n|            [unittest.TestCase.assertIn](https://docs.python.org/3/library/unittest.html#unittest.TestCase.assertIn) | a in b | 3.1 |\n|         [unittest.TestCase.assertNotIn](https://docs.python.org/3/library/unittest.html#unittest.TestCase.assertNotIn) | a not in b | 3.1 |\n|    [unittest.TestCase.assertIsInstance](https://docs.python.org/3/library/unittest.html#unittest.TestCase.assertIsInstance) | isinstance(a, b) | 3.2 |\n| [unittest.TestCase.assertNotIsInstance](https://docs.python.org/3/library/unittest.html#unittest.TestCase.assertNotIsInstance) | not isinstance(a, b) | 3.2 |\n\n### 文档测试\n\n[https://www.liaoxuefeng.com/wiki/1016959663602400/1017605739507840](https://www.liaoxuefeng.com/wiki/1016959663602400/1017605739507840)\n\nPython 的文档测试用的是 `doctest` 库，写法如下：\n\n```python\nclass Dict(dict):\n    '''\n    Simple dict but also support access as x.y style.\n\n    >>> d1 = Dict()\n    >>> d1['x'] = 100\n    >>> d1.x\n    100\n    >>> d1.y = 200\n    >>> d1['y']\n    200\n    >>> d2 = Dict(a=1, b=2, c='3')\n    >>> d2.c\n    '3'\n    >>> d2['empty']\n    Traceback (most recent call last):\n        ...\n    KeyError: 'empty'\n    >>> d2.empty\n    Traceback (most recent call last):\n        ...\n    AttributeError: 'Dict' object has no attribute 'empty'\n    '''\n    def __init__(self, **kw):\n        super(Dict, self).__init__(**kw)\n\n    def __getattr__(self, key):\n        try:\n            return self[key]\n        except KeyError:\n            raise AttributeError(r\"'Dict' object has no attribute '%s'\" % key)\n\n    def __setattr__(self, key, value):\n        self[key] = value\n\nif __name__=='__main__':\n    import doctest\n    doctest.testmod()\n```\n\n在保持 docstring 缩进的前提下，`>>>`  开头的注释会被当作测试运行，紧随其后的行将作为对比基准。只有当预期报错的时候，可以用 `...` 省略中间的报错信息。\n\n### 集成测试\n\n也可以用 `unittest` 完成。\n\n和单元测试的区别在于，一般来说需要构建测试数据集等等。这需要重写 `unittest.TestCase.setup()`\n\n```python\nclass TestComplexData(unittest.TestCase):\n    def setUp(self):\n        # load test data\n        self.app = App(database='fixtures/test_complex.json')\n\n    def test_customer_count(self):\n        self.assertEqual(len(self.app.customers), 10000)\n\n    def test_existence_of_customer(self):\n        customer = self.app.get_customer(id=9999)\n        self.assertEqual(customer.name, u\"バナナ\")\n        self.assertEqual(customer.address, \"10 Red Road, Akihabara, Tokyo\")\n\nif __name__ == '__main__':\n    unittest.main()\n```\n"},{"slug":"python-packaging","filename":"2023-10-13-python-packaging.md","date":"2023-10-13","title":".py | 让自己的代码可以被别人使用","layout":"post","keywords":["md","py"],"excerpt":"这里所说的“别人”，也包括6个月之后，已经不记得当初如何写出这段代码的自己。","content":"\n> 这里所说的“别人”，也包括6个月之后，已经不记得当初如何写出这段代码的自己。\n> \n\n很久以前写过一篇《**[import 引用现成的代码](https://mountaye.github.io/blog/articles/python-import-script-module-package)**》讲如何使用别人的代码，最后讲到一个模块的 `setup.py` 文件就没再往下写，这次继续。\n\n- [https://packaging.python.org/en/latest/tutorials/packaging-projects/](https://packaging.python.org/en/latest/tutorials/packaging-projects/)\n- [https://setuptools.pypa.io/en/latest/userguide/quickstart.html](https://setuptools.pypa.io/en/latest/userguide/quickstart.html)\n\n## 基本流程\n\n- 写代码，且让代码项目文件的结构符合一定的要求（见下一节）\n- 根据项目的文件结构，填写 `pyproject.toml`、`setup.cfg` **或** `setup.py` **文件**\n- 安装 build 这个库，然后运行 `python -m build`，产生 `dist/` 文件夹及下面的文件。（可选）将项目上传到 PyPi 或者 Conda\n\n## 项目文件结构\n\n常用的文件结构有两种：src-layout 和 flat-layout，另外一些小项目只有一个 python 文件。\n\n### src-layout\n\n在 src-layout 里，写有 package 源代码的文件夹上层还套了一个文件夹，这个文件夹习惯上命名为 src，当然也可以是别的。`pyproject.toml` 和 `src/` 文件夹同级。\n\n```\n<project_name>\n├── LICENSE\n├── pyproject.toml\n├── README.md\n├── src/\n│   └── <package_name>/\n│       ├── __init__.py\n│       └── example.py\n└── tests/\n```\n\n### flat-layout\n\nflat-layout 指的是写有 package 源代码的文件夹直接作为开发项目的第一级子文件夹，和 `pyproject.toml` 处于同一级。\n\n这种结构比较古老，不太推荐\n\n```\n<project_name>\n├── pyproject.toml  # and/or setup.cfg/setup.py (depending on the configuration method)\n├── <package_name>\n|   ├── __init__.py\n|   └── ... (other Python files)\n├── test\n|   └── ... (test files)\n├── # README.rst or README.md (a nice description of your package)\n└── # LICENCE (properly chosen license information, e.g. MIT, BSD-3, GPL-3, MPL-2, etc...)\n```\n\n### 单文件项目\n\n可以看作是 flat-layout 的一种特殊情况\n\n```\n<project_name>\n├── pyproject.toml  # and/or setup.cfg/setup.py (depending on the configuration method)\n├── <my_module>.py\n├── # README.rst or README.md (a nice description of your package)\n└── # LICENCE (properly chosen license information, e.g. MIT, BSD-3, GPL-3, MPL-2, etc...)\n```\n\n## 填写 `pyproject.toml`、`setup.cfg` **或** `setup.py` **文件**\n\n要想让构建程序把我们的代码打包成安装包，标题中的三个文件至少有一个要出现在 project 的根目录。\n\n文件中要按照各自拓展名对应的语法，填写项目的有关信息，绝大多数可以顾名思义。\n\n各参数的取值和代码文件结构相关，参数主要包括 `name`, `packages`, `package_dir`。如果文件结构完全满足上一节的结构，那么 `setuptools.find_packages()` 的[自动发现机制](https://setuptools.pypa.io/en/latest/userguide/package_discovery.html#automatic-discovery)就够用了。\n\n### `name`\n\n这是一个必填项。\n\n注意：上一节的文件结构中，有两个名字 `<project_name>` 和 `<package_name>` ——\n\n`<project_name>` 是整个开发项目的名字，如果用了类似 git 的版本控制的话，这个名字就是你的 repository 的名字。\n\n`<package_name>` 比较复杂，它可以是，但不一定是你在其他代码中 `import __` 的名字，import 的名字由 `pyproject.toml` / `setup.cfg` **/** `setup.py` 里面的 `name` 参数指定。不能有连字符，只能用下划线。\n\n如果你的 `name` 参数和 `<package_name>` 不同，还需要填写 `package_dir` 参数，\n\n此外还有第三个名字，就是 `pip install __` 时候的名字，上传到 PyPI 的时候填写，可以带有连字符，比如 scikit-image。\n\n### `packages`\n\n参数是一个 list，但是一般都使用 `setuptools.find_packages()` 的结果。\n\n该函数常用三个参数，都是可选的：\n\n- `where`: 一个路径，相对于 `setup.py`\n- `include`: 一个 list，元素是 glob patterns\n- `exclude`: 一个 list，元素是 glob patterns\n\n不指明任何参数 = 使用自动发现机制\n\n### `package_dir`\n\n参数是一个 dict，两种用法：\n\n- 标准的 src-layout，直接写 `{\"\": \"src/\"}`, 表示所有的代码都在这个文件夹里。\n- 当 python 模块的结构和代码的文件结构不同的时候，用这个 dict 指明 模块-文件夹 之间的关系。\n\n文件的路径相对于 `setup.py` 而言\n\n### `py_modules`\n\n参数是一个文件路径的列表，几乎专为单文件结构而存在。\n\n## 打包和上传\n\n安装 build 工具：`python3 -m pip install --upgrade build`\n\n运行 build：`python3 -m build`\n\n如此会生成一个 `dist/` 文件夹，里面包含打包的结果。\n\n要想让自己的程序可以被别人用 `pip install` 的方式安装，需要将打包成果上传到 PyPI，方法在[这里](https://packaging.python.org/en/latest/tutorials/packaging-projects/#uploading-the-distribution-archives)。\n\n## 安装\n\n### 静态安装\n\n已经上传到 PyPI 的包，可以直接用 `pip install <package>` 安装，这种方法叫做静态安装\n\n### 动态安装\n\n还在开发过程中的包，可以在 `setup.py` 所在的位置，运行 `pip install -e .` 这种安装方法叫做动态安装，因为代码的修改可以实时反映在引用的项目中。\n\n## 思考题\n\n上篇文章提到的一个[数据分析项目](https://gist.github.com/ericmjl/27e50331f24db3e8f957d1fe7bbbe510)，其文件结构是这样的（我稍微改动了一下）：\n\n```\n/path/to/project/directory/\n|-- notebooks/\n    |-- 01-first-logical-notebook.ipynb\n    |-- 02-second-logical-notebook.ipynb\n    |-- prototype-notebook.ipynb\n    |-- archive/\n\t      |-- no-longer-useful.ipynb\n|-- src/\n    |-- projectname/\n\t      |-- __init__.py\n\t      |-- config.py\n\t      |-- data.py\n\t      |-- utils.py\n    |-- setup.py\n|-- README.md\n|-- data/\n    |-- raw/\n    |-- processed/\n    |-- cleaned/\n|-- scripts/\n    |-- script1.py\n    |-- script2.py\n    |-- archive/\n        |-- no-longer-useful.py\n|-- environment.yml\n```\n\n问：\n\n1. 这是一个 flat-layout 还是 src-layout？\n2. setup.py 应该怎么写？执行动态安装时的 `pwd` 结果是什么？\n"},{"slug":"notes-on-TailwindCSS","filename":"2023-10-11-notes-on-TailwindCSS.md","date":"2023-10-11","title":".css | TailwindCSS 笔记","layout":"post","keywords":["md","js"],"excerpt":"一直听说“全栈项目 = Next.js + TailwindCSS + HeadlessUI”，但是 TailwindCSS 到底是啥，之前一直妹整明白","content":"\n> 一直听说“全栈项目 = Next.js + TailwindCSS + HeadlessUI”\n但是 TailwindCSS 到底是啥，之前一直妹整明白\n> \n\n## 思路：utility-first\n\n> [https://tailwindcss.com/docs/utility-first](https://tailwindcss.com/docs/utility-first)\n> \n\n传统设计需要根据 html 中的结构，在 CSS 中给相应的元素/class/id 定义所需要的所有样式 style。\n\n问题很明显：\n\n- 最低效的情况下，每个 `<div/>` 都要定义一个 class。\n- 每个定义里包含若干不同的性质，背景颜色、字体、边框样式等等都挤在一个大括号里，耦合过强。\n\nTailwindCSS 的思路名叫 utility-first, 预先定义一批“性质-取值”的组合，每个组合给出一个有规律命名的类。使用时，一个 `<div/>` 后面声明几个甚至几十个不同的 class。缺点就是不灵活了，每个性质只搭配有限几种取值，且类的数量很多。好处是——\n\n- 不用绞尽脑汁给类取名字\n- CSS 不会再变大了（也可以说已经大得不能再大了）\n- 修改视觉效果时更换一个类，而不是修改类的定义，也就不用担心对类的修改在自己不记得的地方生效。\n\n与之相对的另一种思路，是直接用 html 元素的 style 属性，或者用 module.css 让样式只对某一 component 生效。TailwindCSS 派对这种方法的批评是：\n\n- 每个取值都是设计者拍脑袋想出来的，一个项目要拍太多次脑袋，容易风格不统一。\n- 难以做 responsive design （真的吗？很怀疑）\n- 难以处理鼠标悬浮、聚焦等等状态（这玩意应该由 CSS 处理吗？）\n\nutility-first 在维护性方面收到批评的一点是，很多地方要不断重用相同的组合，少了一点封装和抽象。TailwindCSS 对此的辩护是，可以抽象出 components 和 partials（见下节），或者使用编辑器的多光标功能。（绷……）\n\n## 技术细节\n\n- 样式重用\n- 状态，比如鼠标悬浮、聚焦\n- Responsive design\n- 夜间模式\n- 添加自定义样式\n- 函数和 directives\n\n### 状态，比如鼠标悬浮、聚焦\n\n在正常的类名字之前添加 `<状态>:` 标记，用来指明在相应状态时的样式。这些状态可以叠加，之间用 `:` 分隔。比如 `<button class=\"hover:bg-sky-700\">`\n\n可以标记的状态：[https://tailwindcss.com/docs/hover-focus-and-other-states#appendix](https://tailwindcss.com/docs/hover-focus-and-other-states#appendix)\n\n- Pseudo-classes\n    - 举例： [https://tailwindcss.com/docs/hover-focus-and-other-states#pseudo-class-reference](https://tailwindcss.com/docs/hover-focus-and-other-states#pseudo-class-reference)\n        - `hover:`, `focus:`, `active:`\n        - `first:`, `last:`, `odd:`, `even:`\n        - `required:`, `invalid:`, `disabled:`: 主要用在 `<form>` 中\n    - 需要父元素的状态信息时，\n        - 如果因为嵌套，存在多个 group 时，可以给每一个父元素的类命名 `group/<name>`, 子元素的类名需要写在伪类的后面，有点反直觉 `group-hover/<name>:`\n        - 给父元素添加 `group` 的 class，然后给需要变化的子元素添加 `group-<pseudo-class>:` 前缀。如 `group-hover:`\n        - 当需要更细致的选择时，可以在子元素的 group 后面添加自定义内容，如 `group-[.is-published]:`, `group-[:nth-of-type(3)_&]:`\n    - 需要姊妹元素的状态信息时：\n        - 给被跟踪的姊妹元素添加 `peer` class, 被跟踪的元素只能在跟踪元素的前面。\n        - 其余特性类比 group\n- [Pseudo-elements](https://tailwindcss.com/docs/hover-focus-and-other-states#pseudo-elements), like `::before`, `::after`, `::placeholder`, and `::selection`\n    - 写作 `before:` 等等，默认相当于 `before:content-['*']`\n    - 当想要调整 content 以外的性质时，需要指明 `before:block`, `before:absolute`, `before:-inset-1` 等等\n    - `placeholder:` 用于调整表格中代填内容的样式\n    - `file:` 上传文件按钮的样式\n    - `list:` 列表开头的\n    - `selection:` （鼠标）选中文字之后的样式\n    - `first-line:`, `first-letter:` 杂志常用的首行、首字母的特殊样式\n- [Media and feature queries](https://tailwindcss.com/docs/hover-focus-and-other-states#media-and-feature-queries), like responsive breakpoints, dark mode, and `prefers-reduced-motion`\n    - 结合响应式设计 responsive design 一节，使用 `md:`, `lg:` 等前缀\n    - `dark:` 黑夜模式\n    - `motion-reduce:` 用户选择屏蔽动画效果时的样式，`motion-safe:` 只有不屏蔽动画才会生效的样式\n    - `portrait`,`landscape` 屏幕朝向\n    - `print:` 打印时的样式\n    - `supports-[...]` 当浏览器支持某种特性时启动。也可在 `tailwind.config.js` 文件中设置 `theme.supports` 变量\n- [Attribute selectors](https://tailwindcss.com/docs/hover-focus-and-other-states#attribute-selectors), like `[dir=\"rtl\"]` and `[open]`\n    - `aria-*` modifier to conditionally style things based on [ARIA attributes](https://developer.mozilla.org/en-US/docs/Web/Accessibility/ARIA/Attributes).\n    - `data-[key=value]` data 参数的值\n    - `ltr:` & `rtl:` 从右往左书写的文字\n    - `open:` & `close:` 用于可以展开的元素\n    - 自定义选择符：用中括号包围，`&` 开头选择元素，下划线表示空格，如 `[&:nth-child(3)]:`, `[&_p]:mt-4`, `[@supports(display:grid)]:grid`\n\n### Responsive design\n\n| Breakpoint prefix | Minimum width | CSS |\n| --- | --- | --- |\n| sm | 640px | @media (min-width: 640px) { ... } |\n| md | 768px | @media (min-width: 768px) { ... } |\n| lg | 1024px | @media (min-width: 1024px) { ... } |\n| xl | 1280px | @media (min-width: 1280px) { ... } |\n| 2xl | 1536px | @media (min-width: 1536px) { ... } |\n\n移动端优先的思路，所有尺寸限定的都是大于该宽度时的样式。\n\n要限定上限，要用 `max-<size>:` 比如 `md:max-xl:flex`\n\n要想自定义 breakpoints，可以看 [customizing breakpoints documentation](https://tailwindcss.com/docs/breakpoints).\n\n也可以单独设定 `min-[320px]:`, `max-[600px]:` 等等\n\n### 夜间模式\n\n默认使用操作系统的设定。\n\n要想手动设定，须在 tailwind.config.js 中加入\n\n```jsx\n/** @type {import('tailwindcss').Config} */\nmodule.exports = {\n  darkMode: 'class',\n  // ...\n}\n```\n\n然后含有 `class=’dark’` 的元素的子元素都时夜间模式的效果\n\n这个[链接](https://tailwindcss.com/docs/dark-mode#supporting-system-preference-and-manual-selection)包含了同时兼容系统设置和手动设置的做法\n\n### 样式重用\n\n- 编辑器的多光标功能：[https://code.visualstudio.com/docs/editor/codebasics#_multiple-selections-multicursor](https://code.visualstudio.com/docs/editor/codebasics#_multiple-selections-multicursor)\n- 标记语言的循环语法\n- react 等框架的 component 概念\n- `@apply` and `@layer` in the [Functions & Directives](https://tailwindcss.com/docs/functions-and-directives#layer) documentation.\n- 避免提前过度抽象\n\n### 添加自定义样式\n\n- 编辑 `tailwind.config.js`, 文档在此：[https://tailwindcss.com/docs/theme](https://tailwindcss.com/docs/theme)\n- [Arbitrary properties](https://tailwindcss.com/docs/adding-custom-styles#arbitrary-properties) 和 [arbitrary variants](https://tailwindcss.com/docs/adding-custom-styles#arbitrary-variants)\n- [Using CSS and @layer](https://tailwindcss.com/docs/adding-custom-styles#using-css-and-layer), 使用多个 CSS 文件时，需在 postcss.config.js 文件中添加 `plugins: {’postcss-import’:  {},}` 字段\n- [Writing plugins](https://tailwindcss.com/docs/adding-custom-styles#writing-plugins)\n\n### 函数和 directives\n\ndirectives 是 CSS 文件中的 `@` 开头的语句\n\n`@layer` 用来把一些需要打包的样式绑在一起，后面三个取值：base, components, utilities\n\n`@apply` 后面接 TailwindCSS 已经定义的类，表示把类的定义移植于此处。\n\n**[`@config`](https://tailwindcss.com/docs/functions-and-directives#config)** 指定所在 CSS 文件需要使用的 TailwindCSS 配置文件，放在 @import 语句后面\n\n```css\n@tailwind base;\n@tailwind components;\n@tailwind utilities;\n\n@layer base {\n  h1 {\n    @apply text-2xl;\n  }\n  h2 {\n    @apply text-xl;\n  }\n}\n\n@layer components {\n  .btn-blue {\n    @apply bg-blue-500 hover:bg-blue-700 text-white font-bold py-2 px-4 rounded;\n  }\n}\n\n@layer utilities {\n  .filter-none {\n    filter: none;\n  }\n  .filter-grayscale {\n    filter: grayscale(100%);\n  }\n}\n```\n\nTailwindCSS 还自定义了一些 CSS 函数：\n\n- `theme()`: 返回 config 文件中的参数，比如\n    \n    ```css\n    .content-area {\n      height: calc(100vh - theme(spacing.12));\n    }\n    ```\n    \n- [`screen()`](https://tailwindcss.com/docs/functions-and-directives#screen): 以预定义的 breakpoint 为参数，避免代码中间出现硬编码的数值\n    \n    ```css\n    @media screen(sm) { /* ... */ }\n    ```\n    \n\n## 在 Next.js 项目中安装 TailwindCSS\n\n> [https://nextjs.org/docs/pages/building-your-application/styling/tailwind-css](https://nextjs.org/docs/pages/building-your-application/styling/tailwind-css)\n> \n\n在命令行\n\n```bash\nnpm install -D tailwindcss postcss autoprefixer\nnpx tailwindcss init -p\n```\n\n如此会在项目的根目录新建 `tailwind.config.js` & `postcss.config.js` 文件\n\n然后编辑 `tailwind.config.js` 文件，添加需要用到 TailwaindCSS 的路径\n\n```jsx\n/** @type {import('tailwindcss').Config} */\nmodule.exports = {\n  content: [\n    './app/**/*.{js,ts,jsx,tsx,mdx}', // Note the addition of the `app` directory.\n    './pages/**/*.{js,ts,jsx,tsx,mdx}',\n    './components/**/*.{js,ts,jsx,tsx,mdx}',\n \n    // Or if using `src` directory:\n    './src/**/*.{js,ts,jsx,tsx,mdx}',\n  ],\n  theme: {\n    extend: {},\n  },\n  plugins: [],\n}\n```\n\n在全局样式表 `styles/globals.css` 中引入 TailwaindCSS\n\n```css\n@tailwind base;\n@tailwind components;\n@tailwind utilities;\n```\n\n在 `pages/_app.js` 中引入全局样式表。`@` 的含义不明\n\n```jsx\n// These styles apply to every route in the application\nimport '@/styles/globals.css'\nimport type { AppProps } from 'next/app'\n \nexport default function App({ Component, pageProps }: AppProps) {\n  return <Component {...pageProps} />\n}\n```\n\n在项目的 components 中使用 TailwindCSS 的类：\n\n```tsx\nexport default function Page() {\n  return <h1 className=\"text-3xl font-bold underline\">Hello, Next.js!</h1>\n}\n```"},{"slug":"afaik-generative-ai","filename":"2023-09-11-afaik-generative-ai.md","date":"2023-09-11","title":"·如是我闻 | 生成式人工智能","layout":"post","keywords":["md","ai","rss"],"excerpt":"","content":"\n## 生成式语言模型\n\n### 模型\n\n- OpenAI/GPT\n- Claude\n- `bloomchat`, 可以商用 [[GitHub](https://github.com/sambanova/bloomchat)]\n- `falcon40B`\n    - apache 2.0 许可证，可商用[[huggingface](http://huggingface.co/tiiuae)]\n    - gpt3 的性能，更少的运算资源，其中Falcon 7B可以跑在苹果Mac上 [[推特](https://twitter.com/rickawsb/status/1666148546285043714)]\n- `TigerBot`: 一款国产自研的多语言任务大模型，70亿参数和1800亿参数两个版本 [[GitHub](https://github.com/TigerResearch/TigerBot)]\n- `QLoRA`: 单个GPU，ChatGPT 99%的能力，消费级GPU微调12个小时就可以达到97%的ChatGPT水平，4B就可以保持16B精度的效果 [[论文](https://www.notion.so/Endocytic-trafficking-promotes-vacuolar-enlargements-for-fast-cell-expansion-rates-in-plants-6b8f0a313c184ccba9fb5a035bb04a0e?pvs=21)] [[GitHub](https://www.notion.so/pdf-14a94950d61c42d3b03bb132f7655589?pvs=21)]\n- `MBT 30B`: 开源商用模型为数不多的选择里出现了一个比Falcon 40B更好的模型 [[Twitter](https://twitter.com/fi56622380/status/1672137540281974784)]\n- `GLM-6B` & `GLM2-6B`: 智谱AI发布，对学术研究完全开放，并且在完成企业登记获得授权后，允许免费商业使用。[[Twitter](https://twitter.com/GanymedeNil/status/1679892021807550465)][[微信公众号@GLM大模型](https://mp.weixin.qq.com/s?__biz=MzkxNjMzMjM3NA==&mid=2247484214&idx=1&sn=e42153f987a74d1ffc7882f7cc09670d)]\n- `Llama 2`: Meta开源大语言模型Llama 2，可免费商用. [[微信](https://mp.weixin.qq.com/s/9pcmrCEyp2AQsL3MbPYx-Q?utm_source=pocket_saves)介绍]\n    - Jim Fan 评论 [[推特，翻译](https://twitter.com/dotey/status/1681553916373135362?utm_source=pocket_saves)]\n    - 很多团队几乎都达成共识， RLHF 不重要，SFT 就够了。现在 Llama2 的论文说 RLHF 非常非常重要。[[推特](https://twitter.com/oran_ge/status/1681793774685659136?utm_source=pocket_saves)]\n    - `LLaMA-2-7B-32K`, context为32K的模型 [[推特](https://twitter.com/JefferyTatsuya/status/1685423475979325440)][[Twitter](https://twitter.com/togethercompute/status/1685048832168714240)]\n\n### 基于模型，直接可用的产品\n\n- OpenAI/GPT\n    - ChatGPT\n        - 2023年6月13日，GPT提供了函数调用，让ChatGPT来自己调用函数。[[Twitter](https://twitter.com/cryptonerdcn/status/1668733300070924288)][[OpenAI](https://openai.com/blog/function-calling-and-other-api-updates)][[用法 Twitter@宝玉](https://twitter.com/dotey/status/1668728109376450566)]\n    - ChatGPT - Code Interpreter\n        - 介绍 [[推特](https://twitter.com/fuyufjh/status/1684191835210809344)][[YouTube](https://www.youtube.com/watch?v=4wGlRrir_u4)]\n        - 《ChatGPT 探索：Code Interpreter 高级指南》[[微信@浮之静](https://mp.weixin.qq.com/s/K_csi1oWDv5tEaeeKSlvwA?utm_source=pocket_saves)]\n        - 源码可能被套出。[[Twitter](https://twitter.com/fuergaosi/status/1679457847237820416)]\n        - 对 code interpreter 的逆向工程 [[Twitter](https://twitter.com/Yampeleg/status/1678045605527003136)][[Mem](https://mem.ai/p/xyy8ULiAce1BecTxnU0M)]\n    - OpenAI API\n        - 2023年8月23日，OpenAI 开放了 GPT-3.5 的微调的API [[推特](https://twitter.com/dotey/status/1694207797351616703)]\n    - OpenAI on Azure 内置了一个内容过滤器 [[推特1](https://twitter.com/jw1dev/status/1666613728106938368)][[推特2](https://twitter.com/jw1dev/status/1666622878962548740)]\n    - `forefront`: 完全免费 GPT-4 的工具 [[登录](https://accounts.forefront.ai/)]，大概基于 `gptfree-ts` [[GitHub](https://github.com/xiangsx/gpt4free-ts)]\n    - `BratGPT`: ChatGPT的激进版本。[[官网](https://bratgpt.com/)]\n    - `SmartStudy`: 提供文本文档，创建10个问题的小测验。[[官网](https://smartstudy.streamlit.app/)]\n    - `XrayGPT`: 通过给定的 X 光片来促进围绕胸部 X 光片的自动化分析的研究。[[GitHub](https://twitter.com/CarsonYangk8s/status/1661588037892198401)]\n    - `FinGPT`: 类似BloomBerg的开源方案，RLHF 和 Lora 的低秩技术 [[Twitter](https://twitter.com/JefferyTatsuya/status/1668433680887615488)]\n- 微软\n    - BingAI\n        - 本地部署方案 [[推特](https://twitter.com/geekbb/status/1665692703055552513)][[GitHub](https://github.com/adams549659584/go-proxy-bingai)]\n    - VsCode Copilot\n    - Office 365 Copilot: 每月每名用户30美元. [[verge](https://www.theverge.com/2023/7/18/23798627/microsoft-365-copilot-price-commercial-enterprise)][[微信](https://mp.weixin.qq.com/s/9pcmrCEyp2AQsL3MbPYx-Q?utm_source=pocket_saves)]\n- Claude+\n    - 例子：阅读多份行业报告 [[推特](https://twitter.com/iamshaynez/status/1684398211958730753)]\n- `Llama`\n    - llama2.ai: 一个基于 llama 2 的聊天机器人，非官方。[[网站](https://llama2.ai/)]\n    - WizardCoder 34B based on Code Llama 写代码 [[推特](https://twitter.com/dotey/status/1696202647269785875)]\n- WebGLM: 清华开源的带网络搜索功能的 GLM 实现 [[GitHub](https://www.notion.so/pdf-14a94950d61c42d3b03bb132f7655589?pvs=21)]\n- mendable: 根据开发文档进行问答 [[官网](https://www.mendable.ai/usecases/documentation)]\n- 阅读 PDF 文档\n    - Humata.ai\n    - explainpaper\n    - ChatPDF\n    - [[对比](https://twitter.com/oran_ge/status/1683432444169711616?utm_source=pocket_saves)] Claude2支持超长上下文，摘要信息量更大，更适合长文提炼。ChatDOC 具有页码溯源、表格解析、原文定位功能，数据找得准，也方便二次验证，能够限制大语言的幻觉问题。\n- Obsidian-copliot: 快速获取文字的核心观点\n- 视频内容梗概\n    - Glarity: 浏览器插件，基于ChatGPT和字幕生成Youtube摘要，20秒看完梗概 [[Twitter](https://twitter.com/starzqeth/status/1640867876109422595)]\n    - summarize-tech: 5分钟了解长视频的要点. [[Twitter](https://twitter.com/starzqeth/status/1640867876109422595)]\n- webpilot: 可联网可读网页链接的插件 Webpilot 推出的 Chrome 版插件 [[chrome](https://chrome.google.com/webstore/detail/webpilot-copilot-for-all/biaggnjibplcfekllonekbonhfgchopo?utm_source=link)]\n\n### 模型教程、评论、二次开发\n\n- 一般性原理\n    - 《Prompt 编写模式》[[phodal](https://prompt-patterns.phodal.com)]\n    - 《LLM+Embedding构建问答系统的局限性及优化方案》[[知乎](https://zhuanlan.zhihu.com/p/641132245)]\n    - 基于检索的 LM，外挂一个数据库用来检索。[[推特](https://twitter.com/cosmtrek/status/1678077835418955781)][[GitHub.io](https://acl2023-retrieval-lm.github.io/)]\n    - 一篇泼冷水的论文 [[ACL Anthology](https://aclanthology.org/2023.findings-acl.426/)]\n    - 即刻出的Prompt调试工具。[[Twitter](https://twitter.com/vista8/status/1678784460786135040)][[官网](https://promptknit.com/)]\n- GPT\n    - GPT best practice [[OpenAI](https://platform.openai.com/docs/guides/gpt-best-practices?utm_source=pocket_saves)]\n    - Andrew Ng 吴恩达 & Isa Fulford from OpenAI 《Build system with [#ChatGPT](https://twitter.com/hashtag/ChatGPT?src=hashtag_click) API》[推特@**[金田達也](https://twitter.com/JefferyTatsuya)**]\n        - 借助 CoT 的思路，翻译字幕，返回正确的 JSON 格式 [[推特](https://twitter.com/dotey/status/1665476562219573249)]\n        - 同样的加入了CoT（Chain of Though）的Prompt，如果让GPT打印出来步骤，效果非常好，但是如果不让GPT打印（省点token，以及更容易解析），那么GPT就会偷懒 [[Twitter](https://twitter.com/dotey/status/1668736426286915590)1][[Twitter2](https://twitter.com/dotey/status/1664335473500626946)]\n    - 熊猫吃短信是 Twitter@威力狈 开发的垃圾短信过滤工具。将其与 GPT 结合的一些讨论\n        - [Twitter@威力狈](https://twitter.com/waylybaye/status/1664253928970788864)：尝试了下用 ChatGPT 自动标注数据，效果太差了。\n        - [Twitter@宝玉](https://twitter.com/dotey/status/1669028955842650139)：通常如果我写的话，会做一些小调整\n        - [Twitter@IIInoki](https://twitter.com/IIInoki)：是的，感觉八爷用 API 用得有点糙……就只是很简单的 prompt 达到的效果都还不错\n    - 《ChatGPT 越过山丘之后，再来谈谈 LLM 应用方向》[[橘子汽水铺](https://quail.ink/orange/p/chatgpt-cross-over-the-hills-and-discuss-llm-application-directions)]\n- `LangChain`:\n    - 官方教程 [[推特](https://twitter.com/LangChainAI/status/1665009694627250176)][[streamlit](https://blog.streamlit.io/langchain-tutorial-1-build-an-llm-powered-app-in-18-lines-of-code/)]\n    - 一个使用 LangChain 和 GPT Index 的教程 [[leanpub, 收费](https://leanpub.com/langchain)][[Pocket](https://getpocket.com/read/3839490971)]\n    - LangChain for LLM Application Development 基于LangChain的大语言模型应用开发 [[YouTube](https://t.co/JXV1SBI2OA)]\n        - 基于Embedding的文档问答。stuff, map reduce, refine, map rerank [[Twitter@宝玉](https://twitter.com/dotey/status/1667790801420558342)]\n    - Chanin Nantasenamat: LangChain tutorial #1: Build an LLM-powered app in 18 lines of code [[streamlit](https://blog.streamlit.io/langchain-tutorial-1-build-an-llm-powered-app-in-18-lines-of-code/?utm_source=pocket_saves)]\n    - 把一篇很长的 PDF 内容喂给 ChatGPT，然后向他提问\n        - 纯 JS 开源工具推荐 [[推特](https://twitter.com/Barret_China/status/1638119945749037056)]\n        - 用 `LangChain` 六七行代码就可以搞定了 [[LangChain](https://js.langchain.com/docs/get_started/introduction)]\n- `AutoChain`\n    - 介绍 [[推特](https://twitter.com/zhangjintao9020/status/1683996172980199429)][[GitHub](https://github.com/Forethought-Technologies/AutoChain)]\n    - 《我为什么放弃了 LangChain》[[推特](https://twitter.com/Barret_China/status/1683135367862718465)][[微信](https://mp.weixin.qq.com/s/jIbz9JYc8-_ua-QLENX__A)] 推友提出的 AutoChain 替代方案 [[推特](https://twitter.com/Barret_China/status/1684211570186887170?utm_source=pocket_saves)]\n- `OpenDAN`: 为各类 AI 模块提供运行环境，并提供它们之间的互操作性协议。可创建诸如律师、医生、教师，甚至男女朋友等角色 [[GitHub](https://twitter.com/Barret_China/status/1666455683758161920)]\n- “视频语音↔文字”任务相关\n    - 指定视频URL，识别文字，翻译 [[GitHub](https://github.com/lewangdev/autotranslate)]\n    - `WhisperX`: 按照单词对齐时间戳，生成的字幕都是完整的句子 [[GitHub](https://github.com/m-bain/whisperX)]。[[Twitter@宝玉](https://twitter.com/dotey/status/1667394662628204546)] 写了一个可以根据 YouTube Url 识别 YouTube 字幕的 [Jupyter Notebook](https://github.com/JimLiu/whisper-subtitles/blob/main/whisperx_youtube_subtitle.ipynb)\n    - `audiocraft`: audio processing and generation with deep learning. [[GitHub](https://github.com/facebookresearch/audiocraft)]\n    - [[推特]](https://twitter.com/Barret_China/status/1684218981639413760) 小作文\n    - `yt-dlp` 一行命令下载视频字幕的工具，不需 puppeteer 无头浏览器 [[推特](https://twitter.com/Barret_China/status/1684228477644570624)][[GitHub](https://github.com/yt-dlp/yt-dlp)]\n- ChatGPT + AI agent + ScholarAI + Noteable 写的小综述 [[链接失效](https://t.co/eqVc2LIfSz)]\n- `MusicGen`: 将文本和旋律转化为完整乐曲 [[Twitter](https://twitter.com/Fenng/status/1668141100610248705)][[ReadHub](https://www.notion.so/3753e42dc4204a99ab83a725b655a632?pvs=21)]\n- `MMS`: 一个声音模型 [[HuggingFace](https://huggingface.co/docs/transformers/main/en/model_doc/mms)]\n- `FRVR Forge`: AI-Powered End-to-End Game Creation [[Twitter](https://twitter.com/FRVRGames/status/1669758477789540365)][[官网](https://www.notion.so/ai-University-Cloud-8078b4682e454a5fba982f67e4530498?pvs=21)]\n\n### 开发平台\n\nRunpod: 租用 GPU 跑模型并创建 Serverless API 一站式服务，最低只要0.2刀/hr。[[官网](http://runpod.io)]\n\n### 杂项\n\n- 2023年5月27日、28日，OpenAI 使用 Sentry 审计工具封禁来自中国的用户，解决方案：\n    - 路由器 Clash 规则 [[推特](https://twitter.com/wey_gu/status/1663003950214438912?utm_source=pocket_saves)]\n    - 改用 Azure OpenAI service [[推特](https://twitter.com/zhangjintao9020/status/1662865819041402880)]\n    - Cloudflare WARP [[左耳朵](https://haoel.github.io/)]\n\n## 生成式图像模型\n\n2023年5月31日，Adobe 添加人工智能相关功能 generative fill。[[推特](https://twitter.com/CodeByPoonam/status/1663824055164887040)]\n\n- 配置要求极低，连Win掌机都能跑，但是不能断网。[[推特](https://twitter.com/OfflineHelper/status/1666042746866663424)]\n- 填充将横屏的视频转换为竖屏。[[推特](https://twitter.com/Alex_Cerrato/status/1681677307843432449)]\n\n`MidJourney`\n\n- 在提示词中添加相机镜头信息。[[推特](https://twitter.com/4rtofficial/status/1663310457854099458)]\n- zoom [[Twitter](https://twitter.com/jesselaunz/status/1674210886695923712)]\n\n`StableDiffusion`\n\n- Eric Fu: 训练指南. [[Coding Husky](https://ericfu.me/stable-diffusion-finetune-guide/?utm_source=pocket_reader)]\n- 文字或者符号融合生成图片 [[Twitter](https://twitter.com/op7418/status/1680223090138316800)][[微信](https://mp.weixin.qq.com/s/rvpU4XhToldoec_bABeXJw)]\n\n`StyleDrop`: Google 基于 MUSE 的样式迁移 transformer [[推特](https://twitter.com/recatm/status/1665056017107886080)][[GitHub.io](https://styledrop.github.io/)]\n\n`Redream`: 从视频到二次元动画 [[推特](https://twitter.com/heyBarsee/status/1665034805384290307)][[GitHub](https://github.com/Fictiverse/Redream)]\n\n`Runway` Gen-2: 文本生成视频和图片生成视频, 4 秒钟 [[推特](https://twitter.com/op7418/status/1666461595818504192)][[需注册](https://app.runwayml.com/login)]\n\n一个 AI 视频解决方案，来自南洋理工，代码尚未开源 [[Twitter](https://twitter.com/op7418/status/1669026494885285888)] [[GitHub.io](https://anonymous-31415926.github.io/)][[Twitter2](https://twitter.com/rickawsb/status/1672310994390126593)][[arxiv](https://arxiv.org/abs/2306.07954)]\n\n`AWPortrait1.1`: 图像生成 [[Twitter](https://twitter.com/dynamicwangs/status/1673730591462928385)][[LibLibai](https://www.liblibai.com/modelinfo/721fa2d298b262d7c08f0337ebfe58f8)] \n\n`Anything AI`: 可以取代照片中的任何物体。免费，不需要注册. [[官网](https://www.anything-ai.com/)]\n\n`PixelLab`: 草图创建2D图像. [[官网](https://www.pixellab.ai/)]\n"},{"slug":"what-is-intelligence-not-same-as-intelligence-is-what","filename":"2023-06-17-what-is-intelligence-not-same-as-intelligence-is-what.md","date":"2023-06-17","title":".tex | 什么是智能≠智能是什么","layout":"post","keywords":["tex","doc","md","ai"],"hasMath":true,"excerpt":"“什么是智能”的问题每每得不到回答，是因为它的逆问题“智能是什么”没有答案。","content":"\n## 0\n\n这是一篇酬和之作。\n\n徵文标题说的是：\n\n> **機器會製造「內涵」嗎？**\n> \n\n但是正文提出的问题是：\n\n> AI透過程式組合出回答你問題的文字組合，有「涵義」嗎？\n> \n\n可是，「內涵」和「涵義」两个词的——内涵/涵义——就不完全一样啊……\n\n“内涵”(connotation) 通常指词语或表达方式所隐含的情感、态度、暗示或附加的意义。它涉及到词语或表达方式所引起的情感、联想或隐含的观点。也就是弦外之音。\n\n而“涵义”(meaning) 一般指词语、表达方式或行为所传达的字面意义或字面上的定义。它强调的是直接的、明确的意义。\n\n看热闹不嫌事大，那我们再把问题搞复杂一点——在逻辑学里也有一个“内涵”(intension)，和“外延”(extension) 相对应。用**面向对象编程**的说法来理解，一个类里面定义的所有状态量和内部方法的集合，就构成这个类的“**内涵**”；所有（已经和将来能够）从这个类实例化出来的对象的集合，就构成这个类的“**外延**”。\n\n所以看起来，征文者想问的是日常“内涵”也就是言外之意，但是怕杠精（比如我）用有严格定义的逻辑“内涵”解构掉，所以换了“涵义”一词。\n\n这个问题很显然是因应最近大语言模型掀起的这一波 AI 浪潮。这个问题往前再问一句，就是“大语言模型是智能体/有智能吗？”\n\n- 前两年 DeepMind 的 AlphaGo/AlphaZero 系列 AI 在围棋中击败人类棋手时，人们也在问这个问题。\n- 上世纪四五十年代专家系统 (expert system) 刚刚开发的时候，人们也在问这个问题。\n- 从电子计算机往前追溯到机械计算机，甚至是巴比奇的差分机的时候，人们就已经开始问这样的问题了。\n\n这些问题求并集，然后在问题数量趋近于无穷下的极限，就是“什么是智能”。\n\n这样的问题每每得不到回答，是因为它的逆问题“智能是什么”没有答案。我们并没有智能的准确定义，只能一事一论。而之前的智能和非智能体的区别太明显，以至于作出判断也不能对智能的定义有所启发。\n\n## 1\n\n而对“智能是什么”的探究，哲学、逻辑学、计算机科学、生物学、管理学，不同领域的研究者有着不同的思路。\n\n### 古哲学·洞穴之壁与理念世界\n\n古希腊哲学家柏拉图在《理想国》里提到了“洞穴之壁”的寓言故事。\n\n有一群被囚禁在一个深洞的囚徒，从出生开始就被束缚在这个洞穴里，脖子和腿都被铁链锁住，没办法转身或离开。囚徒身后的洞穴入口处有一道火焰，火焰后有人持物体走过，物体的投射在洞穴内的墙壁上形成了影子。囚徒们就以为这些影子就是唯一的存在。\n\n这里的囚徒代表着人类，洞穴代表着世界，影子则代表着我们对于现象世界的感知和观念。人们的知识和信念往往受限于自己的经验和感知，就像囚徒们只看到了洞穴墙壁上的影子，而在影子之外还存在一个理想的理性世界。柏拉图用这个寓言故事表达了他对于人类认识和智慧的理解。所谓智慧，就是从洞穴的影子反过去推测火把前物体的能力。\n\n当然，这种思想被 Marx 主义定性为一种客观唯心主义、唯理论，是受其批判的。\n\n### 逻辑学·从命题到希尔伯特算符\n\n柏拉图的学生亚里士多德，今天在低年级的物理教科书里基本是个反面典型，但他对逻辑学进行了系统化和全面的研究，提出了许多逻辑学的基本概念和原理。这些成果后来成为了欧洲哲学和逻辑学的基石，对西方哲学和科学的发展产生了深远影响。\n\n所谓逻辑，就是研究命题的对错，以及如何判断命题对错的学问。而命题，就是能被判断对错的句子。但是句子显然可以再分成不同成分，于是就发明/发现了主体、客体、谓词、谓词的量词……等等概念，以及用这些概念构造命题的方法。\n\n但是要注意，虽然逻辑主要由语言来表达，但是逻辑还是和语言不同，主体、客体也不等于句子的主语、宾语。这两者的区别，基本可以类比于之前洞穴之壁寓言里的实体和影子。\n\n这种努力到目前为止的巅峰，基本上要数希尔伯特形式化逻辑系统了。感兴趣的朋友可以自行查阅戈得门特《代数学教程》的第一章，这玩意相当于思想界的引体向上，反正我是一个也拉不上去……\n\n### 计算机·从半导体到抽象语法树\n\n希尔伯特是德国的数学家，《代数学教程》也是数学而不是哲学教材。显而易见，逻辑虽然由哲学家奠基，但是主导权很快落到数学家，至少是哲学家兼数学家手里了。\n\n命题的“真”与“非真”同构于 {1, 0}，各种逻辑运算都可以分解成“或”与“非”两种基本逻辑运算的组合，这就是以数学家乔治·布尔 (George Boole) 命名的布尔代数。因为 {1, 0} 又可以同构于半导体电路的高低电位，和各种类似继电器的门电路组合，所以很容易用计算机在物理世界表示出这些逻辑运算。\n\n我们的电脑由上亿个这样的电位和逻辑门组成，一般的科普文章应该会去介绍芯片啊光刻机之类的东西，本文关注的是另一个方面：虽然生产电脑配件的厂商很多，不同的型号的元器件设计不同，组装出的成品应该千差万别，但是他们可以运行同样的程序，理想条件下（虽然实际工程中常常不理想）我们也可以期望他们跑出同样的结果。\n\n这说明所谓计算机科学，并不等同于研究计算机元件的电子科学和工程，这里电科和电子工程相当于洞穴岩壁上的影子，而计算机科学就相当于火光前的物体。这种超越物理的计算本质，一般用一种叫做“抽象语法树”的数据结构来表示。\n\n### 生物学·从神经元到神经网络\n\n人们发明计算机的时候，基本上还是把它当作工具，就没期望它有什么主体性和智慧。\n\n而随着生物学逐渐发现了神经系统及其作用，也随着物理学在二十世纪初的大发展之后的相对平静，很多物理学家开始插手其他学科。既然生命和非生命体的背后都服从同一套物理规律，既然物理学的众多成功经验说明，搞清楚构成系统的所有微观组成就可以理解宏观的系统，那么搞清楚人类的智力器官的基本单元以及相互作用，按理说也就能够理解什么是智慧。\n\n![a cartoon illustrating a neuron](/photos/2023-06-17-neuron.png)\n\n上图是一个神经细胞的结构示意图。从其他神经细胞释放出来的名为神经递质的化学物质，到达神经元左侧短且密集的树突之后，激活细胞膜表面的离子泵，主动运输离子跨过细胞膜，从而产生电信号。电信号沿细胞膜传导到右侧的树突，刺激凸触释放神经递质给下一个细胞。\n\n![a handdrawing style illustration of perceptron](/photos/2023-06-17-perceptron.png)\n\n上图就是根据神经元的工作原理抽象出的数学模型，名为 perceptron。一个 perceptron 就是一个函数，接受多个输入的自变量，加权求和之后套一个非线性的激活函数，得到一个输出。很多个这样的 perceptron 并连和串联，就构成下图，计算机算法中的神经网络。\n\n![a handdrawing style illustration of a neural network](/photos/2023-06-17-neural-network.png)\n\n而从实验方向研究神经系统，我们隔壁系就有，经常来我们系招人。基本上就是在小鼠的天灵盖上锯开一个天窗，然后给它带上个头盔，头盔上有能从天窗伸进去的电极，采集脑神经的电信号。以前头盔有网线伸到实验室天花板，实时传到数据中心的超算。现在好像进步了，改用 Wi-Fi 了。\n\n这实验怎么通过的伦理审查，咱也不知道，咱也不敢问……\n\n### 管理学·DIKW “数据-信息-知识-智慧”模型\n\n![a pyramid of DIKW model](/photos/2023-06-17-DIKW.png)\n\nDIKW 四个字母分别代表 data, information, knowledge, wisdom，即数据、信息、知识、智慧，是一种知识管理中的心智模型。\n\n四个层次，前一层都是后一层的基础，后一层都是对前一层的理解。\n\n如果是书面文字，数据就是笔画和字母；如果是语言，数据就是人声的响度、频率和音色。由笔画/字母/声音组成的有含义的字词就是信息。表示信息之间的关系的，可以判断对错的命题就是知识。包含和统摄各条知识的思想体系，就是智慧。\n\n反过来说，虽然智慧高于思想，但它仍需要通过把各条知识的表达汇总起来，才能被人感知。对知识的命题的理解依赖于构成名字的各个概念的涵义，属于信息水平的内容。而每个字都有不考虑其涵义的笔画字母构成。\n\n这层与层之间**看似**并没有插入额外的内容，智慧可以直接由笔画构成。但是我们一层层理解的深入，其实是不自觉地借用了我们当前社会约定俗成的解读方式。\n\n比如下面这个图片里的符号，对于现代人就只是数据，无法解读成信息。但是对于苏美尔人，这是用楔形文字表示的数字，是等腰直角三角形的腰和直角边的比值，也就是 $$\\sqrt{2}$$ 的近似值。\n\n![sumerian numerical approximation to square root of two](/photos/2023-06-17-ancient-root-2.png)\n\n约定俗成的数据解读方式，也就是关于**数据的数据**，根据西方的构词法，可以叫做“**元**数据”(meta-data)。\n\n数据和元数据一起构成信息，信息和元信息一起构成知识，知识和元知识一起构成智慧。俺坚持写博客的动机，就是用费曼学习法，把无意间使用的元知识显式地表达出来，而且记录下来，争取学而不退转。\n\n## 2\n\n回顾了这些，再来看大语言模型，就会发现它落在了各方努力的延长线的交点。\n\n大语言模型里有一个重要概念叫做“嵌入”(embedding)，就是把语言的基本字元 (token) 可逆地映射到一个超多维度的向量空间里。本来“国王”和“儿子”之间没办法加减乘除，但是嵌入后的向量空间里有加法和数乘，如果嵌入函数选得好，“国王”的向量 + “儿子”的向量，结果向量就约等于“王子”的向量。\n\n![illustration of vector addition from wikipedia](/photos/2023-06-17-vector-addition.png)\n\n生成式语言模型的核心就是一个超多元函数，接受前一个字嵌入后的向量作为输入，给出另一个向量作为输出，用嵌入函数的逆映射翻译成字元；再把旧的输出作为新的输入，直到输出结果是“语段结束”这样一个特殊字元为止。模型训练的过程，主要就是通过现成的语料，拟合这个超多元函数的参数。\n\n从 DIKW 模型来看，语言模型操作的是最基本的数据，它的输出究竟是什么信息，是不是正确的知识，体现了多少智慧，是人根据当下的社会文化来解读的。\n\n而实现 AI 的电子计算机，或是复杂生命的大脑，他们和智能之间的关系，应该就类似于具体的计算机电路和抽象语法树之间的关系。以此类比，未来的智能科学应该会成为一门独立的专业，它和计算机科学和神经生物学的区别，就像今天的电子科学与工程，和计算机科学之间的区别一样。当下神经生物学的热度，将来恐怕多半会被分流。\n\n这种对字符的计算不同于逻辑运算，语言模型不判断输出结果在逻辑上的正确与错误，这既给了他啥都能说几句的 feature，又给了它经常编假消息的 bug。\n\n想要改掉这种错误，引入对 AI 的纠错机制，治本之道恐怕还是诉诸于对世界的正确描述，与理论相关的还是要靠逻辑，与现实相关的还是要靠科学。\n\n只不过，大语言模型提供了一种数据结构，有希望把人类已知的真理储存在一起。对这种数据结构本身的研究，有可能反过来启发科学的发展。柏拉图的洞穴之壁可能不再是一个比喻，未来更大的语言模型的，亿万维度的参数空间有希望成为洞穴门口的那团火。\n\n只不过这一切都是“可能”，现在还只是 AI 的萌芽阶段，还没有足够的证据来证实或者证伪这种畅想。而且 AI 的参数量再大也是有限的，它所能表达的信息也就有限，而真理应当是无限的，就像科学一样，总要训练更新更大的模型，总要发现已知的未知，然后欣然接受更多未知的未知之存在。\n\n如果电子计算机实现的 AI 独立于人类产生了意识和超出人类的智慧，很难想象他们会继续用人类语言这种对他们来说很不方便的方式来交流。\n\n所以，哪怕是做个 AI 生成内容的质检员，科学家依然有事可做。这算是科学的堕落吗？当然不算，如果算的话，那从计算物理也被当作理论物理的那天起，人类就已经投降了（逃）\n\n## 3\n\n现在正面来回答问题：AI透過程式組合出回答你問題的文字組合，有「涵義」嗎？\n\n答：有。\n\n因为语言的「涵義」来自于语言的内容，和整个社会的文化，并不来自于这句话的作者的身份。即便是人与人之间的交流，诉诸身份也是一种非形式逻辑谬误，是理性不足的表现。只有在信息不足仍不得不下结论的时候才该使用，比如法律判决时的自由心证主义和/或法定证据主义。\n\n而鹿妈眼里真人鹿酱与 AI 鹿酱的区别，如果有的话，好像主要体现在动机的区别。动机这种东西，很多智慧不高的生物，比如小猫小狗都会有；而现在的 AI，似乎还没有展现出超出编程者设计的动机。编程写入的信息有限，现有 AI 的动机也就有限，鹿酱的赢面还是很大的。\n\n而动机是生物与非生物的区别吗？而什么是生物 ≠ 生物是什么，那就是另一个含混而复杂的问题了。\n\n## 4\n\n这篇博文发布的时候，高考应该已经结束了，马上该填报志愿了。\n\n那么，西元 2023 年，AI 来袭的当下，该选个啥专业在 AI 浪潮中幸存，或者选个啥专业给 AI 老爷带路呢？\n\n![a screenshot of a quotation from Three Body about attitudes towards aliens](/photos/2023-06-17-three-body-quotation.png)\n\n我的建议是，不要听别人的建议，按自己的兴趣来就好了。\n\n刚刚改开的时候，有一个超级热门的专业，叫科技英语。科技落下了好多年，对外开放需要语言交流，两者一结合应该是热门又稀缺了。结果呢，你现在还听说过这个专业吗？\n\n科技很重要是不错，语言很重要也不错，但是搞科技的人自己可以学英语，学英语的有几个搞得了科技？社会的进步主要靠创新，而创新的方向难以预测，不论这种预测分析听起来多有道理。\n\n如果真的找不到兴趣，那就在能力范围之内，找个难度最高的。如果想从事智力劳动，那数学含量是个不错的衡量标准；如果不排斥体力劳动，那训练时间越长越值得考虑。\n\n但这只是填志愿来不及时的权宜之计，发掘兴趣是人一生的课题。\n\n兴趣不是为了让你成功的时候更得意，毕竟成功的话不论做什么都很得意；\n\n兴趣是为了你不成功时也可以不失意，毕竟平凡才是人生的真谛。\n"},{"slug":"physics-based-neural-network-review-note","filename":"2023-03-20-physics-based-neural-network-review-note.md","date":"2023-03-20","title":".tex | 基于物理的神经网络 (PINN) 综述笔记","layout":"post","keywords":["tex","phy","md","ai"],"hasMath":true,"excerpt":"本文是《Scientific Machine Learning Through Physics–Informed Neural Networks: Where we are and What’s Next》这篇综述的读书笔记。","content":"\n> 本文是《[Scientific Machine Learning Through Physics–Informed Neural Networks: Where we are and What’s Next](https://link.springer.com/article/10.1007/s10915-022-01939-z)》这篇综述的读书笔记。\n> \n\n年前，今年新入职的天文学方面的一位老师给我们群发邮件，宣传某国家实验室超算的 GPU 编程马拉松活动，他可以担任指导老师。于是毫不意外地，我报了名。该编程马拉松项目还需要专门申请，申请材料里要写清楚打算干什么，于是报名的五六个人七嘴八舌地想创意。基于物理的神经网络 PINN 就是天文老师的点子。\n\n~~写到这里，我才意识到，老哥是不是想拿我们当免费劳动力啊~~~\n\n神经网络可以看作是一个复杂的非线性函数，接受一个（一般来说维度很高的）向量作为输入，一番计算后输出另一个向量。训练神经网络，就是找到这个函数的参数，绝大多数找参数的方法涉及计算网络输出对参数的偏导数，因此神经网络计算框架的核心功能就是自动微分 (auto-differentiation)。\n\n而很多物理问题，都可以用（偏）微分方程来描述，微分方程的解不是变量，而是函数，而且往往是复杂的非线性函数。所以基于物理的神经网络 (PINN) 就是以神经网络来表达这个函数，然后把这个函数带入到物理的微分方程中，把神经网络输出和真正的物理解之间的差距当作损失函数，反向传播回去来优化神经网络的参数。代入方程时的微分计算，正好可以利用现成框架的自动微分功能。\n\n在以 GPT 为代表的 transformer 类神经网络模型出现之前，自然语言处理类的机器学习项目，往往要在网络之外，利用人类的语法知识，对语段进行语义分割等等“中间任务”。Transformer 一出，算力出奇迹，中间任务逐渐变得没有必要了。\n\n在 GPT 崭露头角，并且越来越有迹象表明其将会涌现出通用人工智能的今天，这些基于物理的神经网络，会不会还未成熟就已过时？这种心情，就和《三体》第二卷开始，章北海和吴岳面对焊渍未漆的“唐”号航空母舰时差不多吧……\n\n<hr class=\"slender\">\n\n- Abstract\n    - PINNs are neural networks that encode model equations. a NN must fit observed data while reducing a PDE residual.\n\n1. Introduction\n    - The “curse of dimensionality” was first described by Bellman in the context of optimal control problems. (Bellman R.: Dynamic Programming. Sci. 153(3731), 34-37 (1966))\n    - Early work: MLP ([multilayer perceptron](https://en.wikipedia.org/wiki/Multilayer_perceptron)) with few hidden layers to solve PDEs. ([https://doi.org/10.1109/72.712178](https://doi.org/10.1109/72.712178))\n    - 感觉可能更全面的一篇综述：[https://doi.org/10.1007/s12206-021-0342-5](https://doi.org/10.1007/s12206-021-0342-5)。该文关注 what deep NN is used, how physical knowledge is represented, how physical information is integrated，本文只关于 PINN, a 2017 framework。\n\n    1. What the PINNs are\n        - PINNs solve problems involving PDEs:\n            - approximates PDE solutions by training a NN to minimize a loss function\n            - includes terms reflecting the initial and boundary conditions\n            - and PDE residual at selected points in the domain (called **collocation points**)\n            - given an input point in the integration domain, returns an estimated solution at that point.\n            - incorporates a [residual network](https://en.wikipedia.org/wiki/Residual_neural_network) that encodes the governing physical equations\n            - can be thought of as an **unsupervised strategy** when they are trained solely with physical equations in forward problems, but **supervised learning** when some properties are derived from data\n        - Advantages:\n            - [mesh-free](https://en.wikipedia.org/wiki/Meshfree_methods)? 但是我们给模型喂训练数据的时候往往已经暗含了 mesh 了吧\n            - on-demand computation after training\n            - forward and inverse problem using the same optimization, with minimal modification\n    2. What this Review is About\n        - 提到了一个做综述找文章的方法：本文涉及的文章可以在 Scopus 上进行高级搜索：`((physic* OR physical)) W/2 (informed OR constrained) W/2 “neural network”)`\n2. The Building Blocks of a PINN\n    - question:\n    \n    $$\n    F(u(z);\\gamma)=f(z),\\quad z\\ \\in\\ \\Omega \\\\ B(u(z))=g(z), \\quad z\\ \\in\\ \\partial \\Omega\n    $$\n    \n    - solution:\n    \n    $$\n    \\hat u_{\\theta}(z)\\approx u(z)\\\\ \\theta^* = \\arg\\min_{\\theta}\\left(\\omega_F L_F(\\theta)+\\omega_BL_B(\\theta)+\\omega_{data}L_{data}(\\theta)\\right)\n    $$\n    \n    1. Neural Network Architecture\n        - DNN (deep neural network) is an artificial neural network that is deeper than 2 layers.\n        \n        1. Feed-Forward Neural Network: \n            - $$u_{\\theta}(x) = C_{K} \\circ C_{k-1} ...\\alpha \\circ C_1(x),\\quad C_k(x) = W_k x_k + b_k$$\n            - Just change CNN from convolution to fully connected.\n            - Also known as multi-layer perceptrons (MLP)\n            \n            1. FFNN architectures \n                - Tartakovsky et al used 3 hidden layers, 50 units per layer,  and a hyperbolic tangent activation function. Other people use different numbers but of the same order of magnitude.\n                - A comparison paper: *Blechschmidt, J., Ernst, O.G.: Three ways to solve partial differential equations with neural networks –A review. GAMM-Mitteilungen 44(2), e202100,006 (2021).*\n            2. multiple FFNN: 2 phase [Stephan problem](https://en.wikipedia.org/wiki/Stefan_problem).\n            3. shallow networks: for training costs\n            4. activation function: the swish function in the paper has a learnable parameter, so — [how to add a learnable parameter in PyTorch](https://discuss.pytorch.org/t/how-could-i-create-a-module-with-learnable-parameters/28115)\n        2. Convolutional Neural Networks: \n            - I am most familiar with this one.\n            - $$f_i(x_i;W_i)=\\Phi_i(\\alpha_i(C_i(W_i,x_i)))$$\n            - performs well with multidimensional data such as images and speeches\n            \n            1. CNN architectures: \n                - `PhyGeoNet`: a physics-informed geometry-adaptive convolutional neural network. It uses a coordinate transformation to convert solution fields from irregular physical domains to rectangular reference domains.\n                - According to Fang ([https://doi.org/10.1109/TNNLS.2021.3070878](https://doi.org/10.1109/TNNLS.2021.3070878)), a Laplacian operator can be discretized using the finite volume approach, and the procedures are equivalent to convolution. Padding data can serve as boundary conditions.\n            2. convolutional encoder-decoder network\n        3. Recurrent Neural Network\n            - $$f_i(h_{i-1})=\\alpha\\left(W\\cdot h_{i-1}+U\\cdot x_i+b\\right)$$, where f is the layer-wise function, x is the input, h is the hidden vector state, W is a hidden-to-hidden weight matrix, U is an input-to-hidden matrix and b is a bias vector. 我认为等号左边的 $$h_{i-1}$$ 应当作为下标\n            - 感觉有点像 hidden Markov model，只不过 Markov 中间的 hidden layers 好像与序号无关（记不清了），~~RNN 看起来各个 W 和 H 似乎不同~~。**RNN cell is actually the exact same one and reused throughout.** (from [https://blog.floydhub.com/a-beginners-guide-on-recurrent-neural-networks-with-pytorch/](https://blog.floydhub.com/a-beginners-guide-on-recurrent-neural-networks-with-pytorch/)). Cartoon from Wikipedia:\n                \n                ![RNN 的构成单元](/photos/2023-03-20-rnn-unit.png)\n                \n            - From [https://blog.floydhub.com/a-beginners-guide-on-recurrent-neural-networks-with-pytorch/](https://blog.floydhub.com/a-beginners-guide-on-recurrent-neural-networks-with-pytorch/):\n                \n                ![RNN 的种类](/photos/2023-03-20-rnn-types.png)\n                \n            1. RNN architectures\n                - can be used to perform numerical Euler integration\n                - 基本上输出的第 i 项只与输入的第 i 和 i-1 项相关。\n            2. LSTM architectures\n                - 比 RNN 多更多中间隐变量，至于怎么做到整合长期记忆的，技术细节现在可以先略过\n        4. other architectures for PINN\n            1. Bayesian neural network: weights are distributions rather than deterministic values, and these distributions are learned using Bayesian inference. 只介绍了[一篇文章](https://doi.org/10.1016/j.jcp.2020.109913)\n            2. GAN architectures: \n                - two neural networks compete in a zero-sum game to deceive each other\n                - physics-informed GAN uses automatic differentiation to embed the governing physical laws in stochastic differential equations. The discriminator in PI–GAN is represented by a basic FFNN, while the generators are a combination of FFNNs and a NN induced by the SDE\n            3. multiple PINNs\n    2. Injection of Physical Laws\n        - 既然是要解常/偏微分方程，那么微分计算必不可少。四种方法：hand-coded, symbolic, numerical, auto-differentiation，最后一种显著胜出。所谓 auto-differentiation, 就是利用现成框架，框架自动给出原函数的导数的算法。\n        - Differential equation residual:\n            - $$r_F[\\hat u_\\theta](z)=r_\\theta(z):=F(\\hat u_\\theta(z);\\gamma)-f$$\n            - $$r_F[\\hat u_\\theta](z)=r_\\theta(x,t)=\\frac{\\partial}{\\partial t}\\hat u_\\theta(x,t)+F_x(\\hat u_\\theta(x,t))$$: 原文给出了来源，但是从字面上看不出来与前式的等价性\n        - Boundary condition residual: $$r_B[\\hat u_\\theta](z):=B(\\hat u_\\theta(z))-g(z)$$\n    3. Model Estimation by Learning Approaches\n        1. Observations about the Loss\n            - $$\\omega_F$$ accounts for the fidelity of the PDE model. Setting it to 0 trains the network without knowledge of underlying physics.\n            - In general, the number of $$\\theta$$ is more than the measurements, so regularization is needed.\n            - The number and position of residual points matter a lot.\n        2. Soft and Hard Constraints\n            - Soft: penalty terms. Bad:\n                - satisfying BC is not guaranteed\n                - assignment of the weight of BC affects learning efficiency, no theory for this.\n            - Hard: encoded into the network design. [Zhu et. al](https://doi.org/10.1007/s00466-020-01952-9)\n        3. Optimization methods\n            - minibatch sampling using the Adam algorithm\n            - increased sample size with L-BFGS (limited-memory Broyden-Fletcher-Goldfarb-Shanno)\n    4. Learning theory of PINN: roughly in DE, consistency + stability → convergence\n        1. convergence aspects: related to the number of parameters in NN\n        2. statistical learning error analysis: use *risk* to define *error*\n            - Empirical risk: $$\\hat R[u_\\theta]:=\\frac{1}{N}\\sum_{i=1}^N \\left\\|\\hat u_{\\theta}(z_i)-h_i\\right\\|^2$$\n            - Risk of using approximator: $$R[\\hat u_{\\theta}]:=\\int_{\\bar \\Omega}(\\hat u_{\\theta}(z)-u(z))^2dz$$\n            - Optimization error: the difference between the local and global minimum, is still an open question for PINN. $$E_O:=\\hat R[\\hat u_{\\theta}^*]-\\inf_{\\theta \\in \\Theta}\\hat R[u_\\theta]$$\n            - Generalization error: error when applied to unseen data. $$E_G:=\\sup_{\\theta \\in \\Theta}\\left\\|R[u_\\theta]-\\hat R[u_\\theta]\\right\\|$$\n            - Approximation error: $$E_A:=\\inf_{\\theta \\in \\Theta}R[u_\\theta]$$\n            - Global error between trained deep NN $$u^*_\\theta$$ and the correct solution is bounded: $$R[u^*_\\theta]\\le E_O+2E_G+E_A$$\n            - 有点乱，本来说 error 是误差，结果最后还是用 risk 作为误差\n        3. error analysis results for PINN\n3. Differential Problems Dealt with PINNs：读来感觉这一部分意义不大，将来遇到需要解决的问题时，回来看看之前有没有人做过就行了——另一方面看，一类方程就需要一类特殊构造的神经网络来解，那么说明神经网络解方程的通用性并不好~\n    1. Ordinary differential equations: \n        - Neural ODE as learners, a continuous representation of **ResNet**. [[Lai et al](https://doi.org/10.1016/j.jsv.2021.116196)], into 2 parts: a physics-informed term and an unknown discrepancy\n        - LSTM [[Zhang et al](https://doi.org/10.1016/j.cma.2020.113226)]\n        - [Directed graph models](https://doi.org/10.1016/j.compstruc.2020.106458) to implement ODE, and Euler RNN for numerical integration\n        - Symplectic Taylor neural networks in [Tong et al](https://doi.org/10.1016/j.jcp.2021.110325) use symplectic integrators\n    2. Partial differential equations: steady/unsteady的区别就是是否含时\n        1. steady-state PDEs\n        2. unsteady PDEs\n            1. Advection-diffusion-reaction problems\n                1. diffusion problems\n                2. advection problems\n            2. Flow problems\n                1. Navier-Stokes equations\n                2. hyperbolic equations\n            3. quantum problems\n    3. Other problems\n        1. Differential equations of fractional order\n            - automatic differentiation not applicable to fractional order → [L1 scheme](https://doi.org/10.1515/fca-2019-0086)\n            - [numerical discretization for fractional operators](https://doi.org/10.1137/18M1229845)\n            - [separate network to represent each fractional order](https://doi.org/10.1038/s43588-021-00158-0)\n        2. Uncertainty Estimation: [Bayesian](https://doi.org/10.1016/j.jcp.2020.109913)\n    4.  Solving a Differential Problem with PINN\n        - 1d non-linear Schrödinger equation\n        - dataset by simulation with MATLAB-based Chebfun open-source(?) software\n4. PINNs: Data, Applications, and Software\n    1. Data\n    2. Applications\n        1. Hemodynamics\n        2. Flows Problems\n        3. Optics and Electromagnetic Applications\n        4. Molecular Dynamics and Materials-Related Applications\n        5. Geoscience and Elastiostatic Problems\n        6. Industrial Application\n    3. Software\n        1. `DeepXDE`: initial library by one of the vanilla PINN authors\n        2. `NeuroDiffEq`: PyTorch based used at Harvard IACS\n        3. `Modulus`: previously known as Nvidia SimNet\n        4. `SciANN`: implementation of PINN as Keras wrapper\n        5. `PyDENs`: heat and wave equations\n        6. `NeuralPDE.jl`: part of SciML\n        7. `ADCME`: extending TensorFlow\n        8. `Nangs`: stopped updates, but faster than PyDENs\n        9. `TensorDiffEq`: TensorFlow for multi-worker distributed computing\n        10. `IDRLnet`: a python toolbox inspired by Nvidia SimNet\n        11. `Elvet`: coupled ODEs or PDEs, and variational problems about the minimization of a functional\n        12. Other Packages\n5. PINN Future Challenges and Directions\n    1. Overcoming Theoretical Difficulties in PINN\n    2. Improving Implementation Aspects in PINN\n    3. PINN in the SciML Framework\n    4. PINN in the AI Framework\n6. Conclusion\n"},{"slug":"parameters-in-convolution-in-neural-network-and-transposeconv","filename":"2022-12-29-parameters-in-convolution-in-neural-network-and-transposeconv.md","date":"2022-12-29","title":".ai | 神经网络中的卷积及其参数","layout":"post","keywords":["ai","py","md","m"],"hasMath":true,"excerpt":"在读 PyTorch 的文档和源码的时候，发现写文档的人也不怎么解释啥是卷积，卷积的各个参数是什么意思，只在文档里扔了个链接就完事了……","content":"\n在读 PyTorch 的文档和源码的时候，发现写文档的人也不怎么解释啥是卷积，卷积的各个参数是什么意思，只在文档里扔了个链接就完事了，链接那头是一个 GitHub 上的动图演示仓库，是一篇论文《A guide to convolution arithmetic for deep learning》（链接在文末）的附件。于是这篇文章，基本上就是论文的读书笔记了。\n\n## 数学的卷积：连续 vs. 离散\n\n### 定义\n\n连续的情况，两个单变量函数 $$f(\\cdot)$$ 和 $$g(\\cdot)$$ 的卷积，定义为：\n\n$$\n\\left(f*g\\right)(x):=\\int_{-\\infty}^{\\infty}f(\\tau)g(x-\\tau)d\\tau\n$$\n\n离散的情况，两个向量（也就是一阶张量） $$\\vec f$$ 和 $$\\vec g$$ 的卷积，定义为：\n\n$$\n\\left(\\vec f * \\vec g\\right)_i := \\sum_{j=-\\infty}^{\\infty} f_j g_{i-j}\n$$\n\n多变量函数/高阶张量的情况，只需要多加几重积分/求和号就可以类推了。\n\n看这两个定义——\n\n只看等号左边的话，可以把卷积看作是一种特殊的乘法，也就是一种**运算。**f 和 g 的地位是平等的，卷积甚至还满足交换律，你甚至可以把两者的顺序变一变；\n\n但是看等号右边的话，卷积就应该被看作是一种**变换**。f 和 g 的地位不再平等，f 是被变换的函数/向量，g 是变换的核 (kernel)。函数的情况里，g 把定义在 $$\\tau$$ 空间里的函数 f 变换成了 x 空间里的另一个函数；向量的情况里，g 把一个 J (j 所有可能取值的数量) 维向量 f 变换成了一个 I (i 所有可能取值的数量) 维向量。\n\n神经网络中的卷积，**借用**的主要是第二种**理解**。\n\n### 手算一个例子\n\n例如 $$\\vec f = (1,2,3,4)$$, $$\\vec g = (1,2,3)$$，而且约定下标从 0 开始的话——\n\n&nbsp; $$(\\vec f*\\vec g)_0 = f_0g_0 = 1$$\n\n&nbsp; $$(\\vec f*\\vec g)_1 = f_0g_1 + f_1g_0  = 4$$\n\n&nbsp; $$(\\vec f*\\vec g)_2 = f_0g_2 + f_1g_1 + f_2g_0 = 10$$\n\n&nbsp; $$(\\vec f*\\vec g)_3 = f_1g_2 + f_2g_1 + f_3g_0 = 16$$\n\n&nbsp; $$(\\vec f*\\vec g)_4 = f_2g_2 + f_3g_1 = 17$$\n\n&nbsp; $$(\\vec f*\\vec g)_5 = f_3g_2 = 12$$\n\n不想手算？\n\n```python\nimport numpy as np\nfrom scipy import signal\nsignal.convolve(np.array([1,2,3,4]),np.array([1,2,3]))\n```\n\n### 形象化表示\n\n上面的计算过程，可以看作是——\n\n1. 把 g 向量的**顺序反过来；**\n2. 把 g 的最右一个元素和 f 的最左元素对齐，\n3. 上下两行都有数字的列相乘（也就是把没有数字的地方看作 0），然后把所有乘积相加，得到 f*g 的第一项；\n4. 把 g 向右移动一格\n5. 重复第3、4步\n6. 直到 g 的最左项移动到 f 的最右一个元素。\n\n形如下列各表：\n\n| --- | --- | --- | --- | --- | --- | --- |\n| f |  |  | 1 | 2 | 3 | 4 |\n| g | 3 | 2 | 1 |  |  |  |\n| (f*g)(0) = 1 |  |  | 1 |  |  |  |\n\n<hr class=\"slender\">\n\n| --- | --- | --- | --- | --- | --- |\n| f |  | 1 | 2 | 3 | 4 |\n| g | 3 | 2 | 1 |  |  |\n| (f*g)(1) = 4 |  | 2 | 2 |  |  |\n\n<hr class=\"slender\">\n\n| --- | --- | --- | --- | --- |\n| f | 1 | 2 | 3 | 4 |\n| g | 3 | 2 | 1 |  |\n| (f*g)(2) = 10 | 3 | 4 | 3 |  |\n\n<hr class=\"slender\">\n\n| --- | --- | --- | --- | --- |\n| f | 1 | 2 | 3 | 4 |\n| g |  | 3 | 2 | 1 |\n| (f*g)(3) = 16 |  | 6 | 6 | 4 |\n\n<hr class=\"slender\">\n\n| --- | --- | --- | --- | --- | --- |\n| f | 1 | 2 | 3 | 4 |  |\n| g |  |  | 3 | 2 | 1 |\n| (f*g)(4) = 17 |  |  | 9 | 8 |  |\n\n<hr class=\"slender\">\n\n| --- | --- | --- | --- | --- | --- | --- |\n| f | 1 | 2 | 3 | 4 |  |  |\n| g |  |  |  | 3 | 2 | 1 |\n| (f*g)(5) = 12 |  |  |  | 12 |  |  |\n\n## 机器学习的卷积，是卷积吗？\n\n看论文给出的图 Figure 1.1，在卷积核是灰色 3\\*3 矩阵的情况下，对蓝色 5\\*5 矩阵的卷积就是直接把核对齐到蓝色矩阵上，**并没有把核的元素顺序颠倒过来**。\n\n这玩意能叫卷积吗？\n\n![convolution](/photos/2022-12-29-convolution.png)\n\n有人强行挽尊，说我们画图示的时候已经把核给颠倒过来了，想知道卷积核就把灰色小矩阵再颠倒回去——\n\n但是，不颠倒就对齐相乘的运算也是有名字的，叫 cross correlation。核有没有颠倒，convolution 还是 cross correlation 一组合，可以带来升维打击般的混乱，堪比高中化学的“还原剂被氧化，氧化剂被还原”……所以，对于计算机专业的数学水平，不予置评～\n\n\n## 卷积`torch.nn.Conv` 及其各个参数\n\n### `in_channels` & `out_channels`\n\n“卷积”的意义在于用一种比较省内存的方式，考虑输入张量中各个元素，和空间上相近的邻居元素之间的关系。所以只需要在真的存在空间关系的维度做卷积，其他维度可以留着不动。\n\n比如一张彩色图片，是一个 (颜色*高度*宽度) 的 3 阶张量，我们只需要对高度和宽度两个维度做卷积，颜色就是不参与“卷积”的 channel。\n\n`in_channel` 就是被“卷积”的张量的 channel 数，`out_channel` 是“卷积”结果的 channel 数。比如我们想从一张 RGB 三色图片中分辨出前景和背景两种不同区域，`in_channel=3`, `out_channel=2`。\n\n而 `in_channel` 如何能够与 `out_channel` 取值不同，原理见 Figure 1.3。我们使用 `out_channel` 个不含 channel 维度的“卷积”核，每一个核都与每一个 in channel 做卷积，得到图中的蓝、紫色小矩阵，然后直接把不同的 in channel 暴力求和，得到的结果分别作为卷积结果的 out channel。（这个暴力求和与我以前想得不一样，我以为是什么每一元素都做了一个`in_channel`*`out_channel` 的全联通层）\n\n![channels](/photos/2022-12-29-channels.png)\n\n PyTorch 的习惯，对于一个 N 阶“卷积”，参与卷积的是张量的最后 N 阶，`in_channel` 和 `out_channel` 也就是被卷张量和卷积结果的 `Tensor.shape[-(N+1)]`\n\n后面图示的例子都没有考虑 `in_channel` 和 `out_channel` 的数量，也就是都当作 1 了。\n\n### `kernel_size`\n\n就是灰色矩阵“卷积”核，每边有几个数字。如果不同方向的边长不一，该参数就需要用一个 tuple 来表示。Figure 1.1 的灰色卷积核，`kernel_size=(3,3)`\n\n![kernel](/photos/2022-12-29-kernel.png)\n\n### `padding` & `padding_mode`\n\n前面手算例子的时候很鸡贼地把 0 作为向量下标的起点。如果采用日常 1 开头的下标来算，第 1 项结果为零，整个卷积结果的长度会长很多，而且多出来的后面几项也都是零。\n\n而且在这个过程中，我们实际上是把一个有限长度的向量，看作了一个以所有整数 $$\\Z$$ 为定义域的函数，除了那有限的几项之外，其余地方都定义函数值为 0。\n\n用计算机计算的话显然没法如此奢侈地谈“无限多个”，例子中实际用到的，在 $$\\vec f$$ 左右两边各需要 2 个 0，也就是说 `padding=2`, `padding_mode='zeros'`\n\nFigure 1.2 表示的就是 `padding=(1,1)` 的情况（蓝色是被卷张量，白色是 padding，灰色是卷积核，绿色是卷积结果）：\n\n![padding](/photos/2022-12-29-padding.png)\n\n既然神经网络中的卷积并不是真正的卷积，所以他们索性不装了——\n\n正常卷积的结果往往比被卷张量大一圈（具体大多少取决于  `kernel_size`, `padding`, `stride` 多个参数），但是图像处理的时候经常希望输出图片和输入图片一样大，此时可以用字符串 `“same”` 作为 `padding` 的参数，自动计算 padding 的大小。`“strict”` 则表示 `padding=0`, 这样输出图片尺寸会变小，但是没有 padding，也就没有往图片里掺杂研究者对图片边缘以外信息的臆测。\n\n同时 `padding_mode` 参数表示往被卷张量四周填充的数字也不一定是 0。比如对于图片，0 往往表示纯黑，而绝大多数图片的视野之外，往往是和图片边缘像素值相差不大的值。所以 `padding_mode` 除了 `zeros` 之外，还接受以下取值：\n\n- `reflect`: 以图片边缘为镜面，把边缘附近的像素值对陈反射出去；\n- `replicate`: 只取边缘的像素值作为常数，直接向外延拓；\n- `circular`: 类似于物理中的周期性边界条件，取对边附近的像素值作为 padding 内容。\n\n### `stride`\n\n前面手算卷积的第4步，把卷积核向右移动了1格，如果每次移动超过1格，就需要这个参数指定移动步长。如果不同方向的步长不同，也是用 tuple 来表示。\n\nFigure 1.4 表示的就是 `stride=(2,2)` 的情况（蓝色是被卷张量，蓝色中的深色块是卷积核，绿色是卷积结果）：\n\n![stride](/photos/2022-12-29-stride.png)\n\n### `dilation`\n\n这个参数把“卷积”核撑开，也就相当于在“卷积”核的相邻元素之间加 0。Figure 1.5 表示的就是 `dilation=(1,1)` 的情况（蓝色是被卷张量，蓝色中的深色块是卷积核，绿色是卷积结果）：\n\n![dilation](/photos/2022-12-29-dilation.png)\n\n比如 `dilation=1` 时，(1,2,3) 的卷积核就相当于 (1,0,2,0,3)\n\n比如 `dilation=2` 时，(1,2,3) 的卷积核就相当于 (1,0,0,2,0,0,3)\n\n这样可以让卷积核在尺寸比较小的情况下，覆盖到更大面积的被卷张量。当然具体实现时，不可能直接补 0 这么浪费内存。\n\n### `groups`\n\n该参数必须是 `in_channel` 和 `out_channel` 的公约数，当其不为 1 时，就相当于同时做 `groups` 个卷积，其中每个卷积的 `in_channel=in_channel/groups`, `out_channel=out_channel/groups`\n\n### `bias`\n\n该参数是一个布尔值，卷积类似于一种高维空间里的乘法，这个参数就决定是否要拟合 `y=kx+b` 中的 `b`\n\n## “卷积”的“逆运算”： `TransposeConv`\n\n卷积的结果比 padding 之后的被卷张量要小。尤其当“卷积”的 `stride` 约等于 `kernel_size` 时，卷积的就变成了某些池化 (pooling)（求最大值不是一种线性算子，所以最大值池化不能用卷积表示，但是平均值池化可以）。\n\n那么在类似 U-net 这样的模型里，右半边的数据升维（下图中的绿箭头），就需要一种“卷积”的“逆运算”。有人把这种运算叫做 deconvolution，有人叫做 transposed convolution，还有人叫做 convolution with fractional strides。\n\n![Unet](/photos/2022-12-29-unet.png)\n\nPyTorch 取的是第二种名字。论文解释了为什么这么取名字，笔记以后有时间再补上把……\n\n因为这个与运算本身就是作为“卷积”的逆运算出现的，所以 PyTorch 的文档里这么说：\n\n> This is set so that when a `Conv2d` and a `ConvTranspose2d` are initialized with same parameters, they are inverses of each other in regard to the input and output shapes.\n> \n\n也就是说，把 `ConvTranspose` 的输入和输出反过来，然后按照 `Conv` 的规则确定各个参数，填入 `ConvTranspose` 的括号里就可以了，除了 `output_padding`\n\n### `output_padding`\n\n`ConvTranspose` 的输出就是对应 `Conv` 的输入。看 Figure 2.7：\n\n![padding_output](/photos/2022-12-29-output-padding.png)\n\n当 $$(input+2*padding)/stride$$ 不能整除的时候，最右的几列最下的几行就被卷积核忽略掉了。那么在逆运算 `TransposeConv` 中，这就意味着同一个输入可能对应着 $$stride-1$$ 种可能的输出。`output_padding`参数就可以消除这种歧义，调整 `TransposeConv` 输出张量的尺寸。\n\n## 参考链接\n\n- 给卷积正名: [https://www.kaggle.com/general/225375](https://www.kaggle.com/general/225375)\n- PyTorch Conv2d 源码: [https://pytorch.org/docs/stable/_modules/torch/nn/modules/conv.html#_ConvNd](https://pytorch.org/docs/stable/_modules/torch/nn/modules/conv.html#_ConvNd)\n- 论文: [https://arxiv.org/abs/1603.07285](https://arxiv.org/abs/1603.07285)\n- 动图演示: [https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md](https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md)\n- U-net: [https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/](https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/)\n- PyTorch TransposeConv 文档: [https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html](https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html)\n"},{"slug":"python-decorator","filename":"2022-11-09-python-decorator.md","date":"2022-11-09","title":".py | Python decorator 装饰器","layout":"post","keywords":["md","py"],"excerpt":"所谓装饰器 (decorator)，就是函数前一行 @ 打头的一串字符，是 python 的一种语法糖。","content":"\n最近参加了一个关于如何在 Python 项目中利用 GPU 提高运算效率的培训，里面提到了 `numba` 这个加速科学计算的库，而 `numba` 发挥作用的主要工具就是各种装饰器。\n\n所谓装饰器，就是读一些网上现成的 python 代码的时候会看到的，函数前一行 `@` 打头的一串字符，一般是一个名字，偶尔会附带有参数：\n\n```python\n@decorator\ndef myfunction():\n    # do something...\n    return results\n```\n\n它的实际作用相当于：\n\n```python\ndef myfunction():\n    # do something...\n    return None\n\nmyfunction = decorator(myfunction)\n```\n\nPython 是一种[函数式编程语言](https://program-think.blogspot.com/2012/02/why-choose-python-4-fp.html)，函数和各种类型的变量一样，在 Python 都是一种对象，所以可以把函数赋值给一个变量，可以在函数里定义另一个函数，可以把函数作为参数传递给另一个函数，可以把函数名作为另一个函数的返回值。\n\n`myfunction = decorator(myfunction)` 就是装饰器的定义，是 Python 的一个[语法糖](https://zh.m.wikipedia.org/zh-hans/%E8%AF%AD%E6%B3%95%E7%B3%96)。也就是说装饰器本身也是一个函数，我们的函数被装饰器装饰之后，函数名称不变，在完整实现函数原有功能的同时，额外执行装饰器中的命令。\n\n### 装饰器是如何做到的\n\n要想自己写一个装饰器的话，需要了解一下装饰器的实现原理。一个最简单的装饰器可以这么写：\n\n```python\ndef decorator(func):\n    def inner():\n        # do something\n        func()\n        # do some more\n        return None\n    return inner\n```\n\n也就是在装饰器内部再定义一个函数，这个内部函数的函数体执行被装饰的函数，然后外层装饰器把内层函数名当作返回值。\n\n### 如果一个函数需要多个装饰器\n\n把前面装饰器的定义套在多个装饰器的情况里：\n\n```python\n@decorator1\n@decorator2\ndef myfunction():\n    return None\n\nmyfunction = decorator1(decorator2(myfunction))\n```\n\n### 如果被装饰的函数有传入参数\n\n装饰器不知道自己要装饰的函数长什么样，也就不知道函数接受多少个参数，其中有几个是位置参数，几个是关键词参数。所以需要用单星号打包/解包位置参数，双星号打包/解包关键词参数。`args` 和 `kwargs` 是变量名的代词，可以换成其他自己喜欢的名字。\n\n```python\ndef decorator(func):\n    def inner(*args,**kwargs):\n        # do something\n        func(*args,**kwargs)\n        # something else\n        return None\n    return inner\n\n@decorator\ndef myfunction(x,y,mode=\"normal\",strict=True):\n    # do something...\n    return None\n```\n\n### 如果被装饰的函数有返回值\n\n则装饰器的内层函数需要把被装饰的函数的返回值返回出来：\n\n```python\ndef decorator(func):\n    def inner(*args,**kwargs):\n        # do something\n        func(*args,**kwargs)\n        # something else\n        return func(*args,**kwargs)\n    return inner\n\n@decorator\ndef myfunction(x,y,mode=\"normal\",strict=True):\n    # do something...\n    return results\n```\n\n### 如果想让装饰器本身接受参数\n\n也就是想达到下面的效果：\n\n```python\n@param_decorator(param=\"neat\")\ndef myfunction(x,y,mode=\"normal\",strict=True):\n    # do something...\n    return results\n```\n\n也就是让 `param_decorator(param=\"neat\")` 返回一个装饰器函数，也就是在之前的装饰器外面再加一层：\n\n```python\ndef param_decorator(param):\n    def decorator(func):\n        def inner(*args,**kwargs):\n            if param==\"neat\":\n                print(\"neat\")\n                # ...\n            else:\n                print(\"not neat\")\n                # ...\n            func(*args,**kwargs)\n            return func(*args,**kwargs)\n        return inner\n    return previous_decorator\n```\n\n### 如果想让装饰器既可以接受参数，也可以不接受参数～\n\n实在是有点过于高级了，直接说答案：\n\n```python\ndef flex_decorator(_func=None,*,kw1=\"val1\",kw2=\"val2\"):\n    def decorator(func):\n        def inner(*args,**kwargs):\n            print(kw1,kw2)\n            # do something\n            func(*args,**kwargs)\n            # something else\n            return func(*args,**kwargs)\n        return inner\n    if _func is None:\n        return decorator\n    else:\n        return decorator(_func)\n```\n\n根据 [https://peps.python.org/pep-3102/](https://peps.python.org/pep-3102/)，`*`作为一个单独的函数参数，表示后面所有的参数都是关键词参数，用来限定星号前面位置参数的数量。\n\n- 当 `@flex_decorator` 不加参数使用的时候:\n    - 根据定义 `myfunction = flex_decorator(myfunction)`\n    - `_func=myfunction`\n    - 此时 `else` 生效，`myfunction = decorator(myfunction)`\n- 当 `@flex_decorator(kw1=\"val1\",kw2=\"val2\")` 加上参数使用的时候:\n    - 根据定义 `myfunction = flex_decorator(kw1=\"val1\",kw2=\"val2\")(myfunction)`\n    - `_func=None`\n    - 此时 `if` 生效，`myfunction = decorator(myfunction)`\n\n——**接受的参数必须是关键词参数**，否则和被装饰的函数名无法区分。\n\n### `@functools.wraps`：刻章、办证\n\n以上各节基本完成了常用场景下装饰器的功能。\n\n但是，Python 作为一种动态语言，一大特征就是可以在运行时进行[类型内省](https://en.wikipedia.org/wiki/Type_introspection)。而按照我们上面的写法，被装饰之后的函数，Python 认为它不再是原来的函数，而是装饰器里面定义的那个内部函数，这样可能会出现意想不到的问题。\n\n解决方法是使用一个专门的装饰器根装饰器定义的内部函数办个假身份：\n\n```python\nimport functools\n\ndef decorator(func):\n    @functools.wraps(func)\n    def inner(*args,**kwargs):\n        # do something\n        func(*args,**kwargs)\n        # something else\n        return func(*args,**kwargs)\n    return inner\n```\n"},{"slug":"pytorch-dataset-dataloader-sampler","filename":"2022-09-01-pytorch-dataset-dataloader-sampler.md","date":"2022-09-01","title":".py | PyTorch 数据处理方面的封装","layout":"post","keywords":["md","py","ai"],"excerpt":"一般监督学习的数据结构和处理过程，以及 PyTorch 对上述结构和处理过程的封装","content":"\n## 一般监督学习的数据结构和处理过程\n\n### 训练集、验证集、测试集\n\n所有数据整体构成一个大集合，这个集合的每一个元素都包含一个输入和一个目标，分别记作 x 和 y。\n\n把这个大集合分成互相没有交集的三个子集，分别是训练集 (training set)、验证集 (validation set)、测试集 (test set)。\n\n- 训练集和验证集在训练过程中使用。\n    - 训练集的数据带入模型时，模型处于训练模式，模型输出对参数的导数被记录。通过比较把“模型输出”和“训练目标 y”代入**损失函数**的损失，更新模型的参数。同时记录“模型输出”和“训练目标 y”带入验证函数的结果，和验证集比较。\n    - 验证集的数据代入模型时，模型处于求值模式，模型只根据输入计算输出，对参数的导数不记录。通过观察“模型输出”和“训练目标 y”带入**验证函数**的结果，观察训练是否陷入“过拟合”。当训练集的验证函数结果不断下降，但是验证集的验证函数结果几乎不变时，可以认为模型过拟合。\n- 测试集在训练完成之后使用，代入模型时，模型处于求值模式。用于评价训练结果的好坏。\n\n### epoch vs. batch\n\n如果把所有数据同时进行训练，所需要的空间一般都大于电脑内存。所以一般会将训练集随机分成若干批次 (batch)，一个批次的数据同时塞入模型进行训练，在一个 batch 里每一个模型输出对参数的导数累加在一起，整个 batch 结束后更新模型参数，同时导数清零。因为 batch 这个概念和内存有关，所以数值一般选择为 2 的指数。\n\n将训练集所有的 batches 跑完一次称为而一个 epoch。一次训练一般需要很多 epochs，直到损失函数结果足够低，或验证集显示出现过拟合。\n\n## PyTorch 对上述结构和处理过程的封装\n\n### `Dataset`\n\n前面已经说了，数据集包括输入和目标两部分，`Dateset` 及其子类的作用就是\n\n如果在把数据装入 `Dataset` 之前就已经是规整的两个张量了的话——\n\n```python\nimport torch\nfrom torch.utils import data\n\n# ...\n\nfor x,y in zip(train_x,train_y):\n    # do something with x and y\n\ntrainset = TensorDataset(train_x,train_y)\nfor x,y in trainset:\n    # do something with x and y\n```\n\n——这一步确实没什么意思。\n\n有意思的地方在于可以自己写一个数据集类，继承 `Dataset`，然后重载 `__getitem__()` 和 `__len__()` 方法，这样可以把一些不适合用张量表示的数据塞进 `Dataset` 里面，对图像进行学习的话可以在此处加入图像增强的步骤，并进一步用于 `DataLoader`\n\n### `DataLoader`\n\n`DataLoader` = `Dataset` + `Sampler`，因为一般的教程里只需要讲数据集进行简单随机划分，也就只用到了 `batch_size` 等等参数，用到 Sampler 的地方很少。\n\n最常见的用例就是 `WeightedRandomSampler` 。训练分类器的时候，有时其中一个类别的数据远少于其他，那么训练器就更难判断出这一分类（因为只要无脑排除这个类别就能获得不错的正确率），所以需要平衡不同组别之间的权重。\n\n```python\nlist(WeightedRandomSampler(weights=[0.1, 0.9, 0.4, 0.7, 3.0, 0.6], num_samples=5, replacement=True))\n# [4, 4, 1, 4, 5]\nlist(WeightedRandomSampler(weights=[0.9, 0.4, 0.5, 0.2, 0.3, 0.1], num_samples=5, replacement=False))\n# [0, 1, 4, 3, 2]\n```\n\n平衡完之后转化为 batch，搭配 `BatchSampler`：\n\n```python\nlist(BatchSampler(WeightedRandomSampler(weights=[0.1, 0.9, 0.4, 0.7, 3.0, 0.6], num_samples=5, replacement=True), batch_size=2, drop_last=False))\n# [[4, 4], [1, 4], 5]\nlist(BatchSampler(WeightedRandomSampler(weights=[0.1, 0.9, 0.4, 0.7, 3.0, 0.6], num_samples=5, replacement=True), batch_size=2, drop_last=True))\n# [[0, 1], [4, 3]]\n```\n\n### 汇总一下\n\n```python\nimport torch\nfrom torch.utils import data\n\ntrain_x = torch.rand((100,5))\ntrain_y = torch.rand((100,2))\ntrainset = data.TensorDataset(train_x,train_y)\n\n# either:\ntrainloader = data.DataLoader(\n    trainset,\n    batch_size=2,\n    drop_last=True,\n    sampler=data.WeightedRandomSampler(\n        weights=[0.1, 0.9, 0.4, 0.7, 3.0, 0.6], \n        num_samples=5, \n        replacement=True))\n# or:\ntrainloader = data.DataLoader(\n    trainset,\n    batch_sampler=data.BatchSampler(\n        data.WeightedRandomSampler(\n            weights=[0.1, 0.9, 0.4, 0.7, 3.0, 0.6], \n            num_samples=5, \n            replacement=True), \n        batch_size=2, \n        drop_last=True))\n\nfor epoch in range(100):\n    for x,y in trainloader:\n        train(model,x,y,loss_function)\n```\n\n需要注意的是，`for x,y in trainset` 的 x 和 y 的维度是单个数据的维度，最简单的情况就是是 P 和 Q 维向量，而此时如果把 batch_size 记作 B，`for x,y in trainloader` 中的 x 和 y 是维度分别为 (B,P) 和 (B,Q) 的矩阵。`train()` 函数里面的计算要考虑到多出的这一个维度。\n\n## 参考链接：\n\n- [https://pytorch.org/tutorials/beginner/basics/data_tutorial.html](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html)\n- [https://pytorch.org/tutorials/beginner/nn_tutorial.html](https://pytorch.org/tutorials/beginner/nn_tutorial.html)\n- [https://pytorch.org/tutorials/beginner/ptcheat.html](https://pytorch.org/tutorials/beginner/ptcheat.html)\n- [https://pytorch.org/docs/stable/data.html?highlight=dataset#torch.utils.data.Dataset](https://pytorch.org/docs/stable/data.html?highlight=dataset#torch.utils.data.Dataset)\n- [https://pytorch.org/docs/stable/data.html?highlight=dataloader#torch.utils.data.DataLoader](https://pytorch.org/docs/stable/data.html?highlight=dataloader#torch.utils.data.DataLoader)\n- [https://pytorch.org/docs/stable/data.html?highlight=dataloader#torch.utils.data.Sampler](https://pytorch.org/docs/stable/data.html?highlight=dataloader#torch.utils.data.Sampler)\n- [https://pytorch.org/docs/stable/_modules/torch/utils/data/dataset.html#Dataset](https://pytorch.org/docs/stable/_modules/torch/utils/data/dataset.html#Dataset)\n"},{"slug":"knowledge-structure-machine-learning-image-processing-summer-school","filename":"2022-08-17-knowledge-structure-machine-learning-image-processing-summer-school.md","date":"2022-08-17","title":".py | 深度学习暑期学校知识点","layout":"post","keywords":["md","py","ai"],"excerpt":"机器学习在图像处理当中的应用，知识结构树","content":"\n- 电脑设置、python 入门\n    - NoMachine, ssh, python, conda, jupyter\n    - 文件夹操作 `pathlib`, 图片操作 `skimage`\n    - 数据增强 (data augmentation): `imgaug`\n    - TensorBoard\n- 机器学习简介\n    - Linear Classifier\n    - `sklearn**.**model_selection**.**train_test_split()`\n    - Stochastic gradient descent\n    - TensorFlow:\n        - `tensorflow_addons as tfa`\n        - `tfa.image.rotate()`, `tf.image.random_flip_left_right()`\n        - **`from** tensorflow.keras **import** Model`, **`from** tensorflow.keras.models **import** Sequential`, **`from** tensorflow.keras.layers **import** Input, Flatten, Dense, Activation, BatchNormalization, Conv2D, MaxPool2D, Softmax`\n        - `tf**.**keras**.**losses**.**CategoricalCrossentropy()`, `tf**.**keras**.**optimizers**.**Adam(lr**=**1e-3, clipnorm**=**0.001)`\n        - `linear_classifier **=**``Model(...)`, `linear_classifier.compile()`, `linear_classifier.fit()`, `linear_classifier.predict()`\n- 深度学习简介\n    - Perceptron\n    - Perceptron-based XOR gate\n    - **decision boundary** of your model: `np.meshgrid`\n- 图像恢复 (image restoration)\n    - CARE network\n    - Noise2Nosie, Noise2Void\n- 图像翻译 (image translation)\n    - micro-DL: a tool to generate and train U-net from config files.\n- 图像语义分割 (image semantic segmentation): 比较详细，前两节有点水了。\n    - **`from** PIL **import** Image`, **`import** imageio`, **`from** torchvision **import** transforms`\n    - **`from** torch.utils.data **import** Dataset, DataLoader`, **`import** torch.nn **as** nn`, **`from** torch.nn **import** functional **as** F`,  **`from** torch.utils.tensorboard **import** SummaryWriter`,\n    - U-net on PyTorch\n- 图像实例分割 (instance segmentation)\n    1. Foreground segmentation: \n        - **Receptive Field of View**: The term is borrowed from biology where it describes the \"portion of sensory space that can elicit neuronal responses when stimulated\" (wikipedia). Each output pixel can look at/depends on an input patch with that diameter centered at its position. Based on this patch, the network has to be able to make a decision about the prediction for the respective pixel.\n        - **Early Stopping** to avoid overfitting: define an `EarlyStopping` class\n    2. Instance Segmentation:\n        - Ideas:\n            - Three-class model (foreground, background, boundary),\n            - Distance transform (label for each pixel is the distance to the closest boundary),\n            - Edge affinity (consider not just the pixel but also its direct neighbors, predicts the probability that there is an edge, this is called affinity.) 听的时候懂了，回来看的时候没太看懂\n            - Metric learning (learns to predict an embedding vector for each pixel.)\n    3. **Tile and Stitch：**\n        - 当需要处理的图片过大时，将图片切分成多个小图，分别预测之后拼接在一起。\n        - 文中说图片尺寸不是 某个参数的整数倍的时候拼贴结果会不连续，但是代码注释中说等于这个整数倍的时候会不连续，晕。\n        - [https://arxiv.org/pdf/2101.05846.pdf](https://arxiv.org/pdf/2101.05846.pdf)\n    4. 一个实例，epithelia cells\n- 失败模式：极其之水，就是科普了一下训练参数错误的后果，以及一点对抗学习的内容\n- 追踪：比较水，因为机器学习追踪的运算量极大，且主讲人感觉就是来做广告的，所以就直接用 CoLab 体验了一下就完事了。（就这还加州理工呢~）\n- 知识提取：\n    - 前面的基本上是从像素到像素的映射，这里的知识提取是从图片到标签的映射。\n    - CycleGAN\n    - **Create a balanced Dataloader**\n    "},{"slug":"what-a-PyTorch-project-looks-like","filename":"2022-08-17-what-a-PyTorch-project-looks-like.md","date":"2022-08-17","title":".py | 一个 PyTorch 机器学习项目长什么样","layout":"post","keywords":["md","py","ai"],"excerpt":"官网的一个pytorch教程的笔记，原文先按照第一性原理，尽量用原生 python 写了一遍，然后一步一步重构成接近生产环境的代码。这里我把顺序反过来，先放出重构之后的最终结果","content":"\n自学，或者说一切学习和教学，本质就是在已经掌握的知识和未知的目标知识之间修路。路有两种修法，一是理论或者说是第一性原理路线，从不证自明的公理或者已经掌握的知识出发，通过逻辑推理一步步得到新的知识；另一种是实践或者说工程师路线，拿到一个已经可以工作的产品，划分成各个子系统，通过输入的改变来观察输出的不同，直到子系统简化到自己可以理解的地步，不再是黑箱，借此了解整个系统的功能。\n\n但是当学习的对象复杂到一定程度之后，凭借一个人的自学能力，只用其中一种方法往往难以钻透。又或者两种方法学到的路线并非同一条路。对于机器学习，理论路线就是“让输入数据通过一个带有超多参数的函数，根据函数返回值和输出数据之间的差别修正参数，直到函数能够近似输入数据和输出数据之间的关系”；实践中代码往往会使用很多库作者封装好的函数，只读源码往往一头雾水。\n\n所以，看到 PyTorch 官网的这篇教程 **WHAT IS TORCH.NN *REALLY*?:** [https://pytorch.org/tutorials/beginner/nn_tutorial.html](https://pytorch.org/tutorials/beginner/nn_tutorial.html) 可以说是喜出望外，把两种路线写出的代码都给了出来，对于自学者来说，就像罗塞塔石碑一样可以互相对照。这里我把 CNN 相关的部分抽掉了，毕竟 CNN 只是深度学习的一个子集，深度学习只是机器学习的一个子集，和这篇文章的主题关系不大。\n\n原文先按照第一性原理，尽量用原生 python 写了一遍，然后一步一步重构成接近生产环境的代码。这里我把顺序反过来，先放出重构之后的最终结果：\n\n```python\nfrom pathlib import Path\nimport requests\nimport pickle\nimport gzip\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\nfrom torch import optim\nfrom torch.utils.data import TensorDataset,DataLoader\n\n# Using GPU\n\nprint(torch.cuda.is_available())\ndev = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n\n# Wrapping DataLoader\n# https://pytorch.org/tutorials/beginner/basics/data_tutorial.html?highlight=dataloader\n# https://pytorch.org/tutorials/beginner/data_loading_tutorial.html?highlight=dataloader\n\ndef preprocess(x, y):\n    return x.view(-1, 1, 28, 28).to(dev), y.to(dev)\n\ndef get_data(train_ds, valid_ds, bs):\n    return (\n        DataLoader(train_ds, batch_size=bs, shuffle=True),\n        DataLoader(valid_ds, batch_size=bs * 2),\n    )\n\nclass WrappedDataLoader:\n    def __init__(self, dl, func):\n        self.dl = dl\n        self.func = func\n\n    def __len__(self):\n        return len(self.dl)\n\n    def __iter__(self):\n        batches = iter(self.dl)\n        for b in batches:\n            yield (self.func(*b))\n\n# Define the neural network model to be trained\n\n# # If the model is simple:\n# model = nn.Sequential(nn.Linear(784, 10))\n\n# generally the model is a class that inherites nn.Module and implements forward()\nclass Mnist_Logistic(nn.Module):\n    def __init__(self):\n        super().__init__()\n        # self.weights = nn.Parameter(torch.randn(784, 10) / math.sqrt(784))\n        # self.bias = nn.Parameter(torch.zeros(10))\n        self.lin = nn.Linear(784, 10)\n\n    def forward(self, xb):\n        # return xb @ self.weights + self.bias\n        return self.lin(xb)\n\n# Define the training pipeline in fit()\n\ndef loss_batch(model, loss_func, xb, yb, opt=None):\n    loss = loss_func(model(xb), yb)\n\n    if opt is not None:\n        loss.backward()\n        opt.step()\n        opt.zero_grad()\n\n    return loss.item(), len(xb)\n\ndef fit(epochs, model, loss_func, opt, train_dl, valid_dl):\n    for epoch in range(epochs):\n        model.train()\n        for xb, yb in train_dl:\n            loss_batch(model, loss_func, xb, yb, opt)\n\n        model.eval()\n        with torch.no_grad():\n            losses, nums = zip(\n                *[loss_batch(model, loss_func, xb, yb) for xb, yb in valid_dl]\n            )\n        val_loss = np.sum(np.multiply(losses, nums)) / np.sum(nums)\n\n        print(epoch, val_loss)\n    return None\n\n# __main()__:\n\n# data\nDATA_PATH = Path(\"data\")\nPATH = DATA_PATH / \"mnist\"\n\nPATH.mkdir(parents=True, exist_ok=True)\n\nURL = \"https://github.com/pytorch/tutorials/raw/master/_static/\"\nFILENAME = \"mnist.pkl.gz\"\n\nif not (PATH / FILENAME).exists():\n        content = requests.get(URL + FILENAME).content\n        (PATH / FILENAME).open(\"wb\").write(content)\nwith gzip.open((PATH / FILENAME).as_posix(), \"rb\") as f:\n        ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding=\"latin-1\")\n\nx_train, y_train, x_valid, y_valid = map(\n    torch.tensor, (x_train, y_train, x_valid, y_valid)\n)\n\ntrain_dataset = TensorDataset(x_train, y_train)\nvalid_dataset = TensorDataset(x_valid, y_valid)\ntrain_dataloader, valid_dataloader = get_data(train_ds, valid_ds, bs)\ntrain_dataloader = WrappedDataLoader(train_dataloader, preprocess)\nvalid_dataloader = WrappedDataLoader(valid_dataloader, preprocess)\n\n# hyperparameters/model\nlearning_rate = 0.1\nepochs = 2\nloss_function = F.cross_entropy # loss function\nmodel = Mnist_CNN()\nmodel.to(dev)\noptimizer = optim.SGD(model.parameters(), lr=learning_rate , momentum=0.9)\n\n# training\nfit(epochs, model, loss_function, optimizer, train_dataloader, valid_dataloader)\n```\n\n可以看到，一个项目主干可以分成4部分：\n\n1. 准备数据\n2. 定义模型\n3. 描述流程\n4. 实际运行\n\n下面把各部分拆分开来，把两种思路的代码进行对比。\n\n## 1. 准备数据\n\n### 重构之前\n\n```python\nDATA_PATH = Path(\"data\")\nPATH = DATA_PATH / \"mnist\"\n\nPATH.mkdir(parents=True, exist_ok=True)\n\nURL = \"https://github.com/pytorch/tutorials/raw/master/_static/\"\nFILENAME = \"mnist.pkl.gz\"\n\nif not (PATH / FILENAME).exists():\n        content = requests.get(URL + FILENAME).content\n        (PATH / FILENAME).open(\"wb\").write(content)\nwith gzip.open((PATH / FILENAME).as_posix(), \"rb\") as f:\n        ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding=\"latin-1\")\n\nx_train, y_train, x_valid, y_valid = map(\n    torch.tensor, (x_train, y_train, x_valid, y_valid)\n)\nn, c = x_train.shape\n```\n\n### 重构以后：\n\n```python\n# Wrapping DataLoader\n# https://pytorch.org/tutorials/beginner/basics/data_tutorial.html?highlight=dataloader\n# https://pytorch.org/tutorials/beginner/data_loading_tutorial.html?highlight=dataloader\n\ndef preprocess(x, y):\n    return x.view(-1, 1, 28, 28).to(dev), y.to(dev)\n\ndef get_data(train_ds, valid_ds, bs):\n    return (\n        DataLoader(train_ds, batch_size=bs, shuffle=True),\n        DataLoader(valid_ds, batch_size=bs * 2),\n    )\n\nclass WrappedDataLoader:\n    def __init__(self, dl, func):\n        self.dl = dl\n        self.func = func\n\n    def __len__(self):\n        return len(self.dl)\n\n    def __iter__(self):\n        batches = iter(self.dl)\n        for b in batches:\n            yield (self.func(*b))\n```\n\n## 2. 定义模型\n\n### 重构之前\n\n```python\nweights = torch.randn(784, 10) / math.sqrt(784)\nweights.requires_grad_()\nbias = torch.zeros(10, requires_grad=True)\n\ndef log_softmax(x):\n    return x - x.exp().sum(-1).log().unsqueeze(-1)\n\ndef model(xb):\n    return log_softmax(xb @ weights + bias)\n\ndef nll(input, target):\n    return -input[range(target.shape[0]), target].mean()\nloss_func = nll\n\ndef accuracy(out, yb):\n    preds = torch.argmax(out, dim=1)\n    return (preds == yb).float().mean()\n```\n\n### 重构以后\n\n```python\n# If the model is simple:\nmodel = nn.Sequential(nn.Linear(784, 10))\n\n# generally the model is a class that inherites nn.Module and implements forward()\nclass Mnist_Logistic(nn.Module):\n    def __init__(self):\n        super().__init__()\n        # self.weights = nn.Parameter(torch.randn(784, 10) / math.sqrt(784))\n        # self.bias = nn.Parameter(torch.zeros(10))\n        self.lin = nn.Linear(784, 10)\n\n    def forward(self, xb):\n        # return xb @ self.weights + self.bias\n        return self.lin(xb)\n\n```\n\n## 3. 描述流程\n\n### 重构之前\n\n```python\nlr = 0.5  # learning rate\nepochs = 2  # how many epochs to train for\n\nfor epoch in range(epochs):\n    for i in range((n - 1) // bs + 1):\n        #         set_trace()\n        start_i = i * bs\n        end_i = start_i + bs\n        xb = x_train[start_i:end_i]\n        yb = y_train[start_i:end_i]\n        pred = model(xb)\n        loss = loss_func(pred, yb)\n\n        loss.backward()\n        with torch.no_grad():\n            weights -= weights.grad * lr\n            bias -= bias.grad * lr\n            weights.grad.zero_()\n            bias.grad.zero_()\n```\n\n### 重构以后\n\n```python\n\ndef loss_batch(model, loss_func, xb, yb, opt=None):\n    loss = loss_func(model(xb), yb)\n\n    if opt is not None:\n        loss.backward()\n        opt.step()\n        opt.zero_grad()\n\n    return loss.item(), len(xb)\n\ndef fit(epochs, model, loss_func, opt, train_dl, valid_dl):\n    for epoch in range(epochs):\n        model.train()\n        for xb, yb in train_dl:\n            loss_batch(model, loss_func, xb, yb, opt)\n\n        model.eval()\n        with torch.no_grad():\n            losses, nums = zip(\n                *[loss_batch(model, loss_func, xb, yb) for xb, yb in valid_dl]\n            )\n        val_loss = np.sum(np.multiply(losses, nums)) / np.sum(nums)\n\n        print(epoch, val_loss)\n    return None\n```\n\n## 4. 实际运行\n\n### 重构之前\n\n```python\n# __main()__:\nprint(loss_func(model(xb), yb), accuracy(model(xb), yb))\n```\n\n### 重构以后\n\n```python\n# __main()__:\n\n# data\nDATA_PATH = Path(\"data\")\nPATH = DATA_PATH / \"mnist\"\n\nPATH.mkdir(parents=True, exist_ok=True)\n\nURL = \"https://github.com/pytorch/tutorials/raw/master/_static/\"\nFILENAME = \"mnist.pkl.gz\"\n\nif not (PATH / FILENAME).exists():\n        content = requests.get(URL + FILENAME).content\n        (PATH / FILENAME).open(\"wb\").write(content)\nwith gzip.open((PATH / FILENAME).as_posix(), \"rb\") as f:\n        ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding=\"latin-1\")\n\nx_train, y_train, x_valid, y_valid = map(\n    torch.tensor, (x_train, y_train, x_valid, y_valid)\n)\n\ntrain_dataset = TensorDataset(x_train, y_train)\nvalid_dataset = TensorDataset(x_valid, y_valid)\ntrain_dataloader, valid_dataloader = get_data(train_ds, valid_ds, bs)\ntrain_dataloader = WrappedDataLoader(train_dataloader, preprocess)\nvalid_dataloader = WrappedDataLoader(valid_dataloader, preprocess)\n\n# hyperparameters/model\nlearning_rate = 0.1\nepochs = 2\nloss_function = F.cross_entropy # loss function\nmodel = Mnist_CNN()\nmodel.to(dev)\noptimizer = optim.SGD(model.parameters(), lr=learning_rate , momentum=0.9)\n\n# training\nfit(epochs, model, loss_function, optimizer, train_dataloader, valid_dataloader)\n```"},{"slug":"python-subprocess-run-bash","filename":"2022-07-08-python-subprocess-run-bash.md","date":"2022-07-08","title":".py | python.subprocess执行bash命令","layout":"post","keywords":["md","py"],"excerpt":"让 python 读 bash 的命令结果，写 bash 的命令语句。","content":"\n笔记本的触摸屏被我摔了道裂纹，一开始还不影响使用，但是最近几周情况恶化，有时鼠标光标会突然暴走，不听指挥。所以需要禁用触摸屏作为输入设备。\n\n在 xorg 的桌面环境之下，可以用 `xinput list` 显示出所有输入设备，以及对应的 id 号码。然后把找到的 id 填入 `xinput disable ##` 就可以了。一般来说这个 id 的数值是稳定的，所以我就直接把禁用命令写到 `~/.bashrc` 里面去了。\n\n然而，最近把吃灰很久的树莓派拿出来玩了，所以败家买了个60%布局的小机械键盘，小键盘往笔记本一插，诶，您猜怎么着，新买的键盘输完密码登陆之后就不能用了，着实吓了一跳。\n\n所以需要每次检查一遍输入设备的 id，然后把和触摸屏有关的 id（不止一个）从 `xinput list` 的输出里摘出来赋值给一个变量，然后把变量带入 `xinput disable #` 里面。这一套操作已经超出我的 bash 能力了，所以考虑用 python 完成中间步骤，也就是需要让 python 读 bash 的命令结果，写 bash 的命令语句。\n\nGoogles搜索给到了这个结果：[https://stackoverflow.com/questions/4256107/running-bash-commands-in-python](https://stackoverflow.com/questions/4256107/running-bash-commands-in-python)，稍微看了一下 subprocess 的官方文档，写了下面的一段，存到 `~/disable_touchscreen.py`, 然后在 `~/.bashrc` 里加一句 `python ~/disable_touchscreen.py`\n\n```python\nimport subprocess\ncheck = subprcess.run([\"xinput\",\"list\"],capture_output=True)\nfor line in check.stdout.decode(\"utf-8\").split(\"\\n\"):\n    if \"touchscreen\" in line:\n        device = line.partition(\"id=\")[2].partition(\"\\t\")[0]\n        disable = subprocess.run([\"xinput\",\"disable\",str(device)])\n        if disable.returncode==0:\n            print(f\"Successfully disabled touchscreen device {device}\")\n        else:\n            print(f\"Failed to disable touchscreen device {device}\")\n```\n\n- 这段代码的核心是 `subprocess.run()` ，第一个参数就是传给 bash 的命令，这是一个list，其中每个元素就是 bash 语句用空格分割开的每一部分。要想得到命令的执行结果，需要添加参数 `capture_output=True`\n- 上面函数的返回值是一个特殊的数据结构，命令顺利执行的话，结果会写在 `.stdout` 里，这是一个二进制串 `b'xxx...'`，所以需要 `.decode(\"utf-8\")` 转化成字符串。\n\n如果只用bash的话，\n\n- 我的 bash 水平可以做到 `xinput list | grep \"touchscreen\"`, 这里的 `|` 是一个管道，也就是将前一条命令的输出传递给后一条作为输入。要想在 python 里使用管道，可以看这个问答：[https://stackoverflow.com/questions/13332268/how-to-use-subprocess-command-with-pipes](https://stackoverflow.com/questions/13332268/how-to-use-subprocess-command-with-pipes)\n- 上面管道的结果还需要裁剪出 id 号，也就是 `line.partition(\"id=\")[2].partition(\"\\t\")[0]`，估计需要用到 awk, 虽然难，但可以学，至少知道该学什么；\n- 但是把 id 号传递给一个变量，然后把这个变量填进 `xinput disable`, 这一步就连该学什么也不知道了。\n"},{"slug":"mimic-mathematica-with-wolfram-engine-and-vscode","filename":"2022-06-19-mimic-mathematica-with-wolfram-engine-and-vscode.md","date":"2022-06-19","title":".nb | Mathematica 入门：免费正版、vscode、近似原生体验","layout":"post","keywords":["md","nb","m"],"excerpt":"不用算号器，完全合法的免费手段搭建一个免费的 Wolfram Languange 运行环境，效果尽可能贴近 Mathematica。","content":"\n## Mathematica 的原生体验暨山寨目标\n\n本科的时候老师总是跟我们念叨，让我们学点科学计算软件，可学的不多，不过 MatLab, Mathematica, Maple, Origin 和 Labview.  ~~作为编程语言的 MatLab 是世界上语法最垃圾的（没有之一）~~ ，Maple 实在是太小众了，Origin 和 Labview 不仅应用场景有限而且繁琐还有版权问题，于是 MMA 就成了我主要的折腾对象。不敢说拿手，起码是略懂。\n\nWolfram 和 python 一样也是[动态语言和解释型语言](python-interpreter-editor-virtualenv)，而且默认的新建文件类型就是 `.nb` 笔记本文件，命令按块执行，输出结果直接显示在代码块下方，强烈怀疑 Jupyter Notebook 就是山寨了 Mathematica。当然另有一种 `.m` 文件，用于执行文件内的所有命令，适用于比较大型的独立应用。\n\n![](/photos/2022-06-19-mathematica-notebook.gif)\n\n虽然现在的学校给学生买了正版许可证，但是只能用在一台电脑上，所以笔记本上就安装不了。虽然百度贴吧的精华帖里有传统艺能算号器教程，但是现在 Wolfram 开放了免费的 Wolfram Engine，所以我们还是来点正大光明的，用完全合法的免费手段搭建一个免费的 Wolfram Languange 运行环境，效果尽可能贴近 Mathematica。\n\n需要用到的工具有：\n\n- Wolfram Engine\n- Wolfram Script\n- Wolfram Engine For Jupyter\n- jupyter\n- vscode\n\n## Wolfram Engine 和 Wolfram Script 下载和安装\n\n《How do I install Mathematica on Linux?》：[https://support.wolfram.com/12453](https://support.wolfram.com/12453)\n\n在 Google 上搜索“Wolfram Engine”后可以找到官网的下载地址：[https://www.wolfram.com/engine/](https://www.wolfram.com/engine/)\n\n根据自己的操作系统点击下载之后会弹出获取许可证的页面 ([https://account.wolfram.com/access/wolfram-engine/free](https://account.wolfram.com/access/wolfram-engine/free))，没有 Wolfram 账号的需要注册一个账号。\n\n完成之后在下载文件夹打开 terminal, 输入以下命令，其中 xyz 是下载文件名中的版本号：\n\n```python\nsudo bash WolframEngine_xxx.yy.zz_LINUX\n```\n\n强烈建议按照默认设置完成安装，不做要任何个性化的调整，理由见下方引用块。\n\n> 我把第二个选项，也就是 Wolfram Engine 可执行文件的路径设置成了自己 home 下安放一般独立软件的文件夹，结果激活过程出现了问题：输入 `wolframscript` 之后说找不到 WoflramEngine，填入自己的路径之后提示 Wolfram Engine 尚未激活，手动启动 `WolframEngine` 之后提示输入激活密钥 activation key，但是各处遍寻不得。\n解决方法来自以下 StackOverflow 回答：[https://mathematica.stackexchange.com/questions/198822/the-wolfram-kernel-must-be-activated-for-wolframscript-to-use-it](https://mathematica.stackexchange.com/questions/198822/the-wolfram-kernel-must-be-activated-for-wolframscript-to-use-it)\n在wolfram官网登陆自己的账号之后，在一个新的标签页输入以下网址 [https://www.wolframcloud.com/users/user-current/activationkeys](https://www.wolframcloud.com/users/user-current/activationkeys)，即可看到自己的 activation key，在 terminal 中打开 Wolfram Engine，根据提示把 activation key 复制粘贴到指定位置，即可完成激活。\n但是第二天配置好 vscode 和 Jupyter 之后，再次在命令行打开 WolframScript 的时候提示激活失败，重新按照上述方法操作后，显示 activation key 已被使用。即便是删除后按照默认设置重装，也依然会提示超过许可证限制。\n后来在官网给出的联系方式给客服发了消息，客服回信给了新的激活码。\n> \n\n再下载 [WolframScript](https://account.wolfram.com/products/downloads/wolframscript)，这是 Wolfram 的前端。然后按照各个操作系统自己的规矩安装 WolframScript，我的 fedora 就是双击 rpm 文件然后根据提示操作。完成后在命令行输入 `wolframscript`, 根据提示输入 Wolfram 账号和密码，Wolfram Engine 就会联网激活自己。\n\n激活成功之后，在 terminal 输入 `wolframscript`, 显示的结果如下，即说明 Wolfram Engine 和 Wolfram Script 配置成功。\n\n```bash\nWolfram Language 13.0.1 Engine for Linux x86 (64-bit)\nCopyright 1988-2022 Wolfram Research, Inc.\n\nIn[1]:=\n```\n\n在 `In[1]:=` 处输入 `Exit[]` 并按回车，即可退出 Wolfram 回到命令行。\n\n## 将 Wolfram Engine 设为 Jupyter 的后端\n\n[https://github.com/WolframResearch/WolframLanguageForJupyter](https://github.com/WolframResearch/WolframLanguageForJupyter)\n\n根据上面网址的指示，将官方 repo 克隆到本地，因为我们的 python 分隔成了多个[虚拟环境](https://virtual.env)，所以比官网教程多一步 `workon base`:\n\n```bash\n[me@my_computer dev]$ git clone https://github.com/WolframResearch/WolframLanguageForJupyter.git\n# Cloning into 'WolframLanguageForJupyter'...\n# remote: Enumerating objects: 649, done.\n# remote: Counting objects: 100% (140/140), done.\n# remote: Compressing objects: 100% (52/52), done.\n# remote: Total 649 (delta 93), reused 126 (delta 88), pack-reused 509\n# Receiving objects: 100% (649/649), 321.55 KiB | 2.23 MiB/s, done.\n# Resolving deltas: 100% (411/411), done.\n[me@my_computer dev]$ cd WolframLanguageForJupyter\n[me@my_computer WolframLanguageForJupyter]$ workon base\n(base) [me@my_computer WolframLanguageForJupyter]$ ./configure-jupyter.wls add\n(base) [me@my_computer WolframLanguageForJupyter]$\n```\n\n## 用法和效果\n\n打开 vscode，按 `Ctrl+Shift+P` 呼出命令搜索框，找到 \"Jupyter: Create Interactive Window\":\n\n![](/photos/2022-06-19-jupyter-start.png)\n\n单击 Jupyter 后端内核的图表（下图中右上角的\"base(Python 3.9.12)\"字样），把内核切换为 \"Wolfram Language ##\"\n\n![](/photos/2022-06-19-switch-kernel.png)\n\n等待后端内核切换完成，就可以输入 Mathematica 命令查看效果了：\n\n![](/photos/2022-06-19-final-result.png)\n\n完成！\n"},{"slug":"R-install-and-simple-syntax","filename":"2022-04-29-R-install-and-simple-syntax.md","date":"2022-04-29","title":".r | R语言入门笔记","layout":"post","keywords":["md","r","mpipks-note"],"excerpt":"本文也是马克思·普朗克复杂物理研究所《非平衡态集体过程》第9讲的笔记。","content":"\n> 本文也是马克思·普朗克复杂物理研究所《非平衡态集体过程》第9讲的笔记。\n> \n\n最近在蹭一门数学系的读书课，题目叫“非线性时间序列分析”。本来以为会是数学系的人抢物理系动力学方向的饭碗用的（可能实际上也确实是，或者更有可能是反过来），但是老师在前几讲一直把重点放在何种定理如何证明上面。数学系的嘛，这种对公理化系统的喜爱可以理解。但是发现我们几个上课的对这些证明都不太感兴趣之后，~~直接一个发卡弯漂移，教我们用 R 语言分析现实数据，现在新课还没上，不过可能是以股票数据做例子，我的老天鹅，这也太“经世济民”了吧……~~ 最后期末作业发现大家居然又都选择做 PPT 讲中心极限定理的证明，经世济民计划无疾而终。\n\n以下是为了新课程，我自己提前做的准备。\n\n## 下载、安装、环境配置（精神病版）\n\n- [https://cloud.r-project.org/](https://cloud.r-project.org/)\n- 看这一个链接就够了：[https://marketplace.visualstudio.com/items?itemName=Ikuyadeu.r](https://marketplace.visualstudio.com/items?itemName=Ikuyadeu.r)\n- [https://github.com/randy3k/radian](https://github.com/randy3k/radian)\n- [https://github.com/nx10/httpgd](https://github.com/nx10/httpgd)\n\n`Fedora Linux` + `R` + `radian` + `vscode` + `R extension for vscode`\n\n1. 在官网（[https://cloud.r-project.org/](https://cloud.r-project.org/)）下载并安装 R [解释器](https://python-interpreter)。\n2. 在命令行输入 `R` 打开解释器，在 R 中输入 `install.packages(\"languageserver\")` \n3. 在 vscode 的市场页面搜索 “R”，安装 R 语言插件。\n4. 安装 radian：\n    1. 在合适的 python [虚拟环境](https://pyhton-virtualenv) 里输入 `pip intall radian`\n    2. 用 radian 取代 R：编辑 `~/.bashrc` , 在其中加入一句`alias r=\"radian\"` ，重启命令行\n    3. 在 vscode 的设置中找到 `R>Rterm: Linux`, 输入 radian 的安装路径（在命令行中输入 `which R` 可以找到）\n5. 在 vscode 的市场页面搜索 “R debugger”，安装 `R debugger for vscode`\n6. 在命令行输入 `R` 打开解释器，在 R 中输入 `install.packages(\"httpgd\")`，安装可视化工具 `httpgd`\n\n## 简单语法\n\n来自马克思·普朗克复杂物理研究所《非平衡态集体过程》第9讲，这老师的讲课顺序简直了……\n\nR 的语法也简直了……\n\n### 简单数据结构 Some data types\n\n```r\n# vector\na <- c(1,2,3,NA)\nb <- c(\"m\",\"f\",\"f\",\"m\")\n# using a value\nb[2]\n\n# list\nlist.ab <- list(number=a, gender=b)\nlist.ab$number\nlist.ab[[1]]\n\n# factors (categorial variable)\nf <- factor(b)\nf\n# [1] m f f m f m m\n# levels: f m\nlevels(f) <- c(\"female\", \"male\")\n\n# dataframe (lists of vectors of the same length)\nx <- c(1,2,3)\ny <- c(\"aa\",\"bb\",\"cc\")\nz <- c(TRUE, FALSE, TRUE)\ndf <- data.frame(first=x, second=y, third=z)\nView(df)\n# |      | first | second | third |  \n# |    1 |     2 |     aa |  TRUE |\n# |    2 |     3 |     bb | FALSE |\n# |    3 |     5 |     cc |  TRUE |\n```\n\n### 控制流 Loops, conditional statements\n\n```r\n# for loop\nfor(i in 1:5) {\n\tprint(i)\n}\n# while loop\nwhile(!finished) {\n\tprint(\"Hello\")\n}\n# if statement\nif(i<5) {\n\tprint(\"Hello\")\n} else {\n\tprint(\"Not Hello\")\n}\n```\n\n### 函数 Functions\n\n```r\n# calling a function\nrnorm(5,mean=1,sd=1)\n\n# defining a function\nmysum <- function(a,b,c=1) { a + b + c }\nmysum(1,1)\n```\n\n### 复杂数据类型 `data.table`\n\nSome comparison:\n\n- `data.table`: R package, fast and memory efficient\n- `python.pandas`: python implementation of data frames\n- `dplyr`: highly popular, easy to learn\n\n```r\nlibrary(data.table)\ndt <- as.data.table(df)\n```\n\n```r\n# read and write\nflights <- fread(\"path/to/your/flights.txt\")\nweather <- fread(\"path/tp/your/weather.txt\")\n```\n\nMaking data tidy can simplify the following analysis.\n\n- Every column is an observable.\n- Every row is an observation.\n\n```r\n# making data tidy\nmelt( dt,\n      id.vars = \"ID\",\n      value.name = \"expression\",\n      variable.name = \"cell\"\n)\n# making data messy\ndcast(dt, ID ~ cell+expression) # not understand\n```\n\nGet item\n\n```r\nd[i, j, by] # take `d`, subset rows using `i`, then **calculate** `j` grouped by `by`\n# examples\nplanes[engines == 4]\n\n# slice the 2 columns\nplanes[, .(tailnum, year)] \n# groupby and calculate\nflights[, .(mean_delay = mean(dep_delay, na.rm=T)), by=carrier]\nflights[time_hour>20, .(mean_delay=mean(dep_delay,na.rm=T)), by=.(month, origin)]\n# math calculations\nflights[, speed_kmh := 60*1.61*distance/air_time]\nflights[, resc_distance := distance/mean(distance), by=carrier]\n```\n\nMerge 2 tables\n\n```r\n# left join\nmerge(a,b,all.x=T)\nb[a] # same as above\n\n# right join\nmerge(a,b,all.y=T)\na[b] # same as above\n\n# inner join\nmerge(a,b, all=F)\na[b, nomatch=0]\n\n# full join\nmerge(a,b,all=T)\n```\n\nChaining operations\n\n```r\n# data.table way\nweather[, ws_kmh:=1.61*wind_speed][, .(mean_ws=mean(ws_kmh)), by=month]\n# use operator %>% to take the result on the left as the 1st argument on the right\nlibrary(magrittr)\nweather[, ws_kmh:=1.61*wind_speed] %>%\n.[, .(mean_ws = mean(ws_kmh)), by=month] %>%\nhead()\n```\n"},{"slug":"rust-simple-syntax","filename":"2022-04-29-rust-simple-syntax.md","date":"2022-04-29","title":".rs | Rust 入门笔记","layout":"post","keywords":["md","rs"],"excerpt":"introduction to Rust and some simple syntax","content":"\nOn youtube.\n\n## What is Rust\n\nfast and powerful, system language\n\nweb development through webAssembly\n\nNo garbage collection, also no need to manege memory, which makes language more tedious\n\nUse Cargo to manage package\n\n## Install\n\nWindows: .exe\n\nLinux: run the curl script.\n\n```bash\nrustup --version\nrustc --version \ncargo --version\n```\n\nvscode Rust(rls) plugin\n\n## empty folder and compile\n\n```bash\nmkdir rustsandbox\ncd rustsandbox\ntouch hello.rs\n```\n\n```bash\nfn main() {\n\tprintln(\"Hello World!\");\n}\n```\n\n## initialize a project with cargo\n\n```bash\ncargo init\n```\n\nbettertmil plugin for highlight of `.tmil` file.\n\n```bash\ncargo run # compile and debug\ncargo build\ncargo build --release # for production\n```\n\n## print line\n\n```bash\ntouch src/print/rs\n```\n\n```rust\n// in print.rs\npub fn run() {\n\tprintln!(\"Hello World!\");\n\t// Basic formatting\n\tprintln!(\"Number: {}\",1); \n\tprintln!(\"{} is from {}\",\"Brad\",\"Mass\");\n\t// Positional arguments\n\tprintln!(\"{0} is from {1} and {0} likes to {2}.\",\"Brad\",\"Mass\",\"code\");\n\t// Named argument\n\tprintln!(\"{name} likes to play {activity}.\", name=\"John\", activity=\"baseball\")\n\t//Placeholder traits\n  println!(\"Binary: {:b} Hex: {:x} Octal: {:o}\", 10,10,10);\n\t//Placeholder for debuging\n  println!(\"{:?}\", (10,True,\"hello\"));\n\t//Basic Math\n\tprintln!(\"10+10={}\",10+10)\n}\n```\n\n```rust\n// in main.rs\nmod print;\n\nfn main() {\n\tprint::run();\n}\n```\n\n## Variable\n\n```rust\n// var.rs\npub fn run() {\n\tlet name=\"Brad\";\n\tlet age = 37;\n  age = 38; //cannot assign twice\n\tlet mut age = 37;\n\tage = 38;\n\tprintln!(\"My name is {} and I am {}\",name, age);\n\t// constant\n\tconst ID:i32 = 001;\n\tprintln(\"ID: {}\",ID);\n\t// assign multiple variables\n\tlet ( my_name, my_age ) = (\"Brad\", 37);\n\n}\n```\n\n## Primitive Types\n\n```rust\n// type.rs\n/*\nPrimitive Types:\nIntegers: u8,i8,u16,i16,...u128,i128\nFloats: f32,f64\nBoolean: bool\nCharacters: char\nTuples\nArrays\n*/\n\n// Rust is a static type language, but compiles usually can infer\n \npub fn run() {\n\t// default is i32, f64\n\tlet x = 1; \n\tlet y = 2.5;\n\tlet z: i64 = 454544445554;\n\t// find max size\n\tprintln!(\"Max i32:{}\", std::i32::MAX);\n\tprintln!(\"Max f64:{}\", std::f64::MAX);\n\t// boolean\n\tlet is_active = true;\n\t// get boolean from expression\n\tlet is_greater = 10 > 5;\n\t// char\n\tlet a1 = 'a'; // has to be single quotes\n\tlet a2 = 'ab'; // error, can only be one character\n\tlet a3 = '\\u{1F600}';\n}\n```\n\n## Strings\n\n```rust\n// string.rs\n\n// primitive str = immutable fixedlength string\n// String = growable heap-allocated data structure - use when I need to modify or own srtring data\n\npub fn run() {\n\tlet mut hello = String::from(\"Hello \");\n\t//get length\n\tprintln!(\"Length: {}\", hello.len());\n\thello.push('W');\n\thello.push_str(\"orld\");\n\t// immutable\n\tlet hello2 = \"Hello\";\n\t// methods\n\tprintln!(\"Capacity {}\", hello.capacity());\n\tprintln!(\"Is empty {}\", hello.is_empty());\n\tprintln!(\"Contains 'World' {}\", hello.contains(\"World\"));\n\tprintln!(\"Replace {}\", hello.replace(\"World\",\"there\"));\n\t// loop through string by whitespace\n\tfor word in hello.split_whitespace() {\n\t\tprintln!(\"{}\",word);\n\t}\n\t// Create string with capacity\n\tlet mut s = String::with_capacity(10):\n\ts.push('a');\n\ts.push('b');\n\t// Assertion testing\n\tassert_eq!(2,s.len());\n}\n```\n\n## tuple\n\n```rust\n//tuples.rs\n\n// can be different types\npub fn run() {\n\tlet person: (&str,&str,i8) = (\"Brad\",\"Mass\",37);\n\tprintln!(\"{} is from {} and is {}\", person.0, person.1, person.2);\t\n}\n```\n\n## Array\n\n```rust\n// array.ts\n\nuse std::mem\n\npub fn run() {\n\tlet numbers: [i32,5] = [1,2,3,4,5]; // length has to be exact\n\tprintln!({:?},numbers);\n\t// get single value\n\tprintln!(\"Single value{:?}\",numbers[0]);\n\tlet mut numbers1: [i32,5] = [1,2,3,4,5];\n\tnumbers1[2] = 20;\n\tprintln!(\"Array length: {}\", numbers1.len());\n\t//arrays are static allocated\n\tprintln!(\"Arry occupies {} bytes\", mem::size_of_val(&numbres1));\n\t// get slice\n\tlet slice: &[i32] = &numbres[0..2];\n\tprintln!(\"\")\n}\n```\n\n## Vector\n\n```rust\n//vector.rs\n\n// vectors are resizable arrays\npub fn run() {\n\tlet mut numbers: Vec<i32> = Vec![1,2,3,4];\n\t// add \n\tnumbers.push(5);\n\tnumbers.push(6);\n\t//pop off las values\n\tnumbres.pop();\n\t// loop through vector values\n\tfor x in numbers.iter(){\n\t\tprintln!(\"Number: {}\", x);\n\t}\n\tfor x in numbers.iter_mut(){\n\t\t*x *= 2; // no idea what the 1st * does\n\t\tprintln!(\"Number: {}\", x);\n\t}\n}\n```\n\n## conditional\n\n```rust\n//conditional.rs\n\npub fn run() {\n\tlet age: u8 = 18;\n\tlet check_id: bool = false;\n\tlet knows_person_of_age = true;\n\n\t// if/else\n\tif age >= 21 && check_id || knows_person_of_age {\n\t\tprintln!(\"Bartender: what would like to drink?\");\n\t} else if age < 21 && check_id {\n\t\tprintln!(\"Bartender: sorry you have to leave.\");\n\t} else {\n\t\tprintln!(\"Bartender: I'll need to see your ID.\");\n\t}\n\n\t// short if\n\tlet is_of_age = if age>=21 {true} else {false};\n\tprintln!(\"Is of age: {}\", is_of_age);\n}\n```\n\n## loop\n\n```rust\n//loop.rs\n\npub fn run() {\n\tlet mut count = 0;\n\t//infinite loop\n\tloop {\n\t\tcount += 1;\n\t\tprintln!(\"Number: {}\", count);\n\n\t\tif count > 20 {\n\t\t\tbreak;\n\t\t}\n\t}\n\t// while loop (FizzBuzz)\n\twhile count <=100 {\n\t\tif count%15 === 0 {\n\t\t\tprintln!(\"fizzbuzz\");\n\t\t} else if count%3 == 0 {\n\t\t\tprintln!(\"fizz\");\n\t\t\telse if count%5 == 0 {\n\t\t\tprintln!(\"buzz\");\t\n\t\t} else {\n\t\t\tprintln!(\"{}\",count)\n\t\t}\n\t\tcount += 1;\n\t}\t\n\t// for loop\n\tfor x in 0..100 {\n\t\t// ...\n\t}\n}\n```\n\n## function\n\n```rust\n// function.rs\npub fn run() {\n\tgreeting(\"Hello\",\"Jane\");\n\tlet get_sum = add(5,5);\n\t//closure\n\tlet n3 = 10;\n\tlet add_sums = |n1: i32, n2: i32| n1+ n2 + n3; //need to find out more\n}\n\nfn greeting (greet: &str, name: &str) {\n\tprintln!(\"{} {}, nice to meet you.\", greet, name);\n}\n\nfn add(n1: i32, n2: i32) -> i32 {\n\tn1 + n2 // no semi-colum here\n}\n```\n\n## Pointer/Reference\n\n```rust\n// pointer.rs\n\npub fn run() {\n\t// primitive\n\tlet arr1 = [1,2,3];\n\tlet arr2 = arr1;\n\tprintln!(\"Values: {:?}\", (arr1,arr2));\n\t// with non-primitive, if you assign another variable to a piece of data, the 1st variable will no longer hold that value. You'll need to use a reference (&) to point to the resource\n\tlet vec1 = Vec![1,2,3];\n\tlet vec2 = &vec1;\n\tprintln!(\"Values: {:?}\", (&vec1, vec2))\n}\n```\n\n## Struct\n\n```rust\n// struct.rs\n\n// used to create custome data types\n\n// traditional struct\nstruct Person {\n\tfirst_name: String,\n\tlast_name: String\n};\n\nimpl Person{\n\tfn new(first: &str, last: &str)-> Person {\n\t\tPerson {\n\t\t\tfirst_name: first.to_string(),\n\t\t\tlast_name: last.to_string()\n\t\t}\n\t}\n\tfn full_name(&self) -> String {\n\t\tformat!(\"{} {}\", self.first_name, self.last_name)\n\t}\n\t// set last name\n\tfn set_last_name(&mut self, last: &str){\n\t\tself.last_name = last.to_string();\n\t}\n\t// name to tuple\n\tfn to_tuple(self)->(String,String){\n\t\t(self.first_name,self.last_name)\n\t}\n}\n\nstruct Color {\n\tred: u8,\n\tgreen: u8,\n\tblue: u8,\n};\n\n// tuple struct\nstruct ColorTuple(u8,u8,u8); \n\npub fn run() {\n\tlet mut c = Color{\n\t\tred: 255,\n\t\tgreen: 0,\n\t\tblue: 0,\n\t};\n\tc.red = 200;\n\tprintln!(\"Color: {},{},{}\",);\n\t\n\tlet mut ct = ColorTuple(255,0,0);\n\tct.0 = 200;\n\tprintln!(\"Color: {},{},{}\",ct.0,ct.1,ct.2);\n\n\tlet mut p = Person::new(\"John\",\"Doe\");\n\tprintln!(\"Person {} {}\", p.first_name, p.last_name)\n\tp.set_last_name(\"Williams\");\n\tprintln!(\"Person {}\", p.full_name());\n\tprintln!(\"Person {:?}\", p.to_tuple());\n}\n```\n\n## enumerate\n\n```rust\n//enum.rs\n\n// enum is a type with a few definitive values\n\nenum Movement {\n\t// variants\n\tUp, \n\tDown,\n\tLeft, \n\tRight\n}\n\nfn move_avatar(m:Movement) {\n\tmatch m {\n\t\tMovement::Up => println!(\"Avatar Moving Up\"),\n\t\tMovement::Down => println!(\"Avatar Moving Down\"),\n\t\tMovement::Left => println!(\"Avatar Moving Left\"),\n\t\tMovement::Right => println!(\"Avatar Moving Right\")\n\t}\n}\n\npub fn run() {\n\tlet avatar1 = Movement::Up;\n\tlet avatar2 = Movement::Down;\n\tlet avatar3 = Movement::Left;\n\tlet avatar4 = Movement::Right;\n\n\tmove_avatar(avatar1);\n\tmove_avatar(avatar4);\n\tmove_avatar(avatar2);\n\tmove_avatar(avatar3);\t\n}\n```\n\n## Command line interface\n\n```rust\n// cli.rs\n\nuse std::env;\n\npub fn run() {\n\tlet args: Vec<String> = env::args().collect();\n\tlet command = args[1].clone();\n\tlet name = \"Brad\";\n\tlet status  = \"100%\"\n\n\tprintln!(\"Args: {:?}\", args);\n\tprintln!(\"Command: {}\", command)\n\n\tif command == \"Hello\" {\n\t\tprintln!(\"Hi {}, how are you?\", name);\n\t} else if command == \"status\" {\n\t\tprinln!(\"Status is {}\", status);\n\t} else {\n\t\tprintln!(\"That is not a valid command.\");\n\t}\n}\n```"},{"slug":"typescript-simple-syntax","filename":"2022-04-29-typescript-simple-syntax.md","date":"2022-04-29","title":".ts | TypeScript 入门笔记","layout":"post","keywords":["md","js","ts"],"excerpt":"An introduction to Typescript and some simple syntax","content":"\nWhat is TypeScript:\n\n- static types\n- types from 3rd parties can be added with type definition\n\nDynamic vs Static types\n\n- dynamic: types associated with run-time values, and not explicitly in code\n- static: explicitly assign types\n\nPros and Cons\n\n- pros: more robust, debug, predictability\n- cons: more codes to write, requires compilation, **not true typing**\n\nCompiling\n\n- `.ts` or `.tsx` extension\n- `TSC` is used\n- `tsconfig.json`\n\n# Hands on\n\n## Install\n\n```bash\nsudo npm i -g typescript\ntsc -v\ntouch index.ts\n```\n\n```tsx\nlet id: number = 5 \nid = `5` // error\n\n```\n\n```bash\ntsc index\ntsc -- watch index\n```\n\n```bash\ntsc --init # produces tsconfig.json\n```\n\n```json\n{\n\t\"outDir\": \"./dist\",\n\t\"rootDir\": \"/src\"\n}\n```\n\n## Types\n\n```tsx\nlet id: number = 5\nlet company: string = \"Media\"\nlet isPublished: boolean = true\nlet x: any = \"hello\"\nx = true // no errors\n\n// array\nlet ids: number[] = [1,2,3,4,5]\nids.push(\"hello\") // error\n\nlet arr: any[] = [1, true, 'hello']\n\n//tuple\nlet person: [number, string, boolean] = [1, 'Brad', true]\n\n// tuple array\nlet employee: [number, string][]\nemployee = [\n\t[1, 'Brad'],\n\t[2, 'Sam']\n]\n\n// union\nlet pid: string | number = 22\n\n// enum\nenum Direction1 {\n\tUp, // default is 0\n\tDown,\n\tLeft,\n\tRight\n}\n\nenum Direction2 {\n\tUp   = \"Up\", // default is 0\n\tDown = \"Down\",\n\tLeft = \"Left\",\n\tRight= \"Right\"\n}\n\n//objects\nconst user: {\n\tid: number,\n\tname: string\n} = {\n\tid: 1,\n\tname: 'John'\n}\n\ntype User = {\n\tid: number,\n\tname: string\n}\n\nconst user: User\n\n// type assertion\nlet cid: any = 1\nlet customerId = <number>cid\nlet customerId = cid as number\n\n```\n\n## function\n\n```tsx\nfunction addNum(x: number,y: number): number {\n\treturn x+y\n}\n//void\n\nfunction log(message: number | string): void {\n\tconsole.log(message)\n}\n```\n\n## Interfaces\n\n```tsx\ninterface UserInterface {\n\treadonly id: number,\n\tname: string,\n\tage?: number // optional property\n\tregister(): string\n}\nconst user1: UserInterface = {\n\tid: 1, // error , read only\nname: \"brad\"\n}\n\ninterfce MathFunc {\n\t(x: number, y: number): number\n}\n\nconst add: MathFunc = (x: number, y: number): number => x+y\n```\n\n## Class\n\n```tsx\nclass Person implements UserInterface{\n\tprivate id: number\t // within the class\n\tprotected name: string // within class or extended classes\n\tsalary: number\n\n\tconstructor(id: number, name: string) {\n\t\tthis.id = id\n\t\tthis.name = name\n\t}\n\tregister() {\n\t\treturn `${this.name} is registered.`\n\t}\n}\n\nconst brad = new Person(1,'Brad')\n```\n\n## Subclass\n\n```tsx\nclass Employee extends Person {\n\tposition: string\n\n\tconstructor(id: number, name: string, position: string) {\n\t\tsuper(id,name)\n\t\tthis.position = position\n\t}\n}\nconst emp = new Employee(3,\"Shawn\",\"developer\")\nconsole.log(emp.register())\n\n```\n\n## Generics\n\n```tsx\nfunction getArray(items: any[]): any[] {\n\treturn new Array().concat(items)\n}\nlet numArray = getArray([1,2,3,4])\nlet strArray = getArray(['a','b','c','d'])\n\nnumArray.push('hello') // no error, but not what we want\n\nfunction getArray<T>(items: T[]): T[] {\n\treturn new Array().concat(items)\n}\nlet numArray = getArray<number>([1,2,3,4])\nnumArray.push('hello') // error\n```\n\n## With React\n\n```bash\nnpx create-react-app . --template typescript\nnpm start\ntouch header.tsx\n```\n\n```tsx\nexport interface Props{\n\ttitle: string\n\tcolor?: string\n}\nconst Header = (props: Props) => {\n\treturn (\n\t\t<header>\n\t\t  {% raw %}\n\t\t\t<h1 style={{ color: prps.color ? props.color: \"blue\" }}>\n\t\t\t\t{props.title}\n\t\t\t</h1>\n\t\t  {% endraw %}\n\t\t</header>\n\t)\n}\n```"},{"slug":"tensorboard-on-pytorch","filename":"2022-04-08-tensorboard-on-pytorch.md","date":"2022-04-08","title":".py | TensorBoard 笔记（PyTorch 版）","layout":"post","keywords":["md","py","ai"],"excerpt":"TensorBoard 是 TensorFlow 团队开发的一款可视化工具，方便观察和调整机器学习的数据集、模型、超参数和训练结果等等。但是不知道为什么，PyTorch 调用 TensorBoard，要比 TensorFlow 方便简单得多，<del>这何尝不是一种 NTR</del>……","content":"\n- 官网教程：[https://pytorch.org/tutorials/intermediate/tensorboard_tutorial.html](https://pytorch.org/tutorials/intermediate/tensorboard_tutorial.html)\n- 官方文档：[https://pytorch.org/docs/stable/tensorboard.html](https://pytorch.org/docs/stable/tensorboard.html)\n\nTensorBoard 是 TensorFlow 团队开发的一款可视化工具，方便观察和调整机器学习的数据集、模型、超参数和训练结果等等。但是不知道为什么，PyTorch 调用 TensorBoard，要比 TensorFlow 方便简单得多，~~这何尝不是一种 NTR~~……\n\n---\n\n## 用法和效果\n\n一个使用了 TensorBoard 的 torch 项目的主文件一般是这样的（把和 TensorBoard 无关的部分都注释掉了）：\n\n```python\n# imports\n# import matplotlib.pyplot as plt\n# import numpy as np\n\n# import torch\n# import torchvision\n# import torchvision.transforms as transforms\n\n# import torch.nn as nn\n# import torch.nn.functional as F\n# import torch.optim as optim\n\nfrom torch.utils.tensorboard import SummaryWriter\n\n# # transforms\n# transform = transforms.Compose(\n#     [transforms.ToTensor(),\n#     transforms.Normalize((0.5,), (0.5,))])\n\n# # datasets\n# trainset = torchvision.datasets.FashionMNIST('./data',\n#     download=True,\n#     train=True,\n#     transform=transform)\n# testset = torchvision.datasets.FashionMNIST('./data',\n#     download=True,\n#     train=False,\n#     transform=transform)\n\n# # dataloaders\n# trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n#                                         shuffle=True, num_workers=2)\n# testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n#                                         shuffle=False, num_workers=2)\n\n# # constant for classes\n# classes = ('T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n#         'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle Boot')\n\n# class Net(nn.Module):\n#     def __init__(self):\n#         super(Net, self).__init__()\n#         self.conv1 = nn.Conv2d(1, 6, 5)\n#         self.pool = nn.MaxPool2d(2, 2)\n#         self.conv2 = nn.Conv2d(6, 16, 5)\n#         self.fc1 = nn.Linear(16 * 4 * 4, 120)\n#         self.fc2 = nn.Linear(120, 84)\n#         self.fc3 = nn.Linear(84, 10)\n#     def forward(self, x):\n#         x = self.pool(F.relu(self.conv1(x)))\n#         x = self.pool(F.relu(self.conv2(x)))\n#         x = x.view(-1, 16 * 4 * 4)\n#         x = F.relu(self.fc1(x))\n#         x = F.relu(self.fc2(x))\n#         x = self.fc3(x)\n#         return x\n# net = Net()\n\n# criterion = nn.CrossEntropyLoss()\n# optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n\n# default `log_dir` is \"runs\" - we'll be more specific here\nwriter = SummaryWriter('runs/fashion_mnist_experiment_1')\n\nrunning_loss = 0.0\nfor epoch in range(1):  # loop over the dataset multiple times\n    for i, data in enumerate(trainloader, 0):\n        # # get the inputs; data is a list of [inputs, labels]\n        # inputs, labels = data\n\n        # # zero the parameter gradients\n        # optimizer.zero_grad()\n\n        # # forward + backward + optimize\n        # outputs = net(inputs)\n        # loss = criterion(outputs, labels)\n        # loss.backward()\n        # optimizer.step()\n\n        # running_loss += loss.item()\n        if i % 1000 == 999:    # every 1000 mini-batches...\n            # ...log the running loss\n            writer.add_scalar('training loss',\n                            running_loss / 1000,\n                            epoch * len(trainloader) + i) # 注意这一行！\n# print('Finished Training')\n```\n\n正常情况下，使用了tensorboard 的项目在训练的过程中，可以用网页浏览器打开网址 `localhost:6006`，应该可以看到和下图类似但不同的画面：\n\n![tensorboard](/photos/2022-04-08-tensorboard.png)\n\n下面来仔细分解。\n\n## 代码分解\n\n[引入](python-import-script-module-package) TensorBoard 需要下面一行代码：\n\n```python\nfrom torch.utils.tensorboard import SummaryWriter\n```\n\n从名字就能看出来，`SummaryWriter` 是一个 class。粗略用了一下文档页面的业内搜索，好像整个 `torch.utils.tensorboard` 就只有这一个 class。\n\n新建一个这个类的实例：\n\n```python\nwriter = SummaryWriter('runs/fashion_mnist_experiment_1')\n```\n\n这一步会在当前工作环境下新建一个 `/runs` 文件夹，\n\n要想显示导航栏上的“SCALARS”、“IMAGES”等等选项卡，并且让自己想观察的数据显示在各自类别的选项卡里，需要调用 `SummaryWriter` 下面的各种方法，比如 `add_scalar()`, `add_image()`.\n\n### 各种方法\n\n用法和效果举例如下：\n\n- `add_scalar()`: 在一张图中画出**一个标量**指标随学习迭代的**变化曲线**。\n    - `tag`: 图片的标题。\n    - `scalar_value`: 指标的值，也就是纵坐标。\n    - `global_step`: 全局迭代次数，也就是横坐标。\n- `add_scalars()`: 在一张图中同时画出**多个指标**随学习迭代的**变化曲线**。\n    - `main_tag`: 图片的标题\n    - `tag_scalar_dict`: 一个字典，字典的键是变量的名字，值是各个变量的纵坐标。\n    - `global_step`: 全局迭代次数，也就是横坐标。\n- `add_custome_scalars()`: 没有用过，也没见到例子，不太明白。根据描述像是把之前 `add_scalar()` 和 `add_scalars()` 的结果重新组合，对 SCALARS 选项卡重新排版。每个 `SummaryWriter` 只能运行一次，可以在训练开始前运行，也可以在之后。\n    - `layout`: 只有一个这参数，是一个字典，字典的键像是新图片/章节的名字，值是下一级字典或者是 `add_scalar()` 出现过/将要出现的 `tag` 参数。\n- `add_figure()`: 显示 `matplotlib` 画出的**图表**。\n    - `tag`: 标题\n    - `figure`: 图表，要求类型为 `matplotlib.pyplot.figure`\n    - `global_step`:迭代次数，效果如何 没试过。\n- `add_histogram()`: 在一张图中画出一个样本的**直方图**，以及这个直方图随迭代变化的规律。这是个三维图片，x 轴是直方图的取值范围，y 轴是迭代次数，z 轴是直方图的频率值。\n    - `tag`: 图片标题。\n    - `values`: 一个 `torch.Tensor` 或者 `numpy.array` ，用于绘制直方图的样本.\n    - `global_step`: 迭代次数，y 轴分量。\n    - `bins`: 取样间隔参数，`numpy.histogram()` 中用到的。 ****\n- `add_graph()`: 一般用于在训练前画出神经网络的**图状结构**。\n    - `model`: 要画的模型，类型是 `torch.nn.Module`\n    - `input_to_model=None`: （可选）输入模型的变量。\n- `add_mesh()`: **三维点云**。\n    - `tag`: 表格标题。\n    - `vertices`: 顶点三维坐标的列表。\n    - `colors`: 顶点的颜色。\n    - `faces`: （可选）没看懂 (Indices of vertices within each triangle.)\n    - `config_dict`: 用于画图的 ThreeJS 的参数。\n    - `global_step`: 迭代次数。\n- `add_embeddding()`: 神经网络的输入一般是高维向量，此工具将高维数据**投影到三维**空间，然后画出图像，方便我们感知训练集内样本之间的关系。\n    - `mat`: 一个矩阵，每一行都是一个要处理的向量。\n    - `metadata`: 标记文字，一般是列表。\n    - `label_img`: 标记图片，显示在每个数据点旁边的\n    - `global_step`: 迭代次数，一般没人用。\n    - `tag`: 图片标题。\n- `add_pr_curve()`: 准确率 (precision) -召回率 (call back) 曲线。准确率=真阳性/(真阳性+假阳性)，召回率=真阳性/(真阳性+假阴性)\n    - `tag`: 图片标题。\n    - `labels`: Ground truth 数据，每个数据点对应一个布尔值。\n    - `predictions`: 模型的输出，每个数据点对应一个 [0,1] 之间的实数。\n    - `global_step`: 迭代次数。\n    - `num_thresholds`:用于画出 PR 曲线的阈值的数目。\n- `add_hparams()`: 比较不同次运行之间的超参数。没太看懂。\n    - `hparam_dict`: 超参数的名称和取值\n    - `metric_dict`: metric （不知道怎么翻译）的名称和取值\n    - `hparam_domain_discrete`: （可选）字典，超参数的名称和有限个可能的取值。\n    - `run_name`: 运行的名称，将会成为 `logdir` 的一部分。\n- `add_image()`: 在 IMAGES 选项卡中显示**一张图片**。\n    - `tag`: 图片名称。\n    - `img_tensor`: 一个 `torch.Tensor` 或者 `numpy.array`，被显示的图片。对应于下面的 `dataformats` 参数。\n    - `global_step`: 迭代次数，没见有人在显示图片的时候用过。\n    - `dataformats=’CHW’`: 图片各维度的顺序。默认是“颜色-高度-宽度”。\n- `add_images()`: 并列显示**多张图片**。\n    - `tag`: 图片组的标题。\n    - `img_tensor`: 一个 `torch.Tensor` 或者 `numpy.array`，被显示的图片。图片个维度的含义由 `datadormats` 给出。\n    - `global_step`: 迭代次数，没见有人在显示图片的时候用过。\n    - `datadormats=’NCHW’`: 图片各维度的顺序。默认为“图片序号-颜色-高度-宽度”。\n- `add_video()`: 略\n- `add_audio()`: 略\n- `add_text()`: 略"},{"slug":"git-multiple-users","filename":"2021-12-26-git-multiple-users.md","date":"2021-12-26","title":".git | 管理 GitHub 不同用户身份的仓库","layout":"post","keywords":["md","git"],"excerpt":"多用户多账户，如何告诉 GitHub 某个项目文件夹该由哪个账号来做版本管理。","content":"\n很显然，我不可能把“阿掖山”这个名字写到论文里，与研究相关的项目、组里的代码和数据，都由另一个实名的 GitHub 账号来处理。这就有个问题——如何告诉 GitHub 某个项目文件夹该由哪个账号来做版本管理。\n\n于是 STFW，结果看到了GitHub  官方的回答：[“这边建议您把两个账号合并呢～”](https://docs.github.com/en/account-and-profile/setting-up-and-managing-your-github-user-account/managing-user-account-settings/merging-multiple-user-accounts)\n\n其实这个问题在建立这个博客站之前就解决了，但是因为很长时间我的研究代码都是自己在用，而且只在台式机上用，一直没有推到 GitHub 上去，实名账号一直没在笔记本上用过。具体细节忘得差不多了，这次复习一下。\n\n作为例子，两个账号的用户名分别是 `USER1` 和 `USER2`，注册邮箱分别是 `USER1@EMAIL.com` 和 `USER2@EMAIL.com`。\n\n## 多个计算机用户\n\n当然了，最简单的方法就是新建一个操作系统用户，每个用户登录一个 GitHub 账号。这种方法好处很多：\n\n- 不需要特殊操作。\n- 适用于不同操作系统。\n- 切换身份需要专门切换账号，有助于防止操作者忘记自己所处的身份。\n\n但是这篇文章不会涉及这种方法，因为当初买电脑的时候并没有注意到需要做身份隔离，等到发现事情不妙的时候已经混装和两个身份需要的不同软件，积重难返。于是采用了以下两节的解决方案。\n\n## 单个计算机用户@Windows: GitHub Desktop\n\n![](/photos/2021-12-26-github-desktop.png)\n\n- 在官网下载、安装、打开 [GitHub Desktop](https://desktop.github.com/) 客户端。\n- 在 GitHub 网页版上切换到新的账户。\n- 点击左上角 `File > Options`，默认界面就是账户信息。点击 `Sign Out` 退出登录，然后再点击 `Sign In`，根据弹出窗口的提示操作，就来到了新的账户。\n\n## 单个计算机用户@Linux : `.ssh/config`\n\n具体操作看以下两个连接就够了：\n\n- [https://gist.github.com/JoaquimLey/e6049a12c8fd2923611802384cd2fb4a](https://gist.github.com/JoaquimLey/e6049a12c8fd2923611802384cd2fb4a)\n- [https://docs.github.com/en/authentication/connecting-to-github-with-ssh](https://docs.github.com/en/authentication/connecting-to-github-with-ssh)\n\n~~但是为了水字数，~~ 还是写得详细一点……\n\n### 本地：生成并启用 SSH key\n\n打开命令行，输入以下命令，生成 SSH key：\n\n```bash\n\nssh-keygen -t rsa -b 4096 -C \"USER1\" \nssh-keygen -t rsa -b 4096 -C \"USER2\"\n```\n\n其中 `-t` 指定加密算法，`-b` 指定密钥的位数，`-C` 相当于注释。\n\n然后命令行会弹出几个选项，可以一路按回车使用默认值。\n\n上述命令完成后，在 `~/.ssh` 文件夹应该会有两对四个密钥文件：\n\n- `~/.ssh/USER1`\n- `~/.ssh/USER1.pub`\n- `~/.ssh/USER2`\n- `~/.ssh/USER2.pub`\n\n然后输入以下命令，启用刚刚生成的密钥。\n\n```bash\n\neval \"$(ssh-agent -s)\" # 启动 ssh-agent\nssh-add ~/.ssh/USER1   # 添加 USER1 的密钥\nssh-add ~/.ssh/USER2   # 添加 USER2 的密钥\n```\n\n### 网页：把密钥添加到对应的账号\n\n在网页端以 `USER1` 身份登录 GitHub 之后，在 \"Settings\" 页面找到 \"SSH and GPG keys\" 选项卡，点击绿色的 \"New SSH key\" 按钮之后，将 `~/.ssh/USER1.pub` 中的内容复制到 \"Key\" 填空区，然后起一个名字，点击 \"Add SSH key\" 按钮。\n\n![](/photos/2021-12-26-github-key.png)\n\nUSER2也照此办理。\n\n**千万要注意**复制的应该是 `.pub` 后缀的文件！\n\n### 本地：编辑 `~/.ssh/config` 文件\n\n在 ~/.ssh/ 找到或者新建一个名为 config 的文本文件。打开之后，将以下内容添加到文件中：\n\n```\n\nHost github.com-user1\n\tHostname github.com\n\tUser git\n\tIdentityFile ~/.ssh/user1\nHost github.com-user2\n\tHostname github.com\n\tUser git\n\tIdentityFile ~/.ssh/user2\n```\n\n### `git clone` 时 repo 地址的改动\n\n一般的 git clone, 直接把 GitHub 提供的命令复制粘贴到命令行就行了。\n\n![](/photos/2021-12-26-git-clone.png)\n\n但是我们这个不同，首先是只能选择 SSH 模式，然后是需要在域名`github.com` 后面加上`-user1`:\n\n```bash\n\ngit clone git@github.com-user1:User/Repo.git\n```\n\n这里就体现出之前在 `~/.ssh/config` 把 Host 命名为 `github.com-****` 的好处了。\n\n### `git commit` 前填写在 repo 中填写账户信息\n\n 第一次做完改动推送到 GitHub 之前，需要专门在 repo 级别写明自己的身份，也就是在命令行输入：\n\n```bash\n\ngit config --local user.name  \"USER1\"\ngit config --local user.email \"USER1@EMAIL.com\"\n```\n\n### **注意：** 为了防止操作者忘记自己所处的身份\n\n强烈建议去掉用户信息的全局设置：\n\n```bash\n\ngit config --global --unset user.name\ngit config --global --unset user.email\n```\n\n这样假如忘记之前的 `git config --local`, 第一次 git commit 的时候会报错，提示信息缺失。\n\n这样操作一次之后，之后的操作几乎感受不到账户的不同。\n"},{"slug":"python-import-script-module-package","filename":"2021-12-11-python-import-script-module-package.md","date":"2021-12-11","title":".py | import 引用现成的代码","layout":"post","keywords":["md","py"],"excerpt":"正常的编程语言教程，教人配置完开发环境之后就应该进入正题，开始讲语法了。但是咱不正常，所以先来谈谈怎么用别人已经写好的代码。","content":"\n以官网给出的文件结构为例来说明：\n\n```\n\nsound/                          Top-level package\n      __init__.py               Initialize the sound package\n      formats/                  Subpackage for file format conversions\n              __init__.py\n              wavread.py\n              wavwrite.py\n              aiffread.py\n              aiffwrite.py\n              auread.py\n              auwrite.py\n              ...\n      effects/                  Subpackage for sound effects\n              __init__.py\n              echo.py\n              surround.py\n              reverse.py\n              ...\n      filters/                  Subpackage for filters\n              __init__.py\n              equalizer.py\n              vocoder.py\n              karaoke.py\n              ...\n```\n\n## 使用现成的 python 代码\n\n正常的编程语言教程，教人配置完开发环境之后就应该进入正题，开始讲语法了。但是咱不正常，所以先来谈谈怎么用别人已经写好的代码。其中最简单的，就是可以直接通过包管理程序安装的：\n\n```bash\n\npip install sound\n```\n\n然后想要使用某个文件中的函数，比如假装 `wavwrite.py` 中有个函数叫 `write()`，以下写法都是可以的，注意不同 import 方法对应不同的函数调用写法：\n\n```python\n\nimport sound\nsound.formats.wavwrite.write()\n\nfrom sound import formats\nformats.wavwrite.write() \n\nfrom sound.formats import wavwrite\nwavwrite.write()\n\nfrom sound.formats.wavwrite import write\nwrite()\n```\n\n但是，不是所有的 python 代码都可以直接安装，比如一篇论文的研究成果发表之后，处理数据的代码也往往开源，但是这些作者基本上就只是把自己写代码的文件夹公开出来而已，我们把文件夹下载下来，然后直接 `import sound`, 会报错，提示找不到名为 sound 的库。\n\n## python 如何读取代码文件\n\n仔细想想，找不到才是正常的，之前轻轻松松的一句 `import sound`就解决问题，这才不简单——不同的库往往位于文件系统的不同位置，但我们只要写出他们的名字就行了，不需要指定文件路径。电脑硬盘那么大，找到库却几乎是瞬间完成的。\n\n这是因为 python 并没有搜索整个硬盘。有一个变量，一般名为 `PYTHONPATH`，其变量值是一个列表，表中成员是含有 python 库文件夹的路径。当我们在命令行输入命令的时候，电脑会：\n\n- 搜索当前所在的文件夹，也就是在命令行输入 `python` 时终端所在的文件夹。\n- 遍历 `PYTHONPATH` 中的文件夹。\n- python 包管理程序默认的位置，一般是 `<path to python>/site-package`。\n\n看看有没有我们要引用的库，找到了就引入，找不到就报错。\n\n上一节的错误中，如果我们恰好位于 sound 所在的文件夹，然后运行 python，此时第一条生效， `import sound` 不会报错，但在其他位置就不行了。\n\n## 名词解释：interactive, script, module, package\n\n可执行的 python 命令可以出现在以下四个地方，第一种是接受键盘输入的程序，后三种都是文件：\n\n1. interactive: python **交互式界面**，也叫做 calculator mode，也就是在命令行输入 `python`之后出现的界面。每次输入一句，结果在命令行上显示出来。当 python 退出之后，输入过的命令就消失了。\n2. script: python **脚本**文件，也就是在命令行输入 `python somefile.py`里面的那个`somefile.py`。\n    1. 毕竟 python 是一种很轻量化的语言，在一定程度上可以起到 shell 的作用，有些命令我们并不想要用完就扔，而是保存起来以便以后重复执行，另外很多命令的组合组合成函数也可以极大地简化工作。在这种语境之下, interactive 和 script 的关系，就好像 Linux 命令行和 bash script 的关系一样。\n    2. 但同时 python 又是一种功能很全面的语言，完全可以胜任复杂的面向对象编程。在这种语境之下，script 也可以用来指代 main module，也就是程序执行的主文件和入口，和下面的一般的 module 相区分。\n3. module: python **模块**文件，也就是在命令行输入 `python -m another` 里面的那个 `another`（注意这里不写拓展名 `.py`）。按照官方文档的说法，所有 `.py` 文件都是 module。但是实际上这句话很有误导性，上一节的 main module 和一般的 module 非常不同，下一节会详细展开讲。一般提到 module，都是在强调这个文件定义的变量和函数可以被其他的 python 文件引用。\n4. package: python **包**，互相关联的 modules 构成的更高一级的可供引用的结构，简单理解就是含有 `__init__.py` 的文件夹，但是 python 并不是根据文件夹和文件之间的从属关系来确定 package 和 module 之间的关系的，下一节会详细展开讲。\n\n## script vs. module\n\npython 同时兼具脚本语言的灵活性，和各种重型语言的功能全面性。因为前者，所以它并不要求程序作者一定要在一个叫做 `main.py` 的文件里写一个名叫 `Main` 的类, 然后在里面实现一个 `main()` 方法。但是因为后者，没写不代表 python 不需要知道一个复杂程序执行的起点。\n\n这个起点就是不带有 `-m` 参数的 `python` 命令后面跟着的 `.py` 文件，这就使得这个文件变得比其他 `.py` 文件特殊。底层表现就是 python 会不管这个文件的名字叫什么，都将它的 `__name__` 属性赋值为 `\"__main__\"`。这样，即便这个文件可能是一个大型库中间的一个模块，运行的时候 python 连它的真名都不知道，就更找不到它同级和上下级的其他模块了。\n\n各种普通模块被 python 用到的方法就是通过在主模块 main module（或者说 script）中 import。经过“python 如何读取代码文件”一节中的搜索过程之后找到了所需模块或包，模块的名字、模块之间的关系、模块里定义了哪些属性和函数，就被 python 了解了，从而当主模块召唤他们的时候就知道去哪里找相应的代码。除了在被 import 的时候，`python -m` 命令的宾语也可以告诉 python 被运行的模块和包的相对关系：`python -m sound.formats.wavwrite` ，此时 python 执行了 `wavwrite.py` 中的所有可执行的命令，同时知道从 `sound/` 到 `wavwrite.py` 的各个包之间的关系。\n\n## absolute import vs. relative import\n\n开头使用已经安装过的包使用的语法全都是绝对引用 (absolute import)，表现就是 import 语句里面没有以 `.` 作为开头的。\n\n另外一种 import 方法叫相对引用 (relative import)，`.` 表示模块所在的文件夹，`..` 表示模块的上一级文件夹。主要用在各种明确知道自己是工具代码，而且是一个更高层次结构的组成部分，几乎永远不需要被作为主模块运行的代码。\n\n回到开头例子里的文件结构，假如 sound/effects/surround.py 中想要使用 sound/formats/wavwrite.py 和 sound/effects/echo.py 中的函数，可以写成：\n\n```python\n\n# in sound/effects/surround.py\nfrom ..formats import wavwrite\nfrom . import echo\n```\n\n## 如何组织代码，以便自己重用\n\n研究终于推进到了准备写论文的阶段了（学渣本质暴露了），写草稿之余，之前几年时间里做过的处理和分析，接下来的一两个月里需要把工作流程规范化之后迅速重做一遍确认。\n\n随手写散落各处的分析代码需要整理到一起，之前试图统一到一个项目之下，结果总是在某个模块引用其他模块的时候遇到报错。于是才有了这篇文章。\n\n以下是 [这篇文章](https://gist.github.com/ericmjl/27e50331f24db3e8f957d1fe7bbbe510) 给出的一个推荐的项目文件结构：\n\n```\n\n|- notebooks/\n   |- 01-first-logical-notebook.ipynb\n   |- 02-second-logical-notebook.ipynb\n   |- prototype-notebook.ipynb\n   |- archive/\n\t  |- no-longer-useful.ipynb\n|- projectname/\n   |- projectname/\n\t  |- __init__.py\n\t  |- config.py\n\t  |- data.py\n\t  |- utils.py\n   |- setup.py\n|- README.md\n|- data/\n   |- raw/\n   |- processed/\n   |- cleaned/\n|- scripts/\n   |- script1.py\n   |- script2.py\n   |- archive/\n      |- no-longer-useful.py\n|- environment.yml\n```\n\n学过这篇笔记包含的内容，我才理解作者这样的安排。既然主文件 ~~很难~~ 没办法通过相对引用来找到工具代码，索性就把工具代码写成一个完整可安装的库，然后就像 `numpy`, `pandas` 一样在独立的 notebook 和 scripts 中引用。\n\n实际使用的时候，需要安装 `projectname` 下的代码：\n\n```bash\ncd projectname\npip install -e .\n```\n\n要知道为什么这样做，需要理解 `setup.py` 这个文件。这篇文章已经够长了，所以这个话题还是下次再说吧。\n\n## 参考链接\n\n- [What's the difference between a Python module and a Python package?](https://stackoverflow.com/questions/7948494/whats-the-difference-between-a-python-module-and-a-python-package). all python files re modules, while package is a specific kind of modules. It is a subsection of module in the python documentation.\n- Official explanation of python module: [https://docs.python.org/3/tutorial/modules.html](https://docs.python.org/3/tutorial/modules.html)\n- [This stackoverflow  answer: \"run as module\" is different from \"run as script\".](https://stackoverflow.com/questions/14132789/relative-imports-for-the-billionth-time) run as module sets the \"name\" to the module's name, while running as script sets it to `__main__` . Here we use \"name\" instead of `__name__` because it also contains `__path__` in newer versions.\n- A tutorial for project organization: [https://realpython.com/python-application-layouts/](https://realpython.com/python-application-layouts/)\n- Official about packaging: [https://packaging.python.org/en/latest/tutorials/packaging-projects/](https://packaging.python.org/en/latest/tutorials/packaging-projects/)\n- Gist: How to organize data science project: [https://gist.github.com/ericmjl/27e50331f24db3e8f957d1fe7bbbe510](https://gist.github.com/ericmjl/27e50331f24db3e8f957d1fe7bbbe510)\n- From the gist there is a link: [http://drivendata.github.io/cookiecutter-data-science](http://drivendata.github.io/cookiecutter-data-science)"},{"slug":"blog-update-theme-materalize","filename":"2021-12-06-blog-update-theme-materalize.md","date":"2021-12-06","title":".md | 博客外观现代化升级","layout":"post","keywords":["md"],"excerpt":"<s>本期博文点赞过 5 亿，下篇文章写用 jekyll 在 GitHub Pages 上搭建静态博客的教程。</s>","content":"\n今年感恩节三天假期加上周末，一共五天的时间，别的啥也没干，憋在家里给博客换了个主题模板。现在新版本已经上线，基本上已经能用，明显的 bug 都已经解决了。主要的工作内容如下：\n\n- 从 Tufte 风格转换到 material 风格\n- 评论区迁移到现成的 ~~[utterance](https://utteranc.es/)~~  [giscus](https://giscus.app/)。\n- 将旧模版中的侧边栏注记功能移植到新的模板。\n- 调整了 CSS，包括字体、代码模块、博客标题限制高度、博客博客实现类似纸张的卡片效果。\n- 重写了 index.html, /History.html, /Links.html 等页面，尤其是 /Topics.html，实现了一个响应式的两列结构。\n- 重写了博文前面的元信息。\n- 将之前的 repo 重命名并且归档，新建一个同名的 repo，推送上线。\n\n下面挨个来说。\n\n## 风格转换\n\n![](/photos/2021-12-06-blog-old.png)\n\n之前的风格名字叫做 Tufte，好像是根据一个美国教授的设计，主要设计元素包括一套自己加载的衬线字体（很漂亮，但是对中文没用），还有博文占据页面宽度的大约 60%，右侧剩下的空间可以做边注，有一个设计很精美的横向分割线 `<hr class=\"slender\">`。\n\n这个模板很漂亮，但是衬线字体搭配背景色，就跟人一种上个世纪古董网站的感觉（没有对卢昌海老师不敬的意思）。\n\nMaterial Design 是 Google 推出的一个组件库，提供类似纸张和卡片的视觉效果，就给人一种很现代的感觉，内心觊觎已久。[GitHub 的这个 repository](https://github.com/naveenshaji/material) 就用了这套设计风格，虽然已经不再更新维护了，但是已有的功能，比如点击链接之后的加载页面，页面顶端的阅读进度条，我觉得让我自己来的话，十年之内都不一定能学到做出这些效果的技术，于是就直接拿来用了。\n\n## 评论区\n\n### 旧版：jQuery 搭配 GitHub Milestones\n\n之前的评论区是根据 [farseerfc](https://farseerfc.me/zhs/pages/about.html) 大佬的[博文](https://farseerfc.me/zhs/github-issues-as-comments.html)自己仿写的 JavaScript 函数。原文是把 issue 作为一个博文的评论区，issue 下面的 comment 作为评论，这样每一条评论都是平级的，没办法实现对某一条评论的回复，只能在评论内容中指明回复的对象。\n\n除了像博文里一样自己手搓代码，也有现成的第三方工具 [utterance](https://utteranc.es) 来完成这一工作，但是我还是不喜欢这种单层评论系统，于是决定自己仿写一个类似的系统，但是让一个 milestone 对应一篇文章，一个 issue 对应一条评论，一个 comment 对应一条回复。\n\n这就需要我在博文的 Markdown 页面注明对应的 milestone 的编号，然后在网页加载完成之后向 GitHub 对应的 milestone 发送一个 GET 请求，询问是否存在 issues。 得到肯定的回答之后，再依次发送请求 GET 每一条 issue 的内容，显示在博文下方。代码位于 `[/_includes/comments.html` 页面](https://github.com/MountAye/blog-tufte/blob/source/_includes/comments.html)，当年从异步编程开始学起，颇费了我几个周末。\n\n问题出在 GitHub 的用户权限上，只有作者和管理员才有权给某条 issue 指定一个 milestone，所以读者建立 issue 进行评论之后并不会直接显示在博文下面，还需要我回到 repo 把那条 issue 手动挪到对应的文章，几乎不可用。\n\n### 新版：giscus 搭配 GitHub Discussions\n\n这次改版的时候觉得用户体验比“老子可以手搓评论区代码”的自我满足更重要，直接换用了 utterance。评论区在博客页面就提供了编辑区，新版本第一次上线不久就有朋友留言，说明这个评论区还是很好用的。\n\n后来GitHub推出了 Discussion 功能，每一条 discussion 下方可以有不同评论，评论下面可以有针对的回复，这就和一般的评论区和BBS 的“楼层-单元”同构了。\n\n然后又在阮一峰老师的博客看到了 [giscus，](https://giscus.app/)就是在 Discussions 架构下和 [utterance](https://utteranc.es/) 类似的一个工具，只需要在官网按照流程配置，然后把生成的几行代码嵌入到自己的页面里就好了，比原来方便太多了：\n\n```html\n<script src=\"https://giscus.app/client.js\"\n        data-repo=\"[ENTER REPO HERE]\"\n        data-repo-id=\"[ENTER REPO ID HERE]\"\n        data-category=\"[ENTER CATEGORY NAME HERE]\"\n        data-category-id=\"[ENTER CATEGORY ID HERE]\"\n        data-mapping=\"pathname\"\n        data-reactions-enabled=\"1\"\n        data-emit-metadata=\"0\"\n        data-theme=\"light\"\n        data-lang=\"en\"\n        crossorigin=\"anonymous\"\n        async>\n</script>\n```\n\n之前 utterance 创建的评论也可以直接从 Issue 移动到 Discussion 板块，所以刚刚那条评论可以正常显示。但是因为换了新的 repo，旧版本的两条评论就移植不过来了，请萌狼和 HK 兄弟包涵。 \n\n## 旧版特有功能：侧边栏注记、正文图片\n\n早就想给博客改版了，一直拖着不办的原因主要是直接把博文复制粘贴到新模板的 `/_posts` 文件夹之后会报错，不知道排错需要多长时间，于是压根就不动手了。因为旧版通过 jekyll 实现了一些正常 markdown 文档没有的功能，我用到的主要就是侧边栏的边注 `sidenote`，以及在正文内部插入图片的 `maincolumn`。\n\njekyll 是建立在 ruby on rail 上的一个软件，所以这些功能也是用 ruby 语言写成的。本以为会很难，结果认真一看，其实根本没什么工作量，就直接把旧模板的 `/_plugins`文件夹复制到新模板就不再报错了，当然显示效果需要调整 CSS，下一节会讲到。`{% raw %}{% sidenote %}{% endraw %}` 就可以继续用了。\n\n 至于正文图片，这个功能和 markdown 已有的图片功能重复了，我觉得不值得为了一点点显示效果牺牲可移植性，于是直接把相关组件删除了，然后把博客文章中用到的地方换成了 markdown 插入图片的语法，重复性体力劳动，不提了。\n\n## 调整 CSS\n\n### 新建 `.paper` 取代 `.cover`\n\n![](/photos/2021-12-06-material-card-cover.png)\n\n这个模板主打的一个内容卡片如图所示，很漂亮，但是整个元素的宽度是由封面图片 `.cover` 决定的：\n\n```css\n@media (min-width: 1600px) {\n    .scroll-1 {\n        width: 1200px;\n        margin-left: -600px;\n    }\n    .scroll-1 .card .cover {\n        width: 1200px;\n    }\n}\n```\n\n而我的很多博文都是纯文字的，直接不添加图片会有大半个卡片没什么内容，去掉图片区域之后整个卡片的宽度会缩水到文字的最大宽度，丑。解决方法是新建了一个新的类 `.paper`，把 `.cover` 中和宽度相关的设置移动到新的类，然后把新类应用在整个卡片的 `<div />` 上。\n\n```css\n@media (min-width: 1600px) {\n    .scroll-1 {\n        width: 1200px;\n        margin-left: -600px;\n    }\n    .paper {width: 1065px;}\n    /* .sidenote {width: 300px;} */\n}\n```\n\n### 考虑侧边栏的页面宽度响应式布局\n\n响应式布局指的是同一个网页，在不同的终端设备上都又能够有适合的视觉效果，主要方法就是根据不同的屏幕宽度设定某些元素的不同取值：\n\n```css\n@media (min-width: 1600px) { /* ... */ }\n@media (max-width: 1600px) { /* ... */ }\n@media (max-width: 1200px) { /* ... */ }\n@media (min-width: 768px) and (max-width: 979px) { /* ... */ }\n```\n\n具体到这个博客：\n\n- 屏宽大于一个很大的值之后，博客正文和侧边栏的宽度固定，不再增加；\n- 屏幕小于这个值，但是大于能够正常显示侧边栏的宽度时，博客正文和侧边栏都占据给定的百分比；\n- 屏幕小于能够正常显示侧边栏的宽度时，侧边栏不再显示，正文宽度固定在之前最小的像素值；\n- 正文像素数占据屏幕全部宽度之后，宽度设定为 100%。\n\n## 复习 Liquid，重写非博文页面\n\n非博文页面和之前的布局基本上相同，除了 `/Topics` 从一个无序号列表变成了响应式的两个竖栏。这是得益于模板引用的 `materialize.css`采用了 [flexible grid](https://materializecss.com/grid.html)。\n\n具体来说，一个`container` 可以动态调整内部元素的占宽，以适应不同大小的设备，包含两级子元素，两级子元素顾名思义，分别显示为行和列的元素：\n\n```html\n<div class=\"container\">\n\t<div class=\"row\">\n\t\t<div class=\"col s12 m6 l4\"></div>\n\t\t<div class=\"col s12 m6 l4\"></div>\n\t</div>\n</div>\n```\n\n然后将父元素的宽度分成 12 个基本单位，方便指定某一个元素所占的宽度和高度。比如 `class = \"col s12 m6 l4\"` 表示该元素在小设备 (small) 上占据 12/12=100% 屏宽，在中设备 (medium) 上占据 6/12 = 50% 屏宽，在大设备 (large) 上占据 4/12=1/3 屏宽。 \n\n### Liquid\n\nJekyll 本身的编程语言是 Ruby，但是被 Jekyll 编译之前的网页文档中的特殊标记，是一种叫做 Liquid 的领域专用语言。\n\n一般的命令和控制流结构用 `{% raw %}{% %}{% endraw %}` 括起来 \n\n```html\n for i in (1..len1) %}\n\t assign idx = i | times: 2 | minus:2 %}\n   assign cate = category[idx] %}\n endfor %}\n```\n\n直接在网页中显示内容和变量使用 `{% raw %}{{  }}{% endraw %}`\n\n```html\npost.title }}\n```\n\n函数调用的语法最奇葩，是 `input | function` 或者 `input | function: parameter`：\n\n```html\nassign category = site.data.category %}\nassign length = category | size %}\nassign   len1 = length | plus: 1 | divided_by: 2 %}\nassign   len2 = length | divided_by: 2 %}\n```\n\n## 调整博文元信息\n\n每篇博文的开头两道分割线之间的 YAML 是绑定在页面上的变量，可以被 Liquid 引用 `{% raw %}{{ post.VAR_NAME }}{% endraw %}`。\n\n```yaml\n---\nlayout:  post\ntitle:   .html | 翻译：为什么说物理不是一门学科\nkeywords: html\nexcerpt: 一篇稍微硬核的科普文章，讨论物理在生物学当中的可用性。\ndate:    2019-06-19\ncategories: post\nmilestoneID: 5\n---\n```\n\n新版本简化了很多，删掉了没有用的 `date`, `categories`, `milestoneID` 字段，同时把 `keywords` 变成了一个数组，也就是说同一篇文章可能出现在多个 `/Topics` 页面的卡片中。\n\n```yaml\n---\nlayout:  post\ntitle:   .en | 翻译：为什么说物理不是一门学科\nkeywords: [html,inter]\nexcerpt: 一篇稍微硬核的科普文章，讨论物理在生物学当中的可用性。\n---\n```\n\n## 下一步计划\n\n- 进一步优化排版:\n    - 给博文的标题添加视觉效果，现在的一二级标题之间很难区分。\n    - 超链接的样式不够明显，一眼看不出哪里有链接。\n    - 侧边注距离正文的距离不合适，CSS 相关参数的效果很奇怪。\n    - 现在的 /Links 页面在宽度缩小之后用户头像会和介绍卡片挤到一起去。\n    - 网站整体字号在笔记本上比较合适，在台式机上看起来太大了，移动端更大。\n- 加入 google analytics\n- 欢迎大家提建议。\n- 欢迎有独立博客的朋友互相链接。\n- ~~本期博文点赞过 5 亿，下篇文章写用 jekyll 在 GitHub Pages 上搭建静态博客的教程。~~\n\n"},{"slug":"install-pytorch-cpu-on-fedora","filename":"2021-10-13-install-pytorch-cpu-on-fedora.md","date":"2021-10-13","title":".py | Fedora 上安装 CPU 版 pytorch","layout":"post","keywords":["md","py","ai"],"excerpt":"import torch (as tf)","content":"\n马上要参加一个暑期学校，关于深度学习在显微图像处理当中的应用。\n\n深度学习是机器学习的一个分支，机器学习中的绝大多数数据都可以抽象为向量（一阶张量），绝大多数的算法都可以分解为向量之间的运算，或者对向量的变换，表示为矩阵（二阶张量）。这就对张量计算相关算法的库函数产生了很大的需求。PyTorch 和 TensorFlow，还有其他的一些库，比如 Keras，Caffe 等等等等，都是为此而生。早期版本的 pytorch 和 tensorflow 有很大的区别，但是随着版本的迭代，两者逐渐兼并和挤掉了其他的竞争者，两者的相似之处也越来越多，“变成了自己曾经最讨厌的样子”。lol\n\n不知道课上究竟要使用哪种机器学习的框架，所以决定把 PyTorch 和 TensorFlow 全都安装了（摊手）。正好可以接着上一篇的 [python 教程](python-interpreter-editor-virtualenv) 往下写。\n\n先说一下自己的软硬件环境：Intel 家的 CPU 和集成显卡（玩不了《文明6》）。虽然不在官方支持 Linux 的名单上，但是自己安装了 Fedora，内核更新了几次之后已经没有了兼容性问题。python 版本 3.9.6，包管理器是 pip，编辑器是 vscode。\n\n## 建立虚拟环境\n\n为什么要建立虚拟环境的问题本系列的前一篇已经回答过了，这次直接开干。我给两个虚拟环境分别取名为 `torch` 和`tf` 。关于命令行部分的代码，为了表示各个虚拟环境，特别加上了命令提示符`(env)[me@mycomputer]$`，抄代码的时候注意去掉。\n\n```bash\n\n[me@mycomputer]$ mkvirtualenv torch\n(torch)[me@mycomputer]$\n```\n\n## 安装\n\n在 PyTorch 官网，找到自己的硬件配制对应的安装命令：[https://pytorch.org/get-started/locally/](https://pytorch.org/get-started/locally/)。比如我的就是 `Stable`>`Linux`>`Pip`>`Python`>`CPU`。把生成的命令复制到命令行：\n\n```bash\n\n(torch)[me@mycomputer]$ pip3 install torch==1.9.1+cpu torchvision==0.10.1+cpu torchaudio==0.9.1 -f https://download.pytorch.org/whl/torch_stable.html\n```\n\n等待各种提示信息显示安装完成。\n\n## 验证和退出\n\n按照 [官网给出的方法](https://pytorch.org/get-started/locally/#linux-verification)，验证安装是否成功：\n\n```python\n\nimport torch\nx = torch.rand(5, 3)\nprint(x)\n\n# tensor([[0.3799, 0.4494, 0.4296],\n#       [0.5800, 0.0180, 0.3110],\n#       [0.9847, 0.0125, 0.2648],\n#       [0.0296, 0.3142, 0.9266],\n#       [0.3192, 0.9645, 0.5545]])\n```\n\n为了下一步安装 tensorflow，先要退出到默认的虚拟环境：\n\n```bash\n\n(torch)[me@mycomputer]$ deactivate\n[me@mycomputer]$\n```\n\n## 说好的 TensorFlow 呢\n\n本来这篇文章是打算把  pytorch 和 tensorflow 一起写了，结果 tensorflow 实在是不给力。\n\n- 直接安装\n\n在 [TensorFlow 的官网](https://www.tensorflow.org/install)上方导航栏找到 install 按钮，然后在页面左侧找到 package/pip，[安装命令](https://www.tensorflow.org/install/pip#3.-install-the-tensorflow-pip-package)也是只有一句话\n\n```python\n\npip install --upgrade tensorflow\n```\n\n然而不行，虽然安装过程中没有报错，但是验证安装的时候报出一堆错误。\n\n报错信息里有一句 `Could not load dynamic library 'libcudart.so.11.0'`，怀疑上面命令安装的是 GPU 版本。\n\n- 安装 CPU 版本\n\n在网页正文的“Package Location”一节找到了 CPU 版本的安装文件：`https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow_cpu-2.6.0-cp39-cp39-manylinux2010_x86_64.whl`，于是删除虚拟环境、重建虚拟环境、重新安装。\n\n```bash\n\n(tf)[me@mycomputer]$ deactivate\n[me@mycomputer]$ rmvirtualenv tf\n[me@mycomputer]$ mkvirtualenv tf\n(tf)[me@mycomputer]$ pip install --upgrade pip\n(tf)[me@mycomputer]$ pip install https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow_cpu-2.6.0-cp39-cp39-manylinux2010_x86_64.whl\n```\n\n运行官方提供的测试之后依然会有警告信息：\n\n```bash\n\n(tf) [shixing@yoga-laptop ~]$ python -c \"import tensorflow as tf;print(tf.reduce_sum(tf.random.normal([1000, 1000])))\"\n20XX-XX-XX XX:XX:XX.XXXXXX: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\ntf.Tensor(-1338.4773, shape=(), dtype=float32)\n```\n\n[stackoverflow 的这个回答](https://stackoverflow.com/questions/47068709/your-cpu-supports-instructions-that-this-tensorflow-binary-was-not-compiled-to-u) 说，需要从源码编译 tensorflow，具体的方法在[官网也有](https://www.tensorflow.org/install/source#linux)，但是实在是太麻烦了，~~（还是鸽了）~~ 下次单独写成一篇吧。"},{"slug":"python-interpreter-editor-virtualenv","filename":"2021-06-29-python-interpreter-editor-virtualenv.md","date":"2021-06-29","title":".py | 笔记：python 编辑器、解释器、虚拟环境","layout":"post","keywords":["md","py"],"excerpt":"","content":"\n上一篇 《[在 Windows 10 上配置 python 开发环境](python-installation-and-configuration)》 很惭愧，只做了一点微小的工作，大概三件事：\n- 一个，安装了 python 的解释器；\n- 第二个，把 vscode 安装进了电脑；\n- 第三个，就是我们知道的 virtualenv 虚拟环境。\n\n如果说还有一点什么要讲的，就是在 how 之外，讲一点 what。为了让文章更通顺一点，我打算调整一下顺序，先讲编辑器，再讲解释器，最后再说虚拟环境。虽然文章是立足于 python 来谈，但是这些知识适用于几乎所有通用编程语言。因为我也不是计算机专业出身，这篇文章只是我的学习笔记，如果有不对的地方欢迎大家指出。\n\n## 编辑器\n\n首先我们来做一个实验，把上次教程里创建的 python 文件 `hello.py` 重命名，把拓展名 `.py` 改成 `.txt`, 然后双击鼠标打开文件，会发生什么？在 Windows 系统里，会弹出最最普通的记事本窗口，窗口里是白底黑字的 python 语句，不像 vscode 里面不同语句有不同颜色，但是内容完全一样。\n\n甚至更进一步，你可以在命令行里直接让 python 编译器执行 `.txt` 文件的内容，（`py -m hello.txt`）效果和 `.py` 也是一样的。\n\n`.py` 这个拓展名什么也没有做，和 `.txt` 文件一样，内容就是一串我们人类能够读懂的字符，这样的文件叫做[文本文件](https://zh.wikipedia.org/wiki/%E6%96%87%E6%9C%AC%E6%96%87%E4%BB%B6)。处理文本文件的程序，就叫做[编辑器](https://zh.wikipedia.org/wiki/%E6%96%87%E6%9C%AC%E7%BC%96%E8%BE%91%E5%99%A8)。记事本就是一种编辑器，vscode 也是。\n\n既然记事本就可以编写 python 代码，那我还费劲安装 vscode 干嘛？确实有人真的只用记事本或其他操作系统自带的编辑器写 python 代码，（油管上还有一个视频是用 Microsoft Word 写代码；）然后用命令行调试程序，但是 vscode 毕竟是专为程序员而开发，提供了很多默认编辑器不具备的功能，比如前面提到将函数、变量、保留词显示成不同的颜色的语法高亮功能。\n\n## 编译器 / 解释器\n\n除了文本文件，这篇文章里涉及的第二种文件是[二进制文件](https://en.wikipedia.org/wiki/Binary_file)。\n\n电脑并不能直接看懂人类认识的字符，在一切的最底层，经典计算机认识的是以不同方式表示的 0 和 1。虽然说所有文件在底层都是二进制的文件，但是“二进制文件”这个名词一般专门用来表示除了文本文件之外的文件，又因为图片文件、视频文件啥的都有自己的名字，所以这个词用来指代的文件，基本上都和软件程序，可以让计算机执行的文件有关。\n\n人类只认识字符，计算机只认识 0 和 1，那么最直觉的思路就是把文本文件翻译成二进制文件。这个翻译过程叫做[编译](https://en.wikipedia.org/wiki/Compiler)，能够完成这一过程的软件就叫做编译器。编译器编译完成之后就退出了，要想执行程序，电脑直接执行编译之后的可执行文件就可以了。\n\n但是编译存在一个问题，就是整个软件需要在所有的源代码文本文件都写好的情况下才能被编译成软件，编译耗费的资源和时间随着软件规模的增长而扩大；一旦修改某处，整个项目又要重新编译。很多时候我们只想快速地知道某个大型项目中的某一句命令的效果是什么，编译这种方法就不适合这种场景了。\n\n于是就出现了[解释器](https://en.wikipedia.org/wiki/Interpreter_(computing))，这种程序比编译器复杂的多，在我们执行这种编程语言命令的时候始终运行，允许我们一句一句地输入命令，记得代码的上下文，还记得我们之前命令的结果，代价就是对于大型项目也需要一句一句地分析解释，计算资源的开销和速度都不如编译。\n\npython 就是一种（官方实现）使用解释器的编程语言，我们在官网下载的那个 `python-***.exe` 文件就是 python 的解释器。这也导致了 python 程序的性能往往不如同水平的 C/C++ 程序员写出来的程序，但是由于单句执行适合试错，所以在 ~~经常犯错的~~ 科研领域还挺流行的。\n\n当然了，python 的特点远不仅仅是解释型语言这么简单，它还是一种：\n- [脚本语言](https://program-think.blogspot.com/2009/08/why-choose-python-1-script.html)\n- [动态语言](https://program-think.blogspot.com/2009/08/why-choose-python-2-dynamic.html)\n- [面向对象语言](https://program-think.blogspot.com/2010/08/why-choose-python-3-oop.html)\n- [函数式编程语言](https://program-think.blogspot.com/2010/08/why-choose-python-3-oop.html)\n- [……](https://program-think.blogspot.com/2010/08/why-choose-python-3-oop.html)\n\n## 虚拟环境\n\n有一次我问我女朋友，她写 python 用什么 IDE，她很自豪地回答，她的 Macbook 自带 python，直接在命令行就可以运行……答非所问还不是最大的问题（仔细想想好像也不是答非所问，不过只回答了问题的一部分），而是直接在命令行运行系统自带的 python，或者其他编程语言的解释器或者编译器，本身就是编程初学者常干的一种危险行为。\n\nWindows 还好，毕竟这是一个面向广大家用消费者的操作系统，防呆设计还是挺多的，没有原装的 python。更加极客向的操作系统，比如 Linux 和 BSD 家族可就不一样了，这些操作系统（的发行版）往往预装了 python。这个 python 可不是给用户拿来开发自己的程序用的，而是用来让很多 python 语言写成的操作系统工具调用的。既然如此，这个 python 的版本一般由发行版的安装包管理者来控制，往往落后最新的 python 版本一段时间，为了避免新版本 python 有什么 bug，也为了让操作系统工具的开发者有时间更新自己的代码。所以如果直接用这个版本的 python 做开发，而且不小心自己升级了 python 的版本，很有可能导致系统的某些功能失常。\n\n虚拟环境就是 python 对这个问题的解决方案。我们可以安装不同于原装 python 的版本，但是并不将这个解释器加入系统路径，操作系统也就不知道这个版本 python 的存在。创建虚拟环境的时候，我们指定使用这一特定版本的 python，这样在虚拟环境激活之后就是我们开发需要的 python，退出虚拟环境就是系统工具使用的 python。\n\n另外，即便两个项目适用于同一个 python 版本，而且都是系统自带的这个版本，虚拟环境也有用武之地。绝大多数程序都需要依赖别人写好的工具代码，这些代码叫做库 (library)，不同的项目可能依赖不同的库代码，或者同一个库的不同版本。这个时候，可以创建不同的虚拟环境，并在其中安装各个项目对应的库，项目之间可以互不影响。\n\n## 编辑器 + 解释器 / 编译器 + 虚拟环境管理 + …… = IDE\n\n为了开发 python 程序要安装这么多不同的程序，太麻烦了，就不能一键安装全搞定吗？当然可以了，这种集成了开发过程中用到的各种工具的程序，就叫做[集成开发环境 (IDE)](https://zh.wikipedia.org/zh-cn/%E9%9B%86%E6%88%90%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83)。对于 python，最有名的 IDE 当属 [Anaconda](https://www.anaconda.com/) 了。\n\n那我为什么不用呢？当然用过，但是听说了 vscode 的大名，而且尝鲜之后，就再也回不去了。写 python 需要编辑 `.py` 文件，写博客需要编辑 `.md` 文件，博客的一些功能需要 JavaScript 实现，这些事情本质上都是编辑文本文件这一件事，在 vscode 这样的编辑器里全搞定就再自然不过了，那么 Anaconda 里的 Spyder 和 Jupyter Lab 就显得多余了。\n\n“把一件事做好”，这也是 [Unix 哲学](https://zh.wikipedia.org/wiki/Unix%E5%93%B2%E5%AD%A6)的一部分。但是问题在于，不同的人对于“一件事”的定义是不同的，有些人觉得做早饭是一件事，有些人觉得是热牛奶煎鸡蛋烤面包等等好几件事，谁是对的？\n\n也许都对，但是 编辑器 + 解释器 + …… 比起 IDE 就是处在鄙视链的上游。这一点可以不同意，但是应该要知道，不然别人抖包袱的时候你没捧上哏，挺尴尬的。\n"},{"slug":"qc-hackathon-write-up","filename":"2021-04-22-qc-hackathon-write-up.md","date":"2021-04-22","title":".qs | QC Hack 量子编程马拉松","layout":"post","keywords":["md","phy"],"hasMath":true,"excerpt":"4月初的时候，系秘书转发了一封邮件，耶鲁和斯坦福有两个关于量子计算的学生社团，打算举办为期一周的线上训练营,然后在周末举办一个24小时的编程马拉松","content":"\n\n## 一\n\n4月初的时候，系秘书转发了一封邮件，耶鲁和斯坦福有两个关于量子计算的学生社团，打算举办为期一周的[线上训练营](https://www.quantumcoalition.io/)，然后在周末举办一个24小时的编程马拉松 ([hackathon](https://en.wikipedia.org/wiki/Hackathon)) 的活动。只要年满18岁就可以参加，并不限定本科生。\n\n整个活动由几家从事量子计算的科技公司赞助，前面的线上训练营基本就是各家轮流上来介绍一下自己家的量子计算平台的使用方法，最后的编程马拉松也由他们每家出一套题，所以这个活动也有在学生和公司之间搭桥，给参与者争取实习机会的目的在里面。参与者可以自由组队，但是在项目提交的的时候每个人只能属于一支队伍。虽然参与者可以参加任意数量的题目，但是每一名参与者最终只能成为一家公司的优胜者。如果预感到自己在某一个项目的赢面比较大，可以在提交之前通知自己参加的其他队伍把自己除名。24小时的时间限制还是比较紧迫的，所以基本上认准一家答题就可以了。\n\n女朋友也收到了一样的邮件，所以理所当然地一起组队。我之前在本科阶段上过一门一学期的量子信息和量子通信课程，内容约等于在量子力学之后再上一个学期的习题课，以及在不讲群论的情况下应用 SU(2) 群，并没有接触过这个活动中会用到的编程语言。女朋友没有上过这门课，基本就是物理专业普通研究生的量子力学水平。周中的线上训练营，我只参加了第一天的，是 Microsoft 的 Quantum Development Kit (QDK) 和 Q# 编程语言相关的，顺便介绍了一下量子计算中很有名的 Deutsch 算法。剩下的讲座我基本上都没有参加，一方面是知道前面的规则之后就懒下来了，另一方面是实验室的工作仍然需要继续，再有就是线上活动实在是太容易摸鱼了没有效率。周五的晚上，女朋友看了一晚上我的量子信息笔记，我看了看 Q# 的语法规则，在台式机上安装了开发环境。以上就是我们参加编程马拉松之前的基础和准备。\n\n\n## 二\n\nHackathon 美东时间周六上午10点开始，周日上午10点结束。因为我们只看了 Microsoft 相关的内容，所以直奔[相关题目](https://github.com/quantumcoalition/qchack-microsoft-challenge)。\n\n题目一共分为两部分。\n\n第一部分一共四道题，就像是一般的计算机课程的作业一样，参赛者只需要在举办方写好的主程序文件里的指定区域填入代码，然后运行主办方写好的测试文件检查结果，测试通过即可得分。四道题目要求如下：\n\n1. 判断一个3-5位的2进制数能否被4整除。\n2. 判断一个3比特位当中是否至少有两位不同。\n3. 同第2题，但是要求量子比特门最多只能使用 3-比特，而且 3-比特门最多使用一次。\n4. 给定一个有两种颜色的无向图，判断图当中不含有任何单一颜色的三角形。\n\n第二部分内容比较自由，要求用 Grover's 算法解决一个自己感兴趣的问题，打哪指哪，然后写一篇文章介绍自己的这个项目，并提交相关的代码。根据问题深度(6分)、工具使用(5分)、创新性(4分)、教育价值(5分) 四方面进行评分。\n\n\n## 三\n\n### I.1.\n\n第一道题最简单，但是我们当时约等于0基础，所以做起来也颇费了一些时间。不过由于我听过第一天的课，知道 `oracle` 在 Q# 编程语言中是一个很重要的概念，所以在题目给出的参考教程 [Quantum Kantas](https://github.com/Microsoft/QuantumKatas/) 里找到了[oracle 相关的教程](https://github.com/microsoft/QuantumKatas/tree/main/tutorials/Oracles)。里面有个名为 `ControlledOnBitString` 的 function，可以根据一串量子比特的取值是否等于一个特定的二值串而对另外一个比特做一个特定的操作。前一天晚上又知道了 `Microsoft.Quantum.Convert` 的 namespace 里有各种数据类型转换的函数，搭配 `IntAsBoolArray`，就做出了第一题的初版。后来看到了更简单的 `ControlledOnInt` 函数，就直接用上了。\n\n### I.2.\n\n第二题的初版是女朋友做的。题目要求是找出是否至少两位不同，这一判断的否定就是三位比特全部相同，所以同样用 `ControlledOnBitString` 函数，然后判断一次全 `true` 一次全 `false`，再把最终结果取反就可以了。但是在做第三题的时候，因为两个题目长得太像了，中间不小心把一个能通过第二题测试但是通不过第三题测试的答案直接覆盖在了第二题上面，懒的改回去了，于是就成了最后提交的版本。\n\n### I.3.\n\n第三题和第二题非常不同。第二题的解决思路中，判断全 `true` 和全 `false`有3个控制位1个输出位，这里用了两次 4-量子比特门，所以第三问需要全新的思路。另外我曾经试过在一个 `operation` 里申请一个新的 `Qubit()` 结果测试报错，因为误解了报错信息，所以误认为除了程序的主 operation 之外不能创建新的 qubit，于是被卡住了。这时候已经来到了下午，实在想不出来又很困，于是去床上躺了一会。半睡半醒之间想到，题目虽然要求输入的量子比特不变，但是我们仍然可以直接改动输入，只要在函数结束之前把对输入的改动全部复原就可以了。于是用 CNOT 门分别作用在 1-2, 1-3 对输入的量子比特上，两个门分别以第2、3号比特为输出。然后用一个 3-bit 门判断2、3号比特是否相同，并输出到结果位上。为了复原第2、3号比特，只需要把 CNOT 在两对比特上分别再用一次就行了。\n\n但是这个结果还是无法通过测试（后来成为了第二题的提交版本），报错的提示信息是使用了超过一次 3-量子比特门——这不是开玩笑吗？于是打开了官方提供的测试文件，发现测试代码计算 3-量子比特门的使用次数的时候，会把用户定义的 3-量子比特门的数量，和 `CCNOT` 门的数量做加法，于是看文档，我们定义的那个 “用一个 3-bit 门判断2、3号比特是否相同，并输出到结果位上” 的操作和 `CCNOT` 门是等价的，于是直接换用 `CCNOT` 门，问题解决。\n\n### I.4.\n\n第四题看起来复杂，但是可以分成三个部分：\n\n1. 找出图中所有的三角形，确定每个三角形的三条边，这一步完全可以用经典算法完成；\n2. 创建一个和三角形相同数量的量子比特数列，对每个三角形，把三条边直接带入第二/三题的操作里，结果输入创建的量子比特列中；\n3. 判断量子比特列是否全为 `true`，结果输出到整个程序的结果位上。\n\n第一步由女朋友来想我来写（毕竟只有一台电脑有开发环境），难点在于：\n\n1. Q# 语法改变数列值的语法十分难受\n    <br>`mutable points = [-1,-1,-1,-1,-1,-1];`\n    <br>`set points w/=0..1 <- [0,1];`；\n2. 作为一种强类型语言对元组和数列的区分让我这个 python 选手十分蛋疼\n    <br>`(Int,Int)`/`Int[]`；\n3. 求数列中不重复的值居然不排序不能给出正确结果。\n    <br>`let uniquePoints = Arrays.Unique(EqualI,Arrays.Sorted(LessThanI,points));`\n\n这也是唯一一段用上了 `Message()` 函数来 debug 的部分。\n\n第三步就重新回到了第三题暂时敷衍掉的问题：对于在操作中创建的 `Qubit()`/`Qubit[]`，`Reset()`/`ResetAll()` 函数相当于测量，会破坏操作的 adjoint 性质，不测量则（当时的我）没有办法将这个量子比特列复原。\n\n此时已经午夜，我来解决这个问题，女朋友去看第二部分，后来她看完 Grover‘s 算法的教程去睡了，我还在想这个问题。直接把报错信息复制到 Google，找到了一个[论坛里的问答]()，好像是去年微软在其他地方举办的类似活动的。里面只是提到要“uncompute the qubits”，给出的例子用的是旧版本 Q# 的语法，~~没法直接抄~~ 。最终不抱希望地把之前对那个 `Qubit[]` 做过的循环顺序倒过来重做了一遍，诶，您猜怎么着，还是没通过！绝望了！正序重做一边，诶，通过了！为什么为什么为什么？到现在也没弄清楚。\n\n### II.\n\n然后把女朋友叫醒，让她来讲一讲 Grover's 算法。听完之后我的理解是，对于一个 $$f:(0,1)^N \\rarr (0,1)$$ 的函数，这个算法可以大概率地找到一个解 $$S\\in(0,1)^N$$ 满足 $$f(S)=1$$. \n\n至于这个函数 $$f$$，之前每一道题都是这样一个函数，当时已经夜里两三点了，实在是没时间再想一个新函数了，于是我们直接就拿复杂度最高的第4题来换个皮。换个什么皮呢？为了这个活动翘掉了这周的[《文明6》联机游戏](barrier-forward-keyboard-mouse-to-another-computer)，然后之前看 YouTuber [\"PotatoMcWhiskey\"](https://www.youtube.com/user/PotatoMcWhiskey)介绍过[一个 Mod](https://steamcommunity.com/sharedfiles/filedetails/?id=1753346735&searchtext=diplomacy)，里面可以将文明之间的外交关系可视化为无向图，所以，诶嘿嘿嘿……\n\n女朋友写完文稿就睡了，我把文稿改了改，然后和官方对 Grover's 算法的实现缝合了一下。提交的时候，距离截止时间大约还有一个小时。\n\n\n## 四\n\n之后的周五的时候收到了消息，我们得奖了。优胜者一共6支队伍。从活动结束之后公布的结果看，要想成为优胜，第一部分的4道题必须全部正确，然后第二部分得分在 8-20 分之间。\n\n这个成绩是个什么水平？截止到写这篇文章的此刻，官方题目的 Github 仓库有 80 份 fork，有少数几份 fork 是针对已有的 fork，有可能来自同一队伍，再考虑到可能有些队伍的不同成员分别 fork 了主项目，所以估测 60 支队伍应该是有的，官方给出 6 组优胜者这么一个不零不整的数字，个人猜测是取了前 10%？据主办方在 discord 提供的消息，有一支队伍的第二题成绩高于8分，但是前面没有全对，所以没有得奖；其余队伍的第二题都不超过6分；并不清楚有多少队伍第一题全对，主办方也不打算公布各队的详细成绩。\n\n这大约说明活动的参与者，其成绩基本上符合二八原理——少数人得到的分数，占据了所有参赛者全部得分的大多数。\n\n参加过这个活动之后，我们一下子就从量子计算小白摇身一变，成了优秀人才了？实际上，直到现在，我还是搞不太清楚 oracle 到底是个什么东西，女朋友对量子计算的理解估计比我还差（逃）。美国哲学教授约翰·希尔勒提出过一个叫做[“中文房间”](https://zh.wikipedia.org/wiki/%E4%B8%AD%E6%96%87%E6%88%BF%E9%97%B4)的思想实验，说一个只会说英语的人被关在一间满是汉字字块的房间里，不断从房间外收到写着中文问题的纸条。房间里有一本英文写成的手册，指示如何对输入的汉字进行回复。凭借这个手册，房中人可以在完全不会中文的情况下，与外界进行交流。希尔勒类比外人、房中人、手册，与程序员、计算机、计算机程序，认为房中人不会中文，进而论证计算机不可能通过程序来获得理解力。\n\n希尔勒教授想论证啥是他的事，我倒是对这个类比的本体很感兴趣——如果一个人已经能够熟练运用那个英文写成的汉字使用手册了，我们还能不能，能在多大程度上说他不懂中文呢？就说一般的程序员，工作时间能保证不看 stack overflow 的有几个，所以他们都不会编程？反对中文房间思想实验结论的人，很多都支持用图灵测试超过某一阈值来作为有智能的标志，但是我觉得，智能本身就不是一个非有即无的性质，而是一个连续分布，没有上限的谱。\n\n另一方面，得分名列前茅，和能力名列前茅，又是两回事。本科的时候做建模美赛，我们学校数理金融的一个学神前一年成绩“略有不佳”，没拿到 M 奖，于是我们那年找到了我和风神俩学物理的，准备再次冲击荣誉。巧了这一年的题目正好有一道浴缸放热水的问题，这不就是物理中的扩散方程嘛，那得奖还不是手拿把掐的？结果呢，H 奖，丢人丢到姥姥家去了。合着我们两个成绩还都不错的物理专业学生，在自己的专业里，打不过那么多同龄的非物理专业本科生？\n\n两相对照之下，我想起了很久之前看过的一篇博客文章，文章以一个问题开头——“熟练”的反义词是什么？当然说“生疏”这文章就写不下去了，作者给出的答案是——“应变”。熟练意味着，你对于问题、选项、最优解已经有了充分且完备的了解，只需要重复自己的经验就可以了，但是在自己不了解的战场上，经验至少不能直接派上用场，这时候，脱离具体环境的应变能力就成了生存和取胜的关键，我们当时的专业水平高不成低不就，反而成了掣肘我们的桎梏。\n\n读到这篇文章的时候，我被这种剑走偏锋的观点击中了，从那以后，一直都在注意培养自己的应变能力——如果明天我所研究的这个领域消失了，我还有没有谋生的能力？如果自己正在解决的问题被上帝或者 Matrix 作弊修改成一个新问题，我能不能看到连作弊都改动不了的题眼，然后一击命中？在凌晨两三点的时候，我也没有放弃解决第一题第 4 问的 Qubit 复位问题，虽然当时我并不知道评分标准，但是内心非常确定，这个问题必须解决。\n\n以上两次活动的成绩差别，也可以从得奖难度来看。建模美赛的 M 奖，得奖率应该远小于 10%，即便考虑到二八原理中绝大多数参赛者都只是凑数，而且样本越大凑数者越多，这个差距也还是无法忽略。我们能够得奖，和量子计算领域才刚刚萌芽，连“方兴未艾”都算不上，因此竞争并不激烈也有很大关系，应变能力是切入这些蓝海领域的必要条件，是躲避内卷的利器。我们现在对“内卷”人人喊打，但是培养应变能力是需要牺牲相当多本可以精进专业的时间和精力的。当社会中的大多数人向往着逃离内卷的时候，真的不需要有人咬定一个领域不断深耕？我现在的选择真的正确吗？我不知道。我是打算留在当前的领域继续熟练，还是换个领域应变，抑或是虚掷 PhD 光阴换一张工作签证？我也不知道。\n\n## 五\n\n哦对了，我有女朋友了，而且在 hackathon 的过程中把女朋友惹哭了……问题是我现在已经不记得具体是怎么把人家惹哭的了，连道歉都显得很不诚恳……我确实是一个不擅长合作的人，或者说跟别人说话的我，和想问题的我并不是同一个人，之前本科 CUPT 和建模的时候也一样，需要和人打交道的时候就几乎干不了活儿，严重的时候自己就退化成了鼓励师……总之一切错误在我，希望她不要记仇…… <br>（。・＿・。）ﾉ\n"},{"slug":"python-installation-and-configuration","filename":"2021-03-07-python-installation-and-configuration.md","date":"2021-03-07","title":".py | 在 Windows 10 上配置 python 开发环境","layout":"post","keywords":["md","py"],"excerpt":"截图截到哭……","content":"\n我的 python 版本之前一直停留在 3.7，目前最新已经到了 3.9。最近研究上要用一段别人写的 python 的代码，原作要求 3.6 版本的 python，所以卸载了 3.7 版本，重新安装了 3.9 和 3.6 两个版本的 python。\n\n煽动爸妈学编程很久了，欠他们一篇教程。不论是打算不断精进者的第一门编程语言，还是浅尝辄止者打算学的唯一一门语言，python 都是一个很不错的选择。所以即便爸妈对编程可以说是毫无兴趣，我也还是把这篇教程写完了，希望能帮到其他人。因为是面向纯新手的，所以基于 Windows 10 操作系统。因为这套设置我自己也要用，出于个人偏好，所以没有直接使用 `conda`。\n\n## ~~别看你今天闹得欢~~ 先拉个清单\n- python\n    - 在官网下载 `python` 解释器\n    - 安装 `python` 解释器\n- 虚拟环境\n    - 设置环境变量\n    - 用 `pip` 安装 `virtualenv` \n    - 用 `pip` 安装 `virtualenvwrapper`\n    - 创建一个虚拟环境，安装 `jupyter`\n- 编辑器\n    - 在官网下载 `vscode`\n    - 安装 `vscode` 和相关插件\n    - 测试一下效果\n\n## 教程本体\n\n### 在官网下载 `python` 解释器\n\n在 python 的官网上 ([https://www.python.org/downloads/](https://www.python.org/downloads/)) 下载\n\n![](/photos/2021-03-07-python-official-website.png)\n\n### 安装 `python` 解释器\n\n执行下载的文件，可以看到如下图所示的界面，点击红框中的选项，因为我想更改安装路径。此处似乎也可以勾选最下面的 \"Add Python 3.9 to PATH\"，我安装的时候没有选，后面才发现还要手动完成这一工作。\n\n![](/photos/2021-03-07-python-install.png)\n\n接下来的界面里每个选项都勾选，next。\n\n![](/photos/2021-03-07-python-next.png)\n\n在接下来出现的界面中，选择 \"Install for all users\"，然后选择一个自己喜欢的安装路径。我之所以选择 `C:\\Python39` 这个位置，是因为:\n1. C 盘是系统盘，而且是固态硬盘，比较快；\n2. 默认路径在个人文件夹里，非常难找\n3. `C:\\Program Files` 需要管理员权限，用 `pip` 做软件包管理不方便。\n\n![](/photos/2021-03-07-python-install-path.png)\n\n安装完成后进入命令提示符，输入 `py`，应该可以看到一串 python 版本信息，然后命令提示符变成 `>>>` 字样，此时就已经进入了 python 环境，可以按行运行 python 命令。输入 `exit()` 就可以退出 python。\n\n### 设置环境变量\n\n在 windows 的搜索框里输入 \"environment variables\"，没等你输完，应该就可以看到下图中的联想结果：\n\n![](/photos/2021-03-07-environment-variable-search.png)\n\n再按照下图，依次点击按钮，一共要做两件事:\n1. 给 `Path` 变量添加 `C:\\Python39` 和 `C:\\Python39\\Scripts` 两个新值。\n2. 新建一个文件夹，用于存放 python 的虚拟环境（比如 `C:\\PythonEnvs`）。创建一个名为 `WORKON_HOME` 的新变量，并将其值设为 `C:\\PythonEnvs`.\n\n![](/photos/2021-03-07-environment-variable-box.png)\n\n### 用 `pip` 安装 `virtualenv` 和 `virtualenvwrapper`\n\n按照刚才的步骤，此时 `pip` 应该已经安装，此时在命令行输入途中的命令，应该可以看到类似的返回信息：\n\n![](/photos/2021-03-07-pip-check.png)\n\n如果命令行说没找到 `pip`，可能需要按照[这个链接](https://pip.pypa.io/en/stable/installing/#installing-with-get-pip-py)里的方法重新安装。\n\n在命令行中输入 `py -m pip install virtualenv`， 安装虚拟环境管理器：\n\n![](/photos/2021-03-07-virtualenv-install.png)\n\n显示安装成功之后，在命令行输入 `py -m pip install virtualenvwrapper-win`，结果应该和上图差不多，忘了截图。\n\n### 创建一个虚拟环境，安装 `jupyter`\n\n`virtualenv` 和 `virtualenvwrapper` 安装完成之后，在命令行中输入 `mkvirtualenv base`，新建一个名为 base 的虚拟环境。\n\n然后输入 `workon base`，此时命令提示符的行首应该会多出一个 `(base)` 字样，这说明我们已经工作在了 base 这个虚拟环境里。此时 `pip --version` 的结果说明我们的 python 的地址已经和之前不同了。\n\n![](/photos/2021-03-07-virtualenvwrapper-check.png)\n\n在这个环境之下，安装 jupyter，输入命令 `pip install jupyter`，安装 jupyter。这一步不是必须的，但是这样我们将来安装 vscode 之后，可以获得和 conda 自带的 spyder 类似的体验，菜鸟逐行调试的时候非常方便。\n\n### 下载和安装 `vscode`\n\n从 vscode 的官网下载安装包（[链接在此](https://code.visualstudio.com/)），可以看到大大的下载按钮：\n\n![](/photos/2021-03-07-vscode-download.png)\n\n下在之后的安装过程就和一般的程序安装一样（安装很久了，没有截图）。\n\n安装完成之后打开程序，点击左侧工具栏红框中的那个图标，搜索 python 的相关 vscode 插件。python 开发一般都需要 `python`, `pylance`, `jupyter` 这三个：\n\n![](/photos/2021-03-07-vscode-plugin.png)\n\n以后要运行 vscode 的时候，在你想开始写代码的文件夹里，鼠标右键单击，选择 \"Open with Code\":\n\n![](/photos/2021-03-07-vscode-folder-open.png)\n\n然后在 vscode 的界面，在文件树上单击红框中的按钮，新建一个文件，命名为 `*.py`，这里随便写了个 \"hello\".\n\n![](/photos/2021-03-07-vscode-new-file.png)\n\n在随后打开的文档编辑区输入 `print(\"Hello World\")`，将光标停留在这一行，按 `Shift`+`Enter` 执行这一行：\n\n![](/photos/2021-03-07-vscode-run.png)\n\n因为是第一次执行，又因为安装了 `jupyter`，这时候 vscode 的右下角应该会出现一个提示框，询问是否用 jupyter interactive window 执行代码。然后关掉下方的这个 terminal (下半区右上角的叉号)。重新按 `Shift`+`Enter` 执行，此时的结果应该如下图，点击红圈中的变量浏览器，这就几乎和 spyder 的体验一样了。\n\n![](/photos/2021-03-07-vscode-interactive.png)\n"},{"slug":"barrier-forward-keyboard-mouse-to-another-computer","filename":"2020-12-22-barrier-forward-keyboard-mouse-to-another-computer.md","date":"2020-12-22","title":".md | 扫盲Barrier，用笔记本键鼠无线控制台式机","layout":"post","keywords":["md"],"excerpt":"你看我这个标题它像不像编程随想:-p","content":"\n## 为啥要用 `Barrier`\n\n圣诞节前的周五，我们系在线上办了一个聚会。和学姐闲聊的时候，听说他们高年级的几个朋友每周一起联机玩《文明6》，于是一番理(卑)直(躬)气(屈)壮(膝)的要(恳)求之后，成功加入了组织，等到了 steam 的圣诞特惠（没错我一直没买 new frontier pass，我是云玩家），准备以物理学工作者的严谨态度，研究《文明6》诱导的基于回合的反常时间平移对称现象。\n\n![](/photos/2020-12-22-livingroom.png)\n\n我现在公寓房间的结构如图所示，前任房客走的急，把她的几乎全部家具低价甩卖给我了，四舍五入相当于白捡了一台大电视。电视用 HDMI 线连接电脑，当作第二显示器使用。躺在沙发上，看着大屏幕，玩着文明6，[运用自如](https://www.bilibili.com/video/BV1g441187zq)的话说——“那岂不是非！常！爽！”\n\n所以说，我的要求是：\n\n- 在笔记本和台式机处于同一 Wifi 的情况下——\n- 笔记本电脑和台式机之间不需要任何数据线连接\n- 使用 Linux 笔记本上的触控板、鼠标和键盘\n- 控制 Windows 台式机\n- 正常运行和操作《文明6》，不会因为延迟卡顿被人喷（卡顿是不存在的，只可能掉线lol）\n\n一番搜索之后，锁定了 `Barrier` 这款软件。它可以设定一个 server 和多个 client，用 server 的输入设备控制各个 client。切换方法也很方便，只要提前约定好各个设备的“相对位置”，当 server 的鼠标光标跨过 server 屏幕的边角处，就开始控制对应方向上的设备。（具体可以查看下一节。）\n\n之所以选择它，主要原因如下：\n\n- [开源软件](https://github.com/debauchee/barrier)，堂堂正正地不用花钱。（我知道开源软件≠免费软件，但是……）\n- 目前依然有人维护，GutHub 上还有上百个 open issues，当然 closed issues 更多，有问题可以查，提问也有人回答。\n- 跨平台，Windows 和 Linux 都能用。主流 Linux 发行版都可以在仓库里找到，也可以通过 snap 安装。\n\n## 如何配置 `Barrier`\n\n### 下载和安装：\n\n因为我的发行版是 Fedora，在笔记本的 terminal 上任选一句运行：\n\n```shell\n\nsudo dnf install barrier\nsudo snap install barrier\n```\n\n台式机是 Windows 10，在GitHub 项目（[链接在此](https://github.com/debauchee/barrier/releases)）里找到最新一期的发布版本，找到 `.exe` 结尾的文件（可以用浏览器的页内搜索），下载，双击下载后的文件安装。\n\n### 配置 server （笔记本电脑）：\n\n首先要确定 linux 的桌面环境是基于 xorg 的，当前版本的 GNOME 默认使用的是 Wayland，所以需要 log out 之后重新选择带有 xorg 字样的环境，如下图：\n\n![](/photos/2020-12-22-gnome-environments-new.png)\n\n安装完成之后，在 app 界面（`Win`+`A`）应该可以找到 barrier 的图标，单击即运行。一般来说之后会有相当长一段时间电脑没有反应，这个时候 **千万不要重复点击图标**，会导致无法连接。\n\n![](/photos/2020-12-22-barrier_linux_desktop.png)\n\n之后会看到软件的主界面，选中图中所示的选项：\n\n![](/photos/2020-12-22-barrier_linux_start.png)\n\n然后单击 `configure server` 按钮，进入下图的界面，拖动右上角的电脑屏幕图标，拖到中间屏幕周围的任意一格。我选了右边一格，因为这样在笔记本上向右滑鼠标会进入台式机的显示器，从显示器右边框向右会进入电视，一路反过来就回到了笔记本屏幕，操作比较自然。\n\n![](/photos/2020-12-22-barrier_linux_config.png)\n\n双击图标后会弹出一个新窗口，在最上面的 \"screen name\" 栏填入 client 上显示的本机名称，现在我们还没有配置 client，所以需要等到配置完之后回来填写。\n\n![](/photos/2020-12-22-barrier_linux_naming.png)\n\n注意主窗口的左下角，如果不是 “barrier is running” 的话，需要点击右下方的 reload，还是不行的话就要准备 debug 了。\n\n### 配置 client （台式机）：\n\n安装完成之后，应该能在开始菜单里找到 barrier 的图标，点击运行，应该会看到和笔记本上面差不多的界面。\n\n![](/photos/2020-12-22-barrier_windows_main.png)\n\n选择图中的选项，填入笔记本窗口中显示的本机 IP 地址，选中 `auto config` 选项，然后点击右下方的 `start`，然后窗口左下角同样应该有 \"barrier is running\" 字样。点击菜单栏里的  -> `show log` 打开日志，应该可以看到下图里的 \"connected to server\" 字样。\n\n![](/photos/2020-12-22-barrier_windows_log.png)\n\n### 几个雷点：\n\n- Fedora 的显示管理器默认并不使用 xorg，需要专门切换。\n- Linux 上启动较慢，如果不耐烦多点了几次，可能会重复打开多个实例，造成端口被占用，无法使用。\n    + 诊断方法：打开 log 界面，会发现 \"ERROR: cannot listen for clients: cannot bind address: Address already in use\"  字样。\n    + 解决方法：杀掉多余的进程，不会杀的话就重启电脑吧。\n- server 和各个 client 对是否使用 SSL 的选择必须是一致的。\n- 官方没有按步骤来的配置说明，这么多雷点，我居然看到好几篇博客都在夸 barrier 的界面多么通俗易懂，简直了。 \n\n## `Barrier` 之外的其他方案\n\n- `synergy`：成熟的商业软件，好像要花钱。\n- `Microsoft Garage Mouse without Borders`：要求所有设备都使用 Windows 系统，不适用于我的笔记本电脑\n- `usbip`：来自 farseerfc 大神的[这篇博文](https://farseerfc.me/zhs/usbip-forward-raspberrypi.html)，原文说“设置好的话，就像是一台 PC 多了几个位于树莓派上的 USB 端口，插上树莓派的 USB 设备统统作为 PC 的设备”。\n    + 和 `Barrier` 相比，配置的步骤更繁琐；\n    + 好像一次只能控制一台设备（不太懂，如果转发给多个设备的话可能会一起执行相同的操作？）；\n    + 好像也不适用于非 USB 接口的设备，比如笔记本的原生键盘和触屏笔；\n    + 一旦运行起来，server 的鼠标就不再能够控制自己，所以原文使用了几乎注定吃灰的树莓派，相当于给键盘加了个广播天线。\n\n<!-- 用的是一款叫作 [Gather](https://gather.town/) 的 web 应用，就像 90 年代的 RPG 游戏一样，参加者每人指挥一个像素很糊的人物，在一个像素很糊的房间里走来走去，不同之处在于当角色相互靠近到一定距离之内的时候，可以像 Zoom 一样视频连线。 -->"},{"slug":"py-matplotlib-two-api","filename":"2020-10-04-py-matplotlib-two-api.md","date":"2020-10-04","title":".py | matplotlib笔记：两种API","layout":"post","keywords":["md","py"],"excerpt":"import matplotlib.pyplot as plt","content":"\n“图”这个字在英语中可以对应好几个词，picture, image, figure, plot... 其中的 plot，意思是展示两组或两组以上的数据之间关系的图像。用时髦一点的话说，就是数据可视化的产物。  \n\n所谓`matplotlib`，顾名思义，~~`mat` 表示山寨 MATLAB~~，`plot` 的含义如上所述，`lib` 表示这是 python 的[一个第三方库 (library)，而不是某种领域专用的编程语言 (domain specific languange, DSL)](http://www.yinwang.org/blog-cn/2017/05/25/dsl)。\n\n所谓 API，全称是 application programming interface, 应用程序接口，约等于在你有了自己的数据，想调用 matplotlib 来画图的时候，那些需要写在你自己代码里的语句的语法规则。\n\n因为是代码库，所以在一切开始之前，需要在你的 python 代码开头声明引入\n\n```python\nimport matplotlib.pyplot as plt\n```\n\n`plt` 可以换成你喜欢并且不和其他代码冲突的名字，但是这三个字母是大家的约定俗成的，网上的绝大多数示例代码都这么写，~~照抄就完事了。~~\n\n## `matplotlib` 的两种 API\n\n`matplotlib` 有两种 API，（其实还有第 3 种 `pylab`，但它没能经得起时间的检验，已经处于官方极不推荐的状态），分别是：\n\n- 基于状态的 (state-based)\n- 面向对象的 (object-oriented)\n\n两种风格混用的话大概率没法玩得转，会产生各种出人意料的输出结果，新手 debug 的能力又比较差，所以最好先选边站队，有时间再学剩下的一个。\n\n对于有 MATLAB 基础的朋友，基于状态的 API 语法和 MATLAB 几乎一模一样，几乎可以直接上手，当年 python 算是后起之秀，这一招当初就是为了从 MATLAB 那里吸引用户， ~~相当歹毒。~~ 这套接口本身也比较简单，适合在调试程序的时候快速看一下结果，检查错误。\n\n对于一般的初学者，matplotlib 的代码本身就是用面向对象的编程范式写成的，学习这套 API 可以更好的理解代码，知道自己究竟在干什么，顺便还可以熟悉一下面向对象的编程范式。现在学 python 之前就会 MATLAB 的人越来越少，网上 ~~可供复制粘贴~~ 的示例代码越来越多地使用面向对象的语法，学习面向对象的接口也更加实用。\n\n## 两种 API 的相同任务\n\n![](/photos/2020-10-04_figure-and-axes.png)\n\n上图来自网上随便找的一篇论文，可以看到，一般我们会把信息相关的几幅小图放在一起，在文章排版的时候，这张组合在一起的图片算作一个单位。在 matplotlib 里面，这样一个基本单位叫做 `figure`，而每一幅小图叫做 `axis` （变量名常简写作 `ax`）。平时的单图可以看作只有一个 `axis` 的 `figure`，多图的时候往往用一个 tuple `axes` 的 `__getitem__()` 方法来控制每个子图。\n\n![](/photos/2020-10-04_anatomy-of-figure.png)\n\n上图[来自官网](https://matplotlib.org/gallery/showcase/anatomy.html#anatomy-of-a-figure)，图中的蓝字就是 matplotlib 认为的一张只有一个 axis 的 figure 所包含的元素。\n\n两种 API 要做的事情，就是建立 `figure` 和 `axis`，然后提供函数/方法来生成或者改变各个元素。\n\n## 基于状态 (state-based)\n\n所谓基于状态的 API，不太好解释，前面已经说过，在每个函数前面加上 plt，剩下的就和写 MATLAB 几乎完全一样。\n\n看[官网给出的教程](https://matplotlib.org/tutorials/introductory/pyplot.html#sphx-glr-tutorials-introductory-pyplot-py)，可以观察到两个有趣的现象：\n\n- 几乎没有赋值运算符 `=`\n- 几乎所有的 `.` 前面都是 `plt`\n\n也就是说，与 matplotlib 相关的命令都是函数，而且不需要将返回值赋给任何变量。`figure` 和 `axis` 的概念被隐藏起来了，`plt.figure()` 建立一个 figure；`plt.subplot()`建立多个 axes，并且将程序的注意力放到函数参数指定的子图上；紧跟着的设定各种元素的函数都会作用到之前最新一个 `plt.subplot()` 所指定的子图上。\n\n没有赋值说明函数的返回值并不重要，这些函数都会作用在后台维护的 figure 和 axis 的状态机上面，也就是说这些函数都有副作用，不是纯函数。\n\n## 面向对象 (object-oriented)\n\n[作为对比](https://matplotlib.org/gallery/showcase/anatomy.html#anatomy-of-a-figure)：\n\n- 头几句会有赋值运算符 `=`，被赋值的变量名一般就是 `fig` 和 `ax`。\n- `.` 前面都是 `fig` 和 `ax`，其中 `ax` 居多。\n\n`fig` 和 `ax` 分别是 `matplotlib.figure.Figure` 和 `matplotlib.axes.Axes` 两种对象的实例，画图和调整都是在调用两种对象的方法，主要是 `ax` 的方法。\n\n## 不同之处 Cheat Sheet\n\n绝大多数命令，在两种 API 之下的名字都一样，差别就在于开头究竟是 `plt.` 还是 `ax.`，但是少数命令不同，下面做了一个表格，进行一个不完全的列举：\n\n| State-Based | 任务 | Object-Oriented |\n|---|---|---|\n|`plt.figure(**args)`|__新建 figure__|`fig = plt.figure(**args)`|\n|`plt.subplot(**args)`|__新建 axis__|`ax = fig.add_subplot(**args)`|\n|好像没有|__同时新建 figure 和复数 axes__|`fig,axes = plt.subplots(**args)`|\n|`plt.title(**args)`|__设置 figrue 标题__|`ax.set_title(**args)`|\n|`plt.xlabel(**args)`|__设置 x 轴名称__|`ax.set_xlabel(**args)`|\n|`plt.ylabel(**args)`|__设置 y 轴名称__|`ax.set_ylabel(**args)`|\n|`plt.xlim(**args)`|__设置 x 轴范围__|`ax.set_xlim(**args)`|\n|`plt.ylim(**args)`|__设置 y 轴范围__|`ax.set_ylim(**args)`|\n\n其他不同的命令，以后用到的时候会随手更新。\n"},{"slug":"measure-david-with-imagej","filename":"2020-04-29-measure-david-with-imagej.md","date":"2020-04-29","title":".ijm | 用 ImageJ 给大卫量尺寸","layout":"post","keywords":["md"],"excerpt":"<s>用 ImageJ 测量大卫雕像的丁丁长度</s>","content":"\n前两天看果壳，推送的头条是《古代雕塑的丁丁真的都（像大卫）那么小吗？》，为了立论，作者根据图片测量了大卫像的丁丁长度，虽不十分精确，但是好歹有个定量的结果——“4厘米不到”，还给出了使用的工具——ImageJ。\n\n![](/photos/2020-04-25_guokr-quote.png)\n\n怎么做到的，咱也来试试？\n\n## `ImageJ` 是啥？\n\n上一篇文章（[《“你这是不是原图直出啊？”》](raw-image-or-not)）我们提到了，专业相机可以记录 RAW 格式的图片，这类图片包含的信息比平时常见的图片更多，也需要专门的软件进行处理。不只是消费者水平的相机，实验室里的显微镜、CT机都可以看作是特殊的照相机，它们产生的图像自然也需要专业的软件来读取和处理。\n\nImageJ 就是这样一款软件。相比于直接使用编程语言（比如 MATLAB，python 的 scikit-image 模块），它提供的方框、多边形、椭圆选区工具，直接用编程语言来替代的话要麻烦许多；而且各种操作的结果都可以几乎实时地反馈在画面上，不需要一遍又一遍地 `imshow()`。\n\nImageJ 用 Java 写成，所以跨平台的表现一致，~~用户界面有一股浓郁的 Windows 95 风味，而且在 MacOS 上也一样，Windows 用户幸灾乐祸中~~。代码开源在 GitHub，免费，能让用户知道自己的每个操作究竟在干啥，而且可以自己开发拓展功能，打包成第三方的发行版。我们今天要用的 Fiji，就是一个集成了很多常见拓展功能的 ImageJ 发行版。\n\n## 具体怎么量？\n\n先简单列举一下步骤，详细介绍在后面：\n\n- 下载大卫像的正面照。\n- 运行 `ImageJ`，打开图片。\n- 复制图片，防止我们的操作改变原始数据。\n- 用矩形选择框选定大卫像从头到脚的区域，记录选择框的高度。\n- 用 `Analyze -> Set Scale` 工具确定像素和厘米的换算关系。\n- 用直线工具画出要测量的距离，从主面板读取长度。\n- 用 `Analyze -> Tools -> Scale Bar...` 绘制比例尺。\n\n如果你看到这么简略的介绍就能脑补出如何操作，那么就可以 ~~（关掉这篇文章）~~ 跳过下面的详细介绍了。\n\n我们使用的照片来自维基百科的“大卫像”汉语词条，图片作者是 Jörg Bittner Unna，根据 CC-BY-3.0 协议共享。我们下载的是中等尺寸（480×720）的图片：\n\n![](/photos/2020-04-25-david.jpg)\n\n鼠标右键单击画中的任意一点，在弹出的菜单中选择 `Duplicate`，在对话框中选确定，这样我们就得到一张原图的副本。之后的所有操作都在这个副本上进行，这样即便有任何操作失误，对副本做出了不可逆转的伤害，都不会影响到我们的原始数据。如果数据的安全和完整得不到保证，无意之失叫做学术错误，有意为之叫做学术造假。\n\n选择工具栏中的矩形选择框（图中主面板左一的阴影按钮），在图中画出一个矩形，上下边分别是大卫像的头顶和脚底，**画完后鼠标不要乱动**，然后在主面板的底部读出高度 `h=595`，此处的单位是像素：\n\n![](/photos/2020-04-25_find-height.png)\n\n点击主面板的 `Analyze` 按钮，在下拉菜单中找到 `Set Scale`，在弹出的对话框中，`Distance in pixels` 填入我们读出的 595 像素，`Known distance` 和 `Unit of length` 填入作者假设的 150 cm：\n\n![](/photos/2020-04-25_set-scale.png)\n\n然后就可以开始正式的测量了。再次使用矩形选择工具，**画完后鼠标不要乱动**，主面板底端的 `length=20.93` 就是以 cm 为单位的距离了：\n\n![](/photos/2020-04-25_measure-head.png)\n\n丁丁可以选中直线工具（左起第五个），用鼠标拖拽的方法画线，主面版的底端也会显示结果。\n\n量完之后，要想方便他人，可以在图上画出比例尺。再次点击主面板的 `Analyze` 按钮，在下拉菜单中找到 `Tools`，然后找到 `Set Scale`，在弹出的对话框里选择合适的选项，然后点击确定，就可以在画面中添加一个比例尺：\n\n![](/photos/2020-04-25_menu-scale-bar.png)\n\n如果对操作满意的话，就可以保存图片了。~~（我对这个结果不满意，所以直接没有保存就退出了。）~~ 原图没有改动，可以直接关闭，软件应该也不会发表反对 ~~（通知书）~~ 对话框。\n\n## 量出来个几？\n\n我们的截图显示，按照 150 厘米的身高，大卫的头颅对应的长度是 20.92 厘米。\n\n对于丁丁，由于图片中的阴影部分对起止点的认定有很大的干扰作用，几次测量的结果都不相同，最小值是 3.91 厘米，最大值是 4.45 厘米。\n\n## 这么量对吗？\n\n我们的结果和原文有一定不同，但处于同一数量级 ~~（废话）~~。由于原文并没有介绍自己使用的数据来源，原文中的图片本身并不适合测量这个数据（下面会讨论），而且还给关键区域加了遮挡，所以我们无从比较两个结果的差异。\n\n对于我们自己的测量，我们可以对如下几个误差来源进行讨论：\n\n- 相机视差\n- 深度误差\n- 关于大卫身高的假设\n\n### 相机视差\n\n我们在几何光学里学的相机成的是倒立的实像，物高和像高之间的关系按照牛顿公式取决于物距、像距、透镜的焦距等等知识，都用到了“傍轴条件”这一假设。换句话说，考虑的都是理想状态，要想满足这种理想状态，基本相当于要求镜片的直径无限大，厚度无限小，焦距还可以是任意值。\n\n我们的相机显然不满足这种理想状态（简单证明，理想状态下大卫像和后面的墙壁显然不可能同时被同一个透镜组严格地在同一个平面上成像）。景深范围内，不同深度的物体都可以在图中成像，但是存在着透视关系。同样长度的物体，在不同的深度下，在图片上的成像的长度不同；在画面中央的长度，和在画面边缘时的成像长度也不同。B 站里的摄影师们拍小姐姐的时候，脸尽量靠近画面中间，腿一般出现在画面边缘，就是利用广角镜头的透视效应。\n\n我们的照片目测是用中长焦镜头拍摄的，透视变形不是很明显，但是在测量雕像身高的时候还是遇到了困难，我们用的是脚尖到头顶的距离，如果取脚的中间的话结果就会短一些。\n\n### 深度误差\n\n我们的照片是二维的，所以之前的测量，相当于假设被测距离的起点和终点在同一深度上。如果两点之间有一个深度差，那么测量线、深度差、实际距离三条线会构成一个直角三角形，实际距离需要按照勾股定理进行计算。我们这里得不到深度差，所以这一误差就成了一个系统误差，而且是有偏 (biased) 的，真实结果不小于测量值。\n\n我们选取的照片几乎平视着整座雕像，尤其是我们比较关注的部位，所以深度差几乎可以忽略；果壳提供的图片是仰视的，所以如果这就是测量所用的照片的话，误差应该比我们更大。当然了，还是由于作者没有公布数据和方法，以及原文中图片有马赛克，这种比较没有意义。\n\n### 关于大卫身高的假设\n\n开头的文章截图已经给出了将大卫像对应的身高定为 150 cm 的理由，我们来看一下，这个理由成立吗？\n\n文中说“这相当于现代12岁男童的平均身高”，但是米开朗基罗真的是以 12 岁的人体为原型来创作的吗？我们为什么量了一下大卫的头高呢？是为了算头身比。大卫像头身比是 1:7.5，这已经几乎是一个成年人的比例了。艺术作品为了凸显对象的某些特点和气质，某些特点不符合原型的设定，这在艺术上是完全可能而且可以接受的。《红楼梦》书中的绣像，后来的电视剧中演员的服化道，几乎都比原书的年龄设定更成熟。\n\n像是某一器官的大小，其衡量标准应该是相对于某个基准的（比如体长）的比值，也就是一个无量纲的量。像是4厘米这种带有长度量纲的量，虽然的确是能比一个比例更容易让人有直观的感受，但是结论很容易因为对身高的质疑而动摇。\n\n最后，我觉得最致命的一个问题是，大卫像其实是文艺复兴时期的作品，和罗马仿希腊的艺术品一样，是一个其他文明模仿希腊的产物，而且模仿的过程中融入了很多近代人文主义的精神。这一题材也并非来自于希腊文化，大卫击杀歌利亚是犹太教-基督教文化中的故事。在这种情况下，作者应该额外论证文艺复兴对丁丁的描写照搬了希腊文化的习俗。既然如此，为什么不直接找一个古希腊时期的古希腊作品呢？\n\n而且原文中有“你也许会说大卫像也许只是一个个例，万一只是米开朗基罗的恶趣味呢？一个例子不能说明古希腊雕塑里的小丁丁都很小”字样，不好意思，米开朗琪罗的作品不是古希腊雕塑的“一个例子”。\n\n## 课后作业\n\n![](/photos/2020-04-25_horseman.jpg)\n\n上图是法国画家雅克-路易·大卫的作品《跨越阿尔卑斯山圣伯纳隘道的拿破仑》，请根据该作品完成下列题目：\n\n1. 请寻找资料估测马的体长，并据此估测拿破仑的身高。\n2. 请寻找拿破仑身高相关的史料，并据此估测马的体长。\n3. 比较前两题的结果。\n\n啥？你说拿破仑当时骑的是一头驴？哦，那没事了，下课！\n"},{"slug":"Arch-Linux-in-VirtualBox","filename":"2019-09-03-Arch-Linux-in-VirtualBox.md","date":"2019-09-03","title":".sh | 在 VirutalBox 中安装 Arch Linux","layout":"post","keywords":["md"],"excerpt":"这是一篇速记，相当于实验的原始记录。日后有可能根据这些记录，整理出一些心得体会，或者全流程的教程。","content":"\n> 这是一篇速记，相当于实验的原始记录。日后有可能根据这些记录，整理出一些心得体会，或者全流程的教程。\n\n第四代树莓派（[Raspberry Pi 4](https://www.raspberrypi.org/products/raspberry-pi-4-model-b/)）刚刚推出的时候，脑子一热买了一台。作为一台计算机，只用原装的操作系统有点咸鱼了，所以打算安装 Linux 的一款发行版。既然要追求刺激，那就贯彻到底咯，于是就决定挑战一下 Arch Linux。 ~~（反正早晚也要吃灰）~~\n\n既然要挑战一个很有挑战性的操作系统，那么先在虚拟机里试试水就很有必要了。（后来才发现用这个虚拟机往树莓派里写操作系统，也是官方比较推荐的一种安装方式。）于是就开始了这篇速记中的一系列折腾。\n\n## 我的硬件和软件配置\n\n* **硬件**：蓝厂 CPU、绿厂显卡、2*8 GiB 内存、小 SSD + 大 HDD 硬盘、iTX 主板，机龄不到两年，日常可以流畅使用。\n* **物理机操作系统**：Windows 10 Pro\n* **虚拟机控制软件**：VirtualBox 6.10，短暂用过 5.12 版本\n* **虚拟机镜像文件**：Arch Linux 2019.08.01, Windows 10, Fedora 30 Workstation\n\n## 虚拟机托管软件 VirtualBox\n\n可以看一下编程随想的博客里的 [系列文章](https://program-think.blogspot.com/2012/10/system-vm-0.html)。\n\n我的手头一直有虚拟机托管软件，但是没有编程随想那样的需求，平时几乎用不到。（用虚拟机学 Linux？你可拉倒吧）不过之前很轻松地装过一些 Linux 的发行版，主要是 Fedora，所以没觉得这部分会有什么问题，直接开始了主线任务：下载 Arch Linux 的安装 ISO 镜像、检查校验和（FCIV，教程可以看 [这里](https://program-think.blogspot.com/2013/02/file-integrity-check.html)）、按照官网 [安装指南](https://wiki.archlinux.org/index.php/Installation_guide)（[中文](https://wiki.archlinux.org/index.php/Installation_guide_(%E7%AE%80%E4%BD%93%E4%B8%AD%E6%96%87))）操作，然后发现虚拟机连不上网 :-(\n\n### 问题：虚拟机无法通过 NAT 或 NAT Network 模式联网\n\n具体表现为 `ping www.google.com` 报错：\n\n``` bash\n> ping archlinux.org\n# Temporary failure in name resolution\n```\n\n此时的网络设置没有修改过，就是几乎没有可选参数的 NAT 模式。\n\n第一反应当然是把报错的信息直接复制粘贴到 Google 里，然后看到了 [这个帖子](https://bbs.archlinux.org/viewtopic.php?id=237461)。\n\n回答用一种很有礼貌的方式表述了 RTFM，并给出了官方的 [Network Configuration 页面](https://wiki.archlinux.org/index.php/Network_configuration)。\n\n按照这个页面的步骤，因为我的 ISP 给的是动态的 IP，需要用DHCP，DHCP 是啥我都不知道，搞得我跪着下载了编程随想书单里的《Richard Stevens TCP-IP 详解》，解压后看着 3 卷一百多个 PDF 文件，默默关上了资源管理器……\n\n继续变换关键词搜索解决方案，在这期间改动过防火墙设置，又重置了一次；重装了两次旧版本 VirtualBox，因为第一次重装忘记删除配置信息（Windows 10 Pro 在 `%userprofile%\\.virtualbox`，见[这个帖子](https://superuser.com/a/1429931)）；另外装了 Windows 10 和 Fedora 30 两个虚拟机；找来一个玩过 VirtualBox 而且打包票能解决问题的同学。确定了和虚拟机中的操作系统无关，VirtualBox 的 Bridge 桥接模式是可以联网的，但是总不能放任虚拟机的重要联网功能残废着啊 ~~（何况 NAT 模式在多重代理中不可或缺）~~，还是一筹莫展。\n\n就在折腾了这么几天之后，Windows 推送了一次更新，神奇的一幕发生了！更新之后，不论是 NAT 还是 NAT Network 模式，虚拟机都能正常上网，所以，可能真的是“天下本无事，庸人自扰之”吧……\n\n**解决方法：~~吃饭睡觉打豆豆~~**\n\n## 安装 Arch Linux\n\n继续按照官方指南进行安装，不得不说这份指南写的真的不好，至少对于新手是不友好的。一个典型的例子就是下面这句：\n\n> 你需要安装 Linux 引导程序以在安装后启动系统，你可以使用的的引导程序在 [启动加载器]() 中，请选择一个并且安装并配置它，比如 [GRUB]()。\n\n你该怎么办？公布一下正确答案：点击 `GRUB` 那个链接，在一众分类中找到适用于你的计算机配置的那一节，目力忽略各种介绍，找到那一两行有用的命令。ε=( o｀ω′)ノ\n\n反正单看这一句，以我的理解能力，我是没办法正常通关，于是就有了下面硬盘格式化的问题。\n\n### 问题：硬盘格式化不正确\n\n我是用 fdisk 进行的硬盘分区，在正常模式之下似乎找不到选择分区类型的选项，所以我的 EFI 分区也被当作了 Linux 文件系统，所以在很多个步骤之后报错，具体的报错信息已经不记得了。\n\n还好在编程随想的博客的评论区里见到过萌狼，一个 Arch Linux 用户，顺手去他家的博客逛了逛，知道他写过一系列关于 Arch 的博客。所以找到了《给 GNU/Linux 萌新的 Arch Linux 安装指南》，于是直接重建了一个空白虚拟机，按照新教程的流程走了一遍。\n\n**解决方法：看[萌狼的安装教程](https://blog.yoitsu.moe/arch-linux/installing_arch_linux_for_complete_newbies.html)，用 cgdisk 分区**\n\n### 问题：开机后显示 UEFI interactive shell\n\n本以为万事大吉，结果卸载掉安装 ISO 之后开机，总是会进入 UEFI interactive shell。输入 `exit` 之后重新开始倒计时，然后再次进入这个 shell……\n\n就在写这篇速记的时候，我找到了官方 wiki 的 [这个条目](https://wiki.archlinux.org/index.php/GRUB_(%E7%AE%80%E4%BD%93%E4%B8%AD%E6%96%87)#%E5%90%AF%E5%8A%A8%E6%97%B6%E8%BF%9B%E5%85%A5%E4%BA%86%E6%95%91%E6%80%A5%E6%8E%A7%E5%88%B6%E5%8F%B0)，理论上应该是针对这个问题的，但是看一下就知道，并没有提供可操作的解决方案。\n\n通过 STFW，找到了这篇文章：《[如何在 VirtualBox 内安装 Arch Linux](https://cli.ee/archlinux-virtualbox)》，里面提到这个问题是由 VirtualBox 的 UEFI 引起的，同时提供了解决方法：\n\n```bash\n\nShell> bcfg boot dump -v              # 查看启动的菜单\nShell> bcfg boot rm 0                 # 删除光驱启动目录\nShell> fs0:                           # 进入 EFI 分区\nFS0:\\> ls                             # 查看目录，可以看到 EFI 目录\nFS0:\\> mkdir EFI\\boot                 # 在 EFI 目录下创建子目录\nFS0:\\> ls EFI                         # 查看 EFI 目录，确认是否已创建子目录 boot\nFS0:\\> cp EFI\\grub\\grubx64.efi EFI\\boot\\bootx64.efi # 复制 efi 文件并重命名\nFS0:\\> exit                           # 退出\n```\n\n执行完这一步，开机之后就可以看到 GRUB 的选择操作系统的界面了，因为只安装了 Arch Linux，我们只能看到两个选项，一个 `Arch Linux` 正常启动，一个 `Advanced Options for Arch Linux`。\n\n### 问题：开机后无法进入图形界面\n\n本以为万事大吉，结果开机之后选择 `*Arch Linux` 选项，屏幕上只剩下两行字，然后没有其他反应。\n\n同样是上网搜了一下，发现如果按下 `Ctrl + Atl + F2` 是可以进入命令行模式的，随便执行了几个命令，发现系统工作正常。用 `journalctl` 看一下系统日志，发现在最后一段里经常出现高亮的几行里面，都有 `drm` 这个关键词，再次搜索，发现是和显卡相关的。因为是在虚拟机里，所以就去 VirtualBox 里康康有啥可能会导致问题的选项。\n\n**解决方法：在 VirtualBox 的虚拟机显示设置界面，禁用3D加速**\n\n<hr class=\"slender\">\n\n至此，所有的坑都已经踩完了，开机之后可以看到正常的登录界面，Duang~Duang~~~\n\n![](/photos/arch.png)"}]],["phy",[{"slug":"error-propagation-and-philosophy-of-science","filename":"2024-10-21-error-propagation-and-philosophy-of-science.md","date":"2024-10-21","title":".tex | 误差的传递，科学之所以科学","layout":"post","keywords":["tex","phy","phi"],"excerpt":"多变量测量的误差传递，及其在科学哲学中的作用。","content":"\n如果一个物理量需要用多个直接观测量计算出来：$$y=f(x_1,x_2,...,x_n)$$，这样的量叫做因变量，直接观测量叫做自变量。（比如用直尺测量长方形面积时，长、宽是自变量，面积是因变量，通过长和宽相乘计算面积的方法是一个函数。）\n\n因为中小学减负，因变量的说法不教了，改叫函数值，为了少学一个知识点。\n\n但是学过 C/C++ 的应该知道，对于 $$y=f(x_1,x_2,...)$$\n\n- 因变量 $$y$$ 是一个左值，指向 $$y$$ 的指针 `float *p = &y;` 拿到的地址，位于内存的数据区；\n- 函数值 $$f(\\cdot)$$ 是一个右值，$$f$$ 本身就是一个指针，`void *fp = f;` 拿到的地址，位于内存的指令区。\n\n## 多变量测量的误差传递\n\n先跳过单变量误差的部分（大致原理在《[贝叶斯，从公式到世界观](bayesian-equation-and-view-of-world)》一文频率学派的部分，具体细节以后再写），不论是测量仪器的说明书给出的误差，还是测量者通过独立重复实验取得的统计误差，我们先假设已经拿到了观测量 $$x$$ 的测量值 $$\\bar x$$、误差 $$\\Delta x$$\n\n因为全微分公式，对于 $$y=f(x_1,x_2,...,x_n)$$\n\n$$\n\\mathrm{d}y = \\frac{\\partial f}{\\partial x_1}\\mathrm{d}x_1 + \n\\frac{\\partial f}{\\partial x_2}\\mathrm{d}x_2 + ... + \\frac{\\partial f}{\\partial x_n}\\mathrm{d}x_n = \\sum_i^n \\frac{\\partial f}{\\partial x_i}\\mathrm{d}x_i\n$$\n\n又因为误差相对于真值往往小几个数量级，所以我们把误差看作是真值的微分，用 $$\\Delta$$ 取代 $$\\mathrm{d}$$. （有人问真值为 0 怎么办，绝大多数情况下可以通过平移零点定义的办法来几乎任意地改变测量值的数量级，而误差不会因为这种变换而出现数量级变化。）\n\n还因为对多个自变量的测量是相互独立的，每个自变量 $$(x_1,x_2,...,x_n)$$ 占据相空间中的一个维度，维度之间互相正交。\n\n所以物理上，因变量的误差就是上述 ~~微分~~ 微差向量的“长度”，以  L2 范数 (norm) 来衡量：\n\n$$\n\\begin{array}{rcl}\\Delta y & = & \\sqrt{ \\left(\\frac{\\partial f}{\\partial x_1}\\bigg|_{\\vec x}\\right)^2\\Delta x_1^2 + \\left(\\frac{\\partial f}{\\partial x_2}\\bigg|_{\\vec x}\\right)^2\\Delta x_2^2 +...+ \\left(\\frac{\\partial f}{\\partial x_n}\\bigg|_{\\vec x}\\right)^2\\Delta x_n^2 } \\\\ & = & \\sqrt{\\sum_i^n{\\left(\\frac{\\partial f}{\\partial x_i}\\bigg|_{\\vec x}\\right)^2\\Delta x_i^2}}\\end{array}\n$$\n\n物理学家因此不害怕误差——理论物理的模型哪怕非常复杂，在数学上往往依然“性质优美”，只要理论的自变量可以在实验上测量，误差明确且有限，那理论给出的预测值的误差就同样明确且有限，依然可以指导实践。\n\n## 误差与可证伪性\n\n而根据卡尔·波普尔的科学哲学，具体来说就是可证伪性的划界标准，科学就不只是不害怕误差了，简直是依赖误差而生，靠误差来和伪科学划清界限。\n\n所谓科学的可证伪性，《[科学是什么？——兼谈“非科学、伪科学、反科学”和一些常见谬误](https://program-think.blogspot.com/2015/10/What-is-Science.html)》一文概括为：\n\n- 科学理论是一个相互关联的**命题**的**集合**。\n- 科学理论必须是基于**演绎**法建立整个理论体系的。也就是从不证自明的**定律**出发，依据**逻辑规则**，推论出各种各样的**定理**。\n- 理论中的命题必须是**客观**陈述，也就是能由不同的主体进行独立检验。\n- 检验的方式是**证伪**，也就是寻找现实中的一个现象，说明从理论中某个命题是错的。经过证伪程序，且没能被证伪的命题，就被验证为真。（根据逆否命题的等价性，演绎推论如果被证伪，它的逻辑前提也会连锁被证伪。那科学几百年来靠什么幸存，我们以后再狡辩～）\n- （既然一个存在命题就能否定一个学科理论中的待验证命题，）科学理论中的命题应该是**全称命题**，此即科学的普世性 (universality)。\n- 对全称命题的**特设性修正**（比如把“所有的天鹅都是白色的”修正成“所有北半球的天鹅都是白色的”），应该要提高理论的可证伪程度，否则就是伪科学。\n- 以上各个要求，单独只构成必要条件。\n\n所以定量科学的科学性就体现在，\n\n- 只要理论的自变量和因变量可以在实验上测量，\n- 自变量的误差明确且有限，那理论给出的预测值的误差就同样明确且有限，\n- 因变量的误差同样明确且有限，\n- 将理论预测值和因变量测量值摆在一起，只要差距不大于两者的误差，（技术细节在统计学中的假设检验部分。）\n- 我们就认为对“理论预测和因变量真值相等”的证伪失败了，从而接受他们相等。\n\n因为实验仪器和方法的进步可以缩小误差范围，增加科学理论的可证伪性，一条无限延展且随时可以投入精力的赛道从此出现，科学从相隔几代的少数天才之间的思维接力，变成了夙兴夜寐前赴后继的竞赛。\n\n科学家之间的竞争催生了对制造仪器之工程技术的巨大需求，需求大到部分科学家亲自下场改进甚至发明仪器，科学由此反哺技术；进步的技术使得科学得以产出更高质量的数据，支持更复杂的理论的检验。科学和技术，合成了“科技”一个词。\n\n试想一下，如果科学号称自己绝对精确，不存在误差，要么在弱小之时就被证伪，无法赢得人们的信任；要么任由实验精度低到看不出误差的程度，然后用其他手段维持自己的光辉形象。\n\n之前《[也谈近代科学从西方起步](logical-science-from-west)》一文中说，“物理和数学的区别，在于理论和实验两条腿走路”。如今算是把实验这另一条腿简单介绍完了。\n\n最后需要注意，这里说的是某个哲学理论能够解释科学实践，而不是科学实践必须服从某套哲学理论。\n\n科学只对客观现实负责，不需要对哲学信条负责，不应该对哲学王兼英雄王负责。\n\n## 科学还正确吗？\n\n如果承认可证伪性作为科学与非科学的划界标准，也就意味着，现在包含在科学中的每条知识，都有在未来被更加精确的实验推翻的可能。\n\n这种事也不是没发生过。比如材料的电阻，在相当大的数值范围内，都和温度成线性关系，而且这条线向左延拓到绝对零度时基本为 0。在那个时代，认为电阻来自无规则的热运动，和绝对温标成正比才是符合奥卡姆剃刀原则的理论。但是 1908 年，昂内斯用液氦将汞的温度降到 4.15 K 时，发现汞的电阻突变降低为 0，这就是超导研究的开端。\n\n那还能说科学知识是正确的吗？《费曼物理学讲义》的回答是，不谈科学是不是正确的，只保证科学 (science) 是科学的 (scientific)。也就是保证程序的正确，把正确程序获得的结果交给工程技术，用工程技术上的成就取信于社会，反过来为科学的正确性背书。\n\n所以说，科学家是对科学最不迷信的一批人，一旦实验过程正确，结果和理论不符，那么理论该修改的修改，该放弃的放弃。他们是现有科学最大的破坏者，是成功证伪科学命题最多的一群人。\n\n但同理，科学家又是对科学最坚定的一批人，他们在明知道一个科学命题可能在将来被修正的情况下，依然愿意把它当作前提，继续推理产出新的命题，并试图证伪。\n\n《三体》小说刚开始设置的一大悬疑，大量科学家因为自己正在进行的研究，产出了与理论完全不吻合的随机结果，因为所谓“物理学不存在了”而自杀，这个情节就很成问题。\n\n何况这种事情根本不需要书中的情节设定才会出现，物理学史上早就发生过。比如 β 衰变的质子的动能谱和动量角分布。玻尔想放弃能量守恒定律，泡利想假设一个探测器发现不了的新粒子，这在当时的实验条件下都是尚不能证伪的理论假设，但没听说俩人为这事寻死觅活的。\n\n所以改编成电视剧的时候，几乎重写整个人物关系的网飞版，把自杀改写成了球奸们伪装的他杀；就连以忠实于原作著称的腾讯版，也原创了一段主角主动重启科研装置，直面外星人恐吓的剧情，给原著做了点找补。\n\n## 那我缺的权威性这块谁给我补上啊\n\n坏了，碰到了不该碰的话题，那就先这样吧……\n\n## 报书名儿\n\n- William Lichten.《Data and Error Analysis》\n- 赵凯华《定性和半定量物理学》\n- 卡尔·波普尔《科学发现的逻辑》\n- 理查德·费曼《费曼物理学讲义》\n"},{"slug":"bayesian-equation-and-view-of-world","filename":"2024-09-27-bayesian-equation-and-view-of-world.md","date":"2024-09-27","title":".m | Bayesian 贝叶斯，从公式到世界观","layout":"post","keywords":["tex","m","phy","phi"],"excerpt":"我们老板真是太能吹了，Bro 居然跟隔壁真的在研究物理的课题组 brag abou 我会贝叶斯参数估计，yo know wat ur sayin? 赶紧来补课～","modified":[{"date":"2024-09-27","change":"初稿发布"},{"date":"2024-11-11","change":"修正了对全概率公式的描述。"}],"content":"\n\n## 公式\n\n我上学的时候，贝叶斯公式是概率论里面，少数高中完全不涉及，到了本科才第一次见的公式，所以我从来没背下来过。不过也用不着背，根据条件概率里面的一个平凡结果：\n\n$$\n\\Pr(A|B)\\ \\Pr(B) = \\Pr(B|A)\\ \\Pr(A)\n$$\n\n可以得到 $$\\Pr(A|B)$$ 和 $$\\Pr(B|A)$$ 之间的关系\n\n$$\n\\Pr(A|B) = \\frac{\\Pr(B|A)\\ \\Pr(A)}{\\Pr(B)}\n$$\n\n这就是贝叶斯公式本体。\n\n分母没什么意思，所以一般我们要用全概率公式替换，也就是把整个概率空间 $$\\Omega$$ 划分为全覆盖但是不相交的 $$\\{A_i | \\ A_i \\cap A_{j \\neq i}=\\varnothing,\\ \\bigcup_i A_i=\\Omega\\}$$ （之前写错了，这里的 $$A_i$$ 和上文的 A 没有关系）\n\n$$\n\\Pr(A|B) = \\frac{\\Pr(B|A)\\ \\Pr(A)}{\\sum_i \\Pr(B|A_i) \\Pr(A_i)}\n$$\n\n其中任意一个子事件 $$A_j$$\n\n$$\n\\Pr(A_j|B) = \\frac{\\Pr(B|A_j)\\ \\Pr(A_j)}{\\sum_i \\Pr(B|A_i) \\Pr(A_i)}\n$$\n\n### 根据实验结果筛选理论模型\n\n以上是数学。在科学中，令\n\n- A 为一族理论模型的一组参数取值，记为 $$Param_k$$，下标可任意选取。\n- B 为实验观测数据，记为 *Ob*\n\n$$\n\\Pr(Param_j|Ob) = \\frac{\\Pr(Ob|Param_j)\\ \\Pr(Param_j)}{\\sum_i \\Pr(Ob|Param_i) \\Pr(Param_i)}\n$$\n\n其中 \n\n- $$\\Pr(Param_j)$$ 表示第 j 组参数是模型的正确参数的，未经实验验证，根据零假设计算的 **先验 (prior) 概率；**\n- $$\\Pr(Param_j|Ob)$$ 叫做经过实验观测修正之后的，第 j 组参数正确的 **后验 (posterior) 概率**。\n- $$\\Pr(Ob|Param_j)$$ 在之前的文章中讲过，是当前测量数据下，模型参数的 **似然性 (likelihood)**。\n\n$$\nposterior \\propto likelihood \\cdot prior\n$$\n\n### 举个例子\n\n隔壁组的问题可以简化为下图：\n\n![](/photos/2024-09-27-two-gaussian.png)\n\n- 有两组数据 (x, y1), (x, y2) 可以用同一族函数来拟合。（假设为两个高斯函数的叠加，$$y=f_{A_1,A_2,\\mu_1,\\mu_2,\\sigma_1,\\sigma_2}(x) = A_1e^{-\\frac{(x-\\mu_1)^2}{\\sigma_1^2}} + A_2e^{-\\frac{(x-\\mu_2)^2}{\\sigma_2^2}}$$\n- 两组数据的误差不同。（红色数据点显然比蓝色数据点，相对于理论值偏离得更远一些）\n- 问有没有一个数值，可以衡量每组数据的误差程度。\n\n我给他们的建议是\n\n- 根据自己的专业知识指定先验概率 $$\\Pr(param_j)=\\Pr(A_1,A_2,\\mu_1,\\mu_2,\\sigma_1,\\sigma_2)$$。比如选定一个参数空间的范围，范围之外概率为零，范围之内均匀分布。\n    - $$A_1,A_2 \\in \\left[\\min(\\{Y_1\\}\\cup \\{Y_2\\}),\\max(\\{Y_1\\}\\cup \\{Y_2\\}\\right]$$\n    - $$\\mu_1\\in[\\min\\{X\\},\\max\\{X\\}],\\ \\mu_2\\in[\\mu_1,\\max\\{X\\}]$$\n    - $$\\sigma_1,\\sigma_2\\in[0,\\ \\Sigma_i\\sqrt{|X_i-\\bar X|^2/N}]$$\n- 根据一些假设和统计规律计算 $$\\Pr(Ob|Param_j)$$\n    - 假设误差与 x 变量无关，服从期望为 0 的高斯分布，$$[y_i-f(x_i)]\\sim N(0,\\sigma^2)$$，标准差根据各数据点减去模型预测值的残差估计。\n    - 假设每个数据点的观测相互独立，$$\\Pr(Ob)=\\Pr(\\bigcap_i Ob_i)=\\prod_i\\Pr(Ob_i)$$\n    - 对于模型的每一组参数 ，$$\\Pr(Ob_i|param_j)$$ 取上述高斯分布的绝对值大于残差绝对值的部分，就是钟形曲线两侧尾巴的线下面积。\n- 对参数空间中的每一组值都算出一个后验概率之后，计算整个空间的信息熵（方法见之前的文章）。误差较大的一组数据，应当有更多组参数可以获得类似的拟合结果，从而信息熵更大。\n\n## 世界观\n\n对于概率，有三种理解：\n- 古典的 (classical)、\n- 频率学派的 (Frequentist)、\n- 贝叶斯的 (Bayesian).\n\n### 古典\n\n就是将古典概型推广，成为一种关于可能性的普遍观点——一个随机空间里的随机事件可以分解成若干子事件，子事件还可以再分，直到每个基本事件的概率相等，都等于基本事件总数的倒数，而要计算人们感兴趣的某一事件，只需要数出其包含的基本事件的数量就行了。\n\n让人联想到古希腊古典时代的原子论。时人认为物质世界也不是无限可分的，将任意一种材料打碎研磨，这一过程最终会有一个终点，最终的产物就是这种物质的“原子”。一块材料的大小，就是其所含原子数量的多少。\n\n有人批评这种观点用可能性去定义可能性，有循环论证谬误之嫌。但是看现代化了的概率论，概率被定义成了满足某些条件的函数，公理化是公理化了，逻辑链条是有了坚实的起点，但是那里的概率还能不能被当作可能性的度量，实在是不好说。\n\n有人批评这是机械唯物主义，这种人批判的武器一般是武器的批判，别争辩，先活下来再说。\n\n### 频率学派\n\n这种观点一言以蔽之：概率是频率在样本量趋于无穷时的极限。\n\n科学中（日常生活中也一样，只是人们通常没这么精确），测量误差不可避免，我们每一次的测量哪怕正确，互相之间也会有细微的差别，更不用说和待测的真实值不同了。\n\n解决方法在初中物理实验里学过：多次测量，把平均值当作真值（的估计量），根据标准差计算误差（置信区间、p 值等等……）。\n\n不同的人（假设有 M 个）可以对同一个可观测量进行 N 次测量，对于一个确定的 N，不论这个可观测量本身服从何种概率分布，这 N 个测量值的平均数 $$\\bar X_N$$ 都服从正态分布，这就是中心极限定理（注意不是大数定律）。\n\n当可观测量本身也服从正态分布的时候，就会导致标准差 (standard deviation) 和标准偏误 (standard error of the mean, 常简称为 standard error) 容易让初学者混淆。\n\n而按照这种世界观，所谓一个物理量的真值，就是所有可能的（所有已经发生过的+思想实验中可能发生的）测量的均值 $$\\bar X_\\infin$$。\n\n因为包括可能发生还未发生的测量，所以哪怕我们面对的问题是纯决定论的，客观存在一个确定的真值，无论我们已经进行过多少次测量，都无法保证得到真值。\n\n有人批评这是客观唯心主义，这种人批判的武器一般是武器的批判，别争辩，先活下来再说。\n\n### 贝叶斯\n\n前述世界观好歹还认为真值客观存在——\n\n贝叶斯世界观则直接不再对真值的客观存在下断言，不论先验还是后验，科学理论里的每一条命题，都不再孤单，而是要和所有可能的替代理论打包在一起；也不再“正确”，而是具有一个以概率衡量的可信程度。\n\n实验的作用不再是判断对错，而是在有限的先验知识（现存的科学理论）下，判断新取得的实验结果在多大程度上，更新了旧知识里每条命题的可信权重。\n\n而且每个人掌握的知识不同，先验概率不同，在同样的实验数据面前，所更新出来的知识体系也会不同。\n\n再者，如果先验概率为 0，任你实验数据如何显著，后验概率也一定为 0，所以对“未知的未知”无能为力。实践中，再离谱的先验假设，只要能想到，也要赋一个小而不为 0 的初值。\n\n有人批评这是主观唯心主义，这种人批判的武器一般是武器的批判，别争辩，先活下来再说。\n\n## 送分题\n\n已知本省不超过二十个地级行政单位。一中是本市最好的高中，本科过线人数年年创新高。\n\n已知本市报纸会公布喜报，上有全市前若干名学生的姓名、分数、录取学校等信息。省招办有根据成绩取得全省排名的服务。比如某年本市第十名，全省排名两千名开外。\n\n你能否据此评价母校和家乡的教学质量，以及本省各地区之间教育水平的平均程度？\n\n你该如何评价，从定义原假设和备择假设，到用何种概率分布对先验概率建模？\n\n你有资格评价吗？"},{"slug":"information-entropy-kl-divergence-cross-entropy-mutual-information","filename":"2024-05-14-information-entropy-kl-divergence-cross-entropy-mutual-information.md","date":"2024-05-14","title":".tex | 比较两个概率分布/两条信息","layout":"post","keywords":["tex","phy","m"],"excerpt":"自信息、信息熵、KL Divergence、交叉熵、互信息","hasMath":true,"content":"\n> 自鸣得意了半天，发现这篇文章基本就是维基百科 [Quantities of Information](https://en.wikipedia.org/wiki/Quantities_of_information) 词条英文版的翻译。但是对应的中文词条没有覆盖英文版那么多的内容，所以也不完全是无用功。\n> \n\n## 信息和概率\n\n一条信息由一个命题来表达。（这一个命题可以是对多个命题进行逻辑演算的一个表达式。）\n\n而这个命题解答了人心中的某个疑问。既然这是个疑问，那么在得到确切的信息之前，有众多其他命题，和这条消息一样有可能是问题的答案。既然是有可能，那就是概率论可以派上用场的对方。所有这些可能成为答案的命题一起，构成一个随机变量空间。\n\n比如说一道有 ABCD 四个选项的选择题，如果是单选题，那么答案的随机变量空间就是 {A, B, C, D}，如果是多选题，则是 {A, B, C, D, AB, AC, AD, BC, BD, CD, ABC, ABD, ACD, BCD, ABCD}，如果是排序题、不定项排序题、答案出错了的题……\n\n## 描述一个概率分布的信息量\n\n### 自信息：Self Information\n\n自信息是一个随机事件的性质，也就是针对一个随机变量的**某一个可能取值**而言的。表达式为 \n\n$$\nI(m) = -\\log_n\\left(p(M=m)\\right)\n$$\n\n这是一个无量纲量，但是公式中指数的底数可以任意选择——\n\n- *n* = 2 的时候自信息的单位是 bit，也叫香农 (shannon), 这里的 bit 和二进制位 bit 不完全相同，一个香农是一个二进制位所能表示信息的**上限**：当一个二进制位完全取决于其它位时，这个位不包含任何额外信息，香农数为 0，但这个二进制位依然物理上存在；\n- *n* = *e* 的时候单位是 nat, 因为 $$\\log_e\\equiv\\ln$$ 叫做自然对数；\n- *n* = 10 的时候单位叫 hartley\n\n——单位之间的换算关系由对数的换底公式给出。\n\n这个量在信息论中的意义是，这条消息作为一个不方便问的问题的**答案**，**最少可以**用多少个 n 个选项的单选题套出答案。当 n=2 的时候，每个问题就是一个是非题，也就是一般疑问句。\n\n码农面试的时候经常问一类问题：一堆看起来相同的东西里面有一个不一样，你有一种不能直接测出答案的测量工具，最少需要测量几次才能辨别出来……但是自信息的计算不能提供具体的辨别方法，具体方法还是需要你自己去凑，而面试刷人很多都是在刷这种细枝末节。\n\n当然了，前提是你的面试官懂他自己在问什么，而不是相信美剧《硅谷》里压缩算法可以突破信息论极限的计算机民科～\n\n当 *p* = 0 时，自信息发散为无穷大。不过问题不大，原因在下一节。\n\n### 信息熵：Entropy\n\n信息熵是一个随机变量的概率分布的整体性质。\n\n算法很简单，就是自信息的概率期望，也就是按照随机变量每个取值的概率加权平均：\n\n$$\nS(p(M))=\\mathbb{E}_p[-\\log_n p(M)]=-\\sum_{m\\in M}p(m)\\log_n p(m)\n$$\n\n当 *p* = 0 时，自信息发散，但是概率为零，强行定义两者的积也为零，对信息熵不构成贡献。\n\n当我们只对某一特定的随机事件信息感兴趣，除此以外的所有事件合并为目标事件的补集，就得到二项信息熵 binary entropy:\n\n$$\nS_{binary} = -(1-p)\\log(1-p)-p\\log p = p\\log\\frac{1-p}{p}-\\log(1-p)\n$$\n\n沿着自信息的意义往下走，信息熵在信息论中的意义是，一个将众多信息/命题的集合作为备选答案的**问题**，**最少可以**用多少道 n 个选项的单选题的集合来等价替代。\n\n当这些最优的单选题确定之后，原问题的每一个选项，可以用单选题的答案序号来进行编码。指数的不同底数/信息量的不同单位就是数字的 n 进制，信息量就是相应进制下最大压缩编码后的位数。\n\n当然要讨论压缩的话，还需要另找地方记录各个单选题和选项，也就是压缩字典。\n\n## 比较两个概率分布的信息量\n\n而如何选择单选题，使之成为针对给定问题最优的问题集，会因为各个选项概率分布的不同而变化。即便是同一组信息/备选答案，两套不同的概率分布，各自会给出一套对自己最优的问题集，一套概率分布下的最优问题集不见得是另外一套概率分布下的最优问题集。\n\n> 下面的表达式都只写出了离散变量的形式，连续随机变量需要将求和写成对应的积分。\n> \n\n### 相对熵：Kullback–Leibler (K-L) Divergence\n\n英文里也叫 relative entropy 或者 I-divergence\n\n这里的两个概率分布映射自**同一个**随机变量空间。\n\n$$\nD_{KL}(p(X)|q(X))=\\sum_{x\\in X}p(x)\\log\\frac{p(x)}{q(x)}=-\\sum_{x\\in X}p(x)\\log\\frac{q(x)}{p(x)}\n$$\n\n这个量描述了当 *p*(*X*) 作为各选项的正确概率分布的情况下，用对 *q*(*X*) 最优的单选题去提问，**没问出来的信息**所需要的**额外的**单选题数目/编码数。\n\n在科学应用中，*p*(*X*) 一般是从实验中测量出来的概率分布，*q*(*X*) 是理论模型的预测。\n\n下面的例子计算了一个单选题，选 C、选 B、假想中一群学生的答案统计、胡猜四种概率分布 *p, q ,r , φ* 之间的 KL divergence。因为概率为零会出现发散问题，所以我们取 eps = 10^(-10) 把这些概率值截断：\n\n```python\nimport numpy as np\n\ndef kl_div(p,q,eps=1e-10):\n    p = np.clip(p,eps,1-eps)\n    q = np.clip(q,eps,1-eps)\n    return np.sum(p*np.log2(p/q))\n\np   = np.array([  0,   0,   1,   0])\nq   = np.array([  0,   1,   0,   0])\nr   = np.array([1/6, 1/6, 1/2, 1/6])\nphi = np.array([1/4, 1/4, 1/4, 1/4])\n\nresults = np.empty((4,4))\nfor i,v1 in enumerate([p,q,r,phi]):\n    for j,v2 in enumerate([p,q,r,phi]):\n        results[i,j] = kl_div(v1,v2)\n```\n\n| KL-div(行, 列)/bit | p | q | r | φ |\n| --- | --- | --- | --- | --- |\n| p = [0,0,1,0] | 0 | 33.219 | 1 | 2 |\n| q = [0,1,0,0] | 33.219 | 0 | 2.585 | 2 |\n| r = [1/6, 1/6, 1/2, 1/6] | 14.817 | 25.890 | 0 | 0.208 |\n| φ = [1/4,1/4,1/4,1/4] | 22.914 | 22.914 | 0.189 | 0 |\n\n从结果中我们可以看到：\n\n- 对角线为 0，符合其意义。\n- $$D_{KL}(p,q)$$ 和 $$D_{KL}(q,p)$$ 都应该是 +∞，这里的有限值是 eps 截断的结果\n- 除个别巧合，对称位置的值一般不相等。这个量不同于两点之间的距离。\n\n### 交叉熵：Cross Entropy\n\n这里的两个概率分布映射自**同一个**随机变量空间 X。\n\n概率分布 ***q* 相对于 *p*** 的交叉熵 cross entropy\n\n$$\nCE(p(X),q(X))=-\\sum_{x\\in X}p(x)\\log q(x)=S(p(X))+D_{KL}(p(X)|q(X))\n$$\n\n这个量描述了当 *p*(*X*) 作为各选项的正确概率分布的情况下，用对 *q*(*X*) 最优的单选题去提问，所需要的**总共的**单选题数目/编码数。\n\n类似于二项熵，*p* 和 *q* 之间的 binary cross entropy:\n\n$$\nBCE(p,q)=-p\\log q-(1-p)\\log(1-q)=p\\log\\frac{1-q}{q}-\\log(1-q)\n$$\n\n```python\ndef cross_entropy(p,q,eps=1e-10):\n    p = np.clip(p,eps,1-eps)\n    q = np.clip(q,eps,1-eps)\n    return -np.sum(p*np.log2(q))\n\nresults = np.empty((4,4))\nfor i,v1 in enumerate([p,q,r,phi]):\n    for j,v2 in enumerate([p,q,r,phi]):\n        results[i,j] = cross_entropy(v1,v2)\n```\n\n| Cross Entropy(行, 列)/bit | p      | q      | r   |   $$\\varphi$$   |\n| ---                      | ---    | ---    | ---   | --- |\n| p = [0,0,1,0]            | 0      | 33.219 | 1     | 2   |\n| q = [0,1,0,0]            | 33.219 | 0      | 2.585 |  2  |\n| r = [1/6, 1/6, 1/2, 1/6] | 16.610 | 27.683 | 1.792 | 2   |\n| $$\\varphi$$ = [1/4,1/4,1/4,1/4]    | 24.914 | 24.914 | 2.189 | 2   |\n\n- 对角线上不一定为零，而是自己的信息熵\n- 其他位置和 KL divergence 相差大约为第一个输入分布的信息熵，误差 eps 的截断\n\n### 互信息：Mutual Information\n\n这里的两个概率分布一般来说映射自**不同的**随机变量空间。\n\n$$\nMI(X,Y)=\\sum_{x,y}p(x,y)\\log\\frac{p(x,y)}{p(x)p(y)}=D_{KL}\\left(p(X,Y)|p(X)p(Y)\\right)\n$$\n\n从后一个等号可以看出，这一性质衡量的是 *X, Y* 两个随机变量的联合分布在多大程度上不同于“*X* 和 *Y* 相互独立”的零假设。两个随机变量相互独立时，互相不反映对方的信息，互信息 *MI* = 0。\n\n当从 *X* 所在的随机变量空间取样的难度比较大的时候，我们需要用容易取样的**另一个变量空间**的随机变量 *Y* 来推测 *X* 的情况，互信息就可以用来论证我们这种选择的合理性。\n\n## 扯点闲篇\n\n### PyTorch 中以此为基础的 loss functions\n\n`torch.nn` 中有如下几个和今天的文章相关的 loss functions：\n\n- `torch.[nn.KLDivLoss](https://pytorch.org/docs/stable/generated/torch.nn.KLDivLoss.html#torch.nn.KLDivLoss)`\n- `torch.[nn.CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss)`\n- `torch.[nn.BCELoss](https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html#torch.nn.BCELoss)`\n- `torch.[nn.BCEWithLogitsLoss](https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html#torch.nn.BCEWithLogitsLoss)`\n\n之所以没直接用这些函数计算上面的例子，是因为 `KLDivLoss` 是按元素计算，随后需要自己求和；`CrossEntropyLoss` 又是按类别的，还不需要归一化，而且文档的解释很复杂，我到现在也没看明白；而且还要注意这些函数的设计输入是不是 logit，这是机器学习里的概念，在此不展开了。\n\n### 玻尔兹曼的墓志铭\n\n$$\nS=k\\log W\n$$\n\n其中 *S* 是（微正则系综中的）热力学熵，*k* 是玻尔兹曼常数 $$k_B$$，*W* 是因为刻碑的师傅不会写 *Ω*。\n\nW 或 Ω 是处于相同能量的热力学状态的数量。因为你都需要统计物理了，显然是只知道能量，没办法知道所考虑的微观粒子究竟处于哪一个热力学状态。那此时的零假设就是处于所有状态的可能性相等，*p* = 1/Ω，信息熵 \n\n$$\nS =-\\sum_{m\\in M}p(m)\\log_n p(m)= -\\Omega\\cdot(\\frac{1}{\\Omega}\\log\\frac{1}{\\Omega})=\\log\\Omega\n$$\n\n和热力学熵只相差一个玻尔兹曼常数。这是因为信息熵是无量纲的，熵和温度的量纲相乘之后需要得到能量的量纲，只能由 $$k_B$$ 把量纲凑齐，而数值是自由能相关的实验里测出来的。\n\n好像这就是高中物理里熵的定义式是吧。\n\n上了大学以后，正则系综和巨正则系综中的熵也分别就是各自体系中各状态的概率分布的信息熵，乘上玻尔兹曼常数。~~（我也忘得差不多了，试图萌混过关）~~\n\n### 善卜者无先见之明\n\n公元 451 年，阿提拉 Attila 率领匈人攻入罗马领土，横扫有大量其他民族居住的高卢地区。西罗马帝国将军艾提乌斯 Aetius 联络了众多畏惧匈人的民族组成联军，其中包括西哥特人的王狄奥多里克 Theodoric，两军会战于卡塔隆 Catalaunian 平原。\n\n本来想用这个故事举例子来着，因为我记得阿提拉在战前找了个大师算了一卦，说是一位国王将战死，一个国家将崩塌。于是阿提拉很高兴，以为哥特人和狄奥多里克要玩完了，结果战斗打响，狄奥多里克确实死于乱军，但是罗马和哥特等族的联军击败了匈人，阿提拉的霸业雨打风吹去。\n\n于是试图说明算命的魅力就在于，用文字游戏表达一个自信息比较低的命题，同时误导对方相信一个自信息高得多的命题，在心理疏导之外，赚一个信息熵的差价。\n\n结果查证的时候发现好像不是这么回事，Barbarian Rising 故事片里的预言内容不一样；维基百科上没给出处，说算命的很准，于是阿提拉推迟到下午作战，方便晚上跑路；其他地方甚至压根没有算命的情节。但是写都写了，需要积累高考作文素材的小朋友们还是可以假装被我误导了~\n\n当然了，算命这个事还有一种情况，就是打着不确定的幌子，售卖确定但不方便承认自己确定的信息，那就是另一种生意，和另外的价格了~"},{"slug":"equivlance-between-diffusion-equation-and-random-walk","filename":"2024-04-25-equivlance-between-diffusion-equation-and-random-walk.md","date":"2024-04-25","title":".tex | 扩散方程和随机游走的等价","layout":"post","keywords":["tex","phy","m"],"excerpt":"之前 MCMC 讲错了","hasMath":true,"content":"\n> 这些内容总结自美国研究生级别的《数学物理方法》两次课的笔记，大约两个小时。\n<br>如果是中国大学本科的话，认真的老师半个小时庶几可以讲完;\n<br>念 PPT 就算上课的话 15 分钟可以讲完，附赠一个段子;\n<br>翻转课堂的话也就布置个作业，老师一句话可以讲完。\n<br>以上数据除第一句外纯属揣测，没有黑任何人的意思，love and peace~\n> \n\n### 扩散方程\n\n带有初值条件的扩散方程表述如下：\n\n$$\n\\begin{cases}\nu(x,t=0)=f(x) \\\\\n\\partial u(x,t)/\\partial t=\\sigma \\cdot \\partial^2 u(x,t)/ \\partial x^2\n\\end{cases}\n$$\n\n方程的解为：\n\n$$\nu(x,t) = \\frac{1}{\\sqrt{4\\pi \\sigma t}} \\int_{-\\infty}^{+\\infty} f(s)\\ e^{-\\frac{(x-s)^2}{4\\sigma t}}\\ ds\n$$\n\n解法是将 u(x,t) 对空间变量 x 作傅里叶变换为 U(k,t)，利用傅里叶变换的性质，变换后的方程将是关于时间 t 的一阶常微分方程。求解后作傅里叶逆变换 ~~即为上式。~~ ~~（完蛋，好久没做题了，那个 $$e^{-\\frac{(x-s)^2}{4\\sigma t}}$$ 是怎么凑出来的，为什么我直接给消掉了啊）~~ 凑出来了凑出来了，初值条件代入频域 k 空间里的通解来确定积分常数，可以看到结果 $$F(k) e^{-\\sigma t k^2}$$ 是两项之积，所以根据傅里叶变换的卷积定理，实空间 x 里的解是 f(x) 和 $$\\mathscr{F}_{k\\rightarrow x}^{-1}\\{e^{-\\sigma t k^2}\\}$$ 的卷积（所以上式的指数项以 (x-s) 为宗量），而计算后者的时候需要用到高斯积分～\n\n### 随机游走\n\n随机游走是一个离散过程，为了和连续时空中的扩散方程相对比，将空间变量 x 离散化为相隔 Δ 的格点 i，时间变量 t 离散化为相隔 δ 的 n。\n\n当一个粒子在 n 时刻位于格点 i 时，在下一个时刻 n+1, 它有 1/2 的概率移动到 i-1, 1/2 的概率移动到 i+1.\n\n所以，虽然每个进行随机游走的粒子在任意时刻都只有确定且唯一的位置，但是对于大量同样初始位置和运动规律的例子，n 时刻出现在 i 格点的概率 P(i,n) 有以下关系：\n\n$$\n\\begin{cases}\nP(i,0)= f_i \\\\\nP(i,n)=\\frac{1}{2}\\left[P(i-1,n-1)+P(i+1,n-1)\\right]\n\\end{cases}\n$$\n\n在初值条件为 $$f_i=\\delta_{i=0}$$ 时，递推结果如下：\n\n|  x 轴 — | i = -4 | i = -3 | i = -2 | i = -1 | O | i = 1 | i = 2 | i = 3 | i = 4 | → |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n| n = 0  | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 |  |\n| n = 1  | 0 | 0 | 0 | 1/2 | 0 | 1/2 | 0 | 0 | 0 |  |\n| n = 2  | 0 | 0 | 1/4 | 0 | 1/2 | 0 | 1/4 | 0 | 0 |  |\n| n = 3  | 0 | 1/8 | 0 | 3/8 | 0 | 3/8 | 0 | 1/8 | 0 |  |\n|**t 轴 ↓**|  |  |  |  |  |  |  |  |  |\n\n![](/photos/2024-04-25-random-walk-probabilities.png)\n\n离散的情况，很难对任意的初值条件写出解的表达式，但是对于上面的特殊情况，课上不加证明地给出了（可能是根据上图凑出来的）下面的解：\n\n$$\nP(i,n)=\\frac{1}{2^n}\\frac{n!}{\\left(\\frac{n+i}{2}\\right)!\\left(\\frac{n-i}{2}\\right)!}\n$$\n\n对的，以上关系只能表示 (n+i = 偶数) 的情况，但是康托尔告诉了我们，所有偶数和所有自然数的“数量”一样多，所以也没差太多～\n\n### 方程的等价\n\n概率的递推公式可以变换为：\n\n$$\n\\frac{1}{\\delta}\\left[P(i,n)-P(i,n-1)\\right]=\\frac{1}{2}\\left [\\frac{P(i-1,n-1)-2P(i,n-1)+P(i+1,n-1)}{\\Delta^2}\\right]\\frac{\\Delta^2}{\\delta}\n$$\n\n因为 $$x=i\\Delta,\\ t=n\\delta$$, 对两个变量的微分可以离散化成差分：\n\n$$\n\\frac{\\partial}{\\partial x}\\rightarrow \\frac{1}{\\Delta}\\left[()_i-()_{i-1}\\right],\\ \\frac{\\partial}{\\partial t}\\rightarrow \\frac{1}{\\delta}\\left[()_n-()_{n-1}\\right]\n$$\n\n直接就能看出扩散方程和随机游走的等价，且系数之间存在关系：$$\\sigma = \\frac{\\Delta^2}{2\\delta}$$\n\n### 解的等价\n\n只讨论一个 δ(x) 函数作为初值条件的情况，我们要证明此时扩散方程的解：\n\n$$\nu(x,t) = \\frac{1}{\\sqrt{4\\pi \\sigma t}} e^{-\\frac{x^2}{4\\sigma t}}\\ \\xleftarrow[{\\Delta,\\delta \\rightarrow 0;\\ i,n\\rightarrow \\infty}]{x=i\\Delta,\\ t=n\\delta} \\frac{1}{2^n}\\frac{n!}{\\left(\\frac{n+i}{2}\\right)!\\left(\\frac{n-i}{2}\\right)!} \\frac{1}{2\\Delta}\n$$\n\n只需讨论这一个情况，因为 δ(x-s) 函数可以看作将一个函数 f(x) 在自变量 x=s 时切片为 f(s)，而任何一个（性质比较“优美”的）函数都可以看作把它自己在定义域上的所有点切片后再重新叠加起来：\n\n$$\nf(x) = \\int_{-\\infty}^{+\\infty}f(s)\\delta(x-s)\\ ds\n$$\n\n过程需要用到 Sterling 公式对阶乘的近似：$$n! \\approx \\sqrt{2\\pi n}\\ n^n e^{-n}$$\n\n$$\n\\begin{array}{rcl}\n\\frac{P(i,n)}{2\\Delta} & \\approx & \\frac{1}{2\\Delta} \\frac{1}{2^n} \\frac{\\sqrt{2\\pi n}\\ n^n e^{-n}}{\\sqrt{\\frac{2\\pi (n-i)}{2}}\\ \\left(\\frac{n-i}{2}\\right)^{\\frac{n+i}{2}} e^{-\\frac{n+i}{2}}\\sqrt{\\frac{2\\pi (n+i)}{2}}\\ \\left(\\frac{n+i}{2}\\right)^{\\frac{n+i}{2}} e^{-\\frac{n+i}{2}}} \\\\\n& = & \\frac{1}{2\\Delta}\\frac{\\sqrt{2n}}{\\sqrt{\\pi(n^2-i^2)}}\\frac{n^n}{(n-i)^{\\frac{n}{2}-\\frac{i}{2}}(n+i)^{\\frac{n}{2}+\\frac{i}{2}}} \\\\\n& = & \\frac{1}{2\\Delta}\\frac{\\sqrt{2n}}{\\sqrt{\\pi(n^2-i^2)}}\\frac{n^n/n^n}{(n-i)^{\\frac{n}{2}}(n+i)^{\\frac{n}{2}}(n-i)^{-\\frac{i}{2}}(n+i)^{\\frac{i}{2}}/n^n} \\\\\n& = & \\frac{1}{2\\Delta}\\frac{\\sqrt{2n}}{\\sqrt{\\pi(n^2-i^2)}} \\frac{1}{\\left(1-\\frac{i}{n}\\right)^\\frac{n}{2}\\left(1+\\frac{i}{n}\\right)^\\frac{n}{2}\\left(1-\\frac{i}{n}\\right)^{-\\frac{i}{2}}\\left(1+\\frac{i}{n}\\right)^\\frac{i}{2}} \\\\\n& = & \\frac{1}{\\sqrt{2}\\Delta}\\frac{1}{\\sqrt{\\pi(n-\\frac{i^2}{n})}} \\frac{1}{\\left(1-\\frac{i^2}{n^2}\\right)^\\frac{n}{2}\\left(1-\\frac{i}{n}\\right)^{-\\frac{i}{2}}\\left(1+\\frac{i}{n}\\right)^\\frac{i}{2}} \\\\\n& \\xrightarrow[\\frac{i}{n}=\\frac{x\\Delta}{2\\sigma t},\\ \\frac{i^2}{n}=\\frac{x^2\\delta}{\\Delta^2 t}]{(1+a\\epsilon)^{1/\\epsilon}\\rightarrow e^a} & \\frac{1}{\\sqrt{4\\pi\\sigma t}}\\frac{1}{e^\\frac{x^2}{4\\sigma t} e^\\frac{x^2}{4\\sigma t} e^{-\\frac{x^2}{4\\sigma t}} } \\\\\n& = & \\frac{1}{\\sqrt{4\\pi\\sigma t}}e^{-\\frac{x^2}{4\\sigma t}}\n\\end{array}\n$$\n\n### 之前 MCMC 讲错了\n\n讲 Markov Chain Monte Carlo 模拟的时候举的例子是计算 $$\\int_{-\\infty}^{+\\infty}e^{-x^2}dx$$, 现在系数可以对上了：$$1=4\\sigma t=4 \\frac{\\Delta^2}{2\\delta} n\\delta = 2\\Delta^2n$$, 随机游走的步数和步长之间存在一个确定的关系，在步长确定的情况下，我们需要重复模拟大量粒子作相同步数的随机游走，然后统计这一确定步数走完之后的每个粒子的终末位置。\n\n所以，这并不是一个 Markov Chain Monte Carlo 模拟，只是一个普通的 Monte Carlo 模拟，我们拿到了想要知道的随机变量的原始概率分布，只不过取得符合这一概率分布的每一个样本的过程是一个 Markov 过程。\n\n正经的 MCMC，应该是只模拟一个粒子作随机行走，然后把它每一步的位置记录下来，统计到样本里去。这样的话时间 t 的信息就被抹去了，而且由于扩散方程描述的状态并不是热力学平衡态，并不能通过统计物理中的遍历性 (ergodicity) 来得到正确结果。\n\n采用了 Metropolis 算法的 MCMC, 一个粒子作随机行走只是其中的一个步骤，还要计算这一步之前和之后 $$e^{-x^2}$$ 的值，来决定这一步是否被加入样本，不成立的话要退回前一步继续走。\n"},{"slug":"mc-mcmc-markov-chain-monte-carlo-gibbs-sampling","filename":"2024-04-15-mc-mcmc-markov-chain-monte-carlo-gibbs-sampling.md","date":"2024-04-15","title":".tex | MC→MCMC 蒙特卡洛模拟，基于马尔科夫链采样","layout":"post","keywords":["tex","phy","m"],"excerpt":"蒙特卡洛模拟、马尔科夫链采样、Metropolis-Hastings 算法、吉布斯采样","hasMath":true,"content":"\nMonte Carlo 蒙特卡洛模拟，简称 MC. \n\nMarkov Chain Monte Carlo 是用马尔科夫链采样的蒙特卡洛模拟，简称 MCMC.\n\n## Monte Carlo 模拟\n\n这个比较简单了，举个例子，要计算 π 的近似值，可以在一块正方形板子里画一个内接圆，然后以均匀的概率往正方形里一粒一粒地扔沙子，每扔一粒，就判断并且记录这里沙子在圆内还是圆外，然后把沙子吹掉，如此往复。圆的面积是 πr²，正方形的面积是 4r²，所以落在圆内的概率（圆内沙子的数量和总数的比值）乘 4，就是所求。\n\n![](/photos/2024-04-15-monte-carlo-pi.png)\n\n归纳一下：当问题的解用一个随机变量的概率分布、期望值、二阶矩……等等来表示的时候，就生成一个符合该概率分布的随机样本，用样本的统计量去近似原概率分布。\n\n## Markov Chain Monte Carlo\n\n但是前述例子有一个步骤，就是我们往板子上扔完沙子要把沙子吹掉，每粒沙子，每次扔沙子之间也应该看不出区别，这是为了保证取样之间**相互独立且来自同一个概率分布**。\n\n但是很多取样过程无法满足这种条件，或者达成条件所需的成本很高。比如计算一个高斯积分 $$\\int_{-\\infty}^{+\\infty}e^{-x^2}dx$$，被积函数的取值范围涵盖整个实数集，想找一个在整个实数集上均匀分布的随机数发生器就比较难了。\n\n![](/photos/2024-04-15-monte-carlo-gaussian.png)\n\n但是学过物理的朋友应该知道，上面的被积函数是以狄拉克 δ(x) 函数为初值条件的一个扩散方程的解，在某一时刻的空间分布。（不想凑系数了，将就看吧）\n\n而扩散方程又是随机游走 (random walk) 在连续近似下的极限。\n\n所以我们直接模拟一堆粒子从原点出发作随机行走，向两个方向的概率相同，扩散系数以及积分里的常数对齐，统计粒子在整个过程中出现在不同 x 位置的频率，求和之后乘以步长就是积分结果。这个过程需要的随机数发生器容易获取得多，是一个以 0.5 为阈值的 [0,1) 的均匀分布，比如一个均匀硬币。\n\n而随机行走过程中走完每一步的位置，都只取决于前一步的位置，而与更久远的历史无关——这样的过程叫做马尔可夫过程。用这种方法取样获得随机样本的蒙特卡洛模拟，就是 MCMC.\n\n扩散方程和随机行走只是 MCMC 的一个很特殊很特殊的例子，而对于一般的 MCMC 模拟，有以下通用的 Markov Chain 采样的算法：\n\n### Metropolis-Hastings 算法\n\n已知一个随机变量 x, 和一个与目标概率分布 P(x) 成正比的函数 f(x)（不要求 f 归一化）\n\n1. 初始化\n    1. 选定初始采样点 $$x_0$$ \n    2. 选定一个采样函数 proposal function，也就是在已知当前 x 的取值时，下一个 x’ 取值的概率分布 $$g(x’\\vert x)$$；其中对于 Metropolis 算法，这个采样函数是对称的：$$g(x’\\vert x)=g(x\\vert x’)$$. 常用以两者之差为宗量的高斯函数。\n2. 在得出 t 时刻的 $$x_t$$ 之后：\n    1. 根据 $$g(x'\\vert x_t)$$ 抽样得到一个 x’\n    2. 计算 α = f(x’)/f(x) = P(x’)/P(x)\n    3. 决定是否将 x’ 加入样本\n        1. 如果 α ≥ 1, 直接加入\n        2. 如果 α < 1, 以 α 为概率加入\n\n这种方法不保证采样的早期样本也符合目标概率分布，所以一般会抛弃最先加入的若干样本。\n\n### Gibbs 采样\n\n只是一种思路，不算是完整的算法。\n\n当被采样的随机变量是一个多维向量的情况，在不使用 Gibbs 采样的情况下，在迭代的某一步骤 t，每个分量都应该是前一步骤的函数：$$x_{i,t}=f(\\{x_{j,\\ t-1}\\})$$\n\n而 Gibbs 采样就是说，不必让每个维度 i 都根据前一个步骤的分量来取值，可以把当前 t 已经取样出来的分量直接带入到本回合后面的维度：$$x_{i,t}=f(\\{x_{j,\\ t}\\}_{j<i}\\cup\\{x_{k,\\ t-1}\\}_{k\\ge i})$$"},{"slug":"physics-based-neural-network-review-note","filename":"2023-03-20-physics-based-neural-network-review-note.md","date":"2023-03-20","title":".tex | 基于物理的神经网络 (PINN) 综述笔记","layout":"post","keywords":["tex","phy","md","ai"],"hasMath":true,"excerpt":"本文是《Scientific Machine Learning Through Physics–Informed Neural Networks: Where we are and What’s Next》这篇综述的读书笔记。","content":"\n> 本文是《[Scientific Machine Learning Through Physics–Informed Neural Networks: Where we are and What’s Next](https://link.springer.com/article/10.1007/s10915-022-01939-z)》这篇综述的读书笔记。\n> \n\n年前，今年新入职的天文学方面的一位老师给我们群发邮件，宣传某国家实验室超算的 GPU 编程马拉松活动，他可以担任指导老师。于是毫不意外地，我报了名。该编程马拉松项目还需要专门申请，申请材料里要写清楚打算干什么，于是报名的五六个人七嘴八舌地想创意。基于物理的神经网络 PINN 就是天文老师的点子。\n\n~~写到这里，我才意识到，老哥是不是想拿我们当免费劳动力啊~~~\n\n神经网络可以看作是一个复杂的非线性函数，接受一个（一般来说维度很高的）向量作为输入，一番计算后输出另一个向量。训练神经网络，就是找到这个函数的参数，绝大多数找参数的方法涉及计算网络输出对参数的偏导数，因此神经网络计算框架的核心功能就是自动微分 (auto-differentiation)。\n\n而很多物理问题，都可以用（偏）微分方程来描述，微分方程的解不是变量，而是函数，而且往往是复杂的非线性函数。所以基于物理的神经网络 (PINN) 就是以神经网络来表达这个函数，然后把这个函数带入到物理的微分方程中，把神经网络输出和真正的物理解之间的差距当作损失函数，反向传播回去来优化神经网络的参数。代入方程时的微分计算，正好可以利用现成框架的自动微分功能。\n\n在以 GPT 为代表的 transformer 类神经网络模型出现之前，自然语言处理类的机器学习项目，往往要在网络之外，利用人类的语法知识，对语段进行语义分割等等“中间任务”。Transformer 一出，算力出奇迹，中间任务逐渐变得没有必要了。\n\n在 GPT 崭露头角，并且越来越有迹象表明其将会涌现出通用人工智能的今天，这些基于物理的神经网络，会不会还未成熟就已过时？这种心情，就和《三体》第二卷开始，章北海和吴岳面对焊渍未漆的“唐”号航空母舰时差不多吧……\n\n<hr class=\"slender\">\n\n- Abstract\n    - PINNs are neural networks that encode model equations. a NN must fit observed data while reducing a PDE residual.\n\n1. Introduction\n    - The “curse of dimensionality” was first described by Bellman in the context of optimal control problems. (Bellman R.: Dynamic Programming. Sci. 153(3731), 34-37 (1966))\n    - Early work: MLP ([multilayer perceptron](https://en.wikipedia.org/wiki/Multilayer_perceptron)) with few hidden layers to solve PDEs. ([https://doi.org/10.1109/72.712178](https://doi.org/10.1109/72.712178))\n    - 感觉可能更全面的一篇综述：[https://doi.org/10.1007/s12206-021-0342-5](https://doi.org/10.1007/s12206-021-0342-5)。该文关注 what deep NN is used, how physical knowledge is represented, how physical information is integrated，本文只关于 PINN, a 2017 framework。\n\n    1. What the PINNs are\n        - PINNs solve problems involving PDEs:\n            - approximates PDE solutions by training a NN to minimize a loss function\n            - includes terms reflecting the initial and boundary conditions\n            - and PDE residual at selected points in the domain (called **collocation points**)\n            - given an input point in the integration domain, returns an estimated solution at that point.\n            - incorporates a [residual network](https://en.wikipedia.org/wiki/Residual_neural_network) that encodes the governing physical equations\n            - can be thought of as an **unsupervised strategy** when they are trained solely with physical equations in forward problems, but **supervised learning** when some properties are derived from data\n        - Advantages:\n            - [mesh-free](https://en.wikipedia.org/wiki/Meshfree_methods)? 但是我们给模型喂训练数据的时候往往已经暗含了 mesh 了吧\n            - on-demand computation after training\n            - forward and inverse problem using the same optimization, with minimal modification\n    2. What this Review is About\n        - 提到了一个做综述找文章的方法：本文涉及的文章可以在 Scopus 上进行高级搜索：`((physic* OR physical)) W/2 (informed OR constrained) W/2 “neural network”)`\n2. The Building Blocks of a PINN\n    - question:\n    \n    $$\n    F(u(z);\\gamma)=f(z),\\quad z\\ \\in\\ \\Omega \\\\ B(u(z))=g(z), \\quad z\\ \\in\\ \\partial \\Omega\n    $$\n    \n    - solution:\n    \n    $$\n    \\hat u_{\\theta}(z)\\approx u(z)\\\\ \\theta^* = \\arg\\min_{\\theta}\\left(\\omega_F L_F(\\theta)+\\omega_BL_B(\\theta)+\\omega_{data}L_{data}(\\theta)\\right)\n    $$\n    \n    1. Neural Network Architecture\n        - DNN (deep neural network) is an artificial neural network that is deeper than 2 layers.\n        \n        1. Feed-Forward Neural Network: \n            - $$u_{\\theta}(x) = C_{K} \\circ C_{k-1} ...\\alpha \\circ C_1(x),\\quad C_k(x) = W_k x_k + b_k$$\n            - Just change CNN from convolution to fully connected.\n            - Also known as multi-layer perceptrons (MLP)\n            \n            1. FFNN architectures \n                - Tartakovsky et al used 3 hidden layers, 50 units per layer,  and a hyperbolic tangent activation function. Other people use different numbers but of the same order of magnitude.\n                - A comparison paper: *Blechschmidt, J., Ernst, O.G.: Three ways to solve partial differential equations with neural networks –A review. GAMM-Mitteilungen 44(2), e202100,006 (2021).*\n            2. multiple FFNN: 2 phase [Stephan problem](https://en.wikipedia.org/wiki/Stefan_problem).\n            3. shallow networks: for training costs\n            4. activation function: the swish function in the paper has a learnable parameter, so — [how to add a learnable parameter in PyTorch](https://discuss.pytorch.org/t/how-could-i-create-a-module-with-learnable-parameters/28115)\n        2. Convolutional Neural Networks: \n            - I am most familiar with this one.\n            - $$f_i(x_i;W_i)=\\Phi_i(\\alpha_i(C_i(W_i,x_i)))$$\n            - performs well with multidimensional data such as images and speeches\n            \n            1. CNN architectures: \n                - `PhyGeoNet`: a physics-informed geometry-adaptive convolutional neural network. It uses a coordinate transformation to convert solution fields from irregular physical domains to rectangular reference domains.\n                - According to Fang ([https://doi.org/10.1109/TNNLS.2021.3070878](https://doi.org/10.1109/TNNLS.2021.3070878)), a Laplacian operator can be discretized using the finite volume approach, and the procedures are equivalent to convolution. Padding data can serve as boundary conditions.\n            2. convolutional encoder-decoder network\n        3. Recurrent Neural Network\n            - $$f_i(h_{i-1})=\\alpha\\left(W\\cdot h_{i-1}+U\\cdot x_i+b\\right)$$, where f is the layer-wise function, x is the input, h is the hidden vector state, W is a hidden-to-hidden weight matrix, U is an input-to-hidden matrix and b is a bias vector. 我认为等号左边的 $$h_{i-1}$$ 应当作为下标\n            - 感觉有点像 hidden Markov model，只不过 Markov 中间的 hidden layers 好像与序号无关（记不清了），~~RNN 看起来各个 W 和 H 似乎不同~~。**RNN cell is actually the exact same one and reused throughout.** (from [https://blog.floydhub.com/a-beginners-guide-on-recurrent-neural-networks-with-pytorch/](https://blog.floydhub.com/a-beginners-guide-on-recurrent-neural-networks-with-pytorch/)). Cartoon from Wikipedia:\n                \n                ![RNN 的构成单元](/photos/2023-03-20-rnn-unit.png)\n                \n            - From [https://blog.floydhub.com/a-beginners-guide-on-recurrent-neural-networks-with-pytorch/](https://blog.floydhub.com/a-beginners-guide-on-recurrent-neural-networks-with-pytorch/):\n                \n                ![RNN 的种类](/photos/2023-03-20-rnn-types.png)\n                \n            1. RNN architectures\n                - can be used to perform numerical Euler integration\n                - 基本上输出的第 i 项只与输入的第 i 和 i-1 项相关。\n            2. LSTM architectures\n                - 比 RNN 多更多中间隐变量，至于怎么做到整合长期记忆的，技术细节现在可以先略过\n        4. other architectures for PINN\n            1. Bayesian neural network: weights are distributions rather than deterministic values, and these distributions are learned using Bayesian inference. 只介绍了[一篇文章](https://doi.org/10.1016/j.jcp.2020.109913)\n            2. GAN architectures: \n                - two neural networks compete in a zero-sum game to deceive each other\n                - physics-informed GAN uses automatic differentiation to embed the governing physical laws in stochastic differential equations. The discriminator in PI–GAN is represented by a basic FFNN, while the generators are a combination of FFNNs and a NN induced by the SDE\n            3. multiple PINNs\n    2. Injection of Physical Laws\n        - 既然是要解常/偏微分方程，那么微分计算必不可少。四种方法：hand-coded, symbolic, numerical, auto-differentiation，最后一种显著胜出。所谓 auto-differentiation, 就是利用现成框架，框架自动给出原函数的导数的算法。\n        - Differential equation residual:\n            - $$r_F[\\hat u_\\theta](z)=r_\\theta(z):=F(\\hat u_\\theta(z);\\gamma)-f$$\n            - $$r_F[\\hat u_\\theta](z)=r_\\theta(x,t)=\\frac{\\partial}{\\partial t}\\hat u_\\theta(x,t)+F_x(\\hat u_\\theta(x,t))$$: 原文给出了来源，但是从字面上看不出来与前式的等价性\n        - Boundary condition residual: $$r_B[\\hat u_\\theta](z):=B(\\hat u_\\theta(z))-g(z)$$\n    3. Model Estimation by Learning Approaches\n        1. Observations about the Loss\n            - $$\\omega_F$$ accounts for the fidelity of the PDE model. Setting it to 0 trains the network without knowledge of underlying physics.\n            - In general, the number of $$\\theta$$ is more than the measurements, so regularization is needed.\n            - The number and position of residual points matter a lot.\n        2. Soft and Hard Constraints\n            - Soft: penalty terms. Bad:\n                - satisfying BC is not guaranteed\n                - assignment of the weight of BC affects learning efficiency, no theory for this.\n            - Hard: encoded into the network design. [Zhu et. al](https://doi.org/10.1007/s00466-020-01952-9)\n        3. Optimization methods\n            - minibatch sampling using the Adam algorithm\n            - increased sample size with L-BFGS (limited-memory Broyden-Fletcher-Goldfarb-Shanno)\n    4. Learning theory of PINN: roughly in DE, consistency + stability → convergence\n        1. convergence aspects: related to the number of parameters in NN\n        2. statistical learning error analysis: use *risk* to define *error*\n            - Empirical risk: $$\\hat R[u_\\theta]:=\\frac{1}{N}\\sum_{i=1}^N \\left\\|\\hat u_{\\theta}(z_i)-h_i\\right\\|^2$$\n            - Risk of using approximator: $$R[\\hat u_{\\theta}]:=\\int_{\\bar \\Omega}(\\hat u_{\\theta}(z)-u(z))^2dz$$\n            - Optimization error: the difference between the local and global minimum, is still an open question for PINN. $$E_O:=\\hat R[\\hat u_{\\theta}^*]-\\inf_{\\theta \\in \\Theta}\\hat R[u_\\theta]$$\n            - Generalization error: error when applied to unseen data. $$E_G:=\\sup_{\\theta \\in \\Theta}\\left\\|R[u_\\theta]-\\hat R[u_\\theta]\\right\\|$$\n            - Approximation error: $$E_A:=\\inf_{\\theta \\in \\Theta}R[u_\\theta]$$\n            - Global error between trained deep NN $$u^*_\\theta$$ and the correct solution is bounded: $$R[u^*_\\theta]\\le E_O+2E_G+E_A$$\n            - 有点乱，本来说 error 是误差，结果最后还是用 risk 作为误差\n        3. error analysis results for PINN\n3. Differential Problems Dealt with PINNs：读来感觉这一部分意义不大，将来遇到需要解决的问题时，回来看看之前有没有人做过就行了——另一方面看，一类方程就需要一类特殊构造的神经网络来解，那么说明神经网络解方程的通用性并不好~\n    1. Ordinary differential equations: \n        - Neural ODE as learners, a continuous representation of **ResNet**. [[Lai et al](https://doi.org/10.1016/j.jsv.2021.116196)], into 2 parts: a physics-informed term and an unknown discrepancy\n        - LSTM [[Zhang et al](https://doi.org/10.1016/j.cma.2020.113226)]\n        - [Directed graph models](https://doi.org/10.1016/j.compstruc.2020.106458) to implement ODE, and Euler RNN for numerical integration\n        - Symplectic Taylor neural networks in [Tong et al](https://doi.org/10.1016/j.jcp.2021.110325) use symplectic integrators\n    2. Partial differential equations: steady/unsteady的区别就是是否含时\n        1. steady-state PDEs\n        2. unsteady PDEs\n            1. Advection-diffusion-reaction problems\n                1. diffusion problems\n                2. advection problems\n            2. Flow problems\n                1. Navier-Stokes equations\n                2. hyperbolic equations\n            3. quantum problems\n    3. Other problems\n        1. Differential equations of fractional order\n            - automatic differentiation not applicable to fractional order → [L1 scheme](https://doi.org/10.1515/fca-2019-0086)\n            - [numerical discretization for fractional operators](https://doi.org/10.1137/18M1229845)\n            - [separate network to represent each fractional order](https://doi.org/10.1038/s43588-021-00158-0)\n        2. Uncertainty Estimation: [Bayesian](https://doi.org/10.1016/j.jcp.2020.109913)\n    4.  Solving a Differential Problem with PINN\n        - 1d non-linear Schrödinger equation\n        - dataset by simulation with MATLAB-based Chebfun open-source(?) software\n4. PINNs: Data, Applications, and Software\n    1. Data\n    2. Applications\n        1. Hemodynamics\n        2. Flows Problems\n        3. Optics and Electromagnetic Applications\n        4. Molecular Dynamics and Materials-Related Applications\n        5. Geoscience and Elastiostatic Problems\n        6. Industrial Application\n    3. Software\n        1. `DeepXDE`: initial library by one of the vanilla PINN authors\n        2. `NeuroDiffEq`: PyTorch based used at Harvard IACS\n        3. `Modulus`: previously known as Nvidia SimNet\n        4. `SciANN`: implementation of PINN as Keras wrapper\n        5. `PyDENs`: heat and wave equations\n        6. `NeuralPDE.jl`: part of SciML\n        7. `ADCME`: extending TensorFlow\n        8. `Nangs`: stopped updates, but faster than PyDENs\n        9. `TensorDiffEq`: TensorFlow for multi-worker distributed computing\n        10. `IDRLnet`: a python toolbox inspired by Nvidia SimNet\n        11. `Elvet`: coupled ODEs or PDEs, and variational problems about the minimization of a functional\n        12. Other Packages\n5. PINN Future Challenges and Directions\n    1. Overcoming Theoretical Difficulties in PINN\n    2. Improving Implementation Aspects in PINN\n    3. PINN in the SciML Framework\n    4. PINN in the AI Framework\n6. Conclusion\n"},{"slug":"logical-science-from-west","filename":"2022-08-22-logical-science-from-west.md","date":"2022-08-22","title":".doc | 也谈近代科学从西方起步","layout":"post","keywords":["doc","tex","phy","m","phi"],"excerpt":"为什么近代科学偏偏是在丢过一次古典传统的西方起步的呢？为什么那些成功继承了古典时代智慧的中古文明，比如伊斯兰文明或古中国文明，反而没有成功萌发近代科学思想呢？","content":"\n前不久在公众号转载过“海边的西塞罗”写的《**嗯！您关注的是一个早晚要“凉凉”的公众号**》，标题起得让人不知所云，但是文章内容讨论的是“近代科学为什么从西方起步”的问题，原文说：\n\n> 既然你所讲述的，欧洲从古典时代到文艺复兴、科学曾经出现过一次“断层”，欧洲人是通过翻译阿拉伯人转译的古典时代文献才继承了希腊罗马先贤们的思想的。\n> \n> \n> 那么**，为什么近代科学偏偏是在丢过一次古典传统的西方起步的呢？为什么那些成功继承了古典时代智慧的中古文明，比如伊斯兰文明或古中国文明，反而没有成功萌发近代科学思想呢？**\n> \n\n作者立了一个靶子——\n\n> 我之前听到的比较靠谱的解答，**是古希腊罗马有较好的数学思想，当定量的数学思想与定性的“自然哲学”发生结合，近代科学就诞生了。**\n但这种解释，其实也回答不了一个问题——你可以说古代东方离着希腊远，没有受到希腊某些思想的“药引”的启发。但特别奇怪的是，中世纪的中东却不是这样。\n伊斯兰文明的伍麦叶王朝在公元九世纪曾经掀起过一场声势浩大的“百年翻译运动”，……近代启发西方的那些古典思想典籍，阿拉伯人全有，且早获得了好几百年。\n> \n\n给出的回答是所谓**“托勒密困境”**，即诸文明中的科学技术研究者因为要满足当权者/赞助者的功利性需要，将时间与精力耗费于附会科学（比如天文学）的非科学甚至伪科学（比如占星术）之上，而——\n\n> 这种错误的职业拼接，锁死了天文学的进一步发展的通路，导致其无法实现向近代科学的飞跃——即便托勒密会数学、引入定量计算，也依然没用。\n> \n> **而这种“托勒密困境”，其实也是所有古典时代学者的困境——他们在研究学问时，必须回答“求用”的问题。**\n> \n> ……\n> \n> **于是从托勒密到哥白尼，我们会发现西方在这一轮对天文学的失而复得中，其实并没有增添什么，而是丢掉了一种东西——那就是“求用”的思维。**\n> 欧洲知识分子们研究科学的正义性，来自于他们认定：自然作为一种上帝的造物，其本身就是美的。因此研究它、探索它本身，就是在赞美上帝，所以科学研究不必“求用”也有天然的正义性。\n> \n\n作者写近代西方科学的不求用，是为了托物言志，检讨自己为了读者的关注不得不在历史写作之外“写时评、表达观点、带情绪”，预告自己将来可能会去写作崇高的钻研历史的题目。\n\n给蹭热点找理由这件事，我也做过嘛，感觉写的比这篇文章还简约隽永且立意高远呢～（文人相轻.jpg）\n\n但是科学革命发源于西方这个问题，我也很感兴趣，而且有自己的思考，而且思考的结果和上文不同。\n\n<hr class=\"slender\">\n\n学物理的孩子应该都听说过《费曼物理学讲义》的大名，没听说过的话建议听说一下，自主招生考试面试装逼的时候用的上。费曼先生在引言中也立了个靶子说——\n\n> 你们可能会问，在讲述欧几里德几何时，先是陈述公理，然后作出各种各样的推论，那为什么在讲授物理学的时候不能先直截了当地列出基本规律，然后再就一切可能的情况说明定律的应用呢？\n> \n\n然后上来就讲原子论，开篇问：\n\n> 假如由于某种大灾难，所有的科学和知识都丢失了，只有一句话可传给下一代，那么怎样才能用最少的词汇来传达最多的信息呢？\n> \n\n可惜这个问题仅仅是为了引出原子论，实在是大材小用。这说明费老先生浸淫于西方科学中，“不识庐山真面目，只缘身在此山中”。而我对近代科学起自西方的解释，正好就是这两句话串起来。下面就要兜一个大圈子，把两句话圆起来。\n\n<hr class=\"slender\">\n\n科学者，对世界之正确认知也。\n\n根据这个定义，把人们已知的，关于这个世界的所有知识罗列到一个集合里，这个集合就是科学。我们只谈到了一个集合，不涉及逻辑推演，也不涉及数学带来的定量优势，更不判断从事科学研究的人是否功利。\n\n但是，集合这种知识结构过于简单——\n\n- 集合里的各个元素都是平等的，要想表示出整个集合，除了全默写出来没别的办法；\n- 集合里的元素之间没有顺序，想取得其中的某一条科学命题，只能像抓阄一样，凭运气抽到为止。\n- 一旦由于天灾人祸，集合中的部分内容丢失，除了重新把当初发现它们时经历的艰难困苦重复一遍，也没有别的办法。（哦，也可以去隔壁文明的图书馆翻译。）\n\n所以，必须找到一种更复杂的结构，来组织这些信息，解决上述问题。\n\n<hr class=\"slender\">\n\n计算机专业有门基础课《数据结构与算法》，谈数据结构，最基础的两种就是数组和链表；谈算法，最基础的概念就是函数。注意，这里说的是数据结构，刚才说的是知识结构，两者可以类比，但并非同一概念。\n\n数组，和集合几乎一样，只不过给每个元素标记了一个序号。在计算机里，由于规定数组连续存放，每个元素占用内存长度相等，所以可以通过序号，从数组开头偏置指针，以 O(1) 的时间复杂度取得任意元素，快。\n\n类比到知识结构，语数外理化政史地生，一年级二年级三年级，第一章第二章第三章，第一第二第三个知识点，背吧。列表与列表之间井水不犯河水，你数学老师说你体育老师拉稀了不能上课，你体育老师说你数学老师放屁，两者完全可以在你的知识体系里共存。\n\n链表，和数组一样有顺序，但是并不给每个元素标号，而是在前一个元素的末尾，写上下一个元素的位置指针。找到一个元素需要从链表的开头一个一个往后捋，慢。好处是修改方便，在链表中间塞进去一个新元素，只需要把前面一个的指针指向新元素，新元素的指针指向后一个元素，删除一个旧元素也类似，只对增删点附近一个很小的区域进行改动，整个链表不会伤筋动骨。\n\n但是不论数组还是链表，都需要把所有的知识全写出来，随着时间的积累，科学的总量早晚要超越人脑的记忆力，超越笔记的厚度，对于个人，要皓首穷经，要韦编三绝，才有希望提出一点新内容；对于全人类，图书馆越造越大，一轮战乱，从头再来。\n\n于是函数登场。给定一个/一组输入，根据函数体描述的算法，返回确定的输出。那我们找到一种方法，写一个函数，接收链表的前一个元素作为输入，找到后一个元素输出。这样我们只需要存储第一个元素和这个函数，就可以恢复出整个链表，用计算换空间。\n\n<hr class=\"slender\">\n\n类比到知识结构，这个函数就是逻辑推演。\n\n科学内容中的每一条知识都是一个**命题**。\n\n从少数几条知识出发，这几条在逻辑上就称为**公理**，自然科学里也称之为**定律**。\n\n命题之间可以做**逻辑运算**，**或**、**且**、**非**、**蕴含**等等，运算的结果也是一条新的命题。命题的正确与否，取决于逻辑运算的规定。\n\n通过对公理和已经算出的真命题反复进行逻辑运算，产生的新的真命题，叫做**定理**。\n\n<hr class=\"slender\">\n\n欧几里德几何式的，也就是从有限多个命题出发，承认逻辑推演进行生成的新命题的正确性，这样的一种组织方式——\n\n- 对于学习，科学不再是一家之言，门户之见。一句话的正确性不再由说话者的身份决定，诉诸人身、诉诸权威成了谬误，“我爱吾师，但我更爱真理”一句话有了切实的落脚点。\n- 对于研究，降低了难度，后来者不必从头再来，而是站在前人的终点起跑。发现的新科学有办法整合进现有的科学，证伪的旧科学有办法剔除，而不会让科学整体伤筋动骨。愚弄黔首的矛盾和谬误，真理有办法与之势不两立。\n- 自带有容灾能力，科学得以在摧毁科学记录和科学家人身的重大灾难之后，在几百年的人才断档之后，依然有办法恢复。\n\n<hr class=\"slender\">\n\n刚才说数据结构和知识结构不同，知识管理界有个 DIKW 模型，也就是数据 (Data)、信息 (Information)、知识 (Knowledge)、智慧 (Wisdom)。\n\n纸张上的墨迹组成的字符只是数据，当这些单词按照语法理解为句段篇章之后才构成信息，这些篇章内容指代的概念、关系等等含义构成知识。如何理解知识与知识之间的关系需要智慧。\n\n“继承了古典时代智慧的中古文明，比如伊斯兰文明或古中国文明”——从各个文明没能演化出科学革命来看，**这些文明最多是有一部分学者继承了古典时代的知识，而没能认识到 *用逻辑组织知识* 这一智慧的价值**，而西方发掘出了这种智慧。至于这种发掘发生在西方，是偶然还是必然，由哪些条件促成，那是另一个很有趣的问题了。\n\n“我们会发现西方在这一轮对天文学的失而复得中，其实并没有增添什么，而是丢掉了一种东西——那就是‘求用’的思维。”——西方对天文学的失去，对应的是罗马统治下的和平结束时的战乱与社会崩溃，不论之后的文艺复兴如何光辉灿烂，**这种失落都是对科学乃至整个文明的威胁**，如果没有这种失落，科学革命想必会更早更容易发生。况且这种失落到复兴的整个过程中，对科学有影响的因素实在是太多了，既有正面又有负面，实在是难以分析归因。\n\n至于不求用的思维，有了逻辑推演，科学工作者的产出提高，高到了让社会愿意供养其全职研究的地步，那么不求用的思维，自然会建立起来；不求用对科研效率的提升，良性反哺科学的发展，自然会蔚然成风。反过来，**只有不求用的态度，研究者没有逻辑推演发展科学的能力，资助者没有逻辑推演评价成果的本事，不求用的态度只会鼓励灌水，产出真没用的水货。**\n\n<hr class=\"slender\">\n\n数学对自然科学的作用，定量化只是一个副产品。更重要的是作为逻辑科学的集大成者，发明/发现逻辑推演的规则，探索逻辑推演作为方法论的能力边界。一言以蔽之，欧几里德之后，数学已不只是“数字的学问”。\n\n至于费曼先生，他怎么可能不知道四大力学确实就是按照欧几里德式的，从基本定律出发的方式讲授的呢？面对一伙学普通物理的本科新生，说这种话实在有点骗小孩儿的嫌疑，怪不得那门课上到后来，本科生全都跑了。物理和数学的区别，在于理论和实验两条腿走路，但是理论的这条腿，实实在在地来自于超越了“数字的学问”的数学。\n"},{"slug":"qc-hackathon-write-up","filename":"2021-04-22-qc-hackathon-write-up.md","date":"2021-04-22","title":".qs | QC Hack 量子编程马拉松","layout":"post","keywords":["md","phy"],"hasMath":true,"excerpt":"4月初的时候，系秘书转发了一封邮件，耶鲁和斯坦福有两个关于量子计算的学生社团，打算举办为期一周的线上训练营,然后在周末举办一个24小时的编程马拉松","content":"\n\n## 一\n\n4月初的时候，系秘书转发了一封邮件，耶鲁和斯坦福有两个关于量子计算的学生社团，打算举办为期一周的[线上训练营](https://www.quantumcoalition.io/)，然后在周末举办一个24小时的编程马拉松 ([hackathon](https://en.wikipedia.org/wiki/Hackathon)) 的活动。只要年满18岁就可以参加，并不限定本科生。\n\n整个活动由几家从事量子计算的科技公司赞助，前面的线上训练营基本就是各家轮流上来介绍一下自己家的量子计算平台的使用方法，最后的编程马拉松也由他们每家出一套题，所以这个活动也有在学生和公司之间搭桥，给参与者争取实习机会的目的在里面。参与者可以自由组队，但是在项目提交的的时候每个人只能属于一支队伍。虽然参与者可以参加任意数量的题目，但是每一名参与者最终只能成为一家公司的优胜者。如果预感到自己在某一个项目的赢面比较大，可以在提交之前通知自己参加的其他队伍把自己除名。24小时的时间限制还是比较紧迫的，所以基本上认准一家答题就可以了。\n\n女朋友也收到了一样的邮件，所以理所当然地一起组队。我之前在本科阶段上过一门一学期的量子信息和量子通信课程，内容约等于在量子力学之后再上一个学期的习题课，以及在不讲群论的情况下应用 SU(2) 群，并没有接触过这个活动中会用到的编程语言。女朋友没有上过这门课，基本就是物理专业普通研究生的量子力学水平。周中的线上训练营，我只参加了第一天的，是 Microsoft 的 Quantum Development Kit (QDK) 和 Q# 编程语言相关的，顺便介绍了一下量子计算中很有名的 Deutsch 算法。剩下的讲座我基本上都没有参加，一方面是知道前面的规则之后就懒下来了，另一方面是实验室的工作仍然需要继续，再有就是线上活动实在是太容易摸鱼了没有效率。周五的晚上，女朋友看了一晚上我的量子信息笔记，我看了看 Q# 的语法规则，在台式机上安装了开发环境。以上就是我们参加编程马拉松之前的基础和准备。\n\n\n## 二\n\nHackathon 美东时间周六上午10点开始，周日上午10点结束。因为我们只看了 Microsoft 相关的内容，所以直奔[相关题目](https://github.com/quantumcoalition/qchack-microsoft-challenge)。\n\n题目一共分为两部分。\n\n第一部分一共四道题，就像是一般的计算机课程的作业一样，参赛者只需要在举办方写好的主程序文件里的指定区域填入代码，然后运行主办方写好的测试文件检查结果，测试通过即可得分。四道题目要求如下：\n\n1. 判断一个3-5位的2进制数能否被4整除。\n2. 判断一个3比特位当中是否至少有两位不同。\n3. 同第2题，但是要求量子比特门最多只能使用 3-比特，而且 3-比特门最多使用一次。\n4. 给定一个有两种颜色的无向图，判断图当中不含有任何单一颜色的三角形。\n\n第二部分内容比较自由，要求用 Grover's 算法解决一个自己感兴趣的问题，打哪指哪，然后写一篇文章介绍自己的这个项目，并提交相关的代码。根据问题深度(6分)、工具使用(5分)、创新性(4分)、教育价值(5分) 四方面进行评分。\n\n\n## 三\n\n### I.1.\n\n第一道题最简单，但是我们当时约等于0基础，所以做起来也颇费了一些时间。不过由于我听过第一天的课，知道 `oracle` 在 Q# 编程语言中是一个很重要的概念，所以在题目给出的参考教程 [Quantum Kantas](https://github.com/Microsoft/QuantumKatas/) 里找到了[oracle 相关的教程](https://github.com/microsoft/QuantumKatas/tree/main/tutorials/Oracles)。里面有个名为 `ControlledOnBitString` 的 function，可以根据一串量子比特的取值是否等于一个特定的二值串而对另外一个比特做一个特定的操作。前一天晚上又知道了 `Microsoft.Quantum.Convert` 的 namespace 里有各种数据类型转换的函数，搭配 `IntAsBoolArray`，就做出了第一题的初版。后来看到了更简单的 `ControlledOnInt` 函数，就直接用上了。\n\n### I.2.\n\n第二题的初版是女朋友做的。题目要求是找出是否至少两位不同，这一判断的否定就是三位比特全部相同，所以同样用 `ControlledOnBitString` 函数，然后判断一次全 `true` 一次全 `false`，再把最终结果取反就可以了。但是在做第三题的时候，因为两个题目长得太像了，中间不小心把一个能通过第二题测试但是通不过第三题测试的答案直接覆盖在了第二题上面，懒的改回去了，于是就成了最后提交的版本。\n\n### I.3.\n\n第三题和第二题非常不同。第二题的解决思路中，判断全 `true` 和全 `false`有3个控制位1个输出位，这里用了两次 4-量子比特门，所以第三问需要全新的思路。另外我曾经试过在一个 `operation` 里申请一个新的 `Qubit()` 结果测试报错，因为误解了报错信息，所以误认为除了程序的主 operation 之外不能创建新的 qubit，于是被卡住了。这时候已经来到了下午，实在想不出来又很困，于是去床上躺了一会。半睡半醒之间想到，题目虽然要求输入的量子比特不变，但是我们仍然可以直接改动输入，只要在函数结束之前把对输入的改动全部复原就可以了。于是用 CNOT 门分别作用在 1-2, 1-3 对输入的量子比特上，两个门分别以第2、3号比特为输出。然后用一个 3-bit 门判断2、3号比特是否相同，并输出到结果位上。为了复原第2、3号比特，只需要把 CNOT 在两对比特上分别再用一次就行了。\n\n但是这个结果还是无法通过测试（后来成为了第二题的提交版本），报错的提示信息是使用了超过一次 3-量子比特门——这不是开玩笑吗？于是打开了官方提供的测试文件，发现测试代码计算 3-量子比特门的使用次数的时候，会把用户定义的 3-量子比特门的数量，和 `CCNOT` 门的数量做加法，于是看文档，我们定义的那个 “用一个 3-bit 门判断2、3号比特是否相同，并输出到结果位上” 的操作和 `CCNOT` 门是等价的，于是直接换用 `CCNOT` 门，问题解决。\n\n### I.4.\n\n第四题看起来复杂，但是可以分成三个部分：\n\n1. 找出图中所有的三角形，确定每个三角形的三条边，这一步完全可以用经典算法完成；\n2. 创建一个和三角形相同数量的量子比特数列，对每个三角形，把三条边直接带入第二/三题的操作里，结果输入创建的量子比特列中；\n3. 判断量子比特列是否全为 `true`，结果输出到整个程序的结果位上。\n\n第一步由女朋友来想我来写（毕竟只有一台电脑有开发环境），难点在于：\n\n1. Q# 语法改变数列值的语法十分难受\n    <br>`mutable points = [-1,-1,-1,-1,-1,-1];`\n    <br>`set points w/=0..1 <- [0,1];`；\n2. 作为一种强类型语言对元组和数列的区分让我这个 python 选手十分蛋疼\n    <br>`(Int,Int)`/`Int[]`；\n3. 求数列中不重复的值居然不排序不能给出正确结果。\n    <br>`let uniquePoints = Arrays.Unique(EqualI,Arrays.Sorted(LessThanI,points));`\n\n这也是唯一一段用上了 `Message()` 函数来 debug 的部分。\n\n第三步就重新回到了第三题暂时敷衍掉的问题：对于在操作中创建的 `Qubit()`/`Qubit[]`，`Reset()`/`ResetAll()` 函数相当于测量，会破坏操作的 adjoint 性质，不测量则（当时的我）没有办法将这个量子比特列复原。\n\n此时已经午夜，我来解决这个问题，女朋友去看第二部分，后来她看完 Grover‘s 算法的教程去睡了，我还在想这个问题。直接把报错信息复制到 Google，找到了一个[论坛里的问答]()，好像是去年微软在其他地方举办的类似活动的。里面只是提到要“uncompute the qubits”，给出的例子用的是旧版本 Q# 的语法，~~没法直接抄~~ 。最终不抱希望地把之前对那个 `Qubit[]` 做过的循环顺序倒过来重做了一遍，诶，您猜怎么着，还是没通过！绝望了！正序重做一边，诶，通过了！为什么为什么为什么？到现在也没弄清楚。\n\n### II.\n\n然后把女朋友叫醒，让她来讲一讲 Grover's 算法。听完之后我的理解是，对于一个 $$f:(0,1)^N \\rarr (0,1)$$ 的函数，这个算法可以大概率地找到一个解 $$S\\in(0,1)^N$$ 满足 $$f(S)=1$$. \n\n至于这个函数 $$f$$，之前每一道题都是这样一个函数，当时已经夜里两三点了，实在是没时间再想一个新函数了，于是我们直接就拿复杂度最高的第4题来换个皮。换个什么皮呢？为了这个活动翘掉了这周的[《文明6》联机游戏](barrier-forward-keyboard-mouse-to-another-computer)，然后之前看 YouTuber [\"PotatoMcWhiskey\"](https://www.youtube.com/user/PotatoMcWhiskey)介绍过[一个 Mod](https://steamcommunity.com/sharedfiles/filedetails/?id=1753346735&searchtext=diplomacy)，里面可以将文明之间的外交关系可视化为无向图，所以，诶嘿嘿嘿……\n\n女朋友写完文稿就睡了，我把文稿改了改，然后和官方对 Grover's 算法的实现缝合了一下。提交的时候，距离截止时间大约还有一个小时。\n\n\n## 四\n\n之后的周五的时候收到了消息，我们得奖了。优胜者一共6支队伍。从活动结束之后公布的结果看，要想成为优胜，第一部分的4道题必须全部正确，然后第二部分得分在 8-20 分之间。\n\n这个成绩是个什么水平？截止到写这篇文章的此刻，官方题目的 Github 仓库有 80 份 fork，有少数几份 fork 是针对已有的 fork，有可能来自同一队伍，再考虑到可能有些队伍的不同成员分别 fork 了主项目，所以估测 60 支队伍应该是有的，官方给出 6 组优胜者这么一个不零不整的数字，个人猜测是取了前 10%？据主办方在 discord 提供的消息，有一支队伍的第二题成绩高于8分，但是前面没有全对，所以没有得奖；其余队伍的第二题都不超过6分；并不清楚有多少队伍第一题全对，主办方也不打算公布各队的详细成绩。\n\n这大约说明活动的参与者，其成绩基本上符合二八原理——少数人得到的分数，占据了所有参赛者全部得分的大多数。\n\n参加过这个活动之后，我们一下子就从量子计算小白摇身一变，成了优秀人才了？实际上，直到现在，我还是搞不太清楚 oracle 到底是个什么东西，女朋友对量子计算的理解估计比我还差（逃）。美国哲学教授约翰·希尔勒提出过一个叫做[“中文房间”](https://zh.wikipedia.org/wiki/%E4%B8%AD%E6%96%87%E6%88%BF%E9%97%B4)的思想实验，说一个只会说英语的人被关在一间满是汉字字块的房间里，不断从房间外收到写着中文问题的纸条。房间里有一本英文写成的手册，指示如何对输入的汉字进行回复。凭借这个手册，房中人可以在完全不会中文的情况下，与外界进行交流。希尔勒类比外人、房中人、手册，与程序员、计算机、计算机程序，认为房中人不会中文，进而论证计算机不可能通过程序来获得理解力。\n\n希尔勒教授想论证啥是他的事，我倒是对这个类比的本体很感兴趣——如果一个人已经能够熟练运用那个英文写成的汉字使用手册了，我们还能不能，能在多大程度上说他不懂中文呢？就说一般的程序员，工作时间能保证不看 stack overflow 的有几个，所以他们都不会编程？反对中文房间思想实验结论的人，很多都支持用图灵测试超过某一阈值来作为有智能的标志，但是我觉得，智能本身就不是一个非有即无的性质，而是一个连续分布，没有上限的谱。\n\n另一方面，得分名列前茅，和能力名列前茅，又是两回事。本科的时候做建模美赛，我们学校数理金融的一个学神前一年成绩“略有不佳”，没拿到 M 奖，于是我们那年找到了我和风神俩学物理的，准备再次冲击荣誉。巧了这一年的题目正好有一道浴缸放热水的问题，这不就是物理中的扩散方程嘛，那得奖还不是手拿把掐的？结果呢，H 奖，丢人丢到姥姥家去了。合着我们两个成绩还都不错的物理专业学生，在自己的专业里，打不过那么多同龄的非物理专业本科生？\n\n两相对照之下，我想起了很久之前看过的一篇博客文章，文章以一个问题开头——“熟练”的反义词是什么？当然说“生疏”这文章就写不下去了，作者给出的答案是——“应变”。熟练意味着，你对于问题、选项、最优解已经有了充分且完备的了解，只需要重复自己的经验就可以了，但是在自己不了解的战场上，经验至少不能直接派上用场，这时候，脱离具体环境的应变能力就成了生存和取胜的关键，我们当时的专业水平高不成低不就，反而成了掣肘我们的桎梏。\n\n读到这篇文章的时候，我被这种剑走偏锋的观点击中了，从那以后，一直都在注意培养自己的应变能力——如果明天我所研究的这个领域消失了，我还有没有谋生的能力？如果自己正在解决的问题被上帝或者 Matrix 作弊修改成一个新问题，我能不能看到连作弊都改动不了的题眼，然后一击命中？在凌晨两三点的时候，我也没有放弃解决第一题第 4 问的 Qubit 复位问题，虽然当时我并不知道评分标准，但是内心非常确定，这个问题必须解决。\n\n以上两次活动的成绩差别，也可以从得奖难度来看。建模美赛的 M 奖，得奖率应该远小于 10%，即便考虑到二八原理中绝大多数参赛者都只是凑数，而且样本越大凑数者越多，这个差距也还是无法忽略。我们能够得奖，和量子计算领域才刚刚萌芽，连“方兴未艾”都算不上，因此竞争并不激烈也有很大关系，应变能力是切入这些蓝海领域的必要条件，是躲避内卷的利器。我们现在对“内卷”人人喊打，但是培养应变能力是需要牺牲相当多本可以精进专业的时间和精力的。当社会中的大多数人向往着逃离内卷的时候，真的不需要有人咬定一个领域不断深耕？我现在的选择真的正确吗？我不知道。我是打算留在当前的领域继续熟练，还是换个领域应变，抑或是虚掷 PhD 光阴换一张工作签证？我也不知道。\n\n## 五\n\n哦对了，我有女朋友了，而且在 hackathon 的过程中把女朋友惹哭了……问题是我现在已经不记得具体是怎么把人家惹哭的了，连道歉都显得很不诚恳……我确实是一个不擅长合作的人，或者说跟别人说话的我，和想问题的我并不是同一个人，之前本科 CUPT 和建模的时候也一样，需要和人打交道的时候就几乎干不了活儿，严重的时候自己就退化成了鼓励师……总之一切错误在我，希望她不要记仇…… <br>（。・＿・。）ﾉ\n"},{"slug":"lab-note","filename":"2020-09-21-lab-note.md","date":"2020-09-21","title":".tex | 整理一些关于实验记录的文章","layout":"post","keywords":["tex","phy","bio"],"excerpt":"摸鱼的方式有很多种，琢磨如何完美地进行实验记录就是个挺不错的由头。","content":"\n1. [微信公众号“BioArt植物”，原作者 Elisabeth Pain ，《实验记录到底怎么记？》](https://www.sciencemag.org/careers/2019/09/how-keep-lab-notebook)\n1. [Howard Kannare, 《Writing the Laboratory Notebook》](https://files.eric.ed.gov/fulltext/ED344734.pdf)\n1. [MIT Department of Mechanical Engineering, 《Instructions for Using Your Laboratory Notebook》](http://web.mit.edu/me-ugoffice/communication/labnotebooks.pdf)\n1. [微信公众号“生物学霸”，《颜宁：讲讲如何记实验记录》](https://xw.qq.com/partner/vivoscreen/20200820A00HZI00?vivoRcdMark=1)\n\n微信平台不允许添加指向微信之外的超链接，资源的获取方式见正文。作为报复，以上四条资源中有两条最早是在公众号里看到的，但是博文给出的链接都来自微信之外 :-)\n\n<hr class=\"slender\">\n\n## 0\n\n疫情依旧，摸鱼依旧。摸鱼的方式有很多种，其中比较高级的一种是打着完美主义的旗号，对着一个还没完成，或者根本不存在完成时的东西，疯狂输出时间和精力。琢磨如何完美地进行实验记录就是个挺不错的由头。\n\n说实话，学界对研究记录的要求实际上并不算严格，这一点在《Writing the Laboratory Notebook》里也有佐证。商业机构研发部门的研究记录会成为将来知识产权争端的主要证据，稍有不慎就是真金白银的经济损失，甚至关乎企业的生死存亡。而学界的工作在“科技”中偏重于”科“（即便是工程学科），在”研发“中偏重于”研“。（四者有什么联系和区别？科学认识世界，技术改造世界，研究把钱变成知识，开发把知识变成钱。）自由比起规范显然更有利于在未知领域的探索。\n\n所以，我们实验室对于实验记录并没有成文的规则，大家自己找本子自己记，格式和内容都跟随自己的喜好来，同实验室的同学也很少交流这个问题，仿佛说了就是承认自己的科研能力有问题。\n\n## 1\n\n越不谈越是心虚，于是在看到了《实验记录到底怎么记？》这篇文章之后，下决心要处理掉这个问题。这篇文章的作者访谈了几个科研人员，然后将他们的对话打碎，分到四个问题之下：\n\n- 为什么还要花时间精力去做实验记录？\n- 用传统纸质的记录本，还是电子版，还是都有？\n- 采取什么策略来保证实验记录有条理、完整并且实用？\n- 其他……\n\n并不推荐这篇文章，原因从这四个问题就能看出来：第一条属于幸存者偏见，一个觉得记录不重要的人压根不会认真记录，从而很难成为访谈对象；第二条属于典型的”有的人……有的人……“英式废话文套路；第三个问题太笼统，本应该细分为更明确的子问题；最后一个“其他”说明作者都不知道该怎么总结这些对话。明明是一篇文章，硬生生写出了微博一般的碎片感。\n\n## 2\n\n于是在网上继续找资源，机缘巧合之下，在一个知乎问题之下看到了一个还不错的答案，里面提到了 Howard Kannare 的《Writing the Laboratory Notebook》这本书。真的是“机缘巧合”，因为现在的我已经找不到原来的那个问题和答案了，哪怕专门为了这篇文章搜索了半天……这说明了网络资源的收藏和管理也是一个技术活（又可以水一篇文章了）。\n\n这本书在网上有英文的完整影印版，很容易就能搜到，实在不行的话在微信后台留言\"HowardKannare\"可以收到下载网址（注意回复的关键词没有空格）。\n\n本书各章的标题如下：\n\n1. The Reasons for Note keeping - An Overview\n2. The Hardware of Note keeping - Books, Pens, and Paper\n3. Legal and Ethical Aspects - Ownership, Rights, and Obligations\n4. Management of Notekeeping - Practices for Issuance, Use, and Storage of Notebooks\n5. Organizing and Writing the Notebook - Be Flexible\n6. Examples of Notebook Entries\n7. Patents and Invention Protection\n8. The Electronic Notebook\n9. Appendix A: Some Suggestions for Teaching Laboratory Notekeeping\n10. Appendix B: Photographs from the Historical Laboratory Notebooks of Famous Scientists\n\n书很长，有 150 多页，这导致内容涉及方方面面，包括了那些我们可能不是那么急需的方面；还有很多我们今天可能并不十分需要的冷知识，比如几十年前美国出产的纸张由于某种工艺导致保存期限比较短等等。好在多数章节最后都有总结，可以帮人省下不少时间。\n\n另一方面，这本书出版于1985年，那是一个什么年代呢？苹果在前一年才刚刚推出了 Macintosh 电脑，C++ 在当年才刚刚出版。所以对于电子实验记录，书中只在第8章和纸本笔记进行了一个简单的对比，而且有比较明显的时代局限性。\n\n总之，如果真要读这本书的话，抱着练习英语阅读的目标，要远比学记笔记要少些失望。\n\n## 3\n\n干了半天之后开始怀疑这件事从一开始是否有必要，这可是 PhD 的保留节目了。\n\n尤其是在读过《Writing the Laboratory Notebook》的第5章之后，读到单篇实验报告应该包括 introduction, experimental plan, observations and data, discussion of results 的时候，恍然发现，这不就是本科实验课要交的实验记录的写法吗，之所以没有老师教过我们怎么记实验记录，是不是因为他们觉得这个事情已经教过了？\n\n既然如此，那么第三份材料就是 MIT 机械系给本科生的实验报告要求，不长，只有 6 页，还包括了超过两页的模板笔记，可以当作《Writing the Laboratory Notebook》关于笔记内容部分的精华集锦来看。微信后台回复 \"MITlab\" 可以收到 PDF 下载地址。\n\n但这反而说明了，PI们散养研究生，不对实验记录进行更进一步的要求和培训是不对的。因为“实验记录本”≠“实验记录们”，研究生的工作不同于本科实验课，本科生做实验就只有在固定时间和固定时长的实验课上，一切超出规定范围内的动作大概率都是无用功甚至错误，每个实验要做什么，有哪些步骤，会观察到什么现象，都是事先设计好的，实验记录的各个部分会有哪些内容，大体上没跑。研究生的工作则不然，大到整个博士期间的所有工作都可以算作是一个项目（毕竟会写成一篇毕业论文），小到显微镜从开机到观测到关机的几个小时也可以整出一篇报告来，如何划分研究的基本单元？一个人可能同时在做相对独立的多项工作，是连续记录还是分开平行记录？以纸本为主，电子版主要用于备份，还是主要用电子设备，随手记在纸上的拍照作为附件？\n\n## 4\n\n对于这篇文章没有太多可说的，覆盖范围和《实验记录到底怎么记？》类似，感觉就相当于颜宁女士自己一个人对那篇文章中的问题的回答，由于是一个人的回答，所以不会有前一篇文章中不同观点混在一起的分裂感。看过了《Writing the Laboratory Notebook》之后，会发现文中的每一个点都在书中可以找到。\n"}]]]},"__N_SSG":true}