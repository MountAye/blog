{"pageProps":{"articles":[{"slug":"notes-on-pytorch-autograd","filename":"2024-11-10-notes-on-pytorch-autograd.md","date":"2024-11-10","title":".ai | PyTorch 中的自动微分 autograd","layout":"post","keywords":["md","py","ai"],"excerpt":"我对 PyTorch 中的自动微分还是一知半解，停留在知道计算图和反向传播的概念，以及能看懂和小改别人写好的代码的水平。之所以存在这样的认知断层，是因为介绍自动微分的时候一般是手动计算张量，然后给读者画个计算图；而真正干活的时候一般是写一个神经网络的类，这个类继承自 `torch.nn.Module`。","content":"\n> 装了这么久的 ML/AI 爱好者，我发现我对 PyTorch 中的自动微分还是一知半解，停留在知道计算图和反向传播的概念，以及能看懂和小改别人写好的代码的水平。所以自觉还得再补补课。\n> \n> ~~真实动机是前女友找到了国内的研发工作，马上要回国赚大钱了，我破防了~~\n> \n> 之所以存在这样的认知断层，是因为介绍自动微分的时候一般是手动计算张量，然后给读者画个计算图；而真正干活的时候一般是写一个神经网络的类，这个类继承自 `torch.nn.Module`。\n> \n> 这样一包装，承载微分的张量很少被点名取用；而计算过程被一刀两段，绝大多数计算包装在了这个类的 `forward()` 方法中，但是最后一步计算模型预测和训练目标的差异 (loss)，往往留在了训练的控制流中，让人以为两者之间有什么本质区别。\n> \n> 其实不然——自动微分只针对 PyTorch 张量，`torch.nn.Module` 的实例是一个包含若干参数的函数，其参数要么是 `torch.nn.Parameter`，要么是其他含参的 `torch.nn.Module` 实例的参数，（用 `.parameters()` 方法取得，常见于 optimizer 的首个参数）。每次把训练输入张量带入这个函数，中间的数学计算就会被自动微分机制记录。对于自动微分来说，这些计算和最后对 loss 的计算没什么不同。\n> \n> 这个道理其实在做《[**一个 PyTorch 机器学习项目长什么样**](what-a-PyTorch-project-looks-like)》这篇笔记的时候就应该理解，而实际上没有理解，说明——\n> \n> - 一来信息和知识不是同一层面的概念，相同的信息嵌入在不同元信息中，可以给出不同深度的知识；\n> - 再者费曼学习法虽然像实验记录一样有让人快速恢复学习进度的好处，但也有让人沉溺于流量，忘记学习本心的危险，戒之在躁。\n\n一个机器学习项目的过程包括：准备数据、定义模型、训练、评估效果。先来个简单例子：\n\n```python\nimport torch\nfrom torch import data,nn,optim\n# Data Preparation\ndataset = data.Dataset(...)\ntrainloader = data.DataLoader(dataset,...)\n# Model Definition\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        # ...\n    def forward(self, x):\n        # forward calculation ...\n        return x\nnet = Net()\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n# Training\nfor epoch in range(2):\n    running_loss = 0.0\n    for i, data in enumerate(trainloader, 0):\n        # get the inputs\n        inputs, labels = data\n        # zero the parameter gradients\n        optimizer.zero_grad()\n        # forward + backward + optimize\n        outputs = net(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        # print statistics\n        # ...\nprint('Finished Training')\n# Validation\ncorrect = 0\ntotal = 0\nwith torch.no_grad():\n    for data in testloader:\n        images, labels = data\n        outputs = net(images)\n```\n\n## 张量——自动微分的对象\n\n> https://pytorch.org/tutorials/beginner/introyt/tensors_deeper_tutorial.html\n\n对于基本数据结构——张量，~~不应该~~ 也可以用大写的 `torch.Tensor(data)` 来创建；官方推荐的方法是小写的 `torch.tensor(data)`，产生的张量是 `data` 的拷贝，对一方的改动不反映到另一方。\n\n如果数据已经是 `numpy.ndarray` 了，可以用 `x_tensor = torch.from_numpy(x_array)` 和 `x_array = x_tensor.numpy()` 互相转化，他们使用同一片内存，对一方的改动会反映到另一方。\n\n更推荐的方法是用各种工厂函数，比如 \n\n- `torch.empty()`\n- `torch.zeros()`, `torch.zeros_like()`\n- `torch.ones()`, `torch.ones_like()`\n- `torch.eye()`\n- `torch.rand()`, `torch.rand_like()`\n\n其中在声明随机张量之前使用 `torch.manual_seed(seed)`，可以让距离该函数相同执行距离的随机数代码总是产生相同的数值：\n\n> [https://pytorch.org/docs/stable/notes/randomness.html](https://pytorch.org/docs/stable/notes/randomness.html)\n> \n\n### 尺寸相关\n\n张量的尺寸是一个它本身的一个性质 `x.shape`, 属于 `torch.Size` 类。\n\n张量之间的计算，在尺寸不同时，遵循类似 NumPy 的 broadcasting 规则。\n\n`x.item()` 从一个单元素张量（不一定是标量）里取出它的值，返回值的数据类型不再是 PyTorch 自己的类型，而是 python 原生类型。\n\n`x.unsqueeze(dim)` 在第 dim 个维度增加一个长度为 1 的维度。\n\n更一般的改变形状需要 `x.reshape(*new_shape)` 和 `torch.reshape(x,new_shape)`\n\n### 物理设备相关\n\n一般数学函数也有 `out` 参数，可以原位写入已存在的张量，不改变原张量的 id。\n\n原位计算是在一般数学函数名字后面加一下划线，如 `torch.sin_(x)`, `x1.add_(x2)`\n\n张量的赋值是传递指针，而不是传递数值。数值拷贝需要 `x2 = x1.clone()`. 克隆产物会保持和之前一样的 `requires_grad` 参数值。如果原张量在计算图里，新张量不需要的话应该 `c = a.detach().clone()`\n\n参与运算的两个张量需要在同一设备 (CPU/GPU). 创建张量的时候可以用 `device` 参数声明设备，已经创建的张量可以用 `x.to(device)` 改变设备。\n\n### 自动微分和神经网络相关\n\nPyTorch 中的张量都有 `requires_grad` 参数，默认为 False。\n\n对于带有 `requires_grad=True` 参数的张量，都受到自动微分机制 autograd 的控制。\n\n前面每个每个创建张量的函数都接受这个参数，也自动传染给对自动微分的张量进行数学计算的结果。\n\n神经网络，也就是继承自 `torch.nn.Module` 的对象（假设实例化为 `net`），内部有两个方法`net.parameters()` 和 `net.buffers()`，它们取到的是张量的 iterator，其中前者里面的张量（绝大多数）受 autograd 控制，后者不然。\n\n要想取得某个神经网络的单个参数，需要知道 `__init__()` 方法中的变量名，以及官网文档中的参数名（一般在 Variables 小节，比如[这个例子](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html)）\n\n```python\nclass MyNetwork(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.param1 = torch.nn.Parameter(torch.zeros((5,)),requires_grad=True)\n        self.linear = torch.nn.Linear(in_features=5,out_features=1,bias=True)\n    def forward(self,x):\n        return self.linear(x * self.param1)\nnet = MyNetwork()\nwith torch.no_grad():\n    print(net.param1)\n    print(net.linear.weight)\n    print(net.linear.bias)\n```\n\n## 自动微分\n\n> https://pytorch.org/tutorials/beginner/introyt/autogradyt_tutorial.html\n\nautograd 自动根据代码动态构建计算图：\n\n```python\na = torch.linspace(0., 2. * math.pi, steps=25, requires_grad=True)\nb = torch.sin(a)\nc = 2 * b\nd = c + 1\nout = d.sum()\n```\n\n![上述代码构成的计算图](/photos/2024-11-10-computation-graph.png)\n\n对于数学函数 `b = f(a,*args)` 计算出来的张量 `b` ——\n\n### `.grad_fn` 是一个函数\n\n- 输入只有一个，尺寸需要和 `a` 的尺寸对齐 ~~（我感觉很奇怪）~~\n- 输出的数量等于计算出这个张量的函数的输入的数量。\n- ~~直接运行这个函数 ，基本就是把输入张量重复输出~~。从[这个问答](https://stackoverflow.com/questions/66402331/in-pytorch-what-exactly-does-the-grad-fn-attribute-store-and-how-is-it-used)来看，直接运行这个函数几乎没有意义，因为自动微分是用其他编程语言实现的，`SinBackward0` 并不是 Python 对象。\n- `t.grad_fn.next_functions` 是一个 tuple，成员是 `x1` 等参数的 `.grad_fn`, 可以继续递推到工厂函数创建的张量，也就是这个计算图的叶节点。叶节点的 `.grad_fn` 为 `None`\n\n```python\nprint(a.grad_fn)   # None\nprint(b.grad_fn)   # <SinBackward0 object at 0x000001C8E240F400>\nprint(c.grad_fn)   # <MulBackward0 object at 0x000001C8E240EB30>\nprint(d.grad_fn)   # <AddBackward0 object at 0x000001C8E240F400>\nprint(out.grad_fn) # <SumBackward0 object at 0x000001C8E240F190>\n```\n\n### `.backward()`\n\n- 默认状态下，计算图的最终节点 `out` 是一个标量，也就是 0 阶张量。\n    - 要想让计算图的最终节点是一个高阶张量，需要指定 `out.backward(gradient)` 参数。`gradient`和 `out` 尺寸相同，两者对应元素相乘后求和，从而成为一个标量。\n    - 这种设定可以让一个张量的梯度 `.grad` 的尺寸只和自身的尺寸相等；如果允许计算从 `out` 到自身的 Jacobian 矩阵，会导致计算规模按多项式（一般是平方）增长。\n- 默认状态下，一个计算图只能有一个`.backward()`，且只能执行一次。多次执行会报 `RuntimeError`: “Trying to backward through the graph a second time”。\n    - 要想对完全相同的计算图做多次 `.backward()`, 需指定 `out.backward(retain_graph-True)` 参数。\n\n### `t.grad`\n\n- 偏导数张量 $$\\partial\\mathrm {out}/\\partial \\mathrm t$$，在最终节点运行 `.backward()` 之后，计算图中各节点的 `.grad` 会自动被 autograd 赋值\n- 默认情况下，只有计算图的叶节点可以访问其 `.grad`\n- 要取得中间节点的 `.grad`，需要在 `.backward()` 之前对相应张量执行 `.retain_grad()`\n\n```python\nb.retain_grad()\nout.backward()\n\nprint(a.grad)   # tensor([...],...), 约等于 2 * torch.cos(a)\nprint(b.grad)   # tensor([2.,2.,2.,...2.],...)\nprint(c.grad)   # None，不是计算图的叶节点，也没有 .retain_grad()\nprint(d.grad)   # None，不是计算图的叶节点，也没有 .retain_grad()\nprint(out.grad) # None，不是计算图的叶节点，也没有 .retain_grad()\n```\n\n以上各个操作基本上符合数学的习惯。\n\n## 张量及其微分的更新\n\n以下设定更多的是为了工程上方便：\n\n`.grad` 是一个张量的性质，不是最终节点和该张量两者之间的性质，也不是计算图整体的性质。\n\n不论是 `.backward(retain_graph=True)`，还是另外构建了新的计算图并运行 `.backward()`（比如训练时加载了训练集的新一轮 batch），当一个已有 `.grad` 的张量被新的 `.backward()` 波及时，`.grad` 的新值会是旧值和本轮偏导数的相加。\n\n如果不想要这种累加，需要主动提前将张量 `.grad` 改成 0 或者 None。\n\n除了梯度张量的置 0，\n\n### 手动\n\n正常情况下，`requires_grad=True` 的张量，对其做的所有操作都会被 autograd 记录，包括赋值操作，这就导致正常情况下，不论是清空梯度，还是根据梯度更新模型的参数，会导致 auto grad 的混乱。\n\n所以需要特殊操作，通知 autograd 某些对受控张量的改动，不需要成为计算图的一部分。\n\n所谓的特殊操作就是 `torch.no_grad()`\n\n```python\na = torch.arange(5.,requires_grad=True)\nb = torch.sin(a)\nb.backward(gradient=torch.ones_like(b))\nwith torch.no_grad():\n    a[1] = 10\n    a.grad.zero_()  # a.grad = 0\nprint(a)  # tensor([0,10,2,3,4], requires_grad=True)\n```\n\n在完全不涉及训练，只需要推理的环境，`with torch.inference():` 比 `with torch.no_grad():` 更节省计算资源。\n\n### 自动\n\nPyTorch 提供了上述操作的接口。\n\n神经网络的参数的更新有一整个 `torch.optim` [模块](https://pytorch.org/docs/stable/optim.html#algorithms) 提供的更新参数的方式（`Optimizer` 的子类）。里面的优化器都实现了两个方法：`.step()` 和 `.zero_grad()`，前者修改参数本身的值，后者将各参数的梯度归零。\n\n```python\nclass MyModel(torch.nn.Module):\n    def __init__():\n        # ...\n    def forward(x):\n        # ...\n        return x\nnet = MyModel()\nopt = torch.optim.SGD(net.parameters(),lr=0.01)\nfor e in range(epoch):\n    # ...\n    opt.step()\n    opt.zero_grad()\n```\n\n如果只需要将梯度归零的话，nn.Module 也有一个 `.zero_grad()` 方法。\n\n### `tensor` vs. `tensor.data`\n\n看 SGD 优化器的源码的时候，看到更新权重的时候不是 `with torch.no_grad():`，而是用到了张量的 `a.data` 和 `a.grad.data`\n\n`a` vs. `a.data`, `b.grad` vs. `b.grad.data` 的区别今天不需要知道，不应该知道。这是旧的 Variable 对象的残留，新的接口将 Variable 合并进了 Tensor，`.data` 只是为了兼容旧代码而存在的，不再推荐这种用法。\n"},{"slug":"error-propagation-and-philosophy-of-science","filename":"2024-10-21-error-propagation-and-philosophy-of-science.md","date":"2024-10-21","title":".tex | 误差的传递，科学之所以科学","layout":"post","keywords":["tex","phy","phi"],"excerpt":"多变量测量的误差传递，及其在科学哲学中的作用。","content":"\n如果一个物理量需要用多个直接观测量计算出来：$$y=f(x_1,x_2,...,x_n)$$，这样的量叫做因变量，直接观测量叫做自变量。（比如用直尺测量长方形面积时，长、宽是自变量，面积是因变量，通过长和宽相乘计算面积的方法是一个函数。）\n\n因为中小学减负，因变量的说法不教了，改叫函数值，为了少学一个知识点。\n\n但是学过 C/C++ 的应该知道，对于 $$y=f(x_1,x_2,...)$$\n\n- 因变量 $$y$$ 是一个左值，指向 $$y$$ 的指针 `float *p = &y;` 拿到的地址，位于内存的数据区；\n- 函数值 $$f(\\cdot)$$ 是一个右值，$$f$$ 本身就是一个指针，`void *fp = f;` 拿到的地址，位于内存的指令区。\n\n## 多变量测量的误差传递\n\n先跳过单变量误差的部分（大致原理在《贝叶斯，从公式到世界观》一文频率学派的部分，具体细节以后再写），不论是测量仪器的说明书给出的误差，还是测量者通过独立重复实验取得的统计误差，我们先假设已经拿到了观测量 $$x$$ 的测量值 $$\\bar x$$、误差 $$\\Delta x$$\n\n因为全微分公式，对于 $$y=f(x_1,x_2,...,x_n)$$\n\n$$\n\\mathrm{d}y = \\frac{\\partial f}{\\partial x_1}\\mathrm{d}x_1 + \n\\frac{\\partial f}{\\partial x_2}\\mathrm{d}x_2 + ... + \\frac{\\partial f}{\\partial x_n}\\mathrm{d}x_n = \\sum_i^n \\frac{\\partial f}{\\partial x_i}\\mathrm{d}x_i\n$$\n\n又因为误差相对于真值往往小几个数量级，所以我们把误差看作是真值的微分，用 $$\\Delta$$ 取代 $$\\mathrm{d}$$. （有人问真值为 0 怎么办，绝大多数情况下可以通过平移零点定义的办法来几乎任意地改变测量值的数量级，而误差不会因为这种变换而出现数量级变化。）\n\n还因为对多个自变量的测量是相互独立的，每个自变量 $$(x_1,x_2,...,x_n)$$ 占据相空间中的一个维度，维度之间互相正交。\n\n所以物理上，因变量的误差就是上述 ~~微分~~ 微差向量的“长度”，以  L2 范数 (norm) 来衡量：\n\n$$\n\\begin{array}{rcl}\\Delta y & = & \\sqrt{ \\left(\\frac{\\partial f}{\\partial x_1}\\bigg|_{\\vec x}\\right)^2\\Delta x_1^2 + \\left(\\frac{\\partial f}{\\partial x_2}\\bigg|_{\\vec x}\\right)^2\\Delta x_2^2 +...+ \\left(\\frac{\\partial f}{\\partial x_n}\\bigg|_{\\vec x}\\right)^2\\Delta x_n^2 } \\\\ & = & \\sqrt{\\sum_i^n{\\left(\\frac{\\partial f}{\\partial x_i}\\bigg|_{\\vec x}\\right)^2\\Delta x_i^2}}\\end{array}\n$$\n\n物理学家因此不害怕误差——理论物理的模型哪怕非常复杂，在数学上往往依然“性质优美”，只要理论的自变量可以在实验上测量，误差明确且有限，那理论给出的预测值的误差就同样明确且有限，依然可以指导实践。\n\n## 误差与可证伪性\n\n而根据卡尔·波普尔的科学哲学，具体来说就是可证伪性的划界标准，科学就不只是不害怕误差了，简直是依赖误差而生，靠误差来和伪科学划清界限。\n\n所谓科学的可证伪性，《[科学是什么？——兼谈“非科学、伪科学、反科学”和一些常见谬误](https://program-think.blogspot.com/2015/10/What-is-Science.html)》一文概括为：\n\n- 科学理论是一个相互关联的**命题**的**集合**。\n- 科学理论必须是基于**演绎**法建立整个理论体系的。也就是从不证自明的**定律**出发，依据**逻辑规则**，推论出各种各样的**定理**。\n- 理论中的命题必须是**客观**陈述，也就是能由不同的主体进行独立检验。\n- 检验的方式是**证伪**，也就是寻找现实中的一个现象，说明从理论中某个命题是错的。经过证伪程序，且没能被证伪的命题，就被验证为真。（根据逆否命题的等价性，演绎推论如果被证伪，它的逻辑前提也会连锁被证伪。那科学几百年来靠什么幸存，我们以后再狡辩～）\n- （既然一个存在命题就能否定一个学科理论中的待验证命题，）科学理论中的命题应该是**全称命题**，此即科学的普世性 (universality)。\n- 对全称命题的**特设性修正**（比如把“所有的天鹅都是白色的”修正成“所有北半球的天鹅都是白色的”），应该要提高理论的可证伪程度，否则就是伪科学。\n- 以上各个要求，单独只构成必要条件。\n\n所以定量科学的科学性就体现在，\n\n- 只要理论的自变量和因变量可以在实验上测量，\n- 自变量的误差明确且有限，那理论给出的预测值的误差就同样明确且有限，\n- 因变量的误差同样明确且有限，\n- 将理论预测值和因变量测量值摆在一起，只要差距不大于两者的误差，（技术细节在统计学中的假设检验部分。）\n- 我们就认为对“理论预测和因变量真值相等”的证伪失败了，从而接受他们相等。\n\n因为实验仪器和方法的进步可以缩小误差范围，增加科学理论的可证伪性，一条无限延展且随时可以投入精力的赛道从此出现，科学从相隔几代的少数天才之间的思维接力，变成了夙兴夜寐前赴后继的竞赛。\n\n科学家之间的竞争催生了对制造仪器之工程技术的巨大需求，需求大到部分科学家亲自下场改进甚至发明仪器，科学由此反哺技术；进步的技术使得科学得以产出更高质量的数据，支持更复杂的理论的检验。科学和技术，合成了“科技”一个词。\n\n试想一下，如果科学号称自己绝对精确，不存在误差，要么在弱小之时就被证伪，无法赢得人们的信任；要么任由实验精度低到看不出误差的程度，然后用其他手段维持自己的光辉形象。\n\n之前《也谈近代科学从西方起步》一文中说，“物理和数学的区别，在于理论和实验两条腿走路”。如今算是把实验这另一条腿简单介绍完了。\n\n最后需要注意，这里说的是某个哲学理论能够解释科学实践，而不是科学实践必须服从某套哲学理论。\n\n科学只对客观现实负责，不需要对哲学信条负责，不应该对哲学王兼英雄王负责。\n\n## 科学还正确吗？\n\n如果承认可证伪性作为科学与非科学的划界标准，也就意味着，现在包含在科学中的每条知识，都有在未来被更加精确的实验推翻的可能。\n\n这种事也不是没发生过。比如材料的电阻，在相当大的数值范围内，都和温度成线性关系，而且这条线向左延拓到绝对零度时基本为 0。在那个时代，认为电阻来自无规则的热运动，和绝对温标成正比才是符合奥卡姆剃刀原则的理论。但是 1908 年，昂内斯用液氦将汞的温度降到 4.15 K 时，发现汞的电阻突变降低为 0，这就是超导研究的开端。\n\n那还能说科学知识是正确的吗？《费曼物理学讲义》的回答是，不谈科学是不是正确的，只保证科学 (science)是科学的 (scientific)。也就是保证程序的正确，把正确程序获得的结果交给工程技术，用工程技术上的成就取信于社会，反过来为科学的正确性背书。\n\n所以说，科学家是对科学最不迷信的一批人，一旦实验过程正确，结果和理论不符，那么理论该修改的修改，该放弃的放弃。他们是现有科学最大的破坏者，是成功证伪科学命题最多的一群人。\n\n但同理，科学家又是对科学最坚定的一批人，他们在明知道一个科学命题可能在将来被修正的情况下，依然愿意把它当作前提，继续推理产出新的命题，并试图证伪。\n\n《三体》小说刚开始设置的一大悬疑，大量科学家因为自己正在进行的研究，产出了与理论完全不吻合的随机结果，因为所谓“物理学不存在了”而自杀，这个情节就很成问题。\n\n何况这种事情根本不需要书中的情节设定才会出现，物理学史上早就发生过。比如 β 衰变的质子的动能谱和动量角分布。玻尔想放弃能量守恒定律，泡利想假设一个探测器发现不了的新粒子，这在当时的实验条件下都是尚不能证伪的理论假设，但没听说俩人为这事寻死觅活的。\n\n所以改编成电视剧的时候，几乎重写整个人物关系的网飞版，把自杀改写成了球奸们伪装的他杀；就连以忠实于原作著称的腾讯版，也原创了一段主角主动重启科研装置，直面外星人恐吓的剧情，给原著做了点找补。\n\n## 那我缺的权威性这块谁给我补上啊\n\n坏了，碰到了不该碰的话题，那就先这样吧……\n\n## 报书名儿\n\n- William Lichten.《Data and Error Analysis》\n- 赵凯华《定性和半定量物理学》\n- 卡尔·波普尔《科学发现的逻辑》\n- 理查德·费曼《费曼物理学讲义》\n"},{"slug":"autumn-tour-grand-teton-and-yellowstone","filename":"2024-10-13-autumn-tour-grand-teton-and-yellowstone.md","date":"2024-10-13","title":".xls | 秋游大提顿和黄石公园","layout":"post","keywords":["doc","xls","jpg"],"excerpt":"今年秋假去了 Grand Teton 大提顿和 Yellowstone 黄石国家公园。","cover":"2024-10-13-Yellowstone.png","content":"\n今年秋假去了 Grand Teton 大提顿和 Yellowstone 黄石国家公园。两个公园南北相邻，向南最近的城镇是 Jackson Hole，也就是美联储开会的地方。\n\nGrand Teton 依托于提顿山脉，山的海拔不算特别高，但是胜在山脉东侧的湖泊和盆地，衬托得山势雄伟。园内落叶阔叶林和常绿针叶林混搭，秋季色彩明丽。\n\nYellowstone 整个公园相当于一个活火山的火山口，园内有大量温泉、间歇泉、地热，还有黄石河切削而成的峡谷和瀑布，一年四季的景色都很优美。但是园内植被主要是常绿针叶林，所以秋天并不额外出彩。\n\n另外秋季两个公园内的野生动物都不活跃，不过对于像我这样没有 100 - 400 mm 长焦镜头的普通游客影响不大～\n\n两个公园的门票都是一周内无限次进入，好处是景区内商家不存在行政垄断，吃饭加油的价格并不比外面贵；坏消息是外面和公园里一样贵，提顿村汽油每加仑超过 $5，杰克逊稍微便宜一些但也很贵。\n\n![大提顿公园地图](/photos/2024-10-13-GrandTeton.png)\n\n![黄石公园地图](/photos/2024-10-13-Yellowstone.png)\n\n### 流水账\n\n- 第一天\n    - 在盐湖城中转，期间下载了两个公园的 Google 离线地图\n    - 中午到 Jackson Hole 机场，在租车行取车\n    - 在 Dornans 下车拍照\n    - 下午开车沿 Hwy 191 向北，在沿途的停车区短停拍照，看到 Pack Trail Fire 山火的烟\n    - 经过大提顿和黄石公园南入口，分别买票\n    - 到达黄石公园 Moose 瀑布后返回\n    - 前往提顿村，路上在 Jenny Lake outlook 拍照\n    - 入住酒店\n- 第二天\n    - 从提顿村前往黄石，经过 Teton Park Road，沿途在停车区拍照\n    - 在黄石公园依次游览 West Thumb 间歇泉、Mud Volcano 火山湖、Upper & Lower 瀑布\n    - 试图前往 Dunraven ，因为不太会用离线地图，一直开到了 Roosevelt Lodge 才发现走错了，而且那里秋季不开门\n    - 返回，抵达 Dunraven Pass 时日落，没找到上山途径，在路对面的山坡上看晚霞\n    - 在公园内吃晚饭，不比宾馆贵\n    - 夜行回宾馆\n- 第三天\n    - 试图在宾馆附近拍日出，东方有山，看见太阳时高度角已经很高，未见红日\n    - 在黄石公园依次游览 Old Faithful 间歇泉、Steamboat 间歇泉、Congress pool\n    - 试图返回 Jenny lake 拍日落，没太赶上，发现另一个拍日出的机位 Cathedral Group Turnout，事后证明不是秋季的最佳机位\n- 第四天\n    - 早起前往 Cathedral Group Turnout，\n        - 看提顿峰顶的第一缕日光\n        - 在长焦镜头取景框里看到大角公鹿，没能拍下清晰照片\n        - 回程发现了更好的机位，向东可以看到地平线上的日出，向西有若干金黄色落叶阔叶林作为提顿峰的前景，非常后悔\n    - 在 Jenny lake 拍照\n    - 退房后，大提顿以东的整个平原/盆地隐没于远方山火扩散来的烟霾中\n    - 前往 Mormon Row，是一处早期定居点，遇花栗鼠一只，蓝鸟一对\n    - 穿过 Jackson 镇，进入 Elk Refuge，此处的烟霾比提顿附近略轻。\n    - 返回 Jackson 镇吃午饭，加油，回机场"},{"slug":"switched-domain-name","filename":"2024-09-30-switched-domain-name.md","date":"2024-09-30","title":"通知：本站网址変更","layout":"post","keywords":["md","html"],"excerpt":"未来可能无法自动跳转，请浏览器收藏和 RSS 订阅的读者更新网址。","content":"\n2024 年 9 月 30 日起，本站网址\n- 从 `https://mountaye.github.io/blog/` \n- 变更为 `https://blog.mountaye.com`.\n\n因为网站仍然架设在 GitHub Pages 上，所以旧网址可以自动跳转到新网址。\n\n但是下一步计划把网站迁移到 Cloudflare Pages 上，因为 Next.js 项目不同构建之间文件差异过多，不适合作为 git 仓库的内容进行托管。\n\n所以无法保证将来旧网址依然可以自动跳转。请使用浏览器收藏功能的读者更新收藏夹，使用 RSS 订阅功能的读者更新 RSS 源。\n\n根据初步计划，过渡期将持续到 2024 年 12 月 31 日。\n"},{"slug":"bayesian-equation-and-view-of-world","filename":"2024-09-27-bayesian-equation-and-view-of-world.md","date":"2024-09-27","title":".m | Bayesian 贝叶斯，从公式到世界观","layout":"post","keywords":["tex","m","phy","phi"],"excerpt":"我们老板真是太能吹了，Bro 居然跟隔壁真的在研究物理的课题组 brag abou 我会贝叶斯参数估计，yo know wat ur sayin? 赶紧来补课～","content":"\n\n## 公式\n\n我上学的时候，贝叶斯公式是概率论里面，少数高中完全不涉及，到了本科才第一次见的公式，所以我从来没背下来过。不过也用不着背，根据条件概率里面的一个平凡结果：\n\n$$\n\\Pr(A|B)\\ \\Pr(B) = \\Pr(B|A)\\ \\Pr(A)\n$$\n\n可以得到 $$\\Pr(A|B)$$ 和 $$\\Pr(B|A)$$ 之间的关系\n\n$$\n\\Pr(A|B) = \\frac{\\Pr(B|A)\\ \\Pr(A)}{\\Pr(B)}\n$$\n\n这就是贝叶斯公式本体。\n\n分母没什么意思，所以一般我们要用全概率公式替换，也就是把 $$A$$ 划分为全覆盖但是不相交的 $$\\{A_i | \\ A_i \\cap A_{j \\neq i}=\\varnothing,\\ \\bigcup_i A_i=A\\}$$\n\n$$\n\\Pr(A|B) = \\frac{\\Pr(B|A)\\ \\Pr(A)}{\\sum_i \\Pr(B|A_i) \\Pr(A_i)}\n$$\n\n其中任意一个子事件 $$A_j$$\n\n$$\n\\Pr(A_j|B) = \\frac{\\Pr(B|A_j)\\ \\Pr(A_j)}{\\sum_i \\Pr(B|A_i) \\Pr(A_i)}\n$$\n\n### 根据实验结果筛选理论模型\n\n以上是数学。在科学中，令\n\n- A 为一族理论模型的一组参数取值，记为 $$Param_k$$，下标可任意选取。\n- B 为实验观测数据，记为 *Ob*\n\n$$\n\\Pr(Param_j|Ob) = \\frac{\\Pr(Ob|Param_j)\\ \\Pr(Param_j)}{\\sum_i \\Pr(Ob|Param_i) \\Pr(Param_i)}\n$$\n\n其中 \n\n- $$\\Pr(Param_j)$$ 表示第 j 组参数是模型的正确参数的，未经实验验证，根据零假设计算的 **先验 (prior) 概率；**\n- $$\\Pr(Param_j|Ob)$$ 叫做经过实验观测修正之后的，第 j 组参数正确的 **后验 (posterior) 概率**。\n- $$\\Pr(Ob|Param_j)$$ 在之前的文章中讲过，是当前测量数据下，模型参数的 **似然性 (likelihood)**。\n\n$$\nposterior \\propto likelihood \\cdot prior\n$$\n\n### 举个例子\n\n隔壁组的问题可以简化为下图：\n\n![](/photos/2024-09-27-two-gaussian.png)\n\n- 有两组数据 (x, y1), (x, y2) 可以用同一族函数来拟合。（假设为两个高斯函数的叠加，$$y=f_{A_1,A_2,\\mu_1,\\mu_2,\\sigma_1,\\sigma_2}(x) = A_1e^{-\\frac{(x-\\mu_1)^2}{\\sigma_1^2}} + A_2e^{-\\frac{(x-\\mu_2)^2}{\\sigma_2^2}}$$\n- 两组数据的误差不同。（红色数据点显然比蓝色数据点，相对于理论值偏离得更远一些）\n- 问有没有一个数值，可以衡量每组数据的误差程度。\n\n我给他们的建议是\n\n- 根据自己的专业知识指定先验概率 $$\\Pr(param_j)=\\Pr(A_1,A_2,\\mu_1,\\mu_2,\\sigma_1,\\sigma_2)$$。比如选定一个参数空间的范围，范围之外概率为零，范围之内均匀分布。\n    - $$A_1,A_2 \\in \\left[\\min(\\{Y_1\\}\\cup \\{Y_2\\}),\\max(\\{Y_1\\}\\cup \\{Y_2\\}\\right]$$\n    - $$\\mu_1\\in[\\min\\{X\\},\\max\\{X\\}],\\ \\mu_2\\in[\\mu_1,\\max\\{X\\}]$$\n    - $$\\sigma_1,\\sigma_2\\in[0,\\ \\Sigma_i\\sqrt{|X_i-\\bar X|^2/N}]$$\n- 根据一些假设和统计规律计算 $$\\Pr(Ob|Param_j)$$\n    - 假设误差与 x 变量无关，服从期望为 0 的高斯分布，$$[y_i-f(x_i)]\\sim N(0,\\sigma^2)$$，标准差根据各数据点减去模型预测值的残差估计。\n    - 假设每个数据点的观测相互独立，$$\\Pr(Ob)=\\Pr(\\bigcap_i Ob_i)=\\prod_i\\Pr(Ob_i)$$\n    - 对于模型的每一组参数 ，$$\\Pr(Ob_i|param_j)$$ 取上述高斯分布的绝对值大于残差绝对值的部分，就是钟形曲线两侧尾巴的线下面积。\n- 对参数空间中的每一组值都算出一个后验概率之后，计算整个空间的信息熵（方法见之前的文章）。误差较大的一组数据，应当有更多组参数可以获得类似的拟合结果，从而信息熵更大。\n\n## 世界观\n\n对于概率，有三种理解：\n- 古典的 (classical)、\n- 频率学派的 (Frequentist)、\n- 贝叶斯的 (Bayesian).\n\n### 古典\n\n就是将古典概型推广，成为一种关于可能性的普遍观点——一个随机空间里的随机事件可以分解成若干子事件，子事件还可以再分，直到每个基本事件的概率相等，都等于基本事件总数的倒数，而要计算人们感兴趣的某一事件，只需要数出其包含的基本事件的数量就行了。\n\n让人联想到古希腊古典时代的原子论。时人认为物质世界也不是无限可分的，将任意一种材料打碎研磨，这一过程最终会有一个终点，最终的产物就是这种物质的“原子”。一块材料的大小，就是其所含原子数量的多少。\n\n有人批评这种观点用可能性去定义可能性，有循环论证谬误之嫌。但是看现代化了的概率论，概率被定义成了满足某些条件的函数，公理化是公理化了，逻辑链条是有了坚实的起点，但是那里的概率还能不能被当作可能性的度量，实在是不好说。\n\n有人批评这是机械唯物主义，这种人批判的武器一般是武器的批判，别争辩，先活下来再说。\n\n### 频率学派\n\n这种观点一言以蔽之：概率是频率在样本量趋于无穷时的极限。\n\n科学中（日常生活中也一样，只是人们通常没这么精确），测量误差不可避免，我们每一次的测量哪怕正确，互相之间也会有细微的差别，更不用说和待测的真实值不同了。\n\n解决方法在初中物理实验里学过：多次测量，把平均值当作真值（的估计量），根据标准差计算误差（置信区间、p 值等等……）。\n\n不同的人（假设有 M 个）可以对同一个可观测量进行 N 次测量，对于一个确定的 N，不论这个可观测量本身服从何种概率分布，这 N 个测量值的平均数 $$\\bar X_N$$ 都服从正态分布，这就是中心极限定理（注意不是大数定律）。\n\n当可观测量本身也服从正态分布的时候，就会导致标准差 (standard deviation) 和标准偏误 (standard error of the mean, 常简称为 standard error) 容易让初学者混淆。\n\n而按照这种世界观，所谓一个物理量的真值，就是所有可能的（所有已经发生过的+思想实验中可能发生的）测量的均值 $$\\bar X_\\infin$$。\n\n因为包括可能发生还未发生的测量，所以哪怕我们面对的问题是纯决定论的，客观存在一个确定的真值，无论我们已经进行过多少次测量，都无法保证得到真值。\n\n有人批评这是客观唯心主义，这种人批判的武器一般是武器的批判，别争辩，先活下来再说。\n\n### 贝叶斯\n\n前述世界观好歹还认为真值客观存在——\n\n贝叶斯世界观则直接不再对真值的客观存在下断言，不论先验还是后验，科学理论里的每一条命题，都不再孤单，而是要和所有可能的替代理论打包在一起；也不再“正确”，而是具有一个以概率衡量的可信程度。\n\n实验的作用不再是判断对错，而是在有限的先验知识（现存的科学理论）下，判断新取得的实验结果在多大程度上，更新了旧知识里每条命题的可信权重。\n\n而且每个人掌握的知识不同，先验概率不同，在同样的实验数据面前，所更新出来的知识体系也会不同。\n\n再者，如果先验概率为 0，任你实验数据如何显著，后验概率也一定为 0，所以对“未知的未知”无能为力。实践中，再离谱的先验假设，只要能想到，也要赋一个小而不为 0 的初值。\n\n有人批评这是主观唯心主义，这种人批判的武器一般是武器的批判，别争辩，先活下来再说。\n\n## 送分题\n\n已知本省不超过二十个地级行政单位。一中是本市最好的高中，本科过线人数年年创新高。\n\n已知本市报纸会公布喜报，上有全市前若干名学生的姓名、分数、录取学校等信息。省招办有根据成绩取得全省排名的服务。比如某年本市第十名，全省排名两千名开外。\n\n你能否据此评价母校和家乡的教学质量，以及本省各地区之间教育水平的平均程度？\n\n你该如何评价，从定义原假设和备择假设，到用何种概率分布对先验概率建模？\n\n你有资格评价吗？"}]},"__N_SSG":true}