<!DOCTYPE html><html lang="zh-CN"><head><title>.tex | MC→MCMC 蒙特卡洛模拟，基于马尔科夫链采样 | 阿掖山·博客</title><meta property="og:title" content=".tex | MC→MCMC 蒙特卡洛模拟，基于马尔科夫链采样 | 阿掖山·博客"/><meta name="twitter:title" content=".tex | MC→MCMC 蒙特卡洛模拟，基于马尔科夫链采样 | 阿掖山·博客"/><meta name="description" content="蒙特卡洛模拟、马尔科夫链采样、Metropolis-Hastings 算法、吉布斯采样"/><meta property="og:description" content="蒙特卡洛模拟、马尔科夫链采样、Metropolis-Hastings 算法、吉布斯采样"/><meta name="twitter:description" content="蒙特卡洛模拟、马尔科夫链采样、Metropolis-Hastings 算法、吉布斯采样"/><meta property="og:image" content="https://blog.mountaye.com/assets/img/before_h2.png"/><meta name="twitter:image" content="https://blog.mountaye.com/assets/img/before_h2.png"/><meta charSet="utf-8"/><meta http-equiv="X-UA-Compatible" content="IE=edge"/><meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no"/><meta name="generator" content="Next.js v14.2.6"/><link rel="canonical" href="https://blog.mountaye.com/articles/mc-mcmc-markov-chain-monte-carlo-gibbs-sampling"/><link rel="icon" href="/favicon.ico" type="image/x-icon"/><meta property="og:locale" content="zh-CN"/><meta property="og:url" content="https://blog.mountaye.com/articles/mc-mcmc-markov-chain-monte-carlo-gibbs-sampling"/><meta name="twitter:card" content="summary_large_image"/><meta name="next-head-count" content="17"/><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css"/><link rel="preload" href="/_next/static/css/70a087ca0a2aa809.css" as="style"/><link rel="stylesheet" href="/_next/static/css/70a087ca0a2aa809.css" data-n-g=""/><link rel="preload" href="/_next/static/css/c01db217da1c4d5f.css" as="style"/><link rel="stylesheet" href="/_next/static/css/c01db217da1c4d5f.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-42372ed130431b0a.js"></script><script src="/_next/static/chunks/webpack-2df7a8d27de1794c.js" defer=""></script><script src="/_next/static/chunks/framework-64ad27b21261a9ce.js" defer=""></script><script src="/_next/static/chunks/main-e618b1edbb350739.js" defer=""></script><script src="/_next/static/chunks/pages/_app-82a2279cdebeb61d.js" defer=""></script><script src="/_next/static/chunks/24-03c6412f1cc555e5.js" defer=""></script><script src="/_next/static/chunks/923-e362c0f2482feb7b.js" defer=""></script><script src="/_next/static/chunks/pages/articles/%5Bid%5D-9564c34343a7e645.js" defer=""></script><script src="/_next/static/buildID/_buildManifest.js" defer=""></script><script src="/_next/static/buildID/_ssgManifest.js" defer=""></script></head><body><div id="__next"><div id="all" class="w-full h-max"><nav id="top" class="w-full h-11 sm:h-12 lg:h-14 fixed top-0 flex flex-row justify-between items-center z-50 bg-white dark:bg-black shadow-lg dark:shadow-gray-700 transition-transform"><a class="whitespace-nowrap rounded-md text-sm font-medium ring-offset-background transition-colors focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:pointer-events-none disabled:opacity-50 dark:text-slate-400 text-primary underline-offset-4 hover:underline h-10 py-2 w-max px-2 flex flex-row justify-end items-center grow-0 shrink-0" href="/"><span class="relative flex shrink-0 overflow-hidden rounded-full h-9 w-9 sm:h-10 sm:w-10 md:h-12 md:w-12"><span class="flex h-full w-full items-center justify-center rounded-full bg-muted">Aye</span></span><div id="site title" class="pl-2 hidden sm:block"><span class="text-2xl font-serif dark:font-sans">阿掖山</span><span class="ml-[0.5em] text-base font-sans hidden lg:inline">博客</span></div></a><div class=" grow-0 shrink flex w-[25%]"><input type="text" class="flex h-10 w-full rounded-md border border-input px-3 py-2 text-sm ring-offset-background file:border-0 file:bg-transparent file:text-sm file:font-medium placeholder:text-muted-foreground focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:cursor-not-allowed disabled:opacity-50 rounded-r-none bg-inherit dark:border-gray-700" id="search-input" placeholder="Search"/><button class="inline-flex items-center justify-center whitespace-nowrap text-sm font-medium ring-offset-background transition-colors focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:pointer-events-none disabled:opacity-50 dark:text-slate-400 border border-input hover:bg-accent hover:text-accent-foreground h-10 w-10 rounded-none border-x-0 bg-inherit dark:border-gray-700" type="button" id="radix-:R19km:" aria-haspopup="menu" aria-expanded="false" data-state="closed"><span class="relative flex shrink-0 overflow-hidden h-5 w-5 rounded-none"><span class="flex h-full w-full items-center justify-center rounded-full bg-muted">Aye</span></span></button><button class="inline-flex items-center justify-center whitespace-nowrap rounded-md text-sm font-medium ring-offset-background transition-colors focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:pointer-events-none disabled:opacity-50 dark:text-slate-400 border border-input hover:bg-accent hover:text-accent-foreground h-10 w-10 rounded-l-none bg-inherit dark:border-gray-700"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-search h-4 w-4"><circle cx="11" cy="11" r="8"></circle><path d="m21 21-4.3-4.3"></path></svg></button></div><div class="flex flex-row"><button class="inline-flex items-center justify-center whitespace-nowrap rounded-md text-sm font-medium ring-offset-background transition-colors focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:pointer-events-none disabled:opacity-50 dark:text-slate-400 hover:bg-accent hover:text-accent-foreground h-10 py-2 md:hidden px-1 md:px-2 lg:px-3" type="button" id="radix-:R2tkm:" aria-haspopup="menu" aria-expanded="false" data-state="closed"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-layers"><path d="m12.83 2.18a2 2 0 0 0-1.66 0L2.6 6.08a1 1 0 0 0 0 1.83l8.58 3.91a2 2 0 0 0 1.66 0l8.58-3.9a1 1 0 0 0 0-1.83Z"></path><path d="m22 17.65-9.17 4.16a2 2 0 0 1-1.66 0L2 17.65"></path><path d="m22 12.65-9.17 4.16a2 2 0 0 1-1.66 0L2 12.65"></path></svg></button><div class="hidden md:block "><a class="inline-flex items-center justify-center whitespace-nowrap rounded-md text-sm font-medium ring-offset-background transition-colors focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:pointer-events-none disabled:opacity-50 dark:text-slate-400 hover:bg-accent hover:text-accent-foreground h-10 py-2 px-1 md:px-2 lg:px-3" href="/history"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-history"><path d="M3 12a9 9 0 1 0 9-9 9.75 9.75 0 0 0-6.74 2.74L3 8"></path><path d="M3 3v5h5"></path><path d="M12 7v5l4 2"></path></svg></a><a class="inline-flex items-center justify-center whitespace-nowrap rounded-md text-sm font-medium ring-offset-background transition-colors focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:pointer-events-none disabled:opacity-50 dark:text-slate-400 hover:bg-accent hover:text-accent-foreground h-10 py-2 px-1 md:px-2 lg:px-3" href="/topics"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-tags"><path d="m15 5 6.3 6.3a2.4 2.4 0 0 1 0 3.4L17 19"></path><path d="M9.586 5.586A2 2 0 0 0 8.172 5H3a1 1 0 0 0-1 1v5.172a2 2 0 0 0 .586 1.414L8.29 18.29a2.426 2.426 0 0 0 3.42 0l3.58-3.58a2.426 2.426 0 0 0 0-3.42z"></path><circle cx="6.5" cy="9.5" r=".5" fill="currentColor"></circle></svg></a><a class="inline-flex items-center justify-center whitespace-nowrap rounded-md text-sm font-medium ring-offset-background transition-colors focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:pointer-events-none disabled:opacity-50 dark:text-slate-400 hover:bg-accent hover:text-accent-foreground h-10 py-2 px-1 md:px-2 lg:px-3" href="/discuss"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-messages-square"><path d="M14 9a2 2 0 0 1-2 2H6l-4 4V4c0-1.1.9-2 2-2h8a2 2 0 0 1 2 2z"></path><path d="M18 9h2a2 2 0 0 1 2 2v11l-4-4h-6a2 2 0 0 1-2-2v-1"></path></svg></a></div><button class="inline-flex items-center justify-center whitespace-nowrap rounded-md text-sm font-medium ring-offset-background transition-colors focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:pointer-events-none disabled:opacity-50 dark:text-slate-400 hover:bg-accent hover:text-accent-foreground h-10 py-2 px-1 md:px-2 lg:px-3" type="button" id="radix-:R1dkm:" aria-haspopup="menu" aria-expanded="false" data-state="closed"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-languages w-5 h-5"><path d="m5 8 6 6"></path><path d="m4 14 6-6 2-3"></path><path d="M2 5h12"></path><path d="M7 2h1"></path><path d="m22 22-5-10-5 10"></path><path d="M14 18h6"></path></svg><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-chevron-down w-4 h-4 hidden md:block"><path d="m6 9 6 6 6-6"></path></svg></button><div class="inline-flex items-center justify-center whitespace-nowrap rounded-md text-sm font-medium ring-offset-background transition-colors focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:pointer-events-none disabled:opacity-50 dark:text-slate-400 hover:bg-accent hover:text-accent-foreground h-10 py-2 px-1 md:px-2 lg:px-3 flex items-center space-x-1"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-sun"><circle cx="12" cy="12" r="4"></circle><path d="M12 2v2"></path><path d="M12 20v2"></path><path d="m4.93 4.93 1.41 1.41"></path><path d="m17.66 17.66 1.41 1.41"></path><path d="M2 12h2"></path><path d="M20 12h2"></path><path d="m6.34 17.66-1.41 1.41"></path><path d="m19.07 4.93-1.41 1.41"></path></svg><button type="button" role="switch" aria-checked="false" data-state="unchecked" value="off" class="peer inline-flex h-6 w-11 shrink-0 cursor-pointer items-center rounded-full border-2 border-transparent transition-colors focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 focus-visible:ring-offset-background disabled:cursor-not-allowed disabled:opacity-50 data-[state=checked]:bg-primary data-[state=unchecked]:bg-input" id="night-mode"><span data-state="unchecked" class="pointer-events-none block h-5 w-5 rounded-full bg-background shadow-lg ring-0 transition-transform data-[state=checked]:translate-x-5 data-[state=unchecked]:translate-x-0 dark:bg-slate-500"></span></button><input type="checkbox" aria-hidden="true" style="transform:translateX(-100%);position:absolute;pointer-events:none;opacity:0;margin:0" tabindex="-1" value="off"/><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-moon text-sm font-medium leading-none peer-disabled:cursor-not-allowed peer-disabled:opacity-70 hidden md:block" for="night-mode"><path d="M12 3a6 6 0 0 0 9 9 9 9 0 1 1-9-9Z"></path></svg></div></div></nav><aside id="side" class="
                        hidden md:flex flex-col 
                        w-36 h-screen
                        fixed left-0 bottom-0 
                        z-40 bg-white dark:bg-black shadow-2xl dark:shadow-gray-700         
             "><div class="h-16 grow-0"></div><div class="grow py-20 px-4 text-base font-serif dark:font-sans text-wrap overflow-x-auto utils_right2left__q5Pl7 utils_verticalLines__s6Q8n"><p id="quote-line">智力活动是一种生活态度</p><p>——<span id="quote-author">阿掖山，一个博客</span></p></div><footer class="py-1 grow-0 shrink-0 flex flex-col items-center"><div class="text-sm"><p>阿掖山<!-- --> © <!-- -->2024<br/>作者保留版权</p></div><div class="flex flex-row"><a class="inline-flex items-center justify-center whitespace-nowrap rounded-md text-sm font-medium ring-offset-background transition-colors focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:pointer-events-none disabled:opacity-50 dark:text-slate-400 hover:bg-accent hover:text-accent-foreground h-10 py-2 px-1" href="/feed.xml"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-rss stroke-[#3c70c6]"><path d="M4 11a9 9 0 0 1 9 9"></path><path d="M4 4a16 16 0 0 1 16 16"></path><circle cx="5" cy="19" r="1"></circle></svg></a><a class="inline-flex items-center justify-center whitespace-nowrap rounded-md text-sm font-medium ring-offset-background transition-colors focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:pointer-events-none disabled:opacity-50 dark:text-slate-400 hover:bg-accent hover:text-accent-foreground h-10 py-2 px-1" href="https://www.github.com/MountAye"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-github stroke-[#3c70c6]"><path d="M15 22v-4a4.8 4.8 0 0 0-1-3.5c3 0 6-2 6-5.5.08-1.25-.27-2.48-1-3.5.28-1.15.28-2.35 0-3.5 0 0-1 0-3 1.5-2.64-.5-5.36-.5-8 0C6 2 5 2 5 2c-.3 1.15-.3 2.35 0 3.5A5.403 5.403 0 0 0 4 9c0 3.5 3 5.5 6 5.5-.39.49-.68 1.05-.85 1.65-.17.6-.22 1.23-.15 1.85v4"></path><path d="M9 18c-4.51 2-5-2-7-2"></path></svg></a><a class="inline-flex items-center justify-center whitespace-nowrap rounded-md text-sm font-medium ring-offset-background transition-colors focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:pointer-events-none disabled:opacity-50 dark:text-slate-400 hover:bg-accent hover:text-accent-foreground h-10 py-2 px-1" href="https://www.twitter.com/MountAye"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-twitter stroke-[#3c70c6]"><path d="M22 4s-.7 2.1-2 3.4c1.6 10-9.4 17.3-18 11.6 2.2.1 4.4-.6 6-2C3 15.5.5 9.6 3 5c2.2 2.6 5.6 4.1 9 4-.9-4.2 4-6.6 7-3.8 1.1 0 3-1.2 3-1.2z"></path></svg></a><a class="inline-flex items-center justify-center whitespace-nowrap rounded-md text-sm font-medium ring-offset-background transition-colors focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:pointer-events-none disabled:opacity-50 dark:text-slate-400 hover:bg-accent hover:text-accent-foreground h-10 py-2 px-1" href="https://github.com/MountAye/comments/discussions/categories/blog"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-mail stroke-[#3c70c6]"><rect width="20" height="16" x="2" y="4" rx="2"></rect><path d="m22 7-8.97 5.7a1.94 1.94 0 0 1-2.06 0L2 7"></path></svg></a></div></footer></aside><div id="main" class="
                    block h-full w-screen pt-16
                    sm:w-[640px] sm:ml-auto sm:mr-0 
                    md:w-[calc(100vw-144px)]
         "><div class="rounded-lg bg-card text-card-foreground w-full md:m-auto lg:w-[800px] shadow-2xl relative top-5 border-0 dark:shadow-gray-700"><div class="rounded-lg bg-card text-card-foreground w-full h-full border-0 shadow-lg dark:shadow-gray-700" id="top" style="background-image:none;background-size:cover"><div class="p-6 pt-0 backdrop-blur-sm bg-gray-50/70 dark:bg-black/70"><div data-orientation="vertical"><div data-state="closed" data-orientation="vertical" class="border-b border-none"><h3 data-orientation="vertical" data-state="closed" class="flex"><button type="button" aria-controls="radix-:R4nkm:" aria-expanded="false" data-state="closed" data-orientation="vertical" id="radix-:Rnkm:" class="flex flex-1 items-center justify-between py-4 font-medium transition-all hover:underline [&amp;[data-state=open]&gt;svg]:rotate-180" data-radix-collection-item=""><div style="position:relative;width:100%;padding-bottom:25%" data-radix-aspect-ratio-wrapper=""><div class="h-full flex flex-col justify-end items-start" style="position:absolute;top:0;right:0;bottom:0;left:0"><p id="date">2024 年 4 月 15 日</p><h1 id="title" class="text-start text-lg sm:text-2xl md:text-4xl text-[#3c70c6] font-semibold font-serif dark:font-sans">.tex | MC→MCMC 蒙特卡洛模拟，基于马尔科夫链采样</h1><div id="tags" class="mt-1"><div class="inline-flex items-center rounded-full border px-2.5 py-0.5 font-semibold transition-colors focus:outline-none focus:ring-2 focus:ring-ring focus:ring-offset-2 border-transparent bg-primary hover:bg-primary/80 text-white dark:text-slate-400 text-sm mr-1 font-mono" style="background-color:#329894"><a alt="科学类说明文" href="/articles/mc-mcmc-markov-chain-monte-carlo-gibbs-sampling#collection-tex">tex</a></div><div class="inline-flex items-center rounded-full border px-2.5 py-0.5 font-semibold transition-colors focus:outline-none focus:ring-2 focus:ring-ring focus:ring-offset-2 border-transparent bg-primary hover:bg-primary/80 text-white dark:text-slate-400 text-sm mr-1 font-mono" style="background-color:#3c70c6"><a alt="物理" href="/articles/mc-mcmc-markov-chain-monte-carlo-gibbs-sampling#collection-phy">phy</a></div><div class="inline-flex items-center rounded-full border px-2.5 py-0.5 font-semibold transition-colors focus:outline-none focus:ring-2 focus:ring-ring focus:ring-offset-2 border-transparent bg-primary hover:bg-primary/80 text-white dark:text-slate-400 text-sm mr-1 font-mono" style="background-color:#de2214"><a alt="数学" href="/articles/mc-mcmc-markov-chain-monte-carlo-gibbs-sampling#collection-m">m</a></div></div></div></div><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-chevron-down h-4 w-4 shrink-0 transition-transform duration-200"><path d="m6 9 6 6 6-6"></path></svg></button></h3><div data-state="closed" id="radix-:R4nkm:" hidden="" role="region" aria-labelledby="radix-:Rnkm:" data-orientation="vertical" class="overflow-hidden text-sm transition-all data-[state=closed]:animate-accordion-up data-[state=open]:animate-accordion-down" style="--radix-accordion-content-height:var(--radix-collapsible-content-height);--radix-accordion-content-width:var(--radix-collapsible-content-width)"></div></div></div></div></div><div class="hidden xl:block absolute -left-48 top-0 h-full"><div class="rounded-lg bg-card text-card-foreground sticky top-20 w-48 border-0 shadow-xl"><div class="p-6 py-4 pl-4 pr-0 text-nowrap overflow-clip text-ellipsis font-serif dark:font-sans"><div data-orientation="vertical"><div data-state="open" data-orientation="vertical" class="border-b border-none"><h3 data-orientation="vertical" data-state="open" class="flex"><button type="button" aria-controls="radix-:R57km:" aria-expanded="true" data-state="open" data-orientation="vertical" id="radix-:R17km:" class="flex flex-1 items-center justify-between font-medium transition-all hover:underline [&amp;[data-state=open]&gt;svg]:rotate-180 py-0" data-radix-collection-item=""><a href="#">目录</a><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-chevron-down h-4 w-4 shrink-0 transition-transform duration-200"><path d="m6 9 6 6 6-6"></path></svg></button></h3><div data-state="open" id="radix-:R57km:" role="region" aria-labelledby="radix-:R17km:" data-orientation="vertical" class="overflow-hidden text-sm transition-all data-[state=closed]:animate-accordion-up data-[state=open]:animate-accordion-down" style="--radix-accordion-content-height:var(--radix-collapsible-content-height);--radix-accordion-content-width:var(--radix-collapsible-content-width)"><div class="pt-0 pl-4 pb-0"><ul><li><a href="#monte-carlo-%E6%A8%A1%E6%8B%9F">Monte Carlo 模拟</a></li><li><div data-orientation="vertical"><div data-state="closed" data-orientation="vertical" class="border-b border-none"><h3 data-orientation="vertical" data-state="closed" class="flex"><button type="button" aria-controls="radix-:Rdd7km:" aria-expanded="false" data-state="closed" data-orientation="vertical" id="radix-:R5d7km:" class="flex flex-1 items-center justify-between font-medium transition-all hover:underline [&amp;[data-state=open]&gt;svg]:rotate-180 py-0" data-radix-collection-item=""><a href="#markov-chain-monte-carlo">Markov Chain Monte Carlo</a><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-chevron-down h-4 w-4 shrink-0 transition-transform duration-200"><path d="m6 9 6 6 6-6"></path></svg></button></h3><div data-state="closed" id="radix-:Rdd7km:" hidden="" role="region" aria-labelledby="radix-:R5d7km:" data-orientation="vertical" class="overflow-hidden text-sm transition-all data-[state=closed]:animate-accordion-up data-[state=open]:animate-accordion-down" style="--radix-accordion-content-height:var(--radix-collapsible-content-height);--radix-accordion-content-width:var(--radix-collapsible-content-width)"></div></div></div></li></ul></div></div></div></div></div></div></div><div class="p-6 typeset_written___p4iY pt-10 pb-20"><p>Monte Carlo 蒙特卡洛模拟，简称 MC.</p>
<p>Markov Chain Monte Carlo 是用马尔科夫链采样的蒙特卡洛模拟，简称 MCMC.</p>
<h2 id="monte-carlo-模拟"><a href="#monte-carlo-模拟">Monte Carlo 模拟</a></h2>
<p>这个比较简单了，举个例子，要计算 π 的近似值，可以在一块正方形板子里画一个内接圆，然后以均匀的概率往正方形里一粒一粒地扔沙子，每扔一粒，就判断并且记录这里沙子在圆内还是圆外，然后把沙子吹掉，如此往复。圆的面积是 πr²，正方形的面积是 4r²，所以落在圆内的概率（圆内沙子的数量和总数的比值）乘 4，就是所求。</p>
<p><img src="/photos/2024-04-15-monte-carlo-pi.png" alt=""/></p>
<p>归纳一下：当问题的解用一个随机变量的概率分布、期望值、二阶矩……等等来表示的时候，就生成一个符合该概率分布的随机样本，用样本的统计量去近似原概率分布。</p>
<h2 id="markov-chain-monte-carlo"><a href="#markov-chain-monte-carlo">Markov Chain Monte Carlo</a></h2>
<p>但是前述例子有一个步骤，就是我们往板子上扔完沙子要把沙子吹掉，每粒沙子，每次扔沙子之间也应该看不出区别，这是为了保证取样之间<strong>相互独立且来自同一个概率分布</strong>。</p>
<p>但是很多取样过程无法满足这种条件，或者达成条件所需的成本很高。比如计算一个高斯积分 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mo>∫</mo><mrow><mo>−</mo><mi mathvariant="normal">∞</mi></mrow><mrow><mo>+</mo><mi mathvariant="normal">∞</mi></mrow></msubsup><msup><mi>e</mi><mrow><mo>−</mo><msup><mi>x</mi><mn>2</mn></msup></mrow></msup><mi>d</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">\int_{-\infty}^{+\infty}e^{-x^2}dx</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.4011em;vertical-align:-0.4142em"></span><span class="mop"><span class="mop op-symbol small-op" style="margin-right:0.19445em;position:relative;top:-0.0006em">∫</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9662em"><span style="top:-2.3442em;margin-left:-0.1945em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mtight">∞</span></span></span></span><span style="top:-3.2579em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">+</span><span class="mord mtight">∞</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.4142em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9869em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mtight"><span class="mord mathnormal mtight">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8913em"><span style="top:-2.931em;margin-right:0.0714em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span class="mord mathnormal">d</span><span class="mord mathnormal">x</span></span></span></span>，被积函数的取值范围涵盖整个实数集，想找一个在整个实数集上均匀分布的随机数发生器就比较难了。</p>
<p><img src="/photos/2024-04-15-monte-carlo-gaussian.png" alt=""/></p>
<p>但是学过物理的朋友应该知道，上面的被积函数是以狄拉克 δ(x) 函数为初值条件的一个扩散方程的解，在某一时刻的空间分布。（不想凑系数了，将就看吧）</p>
<p>而扩散方程又是随机游走 (random walk) 在连续近似下的极限。</p>
<p>所以我们直接模拟一堆粒子从原点出发作随机行走，向两个方向的概率相同，扩散系数以及积分里的常数对齐，统计粒子在整个过程中出现在不同 x 位置的频率，求和之后乘以步长就是积分结果。这个过程需要的随机数发生器容易获取得多，是一个以 0.5 为阈值的 [0,1) 的均匀分布，比如一个均匀硬币。</p>
<p>而随机行走过程中走完每一步的位置，都只取决于前一步的位置，而与更久远的历史无关——这样的过程叫做马尔可夫过程。用这种方法取样获得随机样本的蒙特卡洛模拟，就是 MCMC.</p>
<p>扩散方程和随机行走只是 MCMC 的一个很特殊很特殊的例子，而对于一般的 MCMC 模拟，有以下通用的 Markov Chain 采样的算法：</p>
<h3 id="metropolis-hastings-算法"><a href="#metropolis-hastings-算法">Metropolis-Hastings 算法</a></h3>
<p>已知一个随机变量 x, 和一个与目标概率分布 P(x) 成正比的函数 f(x)（不要求 f 归一化）</p>
<ol>
<li>初始化<!-- -->
<ol>
<li>选定初始采样点 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">x_0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></li>
<li>选定一个采样函数 proposal function，也就是在已知当前 x 的取值时，下一个 x’ 取值的概率分布 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>g</mi><mo stretchy="false">(</mo><mi>x</mi><mtext>’</mtext><mi mathvariant="normal">∣</mi><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">g(x’\vert x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.03588em">g</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mord">’∣</span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span></span>；其中对于 Metropolis 算法，这个采样函数是对称的：<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>g</mi><mo stretchy="false">(</mo><mi>x</mi><mtext>’</mtext><mi mathvariant="normal">∣</mi><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mi>g</mi><mo stretchy="false">(</mo><mi>x</mi><mi mathvariant="normal">∣</mi><mi>x</mi><mtext>’</mtext><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">g(x’\vert x)=g(x\vert x’)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.03588em">g</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mord">’∣</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.03588em">g</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mord">∣</span><span class="mord mathnormal">x</span><span class="mord">’</span><span class="mclose">)</span></span></span></span>. 常用以两者之差为宗量的高斯函数。</li>
</ol>
</li>
<li>在得出 t 时刻的 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">x_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span> 之后：<!-- -->
<ol>
<li>根据 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>g</mi><mo stretchy="false">(</mo><msup><mi>x</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mi mathvariant="normal">∣</mi><msub><mi>x</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">g(x&#x27;\vert x_t)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0019em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.03588em">g</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7519em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mord">∣</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span> 抽样得到一个 x’</li>
<li>计算 α = f(x’)/f(x) = P(x’)/P(x)</li>
<li>决定是否将 x’ 加入样本<!-- -->
<ol>
<li>如果 α ≥ 1, 直接加入</li>
<li>如果 α &lt; 1, 以 α 为概率加入</li>
</ol>
</li>
</ol>
</li>
</ol>
<p>这种方法不保证采样的早期样本也符合目标概率分布，所以一般会抛弃最先加入的若干样本。</p>
<h3 id="gibbs-采样"><a href="#gibbs-采样">Gibbs 采样</a></h3>
<p>只是一种思路，不算是完整的算法。</p>
<p>当被采样的随机变量是一个多维向量的情况，在不使用 Gibbs 采样的情况下，在迭代的某一步骤 t，每个分量都应该是前一步骤的函数：<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mrow><mi>i</mi><mo separator="true">,</mo><mi>t</mi></mrow></msub><mo>=</mo><mi>f</mi><mo stretchy="false">(</mo><mo stretchy="false">{</mo><msub><mi>x</mi><mrow><mi>j</mi><mo separator="true">,</mo><mtext> </mtext><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo stretchy="false">}</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">x_{i,t}=f(\{x_{j,\ t-1}\})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7167em;vertical-align:-0.2861em"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1.0361em;vertical-align:-0.2861em"></span><span class="mord mathnormal" style="margin-right:0.10764em">f</span><span class="mopen">({</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em">j</span><span class="mpunct mtight">,</span><span class="mspace mtight"><span class="mtight"> </span></span><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span><span class="mclose">})</span></span></span></span></p>
<p>而 Gibbs 采样就是说，不必让每个维度 i 都根据前一个步骤的分量来取值，可以把当前 t 已经取样出来的分量直接带入到本回合后面的维度：<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mrow><mi>i</mi><mo separator="true">,</mo><mi>t</mi></mrow></msub><mo>=</mo><mi>f</mi><mo stretchy="false">(</mo><mo stretchy="false">{</mo><msub><mi>x</mi><mrow><mi>j</mi><mo separator="true">,</mo><mtext> </mtext><mi>t</mi></mrow></msub><msub><mo stretchy="false">}</mo><mrow><mi>j</mi><mo>&lt;</mo><mi>i</mi></mrow></msub><mo>∪</mo><mo stretchy="false">{</mo><msub><mi>x</mi><mrow><mi>k</mi><mo separator="true">,</mo><mtext> </mtext><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><msub><mo stretchy="false">}</mo><mrow><mi>k</mi><mo>≥</mo><mi>i</mi></mrow></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">x_{i,t}=f(\{x_{j,\ t}\}_{j&lt;i}\cup\{x_{k,\ t-1}\}_{k\ge i})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7167em;vertical-align:-0.2861em"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1.0361em;vertical-align:-0.2861em"></span><span class="mord mathnormal" style="margin-right:0.10764em">f</span><span class="mopen">({</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em">j</span><span class="mpunct mtight">,</span><span class="mspace mtight"><span class="mtight"> </span></span><span class="mord mathnormal mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span><span class="mclose"><span class="mclose">}</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em">j</span><span class="mrel mtight">&lt;</span><span class="mord mathnormal mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">∪</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:1.0361em;vertical-align:-0.2861em"></span><span class="mopen">{</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em">k</span><span class="mpunct mtight">,</span><span class="mspace mtight"><span class="mtight"> </span></span><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span><span class="mclose"><span class="mclose">}</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em">k</span><span class="mrel mtight">≥</span><span class="mord mathnormal mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2452em"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></p></div><div class="absolute right-0 lg:-right-20 top-0 h-full"><div class="sticky top-[calc(100vh-60px)] lg:top-[calc(100vh-250px)] flex flex-row-reverse lg:flex-col"><a class="inline-flex items-center justify-center whitespace-nowrap text-sm font-medium ring-offset-background transition-colors focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:pointer-events-none disabled:opacity-50 dark:text-slate-400 text-primary-foreground hover:bg-primary/90 bg-[#3c70c6] h-fit w-fit px-2 py-2 m-2 rounded-full opacity-75 lg:opacity-100 hover:opacity-100" title="顶部" href="/articles/mc-mcmc-markov-chain-monte-carlo-gibbs-sampling#top"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-arrow-big-up stroke-white dark:stroke-slate-400"><path d="M9 18v-6H5l7-7 7 7h-4v6H9z"></path></svg></a><a class="inline-flex items-center justify-center whitespace-nowrap text-sm font-medium ring-offset-background transition-colors focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:pointer-events-none disabled:opacity-50 dark:text-slate-400 text-primary-foreground hover:bg-primary/90 bg-[#3c70c6] h-fit w-fit px-2 py-2 m-2 rounded-full opacity-75 lg:opacity-100 hover:opacity-100" title="评论" href="/articles/mc-mcmc-markov-chain-monte-carlo-gibbs-sampling#comments"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-message-circle-more stroke-white dark:stroke-slate-400"><path d="M7.9 20A9 9 0 1 0 4 16.1L2 22Z"></path><path d="M8 12h.01"></path><path d="M12 12h.01"></path><path d="M16 12h.01"></path></svg></a><button class="inline-flex items-center justify-center whitespace-nowrap text-sm font-medium ring-offset-background transition-colors focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:pointer-events-none disabled:opacity-50 dark:text-slate-400 text-primary-foreground hover:bg-primary/90 bg-[#3c70c6] h-fit w-fit px-2 py-2 m-2 rounded-full opacity-75 lg:opacity-100 hover:opacity-100" title="分享"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-share2 stroke-white dark:stroke-slate-400"><circle cx="18" cy="5" r="3"></circle><circle cx="6" cy="12" r="3"></circle><circle cx="18" cy="19" r="3"></circle><line x1="8.59" x2="15.42" y1="13.51" y2="17.49"></line><line x1="15.41" x2="8.59" y1="6.51" y2="10.49"></line></svg></button><button class="inline-flex items-center justify-center whitespace-nowrap text-sm font-medium ring-offset-background transition-colors focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:pointer-events-none disabled:opacity-50 dark:text-slate-400 text-primary-foreground hover:bg-primary/90 bg-[#3c70c6] h-fit w-fit px-2 py-2 m-2 rounded-full opacity-75 lg:opacity-100 hover:opacity-100" title="恰饭"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-hand-heart stroke-white dark:stroke-slate-400"><path d="M11 14h2a2 2 0 1 0 0-4h-3c-.6 0-1.1.2-1.4.6L3 16"></path><path d="m7 20 1.6-1.4c.3-.4.8-.6 1.4-.6h4c1.1 0 2.1-.4 2.8-1.2l4.6-4.4a2 2 0 0 0-2.75-2.91l-4.2 3.9"></path><path d="m2 15 6 6"></path><path d="M19.5 8.5c.7-.7 1.5-1.6 1.5-2.7A2.73 2.73 0 0 0 16 4a2.78 2.78 0 0 0-5 1.8c0 1.2.8 2 1.5 2.8L16 12Z"></path></svg></button></div></div></div><div class="rounded-lg bg-card text-card-foreground w-full md:m-auto lg:w-[800px] relative top-14 shadow-2xl border-0 dark:shadow-gray-700" id="collections"><div class="flex flex-col space-y-1.5 p-6">本文收录于以下合集：</div><div class="p-6 pt-0"><div data-orientation="vertical"><div data-state="closed" data-orientation="vertical" class="border-b border-none"><h3 data-orientation="vertical" data-state="closed" class="flex"><button type="button" aria-controls="radix-:Rbbkm:" aria-expanded="false" data-state="closed" data-orientation="vertical" id="collection-tex" class="flex flex-1 items-center justify-between py-4 font-medium transition-all hover:underline [&amp;[data-state=open]&gt;svg]:rotate-180" data-radix-collection-item=""><p><span class="scroll-m-20 text-2xl font-semibold tracking-tight">.<!-- -->tex</span><span class="ml-4">科学类说明文</span></p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-chevron-down h-4 w-4 shrink-0 transition-transform duration-200"><path d="m6 9 6 6 6-6"></path></svg></button></h3><div data-state="closed" id="radix-:Rbbkm:" hidden="" role="region" aria-labelledby="radix-:R3bkm:" data-orientation="vertical" class="overflow-hidden text-sm transition-all data-[state=closed]:animate-accordion-up data-[state=open]:animate-accordion-down" style="--radix-accordion-content-height:var(--radix-collapsible-content-height);--radix-accordion-content-width:var(--radix-collapsible-content-width)"></div></div><div data-state="closed" data-orientation="vertical" class="border-b border-none"><h3 data-orientation="vertical" data-state="closed" class="flex"><button type="button" aria-controls="radix-:Rdbkm:" aria-expanded="false" data-state="closed" data-orientation="vertical" id="collection-phy" class="flex flex-1 items-center justify-between py-4 font-medium transition-all hover:underline [&amp;[data-state=open]&gt;svg]:rotate-180" data-radix-collection-item=""><p><span class="scroll-m-20 text-2xl font-semibold tracking-tight">.<!-- -->phy</span><span class="ml-4">物理</span></p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-chevron-down h-4 w-4 shrink-0 transition-transform duration-200"><path d="m6 9 6 6 6-6"></path></svg></button></h3><div data-state="closed" id="radix-:Rdbkm:" hidden="" role="region" aria-labelledby="radix-:R5bkm:" data-orientation="vertical" class="overflow-hidden text-sm transition-all data-[state=closed]:animate-accordion-up data-[state=open]:animate-accordion-down" style="--radix-accordion-content-height:var(--radix-collapsible-content-height);--radix-accordion-content-width:var(--radix-collapsible-content-width)"></div></div><div data-state="closed" data-orientation="vertical" class="border-b border-none"><h3 data-orientation="vertical" data-state="closed" class="flex"><button type="button" aria-controls="radix-:Rfbkm:" aria-expanded="false" data-state="closed" data-orientation="vertical" id="collection-m" class="flex flex-1 items-center justify-between py-4 font-medium transition-all hover:underline [&amp;[data-state=open]&gt;svg]:rotate-180" data-radix-collection-item=""><p><span class="scroll-m-20 text-2xl font-semibold tracking-tight">.<!-- -->m</span><span class="ml-4">数学</span></p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-chevron-down h-4 w-4 shrink-0 transition-transform duration-200"><path d="m6 9 6 6 6-6"></path></svg></button></h3><div data-state="closed" id="radix-:Rfbkm:" hidden="" role="region" aria-labelledby="radix-:R7bkm:" data-orientation="vertical" class="overflow-hidden text-sm transition-all data-[state=closed]:animate-accordion-up data-[state=open]:animate-accordion-down" style="--radix-accordion-content-height:var(--radix-collapsible-content-height);--radix-accordion-content-width:var(--radix-collapsible-content-width)"></div></div></div></div></div><div class="rounded-lg bg-card text-card-foreground w-full md:m-auto lg:w-[800px] relative top-20 shadow-2xl border-0 dark:shadow-gray-700" id="comments"><div class="p-6 pt-0"><div id="giscus" class="pt-16 utils_giscus__lppxx"></div></div></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"metadata":{"slug":"mc-mcmc-markov-chain-monte-carlo-gibbs-sampling","filename":"2024-04-15-mc-mcmc-markov-chain-monte-carlo-gibbs-sampling.md","date":"2024-04-15","title":".tex | MC→MCMC 蒙特卡洛模拟，基于马尔科夫链采样","layout":"post","keywords":["tex","phy","m"],"excerpt":"蒙特卡洛模拟、马尔科夫链采样、Metropolis-Hastings 算法、吉布斯采样","hasMath":true,"content":"\nMonte Carlo 蒙特卡洛模拟，简称 MC. \n\nMarkov Chain Monte Carlo 是用马尔科夫链采样的蒙特卡洛模拟，简称 MCMC.\n\n## Monte Carlo 模拟\n\n这个比较简单了，举个例子，要计算 π 的近似值，可以在一块正方形板子里画一个内接圆，然后以均匀的概率往正方形里一粒一粒地扔沙子，每扔一粒，就判断并且记录这里沙子在圆内还是圆外，然后把沙子吹掉，如此往复。圆的面积是 πr²，正方形的面积是 4r²，所以落在圆内的概率（圆内沙子的数量和总数的比值）乘 4，就是所求。\n\n![](/photos/2024-04-15-monte-carlo-pi.png)\n\n归纳一下：当问题的解用一个随机变量的概率分布、期望值、二阶矩……等等来表示的时候，就生成一个符合该概率分布的随机样本，用样本的统计量去近似原概率分布。\n\n## Markov Chain Monte Carlo\n\n但是前述例子有一个步骤，就是我们往板子上扔完沙子要把沙子吹掉，每粒沙子，每次扔沙子之间也应该看不出区别，这是为了保证取样之间**相互独立且来自同一个概率分布**。\n\n但是很多取样过程无法满足这种条件，或者达成条件所需的成本很高。比如计算一个高斯积分 $$\\int_{-\\infty}^{+\\infty}e^{-x^2}dx$$，被积函数的取值范围涵盖整个实数集，想找一个在整个实数集上均匀分布的随机数发生器就比较难了。\n\n![](/photos/2024-04-15-monte-carlo-gaussian.png)\n\n但是学过物理的朋友应该知道，上面的被积函数是以狄拉克 δ(x) 函数为初值条件的一个扩散方程的解，在某一时刻的空间分布。（不想凑系数了，将就看吧）\n\n而扩散方程又是随机游走 (random walk) 在连续近似下的极限。\n\n所以我们直接模拟一堆粒子从原点出发作随机行走，向两个方向的概率相同，扩散系数以及积分里的常数对齐，统计粒子在整个过程中出现在不同 x 位置的频率，求和之后乘以步长就是积分结果。这个过程需要的随机数发生器容易获取得多，是一个以 0.5 为阈值的 [0,1) 的均匀分布，比如一个均匀硬币。\n\n而随机行走过程中走完每一步的位置，都只取决于前一步的位置，而与更久远的历史无关——这样的过程叫做马尔可夫过程。用这种方法取样获得随机样本的蒙特卡洛模拟，就是 MCMC.\n\n扩散方程和随机行走只是 MCMC 的一个很特殊很特殊的例子，而对于一般的 MCMC 模拟，有以下通用的 Markov Chain 采样的算法：\n\n### Metropolis-Hastings 算法\n\n已知一个随机变量 x, 和一个与目标概率分布 P(x) 成正比的函数 f(x)（不要求 f 归一化）\n\n1. 初始化\n    1. 选定初始采样点 $$x_0$$ \n    2. 选定一个采样函数 proposal function，也就是在已知当前 x 的取值时，下一个 x’ 取值的概率分布 $$g(x’\\vert x)$$；其中对于 Metropolis 算法，这个采样函数是对称的：$$g(x’\\vert x)=g(x\\vert x’)$$. 常用以两者之差为宗量的高斯函数。\n2. 在得出 t 时刻的 $$x_t$$ 之后：\n    1. 根据 $$g(x'\\vert x_t)$$ 抽样得到一个 x’\n    2. 计算 α = f(x’)/f(x) = P(x’)/P(x)\n    3. 决定是否将 x’ 加入样本\n        1. 如果 α ≥ 1, 直接加入\n        2. 如果 α \u003c 1, 以 α 为概率加入\n\n这种方法不保证采样的早期样本也符合目标概率分布，所以一般会抛弃最先加入的若干样本。\n\n### Gibbs 采样\n\n只是一种思路，不算是完整的算法。\n\n当被采样的随机变量是一个多维向量的情况，在不使用 Gibbs 采样的情况下，在迭代的某一步骤 t，每个分量都应该是前一步骤的函数：$$x_{i,t}=f(\\{x_{j,\\ t-1}\\})$$\n\n而 Gibbs 采样就是说，不必让每个维度 i 都根据前一个步骤的分量来取值，可以把当前 t 已经取样出来的分量直接带入到本回合后面的维度：$$x_{i,t}=f(\\{x_{j,\\ t}\\}_{j\u003ci}\\cup\\{x_{k,\\ t-1}\\}_{k\\ge i})$$"},"tocStr":"{\"index\":null,\"endIndex\":null,\"map\":{\"type\":\"list\",\"ordered\":false,\"spread\":true,\"children\":[{\"type\":\"listItem\",\"spread\":false,\"children\":[{\"type\":\"paragraph\",\"children\":[{\"type\":\"link\",\"title\":null,\"url\":\"#monte-carlo-模拟\",\"children\":[{\"type\":\"text\",\"value\":\"Monte Carlo 模拟\"}]}]}]},{\"type\":\"listItem\",\"spread\":true,\"children\":[{\"type\":\"paragraph\",\"children\":[{\"type\":\"link\",\"title\":null,\"url\":\"#markov-chain-monte-carlo\",\"children\":[{\"type\":\"text\",\"value\":\"Markov Chain Monte Carlo\"}]}]},{\"type\":\"list\",\"ordered\":false,\"spread\":false,\"children\":[{\"type\":\"listItem\",\"spread\":false,\"children\":[{\"type\":\"paragraph\",\"children\":[{\"type\":\"link\",\"title\":null,\"url\":\"#metropolis-hastings-算法\",\"children\":[{\"type\":\"text\",\"value\":\"Metropolis-Hastings 算法\"}]}]}]},{\"type\":\"listItem\",\"spread\":false,\"children\":[{\"type\":\"paragraph\",\"children\":[{\"type\":\"link\",\"title\":null,\"url\":\"#gibbs-采样\",\"children\":[{\"type\":\"text\",\"value\":\"Gibbs 采样\"}]}]}]}]}]}]}}","htmlAst":{"type":"root","children":[{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"Monte Carlo 蒙特卡洛模拟，简称 MC.","position":{"start":{"line":2,"column":1,"offset":1},"end":{"line":2,"column":26,"offset":26}}}],"position":{"start":{"line":2,"column":1,"offset":1},"end":{"line":2,"column":27,"offset":27}}},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"Markov Chain Monte Carlo 是用马尔科夫链采样的蒙特卡洛模拟，简称 MCMC.","position":{"start":{"line":4,"column":1,"offset":29},"end":{"line":4,"column":51,"offset":79}}}],"position":{"start":{"line":4,"column":1,"offset":29},"end":{"line":4,"column":51,"offset":79}}},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{"id":"monte-carlo-模拟"},"children":[{"type":"element","tagName":"a","properties":{"href":"#monte-carlo-模拟"},"children":[{"type":"text","value":"Monte Carlo 模拟","position":{"start":{"line":6,"column":4,"offset":84},"end":{"line":6,"column":18,"offset":98}}}]}],"position":{"start":{"line":6,"column":1,"offset":81},"end":{"line":6,"column":18,"offset":98}}},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"这个比较简单了，举个例子，要计算 π 的近似值，可以在一块正方形板子里画一个内接圆，然后以均匀的概率往正方形里一粒一粒地扔沙子，每扔一粒，就判断并且记录这里沙子在圆内还是圆外，然后把沙子吹掉，如此往复。圆的面积是 πr²，正方形的面积是 4r²，所以落在圆内的概率（圆内沙子的数量和总数的比值）乘 4，就是所求。","position":{"start":{"line":8,"column":1,"offset":100},"end":{"line":8,"column":157,"offset":256}}}],"position":{"start":{"line":8,"column":1,"offset":100},"end":{"line":8,"column":157,"offset":256}}},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"element","tagName":"img","properties":{"src":"/photos/2024-04-15-monte-carlo-pi.png","alt":""},"children":[],"position":{"start":{"line":10,"column":1,"offset":258},"end":{"line":10,"column":43,"offset":300}}}],"position":{"start":{"line":10,"column":1,"offset":258},"end":{"line":10,"column":43,"offset":300}}},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"归纳一下：当问题的解用一个随机变量的概率分布、期望值、二阶矩……等等来表示的时候，就生成一个符合该概率分布的随机样本，用样本的统计量去近似原概率分布。","position":{"start":{"line":12,"column":1,"offset":302},"end":{"line":12,"column":76,"offset":377}}}],"position":{"start":{"line":12,"column":1,"offset":302},"end":{"line":12,"column":76,"offset":377}}},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{"id":"markov-chain-monte-carlo"},"children":[{"type":"element","tagName":"a","properties":{"href":"#markov-chain-monte-carlo"},"children":[{"type":"text","value":"Markov Chain Monte Carlo","position":{"start":{"line":14,"column":4,"offset":382},"end":{"line":14,"column":28,"offset":406}}}]}],"position":{"start":{"line":14,"column":1,"offset":379},"end":{"line":14,"column":28,"offset":406}}},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"但是前述例子有一个步骤，就是我们往板子上扔完沙子要把沙子吹掉，每粒沙子，每次扔沙子之间也应该看不出区别，这是为了保证取样之间","position":{"start":{"line":16,"column":1,"offset":408},"end":{"line":16,"column":63,"offset":470}}},{"type":"element","tagName":"strong","properties":{},"children":[{"type":"text","value":"相互独立且来自同一个概率分布","position":{"start":{"line":16,"column":65,"offset":472},"end":{"line":16,"column":79,"offset":486}}}],"position":{"start":{"line":16,"column":63,"offset":470},"end":{"line":16,"column":81,"offset":488}}},{"type":"text","value":"。","position":{"start":{"line":16,"column":81,"offset":488},"end":{"line":16,"column":82,"offset":489}}}],"position":{"start":{"line":16,"column":1,"offset":408},"end":{"line":16,"column":82,"offset":489}}},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"但是很多取样过程无法满足这种条件，或者达成条件所需的成本很高。比如计算一个高斯积分 ","position":{"start":{"line":18,"column":1,"offset":491},"end":{"line":18,"column":43,"offset":533}}},{"type":"element","tagName":"span","properties":{"className":["katex"]},"children":[{"type":"element","tagName":"span","properties":{"className":["katex-mathml"]},"children":[{"type":"element","tagName":"math","properties":{"xmlns":"http://www.w3.org/1998/Math/MathML"},"children":[{"type":"element","tagName":"semantics","properties":{},"children":[{"type":"element","tagName":"mrow","properties":{},"children":[{"type":"element","tagName":"msubsup","properties":{},"children":[{"type":"element","tagName":"mo","properties":{},"children":[{"type":"text","value":"∫"}]},{"type":"element","tagName":"mrow","properties":{},"children":[{"type":"element","tagName":"mo","properties":{},"children":[{"type":"text","value":"−"}]},{"type":"element","tagName":"mi","properties":{"mathvariant":"normal"},"children":[{"type":"text","value":"∞"}]}]},{"type":"element","tagName":"mrow","properties":{},"children":[{"type":"element","tagName":"mo","properties":{},"children":[{"type":"text","value":"+"}]},{"type":"element","tagName":"mi","properties":{"mathvariant":"normal"},"children":[{"type":"text","value":"∞"}]}]}]},{"type":"element","tagName":"msup","properties":{},"children":[{"type":"element","tagName":"mi","properties":{},"children":[{"type":"text","value":"e"}]},{"type":"element","tagName":"mrow","properties":{},"children":[{"type":"element","tagName":"mo","properties":{},"children":[{"type":"text","value":"−"}]},{"type":"element","tagName":"msup","properties":{},"children":[{"type":"element","tagName":"mi","properties":{},"children":[{"type":"text","value":"x"}]},{"type":"element","tagName":"mn","properties":{},"children":[{"type":"text","value":"2"}]}]}]}]},{"type":"element","tagName":"mi","properties":{},"children":[{"type":"text","value":"d"}]},{"type":"element","tagName":"mi","properties":{},"children":[{"type":"text","value":"x"}]}]},{"type":"element","tagName":"annotation","properties":{"encoding":"application/x-tex"},"children":[{"type":"text","value":"\\int_{-\\infty}^{+\\infty}e^{-x^2}dx"}]}]}]}]},{"type":"element","tagName":"span","properties":{"className":["katex-html"],"ariaHidden":"true"},"children":[{"type":"element","tagName":"span","properties":{"className":["base"]},"children":[{"type":"element","tagName":"span","properties":{"className":["strut"],"style":"height:1.4011em;vertical-align:-0.4142em;"},"children":[]},{"type":"element","tagName":"span","properties":{"className":["mop"]},"children":[{"type":"element","tagName":"span","properties":{"className":["mop","op-symbol","small-op"],"style":"margin-right:0.19445em;position:relative;top:-0.0006em;"},"children":[{"type":"text","value":"∫"}]},{"type":"element","tagName":"span","properties":{"className":["msupsub"]},"children":[{"type":"element","tagName":"span","properties":{"className":["vlist-t","vlist-t2"]},"children":[{"type":"element","tagName":"span","properties":{"className":["vlist-r"]},"children":[{"type":"element","tagName":"span","properties":{"className":["vlist"],"style":"height:0.9662em;"},"children":[{"type":"element","tagName":"span","properties":{"style":"top:-2.3442em;margin-left:-0.1945em;margin-right:0.05em;"},"children":[{"type":"element","tagName":"span","properties":{"className":["pstrut"],"style":"height:2.7em;"},"children":[]},{"type":"element","tagName":"span","properties":{"className":["sizing","reset-size6","size3","mtight"]},"children":[{"type":"element","tagName":"span","properties":{"className":["mord","mtight"]},"children":[{"type":"element","tagName":"span","properties":{"className":["mord","mtight"]},"children":[{"type":"text","value":"−"}]},{"type":"element","tagName":"span","properties":{"className":["mord","mtight"]},"children":[{"type":"text","value":"∞"}]}]}]}]},{"type":"element","tagName":"span","properties":{"style":"top:-3.2579em;margin-right:0.05em;"},"children":[{"type":"element","tagName":"span","properties":{"className":["pstrut"],"style":"height:2.7em;"},"children":[]},{"type":"element","tagName":"span","properties":{"className":["sizing","reset-size6","size3","mtight"]},"children":[{"type":"element","tagName":"span","properties":{"className":["mord","mtight"]},"children":[{"type":"element","tagName":"span","properties":{"className":["mord","mtight"]},"children":[{"type":"text","value":"+"}]},{"type":"element","tagName":"span","properties":{"className":["mord","mtight"]},"children":[{"type":"text","value":"∞"}]}]}]}]}]},{"type":"element","tagName":"span","properties":{"className":["vlist-s"]},"children":[{"type":"text","value":"​"}]}]},{"type":"element","tagName":"span","properties":{"className":["vlist-r"]},"children":[{"type":"element","tagName":"span","properties":{"className":["vlist"],"style":"height:0.4142em;"},"children":[{"type":"element","tagName":"span","properties":{},"children":[]}]}]}]}]}]},{"type":"element","tagName":"span","properties":{"className":["mspace"],"style":"margin-right:0.1667em;"},"children":[]},{"type":"element","tagName":"span","properties":{"className":["mord"]},"children":[{"type":"element","tagName":"span","properties":{"className":["mord","mathnormal"]},"children":[{"type":"text","value":"e"}]},{"type":"element","tagName":"span","properties":{"className":["msupsub"]},"children":[{"type":"element","tagName":"span","properties":{"className":["vlist-t"]},"children":[{"type":"element","tagName":"span","properties":{"className":["vlist-r"]},"children":[{"type":"element","tagName":"span","properties":{"className":["vlist"],"style":"height:0.9869em;"},"children":[{"type":"element","tagName":"span","properties":{"style":"top:-3.063em;margin-right:0.05em;"},"children":[{"type":"element","tagName":"span","properties":{"className":["pstrut"],"style":"height:2.7em;"},"children":[]},{"type":"element","tagName":"span","properties":{"className":["sizing","reset-size6","size3","mtight"]},"children":[{"type":"element","tagName":"span","properties":{"className":["mord","mtight"]},"children":[{"type":"element","tagName":"span","properties":{"className":["mord","mtight"]},"children":[{"type":"text","value":"−"}]},{"type":"element","tagName":"span","properties":{"className":["mord","mtight"]},"children":[{"type":"element","tagName":"span","properties":{"className":["mord","mathnormal","mtight"]},"children":[{"type":"text","value":"x"}]},{"type":"element","tagName":"span","properties":{"className":["msupsub"]},"children":[{"type":"element","tagName":"span","properties":{"className":["vlist-t"]},"children":[{"type":"element","tagName":"span","properties":{"className":["vlist-r"]},"children":[{"type":"element","tagName":"span","properties":{"className":["vlist"],"style":"height:0.8913em;"},"children":[{"type":"element","tagName":"span","properties":{"style":"top:-2.931em;margin-right:0.0714em;"},"children":[{"type":"element","tagName":"span","properties":{"className":["pstrut"],"style":"height:2.5em;"},"children":[]},{"type":"element","tagName":"span","properties":{"className":["sizing","reset-size3","size1","mtight"]},"children":[{"type":"element","tagName":"span","properties":{"className":["mord","mtight"]},"children":[{"type":"text","value":"2"}]}]}]}]}]}]}]}]}]}]}]}]}]}]}]}]},{"type":"element","tagName":"span","properties":{"className":["mord","mathnormal"]},"children":[{"type":"text","value":"d"}]},{"type":"element","tagName":"span","properties":{"className":["mord","mathnormal"]},"children":[{"type":"text","value":"x"}]}]}]}]},{"type":"text","value":"，被积函数的取值范围涵盖整个实数集，想找一个在整个实数集上均匀分布的随机数发生器就比较难了。","position":{"start":{"line":18,"column":81,"offset":571},"end":{"line":18,"column":127,"offset":617}}}],"position":{"start":{"line":18,"column":1,"offset":491},"end":{"line":18,"column":127,"offset":617}}},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"element","tagName":"img","properties":{"src":"/photos/2024-04-15-monte-carlo-gaussian.png","alt":""},"children":[],"position":{"start":{"line":20,"column":1,"offset":619},"end":{"line":20,"column":49,"offset":667}}}],"position":{"start":{"line":20,"column":1,"offset":619},"end":{"line":20,"column":49,"offset":667}}},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"但是学过物理的朋友应该知道，上面的被积函数是以狄拉克 δ(x) 函数为初值条件的一个扩散方程的解，在某一时刻的空间分布。（不想凑系数了，将就看吧）","position":{"start":{"line":22,"column":1,"offset":669},"end":{"line":22,"column":74,"offset":742}}}],"position":{"start":{"line":22,"column":1,"offset":669},"end":{"line":22,"column":74,"offset":742}}},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"而扩散方程又是随机游走 (random walk) 在连续近似下的极限。","position":{"start":{"line":24,"column":1,"offset":744},"end":{"line":24,"column":37,"offset":780}}}],"position":{"start":{"line":24,"column":1,"offset":744},"end":{"line":24,"column":37,"offset":780}}},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"所以我们直接模拟一堆粒子从原点出发作随机行走，向两个方向的概率相同，扩散系数以及积分里的常数对齐，统计粒子在整个过程中出现在不同 x 位置的频率，求和之后乘以步长就是积分结果。这个过程需要的随机数发生器容易获取得多，是一个以 0.5 为阈值的 [0,1) 的均匀分布，比如一个均匀硬币。","position":{"start":{"line":26,"column":1,"offset":782},"end":{"line":26,"column":144,"offset":925}}}],"position":{"start":{"line":26,"column":1,"offset":782},"end":{"line":26,"column":144,"offset":925}}},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"而随机行走过程中走完每一步的位置，都只取决于前一步的位置，而与更久远的历史无关——这样的过程叫做马尔可夫过程。用这种方法取样获得随机样本的蒙特卡洛模拟，就是 MCMC.","position":{"start":{"line":28,"column":1,"offset":927},"end":{"line":28,"column":85,"offset":1011}}}],"position":{"start":{"line":28,"column":1,"offset":927},"end":{"line":28,"column":85,"offset":1011}}},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"扩散方程和随机行走只是 MCMC 的一个很特殊很特殊的例子，而对于一般的 MCMC 模拟，有以下通用的 Markov Chain 采样的算法：","position":{"start":{"line":30,"column":1,"offset":1013},"end":{"line":30,"column":72,"offset":1084}}}],"position":{"start":{"line":30,"column":1,"offset":1013},"end":{"line":30,"column":72,"offset":1084}}},{"type":"text","value":"\n"},{"type":"element","tagName":"h3","properties":{"id":"metropolis-hastings-算法"},"children":[{"type":"element","tagName":"a","properties":{"href":"#metropolis-hastings-算法"},"children":[{"type":"text","value":"Metropolis-Hastings 算法","position":{"start":{"line":32,"column":5,"offset":1090},"end":{"line":32,"column":27,"offset":1112}}}]}],"position":{"start":{"line":32,"column":1,"offset":1086},"end":{"line":32,"column":27,"offset":1112}}},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"已知一个随机变量 x, 和一个与目标概率分布 P(x) 成正比的函数 f(x)（不要求 f 归一化）","position":{"start":{"line":34,"column":1,"offset":1114},"end":{"line":34,"column":51,"offset":1164}}}],"position":{"start":{"line":34,"column":1,"offset":1114},"end":{"line":34,"column":51,"offset":1164}}},{"type":"text","value":"\n"},{"type":"element","tagName":"ol","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"初始化","position":{"start":{"line":36,"column":4,"offset":1169},"end":{"line":36,"column":7,"offset":1172}}},{"type":"text","value":"\n"},{"type":"element","tagName":"ol","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"选定初始采样点 ","position":{"start":{"line":37,"column":8,"offset":1180},"end":{"line":37,"column":16,"offset":1188}}},{"type":"element","tagName":"span","properties":{"className":["katex"]},"children":[{"type":"element","tagName":"span","properties":{"className":["katex-mathml"]},"children":[{"type":"element","tagName":"math","properties":{"xmlns":"http://www.w3.org/1998/Math/MathML"},"children":[{"type":"element","tagName":"semantics","properties":{},"children":[{"type":"element","tagName":"mrow","properties":{},"children":[{"type":"element","tagName":"msub","properties":{},"children":[{"type":"element","tagName":"mi","properties":{},"children":[{"type":"text","value":"x"}]},{"type":"element","tagName":"mn","properties":{},"children":[{"type":"text","value":"0"}]}]}]},{"type":"element","tagName":"annotation","properties":{"encoding":"application/x-tex"},"children":[{"type":"text","value":"x_0"}]}]}]}]},{"type":"element","tagName":"span","properties":{"className":["katex-html"],"ariaHidden":"true"},"children":[{"type":"element","tagName":"span","properties":{"className":["base"]},"children":[{"type":"element","tagName":"span","properties":{"className":["strut"],"style":"height:0.5806em;vertical-align:-0.15em;"},"children":[]},{"type":"element","tagName":"span","properties":{"className":["mord"]},"children":[{"type":"element","tagName":"span","properties":{"className":["mord","mathnormal"]},"children":[{"type":"text","value":"x"}]},{"type":"element","tagName":"span","properties":{"className":["msupsub"]},"children":[{"type":"element","tagName":"span","properties":{"className":["vlist-t","vlist-t2"]},"children":[{"type":"element","tagName":"span","properties":{"className":["vlist-r"]},"children":[{"type":"element","tagName":"span","properties":{"className":["vlist"],"style":"height:0.3011em;"},"children":[{"type":"element","tagName":"span","properties":{"style":"top:-2.55em;margin-left:0em;margin-right:0.05em;"},"children":[{"type":"element","tagName":"span","properties":{"className":["pstrut"],"style":"height:2.7em;"},"children":[]},{"type":"element","tagName":"span","properties":{"className":["sizing","reset-size6","size3","mtight"]},"children":[{"type":"element","tagName":"span","properties":{"className":["mord","mtight"]},"children":[{"type":"text","value":"0"}]}]}]}]},{"type":"element","tagName":"span","properties":{"className":["vlist-s"]},"children":[{"type":"text","value":"​"}]}]},{"type":"element","tagName":"span","properties":{"className":["vlist-r"]},"children":[{"type":"element","tagName":"span","properties":{"className":["vlist"],"style":"height:0.15em;"},"children":[{"type":"element","tagName":"span","properties":{},"children":[]}]}]}]}]}]}]}]}]}],"position":{"start":{"line":37,"column":5,"offset":1177},"end":{"line":37,"column":24,"offset":1196}}},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"选定一个采样函数 proposal function，也就是在已知当前 x 的取值时，下一个 x’ 取值的概率分布 ","position":{"start":{"line":38,"column":8,"offset":1204},"end":{"line":38,"column":66,"offset":1262}}},{"type":"element","tagName":"span","properties":{"className":["katex"]},"children":[{"type":"element","tagName":"span","properties":{"className":["katex-mathml"]},"children":[{"type":"element","tagName":"math","properties":{"xmlns":"http://www.w3.org/1998/Math/MathML"},"children":[{"type":"element","tagName":"semantics","properties":{},"children":[{"type":"element","tagName":"mrow","properties":{},"children":[{"type":"element","tagName":"mi","properties":{},"children":[{"type":"text","value":"g"}]},{"type":"element","tagName":"mo","properties":{"stretchy":"false"},"children":[{"type":"text","value":"("}]},{"type":"element","tagName":"mi","properties":{},"children":[{"type":"text","value":"x"}]},{"type":"element","tagName":"mtext","properties":{},"children":[{"type":"text","value":"’"}]},{"type":"element","tagName":"mi","properties":{"mathvariant":"normal"},"children":[{"type":"text","value":"∣"}]},{"type":"element","tagName":"mi","properties":{},"children":[{"type":"text","value":"x"}]},{"type":"element","tagName":"mo","properties":{"stretchy":"false"},"children":[{"type":"text","value":")"}]}]},{"type":"element","tagName":"annotation","properties":{"encoding":"application/x-tex"},"children":[{"type":"text","value":"g(x’\\vert x)"}]}]}]}]},{"type":"element","tagName":"span","properties":{"className":["katex-html"],"ariaHidden":"true"},"children":[{"type":"element","tagName":"span","properties":{"className":["base"]},"children":[{"type":"element","tagName":"span","properties":{"className":["strut"],"style":"height:1em;vertical-align:-0.25em;"},"children":[]},{"type":"element","tagName":"span","properties":{"className":["mord","mathnormal"],"style":"margin-right:0.03588em;"},"children":[{"type":"text","value":"g"}]},{"type":"element","tagName":"span","properties":{"className":["mopen"]},"children":[{"type":"text","value":"("}]},{"type":"element","tagName":"span","properties":{"className":["mord","mathnormal"]},"children":[{"type":"text","value":"x"}]},{"type":"element","tagName":"span","properties":{"className":["mord"]},"children":[{"type":"text","value":"’∣"}]},{"type":"element","tagName":"span","properties":{"className":["mord","mathnormal"]},"children":[{"type":"text","value":"x"}]},{"type":"element","tagName":"span","properties":{"className":["mclose"]},"children":[{"type":"text","value":")"}]}]}]}]},{"type":"text","value":"；其中对于 Metropolis 算法，这个采样函数是对称的：","position":{"start":{"line":38,"column":82,"offset":1278},"end":{"line":38,"column":113,"offset":1309}}},{"type":"element","tagName":"span","properties":{"className":["katex"]},"children":[{"type":"element","tagName":"span","properties":{"className":["katex-mathml"]},"children":[{"type":"element","tagName":"math","properties":{"xmlns":"http://www.w3.org/1998/Math/MathML"},"children":[{"type":"element","tagName":"semantics","properties":{},"children":[{"type":"element","tagName":"mrow","properties":{},"children":[{"type":"element","tagName":"mi","properties":{},"children":[{"type":"text","value":"g"}]},{"type":"element","tagName":"mo","properties":{"stretchy":"false"},"children":[{"type":"text","value":"("}]},{"type":"element","tagName":"mi","properties":{},"children":[{"type":"text","value":"x"}]},{"type":"element","tagName":"mtext","properties":{},"children":[{"type":"text","value":"’"}]},{"type":"element","tagName":"mi","properties":{"mathvariant":"normal"},"children":[{"type":"text","value":"∣"}]},{"type":"element","tagName":"mi","properties":{},"children":[{"type":"text","value":"x"}]},{"type":"element","tagName":"mo","properties":{"stretchy":"false"},"children":[{"type":"text","value":")"}]},{"type":"element","tagName":"mo","properties":{},"children":[{"type":"text","value":"="}]},{"type":"element","tagName":"mi","properties":{},"children":[{"type":"text","value":"g"}]},{"type":"element","tagName":"mo","properties":{"stretchy":"false"},"children":[{"type":"text","value":"("}]},{"type":"element","tagName":"mi","properties":{},"children":[{"type":"text","value":"x"}]},{"type":"element","tagName":"mi","properties":{"mathvariant":"normal"},"children":[{"type":"text","value":"∣"}]},{"type":"element","tagName":"mi","properties":{},"children":[{"type":"text","value":"x"}]},{"type":"element","tagName":"mtext","properties":{},"children":[{"type":"text","value":"’"}]},{"type":"element","tagName":"mo","properties":{"stretchy":"false"},"children":[{"type":"text","value":")"}]}]},{"type":"element","tagName":"annotation","properties":{"encoding":"application/x-tex"},"children":[{"type":"text","value":"g(x’\\vert x)=g(x\\vert x’)"}]}]}]}]},{"type":"element","tagName":"span","properties":{"className":["katex-html"],"ariaHidden":"true"},"children":[{"type":"element","tagName":"span","properties":{"className":["base"]},"children":[{"type":"element","tagName":"span","properties":{"className":["strut"],"style":"height:1em;vertical-align:-0.25em;"},"children":[]},{"type":"element","tagName":"span","properties":{"className":["mord","mathnormal"],"style":"margin-right:0.03588em;"},"children":[{"type":"text","value":"g"}]},{"type":"element","tagName":"span","properties":{"className":["mopen"]},"children":[{"type":"text","value":"("}]},{"type":"element","tagName":"span","properties":{"className":["mord","mathnormal"]},"children":[{"type":"text","value":"x"}]},{"type":"element","tagName":"span","properties":{"className":["mord"]},"children":[{"type":"text","value":"’∣"}]},{"type":"element","tagName":"span","properties":{"className":["mord","mathnormal"]},"children":[{"type":"text","value":"x"}]},{"type":"element","tagName":"span","properties":{"className":["mclose"]},"children":[{"type":"text","value":")"}]},{"type":"element","tagName":"span","properties":{"className":["mspace"],"style":"margin-right:0.2778em;"},"children":[]},{"type":"element","tagName":"span","properties":{"className":["mrel"]},"children":[{"type":"text","value":"="}]},{"type":"element","tagName":"span","properties":{"className":["mspace"],"style":"margin-right:0.2778em;"},"children":[]}]},{"type":"element","tagName":"span","properties":{"className":["base"]},"children":[{"type":"element","tagName":"span","properties":{"className":["strut"],"style":"height:1em;vertical-align:-0.25em;"},"children":[]},{"type":"element","tagName":"span","properties":{"className":["mord","mathnormal"],"style":"margin-right:0.03588em;"},"children":[{"type":"text","value":"g"}]},{"type":"element","tagName":"span","properties":{"className":["mopen"]},"children":[{"type":"text","value":"("}]},{"type":"element","tagName":"span","properties":{"className":["mord","mathnormal"]},"children":[{"type":"text","value":"x"}]},{"type":"element","tagName":"span","properties":{"className":["mord"]},"children":[{"type":"text","value":"∣"}]},{"type":"element","tagName":"span","properties":{"className":["mord","mathnormal"]},"children":[{"type":"text","value":"x"}]},{"type":"element","tagName":"span","properties":{"className":["mord"]},"children":[{"type":"text","value":"’"}]},{"type":"element","tagName":"span","properties":{"className":["mclose"]},"children":[{"type":"text","value":")"}]}]}]}]},{"type":"text","value":". 常用以两者之差为宗量的高斯函数。","position":{"start":{"line":38,"column":142,"offset":1338},"end":{"line":38,"column":160,"offset":1356}}}],"position":{"start":{"line":38,"column":5,"offset":1201},"end":{"line":38,"column":160,"offset":1356}}},{"type":"text","value":"\n"}],"position":{"start":{"line":37,"column":5,"offset":1177},"end":{"line":38,"column":160,"offset":1356}}},{"type":"text","value":"\n"}],"position":{"start":{"line":36,"column":1,"offset":1166},"end":{"line":38,"column":160,"offset":1356}}},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"在得出 t 时刻的 ","position":{"start":{"line":39,"column":4,"offset":1360},"end":{"line":39,"column":14,"offset":1370}}},{"type":"element","tagName":"span","properties":{"className":["katex"]},"children":[{"type":"element","tagName":"span","properties":{"className":["katex-mathml"]},"children":[{"type":"element","tagName":"math","properties":{"xmlns":"http://www.w3.org/1998/Math/MathML"},"children":[{"type":"element","tagName":"semantics","properties":{},"children":[{"type":"element","tagName":"mrow","properties":{},"children":[{"type":"element","tagName":"msub","properties":{},"children":[{"type":"element","tagName":"mi","properties":{},"children":[{"type":"text","value":"x"}]},{"type":"element","tagName":"mi","properties":{},"children":[{"type":"text","value":"t"}]}]}]},{"type":"element","tagName":"annotation","properties":{"encoding":"application/x-tex"},"children":[{"type":"text","value":"x_t"}]}]}]}]},{"type":"element","tagName":"span","properties":{"className":["katex-html"],"ariaHidden":"true"},"children":[{"type":"element","tagName":"span","properties":{"className":["base"]},"children":[{"type":"element","tagName":"span","properties":{"className":["strut"],"style":"height:0.5806em;vertical-align:-0.15em;"},"children":[]},{"type":"element","tagName":"span","properties":{"className":["mord"]},"children":[{"type":"element","tagName":"span","properties":{"className":["mord","mathnormal"]},"children":[{"type":"text","value":"x"}]},{"type":"element","tagName":"span","properties":{"className":["msupsub"]},"children":[{"type":"element","tagName":"span","properties":{"className":["vlist-t","vlist-t2"]},"children":[{"type":"element","tagName":"span","properties":{"className":["vlist-r"]},"children":[{"type":"element","tagName":"span","properties":{"className":["vlist"],"style":"height:0.2806em;"},"children":[{"type":"element","tagName":"span","properties":{"style":"top:-2.55em;margin-left:0em;margin-right:0.05em;"},"children":[{"type":"element","tagName":"span","properties":{"className":["pstrut"],"style":"height:2.7em;"},"children":[]},{"type":"element","tagName":"span","properties":{"className":["sizing","reset-size6","size3","mtight"]},"children":[{"type":"element","tagName":"span","properties":{"className":["mord","mathnormal","mtight"]},"children":[{"type":"text","value":"t"}]}]}]}]},{"type":"element","tagName":"span","properties":{"className":["vlist-s"]},"children":[{"type":"text","value":"​"}]}]},{"type":"element","tagName":"span","properties":{"className":["vlist-r"]},"children":[{"type":"element","tagName":"span","properties":{"className":["vlist"],"style":"height:0.15em;"},"children":[{"type":"element","tagName":"span","properties":{},"children":[]}]}]}]}]}]}]}]}]},{"type":"text","value":" 之后：","position":{"start":{"line":39,"column":21,"offset":1377},"end":{"line":39,"column":25,"offset":1381}}},{"type":"text","value":"\n"},{"type":"element","tagName":"ol","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"根据 ","position":{"start":{"line":40,"column":8,"offset":1389},"end":{"line":40,"column":11,"offset":1392}}},{"type":"element","tagName":"span","properties":{"className":["katex"]},"children":[{"type":"element","tagName":"span","properties":{"className":["katex-mathml"]},"children":[{"type":"element","tagName":"math","properties":{"xmlns":"http://www.w3.org/1998/Math/MathML"},"children":[{"type":"element","tagName":"semantics","properties":{},"children":[{"type":"element","tagName":"mrow","properties":{},"children":[{"type":"element","tagName":"mi","properties":{},"children":[{"type":"text","value":"g"}]},{"type":"element","tagName":"mo","properties":{"stretchy":"false"},"children":[{"type":"text","value":"("}]},{"type":"element","tagName":"msup","properties":{},"children":[{"type":"element","tagName":"mi","properties":{},"children":[{"type":"text","value":"x"}]},{"type":"element","tagName":"mo","properties":{"mathvariant":"normal","lspace":"0em","rspace":"0em"},"children":[{"type":"text","value":"′"}]}]},{"type":"element","tagName":"mi","properties":{"mathvariant":"normal"},"children":[{"type":"text","value":"∣"}]},{"type":"element","tagName":"msub","properties":{},"children":[{"type":"element","tagName":"mi","properties":{},"children":[{"type":"text","value":"x"}]},{"type":"element","tagName":"mi","properties":{},"children":[{"type":"text","value":"t"}]}]},{"type":"element","tagName":"mo","properties":{"stretchy":"false"},"children":[{"type":"text","value":")"}]}]},{"type":"element","tagName":"annotation","properties":{"encoding":"application/x-tex"},"children":[{"type":"text","value":"g(x'\\vert x_t)"}]}]}]}]},{"type":"element","tagName":"span","properties":{"className":["katex-html"],"ariaHidden":"true"},"children":[{"type":"element","tagName":"span","properties":{"className":["base"]},"children":[{"type":"element","tagName":"span","properties":{"className":["strut"],"style":"height:1.0019em;vertical-align:-0.25em;"},"children":[]},{"type":"element","tagName":"span","properties":{"className":["mord","mathnormal"],"style":"margin-right:0.03588em;"},"children":[{"type":"text","value":"g"}]},{"type":"element","tagName":"span","properties":{"className":["mopen"]},"children":[{"type":"text","value":"("}]},{"type":"element","tagName":"span","properties":{"className":["mord"]},"children":[{"type":"element","tagName":"span","properties":{"className":["mord","mathnormal"]},"children":[{"type":"text","value":"x"}]},{"type":"element","tagName":"span","properties":{"className":["msupsub"]},"children":[{"type":"element","tagName":"span","properties":{"className":["vlist-t"]},"children":[{"type":"element","tagName":"span","properties":{"className":["vlist-r"]},"children":[{"type":"element","tagName":"span","properties":{"className":["vlist"],"style":"height:0.7519em;"},"children":[{"type":"element","tagName":"span","properties":{"style":"top:-3.063em;margin-right:0.05em;"},"children":[{"type":"element","tagName":"span","properties":{"className":["pstrut"],"style":"height:2.7em;"},"children":[]},{"type":"element","tagName":"span","properties":{"className":["sizing","reset-size6","size3","mtight"]},"children":[{"type":"element","tagName":"span","properties":{"className":["mord","mtight"]},"children":[{"type":"element","tagName":"span","properties":{"className":["mord","mtight"]},"children":[{"type":"text","value":"′"}]}]}]}]}]}]}]}]}]},{"type":"element","tagName":"span","properties":{"className":["mord"]},"children":[{"type":"text","value":"∣"}]},{"type":"element","tagName":"span","properties":{"className":["mord"]},"children":[{"type":"element","tagName":"span","properties":{"className":["mord","mathnormal"]},"children":[{"type":"text","value":"x"}]},{"type":"element","tagName":"span","properties":{"className":["msupsub"]},"children":[{"type":"element","tagName":"span","properties":{"className":["vlist-t","vlist-t2"]},"children":[{"type":"element","tagName":"span","properties":{"className":["vlist-r"]},"children":[{"type":"element","tagName":"span","properties":{"className":["vlist"],"style":"height:0.2806em;"},"children":[{"type":"element","tagName":"span","properties":{"style":"top:-2.55em;margin-left:0em;margin-right:0.05em;"},"children":[{"type":"element","tagName":"span","properties":{"className":["pstrut"],"style":"height:2.7em;"},"children":[]},{"type":"element","tagName":"span","properties":{"className":["sizing","reset-size6","size3","mtight"]},"children":[{"type":"element","tagName":"span","properties":{"className":["mord","mathnormal","mtight"]},"children":[{"type":"text","value":"t"}]}]}]}]},{"type":"element","tagName":"span","properties":{"className":["vlist-s"]},"children":[{"type":"text","value":"​"}]}]},{"type":"element","tagName":"span","properties":{"className":["vlist-r"]},"children":[{"type":"element","tagName":"span","properties":{"className":["vlist"],"style":"height:0.15em;"},"children":[{"type":"element","tagName":"span","properties":{},"children":[]}]}]}]}]}]},{"type":"element","tagName":"span","properties":{"className":["mclose"]},"children":[{"type":"text","value":")"}]}]}]}]},{"type":"text","value":" 抽样得到一个 x’","position":{"start":{"line":40,"column":29,"offset":1410},"end":{"line":40,"column":39,"offset":1420}}}],"position":{"start":{"line":40,"column":5,"offset":1386},"end":{"line":40,"column":39,"offset":1420}}},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"计算 α = f(x’)/f(x) = P(x’)/P(x)","position":{"start":{"line":41,"column":8,"offset":1428},"end":{"line":41,"column":38,"offset":1458}}}],"position":{"start":{"line":41,"column":5,"offset":1425},"end":{"line":41,"column":38,"offset":1458}}},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"决定是否将 x’ 加入样本","position":{"start":{"line":42,"column":8,"offset":1466},"end":{"line":42,"column":21,"offset":1479}}},{"type":"text","value":"\n"},{"type":"element","tagName":"ol","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"如果 α ≥ 1, 直接加入","position":{"start":{"line":43,"column":12,"offset":1491},"end":{"line":43,"column":26,"offset":1505}}}],"position":{"start":{"line":43,"column":9,"offset":1488},"end":{"line":43,"column":26,"offset":1505}}},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"如果 α \u003c 1, 以 α 为概率加入","position":{"start":{"line":44,"column":12,"offset":1517},"end":{"line":44,"column":31,"offset":1536}}}],"position":{"start":{"line":44,"column":9,"offset":1514},"end":{"line":44,"column":31,"offset":1536}}},{"type":"text","value":"\n"}],"position":{"start":{"line":43,"column":9,"offset":1488},"end":{"line":44,"column":31,"offset":1536}}},{"type":"text","value":"\n"}],"position":{"start":{"line":42,"column":5,"offset":1463},"end":{"line":44,"column":31,"offset":1536}}},{"type":"text","value":"\n"}],"position":{"start":{"line":40,"column":5,"offset":1386},"end":{"line":44,"column":31,"offset":1536}}},{"type":"text","value":"\n"}],"position":{"start":{"line":39,"column":1,"offset":1357},"end":{"line":44,"column":31,"offset":1536}}},{"type":"text","value":"\n"}],"position":{"start":{"line":36,"column":1,"offset":1166},"end":{"line":44,"column":31,"offset":1536}}},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"这种方法不保证采样的早期样本也符合目标概率分布，所以一般会抛弃最先加入的若干样本。","position":{"start":{"line":46,"column":1,"offset":1538},"end":{"line":46,"column":42,"offset":1579}}}],"position":{"start":{"line":46,"column":1,"offset":1538},"end":{"line":46,"column":42,"offset":1579}}},{"type":"text","value":"\n"},{"type":"element","tagName":"h3","properties":{"id":"gibbs-采样"},"children":[{"type":"element","tagName":"a","properties":{"href":"#gibbs-采样"},"children":[{"type":"text","value":"Gibbs 采样","position":{"start":{"line":48,"column":5,"offset":1585},"end":{"line":48,"column":13,"offset":1593}}}]}],"position":{"start":{"line":48,"column":1,"offset":1581},"end":{"line":48,"column":13,"offset":1593}}},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"只是一种思路，不算是完整的算法。","position":{"start":{"line":50,"column":1,"offset":1595},"end":{"line":50,"column":17,"offset":1611}}}],"position":{"start":{"line":50,"column":1,"offset":1595},"end":{"line":50,"column":17,"offset":1611}}},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"当被采样的随机变量是一个多维向量的情况，在不使用 Gibbs 采样的情况下，在迭代的某一步骤 t，每个分量都应该是前一步骤的函数：","position":{"start":{"line":52,"column":1,"offset":1613},"end":{"line":52,"column":66,"offset":1678}}},{"type":"element","tagName":"span","properties":{"className":["katex"]},"children":[{"type":"element","tagName":"span","properties":{"className":["katex-mathml"]},"children":[{"type":"element","tagName":"math","properties":{"xmlns":"http://www.w3.org/1998/Math/MathML"},"children":[{"type":"element","tagName":"semantics","properties":{},"children":[{"type":"element","tagName":"mrow","properties":{},"children":[{"type":"element","tagName":"msub","properties":{},"children":[{"type":"element","tagName":"mi","properties":{},"children":[{"type":"text","value":"x"}]},{"type":"element","tagName":"mrow","properties":{},"children":[{"type":"element","tagName":"mi","properties":{},"children":[{"type":"text","value":"i"}]},{"type":"element","tagName":"mo","properties":{"separator":"true"},"children":[{"type":"text","value":","}]},{"type":"element","tagName":"mi","properties":{},"children":[{"type":"text","value":"t"}]}]}]},{"type":"element","tagName":"mo","properties":{},"children":[{"type":"text","value":"="}]},{"type":"element","tagName":"mi","properties":{},"children":[{"type":"text","value":"f"}]},{"type":"element","tagName":"mo","properties":{"stretchy":"false"},"children":[{"type":"text","value":"("}]},{"type":"element","tagName":"mo","properties":{"stretchy":"false"},"children":[{"type":"text","value":"{"}]},{"type":"element","tagName":"msub","properties":{},"children":[{"type":"element","tagName":"mi","properties":{},"children":[{"type":"text","value":"x"}]},{"type":"element","tagName":"mrow","properties":{},"children":[{"type":"element","tagName":"mi","properties":{},"children":[{"type":"text","value":"j"}]},{"type":"element","tagName":"mo","properties":{"separator":"true"},"children":[{"type":"text","value":","}]},{"type":"element","tagName":"mtext","properties":{},"children":[{"type":"text","value":" "}]},{"type":"element","tagName":"mi","properties":{},"children":[{"type":"text","value":"t"}]},{"type":"element","tagName":"mo","properties":{},"children":[{"type":"text","value":"−"}]},{"type":"element","tagName":"mn","properties":{},"children":[{"type":"text","value":"1"}]}]}]},{"type":"element","tagName":"mo","properties":{"stretchy":"false"},"children":[{"type":"text","value":"}"}]},{"type":"element","tagName":"mo","properties":{"stretchy":"false"},"children":[{"type":"text","value":")"}]}]},{"type":"element","tagName":"annotation","properties":{"encoding":"application/x-tex"},"children":[{"type":"text","value":"x_{i,t}=f(\\{x_{j,\\ t-1}\\})"}]}]}]}]},{"type":"element","tagName":"span","properties":{"className":["katex-html"],"ariaHidden":"true"},"children":[{"type":"element","tagName":"span","properties":{"className":["base"]},"children":[{"type":"element","tagName":"span","properties":{"className":["strut"],"style":"height:0.7167em;vertical-align:-0.2861em;"},"children":[]},{"type":"element","tagName":"span","properties":{"className":["mord"]},"children":[{"type":"element","tagName":"span","properties":{"className":["mord","mathnormal"]},"children":[{"type":"text","value":"x"}]},{"type":"element","tagName":"span","properties":{"className":["msupsub"]},"children":[{"type":"element","tagName":"span","properties":{"className":["vlist-t","vlist-t2"]},"children":[{"type":"element","tagName":"span","properties":{"className":["vlist-r"]},"children":[{"type":"element","tagName":"span","properties":{"className":["vlist"],"style":"height:0.3117em;"},"children":[{"type":"element","tagName":"span","properties":{"style":"top:-2.55em;margin-left:0em;margin-right:0.05em;"},"children":[{"type":"element","tagName":"span","properties":{"className":["pstrut"],"style":"height:2.7em;"},"children":[]},{"type":"element","tagName":"span","properties":{"className":["sizing","reset-size6","size3","mtight"]},"children":[{"type":"element","tagName":"span","properties":{"className":["mord","mtight"]},"children":[{"type":"element","tagName":"span","properties":{"className":["mord","mathnormal","mtight"]},"children":[{"type":"text","value":"i"}]},{"type":"element","tagName":"span","properties":{"className":["mpunct","mtight"]},"children":[{"type":"text","value":","}]},{"type":"element","tagName":"span","properties":{"className":["mord","mathnormal","mtight"]},"children":[{"type":"text","value":"t"}]}]}]}]}]},{"type":"element","tagName":"span","properties":{"className":["vlist-s"]},"children":[{"type":"text","value":"​"}]}]},{"type":"element","tagName":"span","properties":{"className":["vlist-r"]},"children":[{"type":"element","tagName":"span","properties":{"className":["vlist"],"style":"height:0.2861em;"},"children":[{"type":"element","tagName":"span","properties":{},"children":[]}]}]}]}]}]},{"type":"element","tagName":"span","properties":{"className":["mspace"],"style":"margin-right:0.2778em;"},"children":[]},{"type":"element","tagName":"span","properties":{"className":["mrel"]},"children":[{"type":"text","value":"="}]},{"type":"element","tagName":"span","properties":{"className":["mspace"],"style":"margin-right:0.2778em;"},"children":[]}]},{"type":"element","tagName":"span","properties":{"className":["base"]},"children":[{"type":"element","tagName":"span","properties":{"className":["strut"],"style":"height:1.0361em;vertical-align:-0.2861em;"},"children":[]},{"type":"element","tagName":"span","properties":{"className":["mord","mathnormal"],"style":"margin-right:0.10764em;"},"children":[{"type":"text","value":"f"}]},{"type":"element","tagName":"span","properties":{"className":["mopen"]},"children":[{"type":"text","value":"({"}]},{"type":"element","tagName":"span","properties":{"className":["mord"]},"children":[{"type":"element","tagName":"span","properties":{"className":["mord","mathnormal"]},"children":[{"type":"text","value":"x"}]},{"type":"element","tagName":"span","properties":{"className":["msupsub"]},"children":[{"type":"element","tagName":"span","properties":{"className":["vlist-t","vlist-t2"]},"children":[{"type":"element","tagName":"span","properties":{"className":["vlist-r"]},"children":[{"type":"element","tagName":"span","properties":{"className":["vlist"],"style":"height:0.3117em;"},"children":[{"type":"element","tagName":"span","properties":{"style":"top:-2.55em;margin-left:0em;margin-right:0.05em;"},"children":[{"type":"element","tagName":"span","properties":{"className":["pstrut"],"style":"height:2.7em;"},"children":[]},{"type":"element","tagName":"span","properties":{"className":["sizing","reset-size6","size3","mtight"]},"children":[{"type":"element","tagName":"span","properties":{"className":["mord","mtight"]},"children":[{"type":"element","tagName":"span","properties":{"className":["mord","mathnormal","mtight"],"style":"margin-right:0.05724em;"},"children":[{"type":"text","value":"j"}]},{"type":"element","tagName":"span","properties":{"className":["mpunct","mtight"]},"children":[{"type":"text","value":","}]},{"type":"element","tagName":"span","properties":{"className":["mspace","mtight"]},"children":[{"type":"element","tagName":"span","properties":{"className":["mtight"]},"children":[{"type":"text","value":" "}]}]},{"type":"element","tagName":"span","properties":{"className":["mord","mathnormal","mtight"]},"children":[{"type":"text","value":"t"}]},{"type":"element","tagName":"span","properties":{"className":["mbin","mtight"]},"children":[{"type":"text","value":"−"}]},{"type":"element","tagName":"span","properties":{"className":["mord","mtight"]},"children":[{"type":"text","value":"1"}]}]}]}]}]},{"type":"element","tagName":"span","properties":{"className":["vlist-s"]},"children":[{"type":"text","value":"​"}]}]},{"type":"element","tagName":"span","properties":{"className":["vlist-r"]},"children":[{"type":"element","tagName":"span","properties":{"className":["vlist"],"style":"height:0.2861em;"},"children":[{"type":"element","tagName":"span","properties":{},"children":[]}]}]}]}]}]},{"type":"element","tagName":"span","properties":{"className":["mclose"]},"children":[{"type":"text","value":"})"}]}]}]}]}],"position":{"start":{"line":52,"column":1,"offset":1613},"end":{"line":52,"column":96,"offset":1708}}},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"而 Gibbs 采样就是说，不必让每个维度 i 都根据前一个步骤的分量来取值，可以把当前 t 已经取样出来的分量直接带入到本回合后面的维度：","position":{"start":{"line":54,"column":1,"offset":1710},"end":{"line":54,"column":71,"offset":1780}}},{"type":"element","tagName":"span","properties":{"className":["katex"]},"children":[{"type":"element","tagName":"span","properties":{"className":["katex-mathml"]},"children":[{"type":"element","tagName":"math","properties":{"xmlns":"http://www.w3.org/1998/Math/MathML"},"children":[{"type":"element","tagName":"semantics","properties":{},"children":[{"type":"element","tagName":"mrow","properties":{},"children":[{"type":"element","tagName":"msub","properties":{},"children":[{"type":"element","tagName":"mi","properties":{},"children":[{"type":"text","value":"x"}]},{"type":"element","tagName":"mrow","properties":{},"children":[{"type":"element","tagName":"mi","properties":{},"children":[{"type":"text","value":"i"}]},{"type":"element","tagName":"mo","properties":{"separator":"true"},"children":[{"type":"text","value":","}]},{"type":"element","tagName":"mi","properties":{},"children":[{"type":"text","value":"t"}]}]}]},{"type":"element","tagName":"mo","properties":{},"children":[{"type":"text","value":"="}]},{"type":"element","tagName":"mi","properties":{},"children":[{"type":"text","value":"f"}]},{"type":"element","tagName":"mo","properties":{"stretchy":"false"},"children":[{"type":"text","value":"("}]},{"type":"element","tagName":"mo","properties":{"stretchy":"false"},"children":[{"type":"text","value":"{"}]},{"type":"element","tagName":"msub","properties":{},"children":[{"type":"element","tagName":"mi","properties":{},"children":[{"type":"text","value":"x"}]},{"type":"element","tagName":"mrow","properties":{},"children":[{"type":"element","tagName":"mi","properties":{},"children":[{"type":"text","value":"j"}]},{"type":"element","tagName":"mo","properties":{"separator":"true"},"children":[{"type":"text","value":","}]},{"type":"element","tagName":"mtext","properties":{},"children":[{"type":"text","value":" "}]},{"type":"element","tagName":"mi","properties":{},"children":[{"type":"text","value":"t"}]}]}]},{"type":"element","tagName":"msub","properties":{},"children":[{"type":"element","tagName":"mo","properties":{"stretchy":"false"},"children":[{"type":"text","value":"}"}]},{"type":"element","tagName":"mrow","properties":{},"children":[{"type":"element","tagName":"mi","properties":{},"children":[{"type":"text","value":"j"}]},{"type":"element","tagName":"mo","properties":{},"children":[{"type":"text","value":"\u003c"}]},{"type":"element","tagName":"mi","properties":{},"children":[{"type":"text","value":"i"}]}]}]},{"type":"element","tagName":"mo","properties":{},"children":[{"type":"text","value":"∪"}]},{"type":"element","tagName":"mo","properties":{"stretchy":"false"},"children":[{"type":"text","value":"{"}]},{"type":"element","tagName":"msub","properties":{},"children":[{"type":"element","tagName":"mi","properties":{},"children":[{"type":"text","value":"x"}]},{"type":"element","tagName":"mrow","properties":{},"children":[{"type":"element","tagName":"mi","properties":{},"children":[{"type":"text","value":"k"}]},{"type":"element","tagName":"mo","properties":{"separator":"true"},"children":[{"type":"text","value":","}]},{"type":"element","tagName":"mtext","properties":{},"children":[{"type":"text","value":" "}]},{"type":"element","tagName":"mi","properties":{},"children":[{"type":"text","value":"t"}]},{"type":"element","tagName":"mo","properties":{},"children":[{"type":"text","value":"−"}]},{"type":"element","tagName":"mn","properties":{},"children":[{"type":"text","value":"1"}]}]}]},{"type":"element","tagName":"msub","properties":{},"children":[{"type":"element","tagName":"mo","properties":{"stretchy":"false"},"children":[{"type":"text","value":"}"}]},{"type":"element","tagName":"mrow","properties":{},"children":[{"type":"element","tagName":"mi","properties":{},"children":[{"type":"text","value":"k"}]},{"type":"element","tagName":"mo","properties":{},"children":[{"type":"text","value":"≥"}]},{"type":"element","tagName":"mi","properties":{},"children":[{"type":"text","value":"i"}]}]}]},{"type":"element","tagName":"mo","properties":{"stretchy":"false"},"children":[{"type":"text","value":")"}]}]},{"type":"element","tagName":"annotation","properties":{"encoding":"application/x-tex"},"children":[{"type":"text","value":"x_{i,t}=f(\\{x_{j,\\ t}\\}_{j\u003ci}\\cup\\{x_{k,\\ t-1}\\}_{k\\ge i})"}]}]}]}]},{"type":"element","tagName":"span","properties":{"className":["katex-html"],"ariaHidden":"true"},"children":[{"type":"element","tagName":"span","properties":{"className":["base"]},"children":[{"type":"element","tagName":"span","properties":{"className":["strut"],"style":"height:0.7167em;vertical-align:-0.2861em;"},"children":[]},{"type":"element","tagName":"span","properties":{"className":["mord"]},"children":[{"type":"element","tagName":"span","properties":{"className":["mord","mathnormal"]},"children":[{"type":"text","value":"x"}]},{"type":"element","tagName":"span","properties":{"className":["msupsub"]},"children":[{"type":"element","tagName":"span","properties":{"className":["vlist-t","vlist-t2"]},"children":[{"type":"element","tagName":"span","properties":{"className":["vlist-r"]},"children":[{"type":"element","tagName":"span","properties":{"className":["vlist"],"style":"height:0.3117em;"},"children":[{"type":"element","tagName":"span","properties":{"style":"top:-2.55em;margin-left:0em;margin-right:0.05em;"},"children":[{"type":"element","tagName":"span","properties":{"className":["pstrut"],"style":"height:2.7em;"},"children":[]},{"type":"element","tagName":"span","properties":{"className":["sizing","reset-size6","size3","mtight"]},"children":[{"type":"element","tagName":"span","properties":{"className":["mord","mtight"]},"children":[{"type":"element","tagName":"span","properties":{"className":["mord","mathnormal","mtight"]},"children":[{"type":"text","value":"i"}]},{"type":"element","tagName":"span","properties":{"className":["mpunct","mtight"]},"children":[{"type":"text","value":","}]},{"type":"element","tagName":"span","properties":{"className":["mord","mathnormal","mtight"]},"children":[{"type":"text","value":"t"}]}]}]}]}]},{"type":"element","tagName":"span","properties":{"className":["vlist-s"]},"children":[{"type":"text","value":"​"}]}]},{"type":"element","tagName":"span","properties":{"className":["vlist-r"]},"children":[{"type":"element","tagName":"span","properties":{"className":["vlist"],"style":"height:0.2861em;"},"children":[{"type":"element","tagName":"span","properties":{},"children":[]}]}]}]}]}]},{"type":"element","tagName":"span","properties":{"className":["mspace"],"style":"margin-right:0.2778em;"},"children":[]},{"type":"element","tagName":"span","properties":{"className":["mrel"]},"children":[{"type":"text","value":"="}]},{"type":"element","tagName":"span","properties":{"className":["mspace"],"style":"margin-right:0.2778em;"},"children":[]}]},{"type":"element","tagName":"span","properties":{"className":["base"]},"children":[{"type":"element","tagName":"span","properties":{"className":["strut"],"style":"height:1.0361em;vertical-align:-0.2861em;"},"children":[]},{"type":"element","tagName":"span","properties":{"className":["mord","mathnormal"],"style":"margin-right:0.10764em;"},"children":[{"type":"text","value":"f"}]},{"type":"element","tagName":"span","properties":{"className":["mopen"]},"children":[{"type":"text","value":"({"}]},{"type":"element","tagName":"span","properties":{"className":["mord"]},"children":[{"type":"element","tagName":"span","properties":{"className":["mord","mathnormal"]},"children":[{"type":"text","value":"x"}]},{"type":"element","tagName":"span","properties":{"className":["msupsub"]},"children":[{"type":"element","tagName":"span","properties":{"className":["vlist-t","vlist-t2"]},"children":[{"type":"element","tagName":"span","properties":{"className":["vlist-r"]},"children":[{"type":"element","tagName":"span","properties":{"className":["vlist"],"style":"height:0.3117em;"},"children":[{"type":"element","tagName":"span","properties":{"style":"top:-2.55em;margin-left:0em;margin-right:0.05em;"},"children":[{"type":"element","tagName":"span","properties":{"className":["pstrut"],"style":"height:2.7em;"},"children":[]},{"type":"element","tagName":"span","properties":{"className":["sizing","reset-size6","size3","mtight"]},"children":[{"type":"element","tagName":"span","properties":{"className":["mord","mtight"]},"children":[{"type":"element","tagName":"span","properties":{"className":["mord","mathnormal","mtight"],"style":"margin-right:0.05724em;"},"children":[{"type":"text","value":"j"}]},{"type":"element","tagName":"span","properties":{"className":["mpunct","mtight"]},"children":[{"type":"text","value":","}]},{"type":"element","tagName":"span","properties":{"className":["mspace","mtight"]},"children":[{"type":"element","tagName":"span","properties":{"className":["mtight"]},"children":[{"type":"text","value":" "}]}]},{"type":"element","tagName":"span","properties":{"className":["mord","mathnormal","mtight"]},"children":[{"type":"text","value":"t"}]}]}]}]}]},{"type":"element","tagName":"span","properties":{"className":["vlist-s"]},"children":[{"type":"text","value":"​"}]}]},{"type":"element","tagName":"span","properties":{"className":["vlist-r"]},"children":[{"type":"element","tagName":"span","properties":{"className":["vlist"],"style":"height:0.2861em;"},"children":[{"type":"element","tagName":"span","properties":{},"children":[]}]}]}]}]}]},{"type":"element","tagName":"span","properties":{"className":["mclose"]},"children":[{"type":"element","tagName":"span","properties":{"className":["mclose"]},"children":[{"type":"text","value":"}"}]},{"type":"element","tagName":"span","properties":{"className":["msupsub"]},"children":[{"type":"element","tagName":"span","properties":{"className":["vlist-t","vlist-t2"]},"children":[{"type":"element","tagName":"span","properties":{"className":["vlist-r"]},"children":[{"type":"element","tagName":"span","properties":{"className":["vlist"],"style":"height:0.3117em;"},"children":[{"type":"element","tagName":"span","properties":{"style":"top:-2.55em;margin-left:0em;margin-right:0.05em;"},"children":[{"type":"element","tagName":"span","properties":{"className":["pstrut"],"style":"height:2.7em;"},"children":[]},{"type":"element","tagName":"span","properties":{"className":["sizing","reset-size6","size3","mtight"]},"children":[{"type":"element","tagName":"span","properties":{"className":["mord","mtight"]},"children":[{"type":"element","tagName":"span","properties":{"className":["mord","mathnormal","mtight"],"style":"margin-right:0.05724em;"},"children":[{"type":"text","value":"j"}]},{"type":"element","tagName":"span","properties":{"className":["mrel","mtight"]},"children":[{"type":"text","value":"\u003c"}]},{"type":"element","tagName":"span","properties":{"className":["mord","mathnormal","mtight"]},"children":[{"type":"text","value":"i"}]}]}]}]}]},{"type":"element","tagName":"span","properties":{"className":["vlist-s"]},"children":[{"type":"text","value":"​"}]}]},{"type":"element","tagName":"span","properties":{"className":["vlist-r"]},"children":[{"type":"element","tagName":"span","properties":{"className":["vlist"],"style":"height:0.2861em;"},"children":[{"type":"element","tagName":"span","properties":{},"children":[]}]}]}]}]}]},{"type":"element","tagName":"span","properties":{"className":["mspace"],"style":"margin-right:0.2222em;"},"children":[]},{"type":"element","tagName":"span","properties":{"className":["mbin"]},"children":[{"type":"text","value":"∪"}]},{"type":"element","tagName":"span","properties":{"className":["mspace"],"style":"margin-right:0.2222em;"},"children":[]}]},{"type":"element","tagName":"span","properties":{"className":["base"]},"children":[{"type":"element","tagName":"span","properties":{"className":["strut"],"style":"height:1.0361em;vertical-align:-0.2861em;"},"children":[]},{"type":"element","tagName":"span","properties":{"className":["mopen"]},"children":[{"type":"text","value":"{"}]},{"type":"element","tagName":"span","properties":{"className":["mord"]},"children":[{"type":"element","tagName":"span","properties":{"className":["mord","mathnormal"]},"children":[{"type":"text","value":"x"}]},{"type":"element","tagName":"span","properties":{"className":["msupsub"]},"children":[{"type":"element","tagName":"span","properties":{"className":["vlist-t","vlist-t2"]},"children":[{"type":"element","tagName":"span","properties":{"className":["vlist-r"]},"children":[{"type":"element","tagName":"span","properties":{"className":["vlist"],"style":"height:0.3361em;"},"children":[{"type":"element","tagName":"span","properties":{"style":"top:-2.55em;margin-left:0em;margin-right:0.05em;"},"children":[{"type":"element","tagName":"span","properties":{"className":["pstrut"],"style":"height:2.7em;"},"children":[]},{"type":"element","tagName":"span","properties":{"className":["sizing","reset-size6","size3","mtight"]},"children":[{"type":"element","tagName":"span","properties":{"className":["mord","mtight"]},"children":[{"type":"element","tagName":"span","properties":{"className":["mord","mathnormal","mtight"],"style":"margin-right:0.03148em;"},"children":[{"type":"text","value":"k"}]},{"type":"element","tagName":"span","properties":{"className":["mpunct","mtight"]},"children":[{"type":"text","value":","}]},{"type":"element","tagName":"span","properties":{"className":["mspace","mtight"]},"children":[{"type":"element","tagName":"span","properties":{"className":["mtight"]},"children":[{"type":"text","value":" "}]}]},{"type":"element","tagName":"span","properties":{"className":["mord","mathnormal","mtight"]},"children":[{"type":"text","value":"t"}]},{"type":"element","tagName":"span","properties":{"className":["mbin","mtight"]},"children":[{"type":"text","value":"−"}]},{"type":"element","tagName":"span","properties":{"className":["mord","mtight"]},"children":[{"type":"text","value":"1"}]}]}]}]}]},{"type":"element","tagName":"span","properties":{"className":["vlist-s"]},"children":[{"type":"text","value":"​"}]}]},{"type":"element","tagName":"span","properties":{"className":["vlist-r"]},"children":[{"type":"element","tagName":"span","properties":{"className":["vlist"],"style":"height:0.2861em;"},"children":[{"type":"element","tagName":"span","properties":{},"children":[]}]}]}]}]}]},{"type":"element","tagName":"span","properties":{"className":["mclose"]},"children":[{"type":"element","tagName":"span","properties":{"className":["mclose"]},"children":[{"type":"text","value":"}"}]},{"type":"element","tagName":"span","properties":{"className":["msupsub"]},"children":[{"type":"element","tagName":"span","properties":{"className":["vlist-t","vlist-t2"]},"children":[{"type":"element","tagName":"span","properties":{"className":["vlist-r"]},"children":[{"type":"element","tagName":"span","properties":{"className":["vlist"],"style":"height:0.3361em;"},"children":[{"type":"element","tagName":"span","properties":{"style":"top:-2.55em;margin-left:0em;margin-right:0.05em;"},"children":[{"type":"element","tagName":"span","properties":{"className":["pstrut"],"style":"height:2.7em;"},"children":[]},{"type":"element","tagName":"span","properties":{"className":["sizing","reset-size6","size3","mtight"]},"children":[{"type":"element","tagName":"span","properties":{"className":["mord","mtight"]},"children":[{"type":"element","tagName":"span","properties":{"className":["mord","mathnormal","mtight"],"style":"margin-right:0.03148em;"},"children":[{"type":"text","value":"k"}]},{"type":"element","tagName":"span","properties":{"className":["mrel","mtight"]},"children":[{"type":"text","value":"≥"}]},{"type":"element","tagName":"span","properties":{"className":["mord","mathnormal","mtight"]},"children":[{"type":"text","value":"i"}]}]}]}]}]},{"type":"element","tagName":"span","properties":{"className":["vlist-s"]},"children":[{"type":"text","value":"​"}]}]},{"type":"element","tagName":"span","properties":{"className":["vlist-r"]},"children":[{"type":"element","tagName":"span","properties":{"className":["vlist"],"style":"height:0.2452em;"},"children":[{"type":"element","tagName":"span","properties":{},"children":[]}]}]}]}]}]},{"type":"element","tagName":"span","properties":{"className":["mclose"]},"children":[{"type":"text","value":")"}]}]}]}]}],"position":{"start":{"line":54,"column":1,"offset":1710},"end":{"line":54,"column":133,"offset":1842}}}],"position":{"start":{"line":1,"column":1,"offset":0},"end":{"line":54,"column":133,"offset":1842}}},"collectedBy":[["tex",[{"slug":"error-propagation-and-philosophy-of-science","filename":"2024-10-21-error-propagation-and-philosophy-of-science.md","date":"2024-10-21","title":".tex | 误差的传递，科学之所以科学","layout":"post","keywords":["tex","phy","phi"],"excerpt":"多变量测量的误差传递，及其在科学哲学中的作用。","content":"\n如果一个物理量需要用多个直接观测量计算出来：$$y=f(x_1,x_2,...,x_n)$$，这样的量叫做因变量，直接观测量叫做自变量。（比如用直尺测量长方形面积时，长、宽是自变量，面积是因变量，通过长和宽相乘计算面积的方法是一个函数。）\n\n因为中小学减负，因变量的说法不教了，改叫函数值，为了少学一个知识点。\n\n但是学过 C/C++ 的应该知道，对于 $$y=f(x_1,x_2,...)$$\n\n- 因变量 $$y$$ 是一个左值，指向 $$y$$ 的指针 `float *p = \u0026y;` 拿到的地址，位于内存的数据区；\n- 函数值 $$f(\\cdot)$$ 是一个右值，$$f$$ 本身就是一个指针，`void *fp = f;` 拿到的地址，位于内存的指令区。\n\n## 多变量测量的误差传递\n\n先跳过单变量误差的部分（大致原理在《贝叶斯，从公式到世界观》一文频率学派的部分，具体细节以后再写），不论是测量仪器的说明书给出的误差，还是测量者通过独立重复实验取得的统计误差，我们先假设已经拿到了观测量 $$x$$ 的测量值 $$\\bar x$$、误差 $$\\Delta x$$\n\n因为全微分公式，对于 $$y=f(x_1,x_2,...,x_n)$$\n\n$$\n\\mathrm{d}y = \\frac{\\partial f}{\\partial x_1}\\mathrm{d}x_1 + \n\\frac{\\partial f}{\\partial x_2}\\mathrm{d}x_2 + ... + \\frac{\\partial f}{\\partial x_n}\\mathrm{d}x_n = \\sum_i^n \\frac{\\partial f}{\\partial x_i}\\mathrm{d}x_i\n$$\n\n又因为误差相对于真值往往小几个数量级，所以我们把误差看作是真值的微分，用 $$\\Delta$$ 取代 $$\\mathrm{d}$$. （有人问真值为 0 怎么办，绝大多数情况下可以通过平移零点定义的办法来几乎任意地改变测量值的数量级，而误差不会因为这种变换而出现数量级变化。）\n\n还因为对多个自变量的测量是相互独立的，每个自变量 $$(x_1,x_2,...,x_n)$$ 占据相空间中的一个维度，维度之间互相正交。\n\n所以物理上，因变量的误差就是上述 ~~微分~~ 微差向量的“长度”，以  L2 范数 (norm) 来衡量：\n\n$$\n\\begin{array}{rcl}\\Delta y \u0026 = \u0026 \\sqrt{ \\left(\\frac{\\partial f}{\\partial x_1}\\bigg|_{\\vec x}\\right)^2\\Delta x_1^2 + \\left(\\frac{\\partial f}{\\partial x_2}\\bigg|_{\\vec x}\\right)^2\\Delta x_2^2 +...+ \\left(\\frac{\\partial f}{\\partial x_n}\\bigg|_{\\vec x}\\right)^2\\Delta x_n^2 } \\\\ \u0026 = \u0026 \\sqrt{\\sum_i^n{\\left(\\frac{\\partial f}{\\partial x_i}\\bigg|_{\\vec x}\\right)^2\\Delta x_i^2}}\\end{array}\n$$\n\n物理学家因此不害怕误差——理论物理的模型哪怕非常复杂，在数学上往往依然“性质优美”，只要理论的自变量可以在实验上测量，误差明确且有限，那理论给出的预测值的误差就同样明确且有限，依然可以指导实践。\n\n## 误差与可证伪性\n\n而根据卡尔·波普尔的科学哲学，具体来说就是可证伪性的划界标准，科学就不只是不害怕误差了，简直是依赖误差而生，靠误差来和伪科学划清界限。\n\n所谓科学的可证伪性，《[科学是什么？——兼谈“非科学、伪科学、反科学”和一些常见谬误](https://program-think.blogspot.com/2015/10/What-is-Science.html)》一文概括为：\n\n- 科学理论是一个相互关联的**命题**的**集合**。\n- 科学理论必须是基于**演绎**法建立整个理论体系的。也就是从不证自明的**定律**出发，依据**逻辑规则**，推论出各种各样的**定理**。\n- 理论中的命题必须是**客观**陈述，也就是能由不同的主体进行独立检验。\n- 检验的方式是**证伪**，也就是寻找现实中的一个现象，说明从理论中某个命题是错的。经过证伪程序，且没能被证伪的命题，就被验证为真。（根据逆否命题的等价性，演绎推论如果被证伪，它的逻辑前提也会连锁被证伪。那科学几百年来靠什么幸存，我们以后再狡辩～）\n- （既然一个存在命题就能否定一个学科理论中的待验证命题，）科学理论中的命题应该是**全称命题**，此即科学的普世性 (universality)。\n- 对全称命题的**特设性修正**（比如把“所有的天鹅都是白色的”修正成“所有北半球的天鹅都是白色的”），应该要提高理论的可证伪程度，否则就是伪科学。\n- 以上各个要求，单独只构成必要条件。\n\n所以定量科学的科学性就体现在，\n\n- 只要理论的自变量和因变量可以在实验上测量，\n- 自变量的误差明确且有限，那理论给出的预测值的误差就同样明确且有限，\n- 因变量的误差同样明确且有限，\n- 将理论预测值和因变量测量值摆在一起，只要差距不大于两者的误差，（技术细节在统计学中的假设检验部分。）\n- 我们就认为对“理论预测和因变量真值相等”的证伪失败了，从而接受他们相等。\n\n因为实验仪器和方法的进步可以缩小误差范围，增加科学理论的可证伪性，一条无限延展且随时可以投入精力的赛道从此出现，科学从相隔几代的少数天才之间的思维接力，变成了夙兴夜寐前赴后继的竞赛。\n\n科学家之间的竞争催生了对制造仪器之工程技术的巨大需求，需求大到部分科学家亲自下场改进甚至发明仪器，科学由此反哺技术；进步的技术使得科学得以产出更高质量的数据，支持更复杂的理论的检验。科学和技术，合成了“科技”一个词。\n\n试想一下，如果科学号称自己绝对精确，不存在误差，要么在弱小之时就被证伪，无法赢得人们的信任；要么任由实验精度低到看不出误差的程度，然后用其他手段维持自己的光辉形象。\n\n之前《也谈近代科学从西方起步》一文中说，“物理和数学的区别，在于理论和实验两条腿走路”。如今算是把实验这另一条腿简单介绍完了。\n\n最后需要注意，这里说的是某个哲学理论能够解释科学实践，而不是科学实践必须服从某套哲学理论。\n\n科学只对客观现实负责，不需要对哲学信条负责，不应该对哲学王兼英雄王负责。\n\n## 科学还正确吗？\n\n如果承认可证伪性作为科学与非科学的划界标准，也就意味着，现在包含在科学中的每条知识，都有在未来被更加精确的实验推翻的可能。\n\n这种事也不是没发生过。比如材料的电阻，在相当大的数值范围内，都和温度成线性关系，而且这条线向左延拓到绝对零度时基本为 0。在那个时代，认为电阻来自无规则的热运动，和绝对温标成正比才是符合奥卡姆剃刀原则的理论。但是 1908 年，昂内斯用液氦将汞的温度降到 4.15 K 时，发现汞的电阻突变降低为 0，这就是超导研究的开端。\n\n那还能说科学知识是正确的吗？《费曼物理学讲义》的回答是，不谈科学是不是正确的，只保证科学 (science)是科学的 (scientific)。也就是保证程序的正确，把正确程序获得的结果交给工程技术，用工程技术上的成就取信于社会，反过来为科学的正确性背书。\n\n所以说，科学家是对科学最不迷信的一批人，一旦实验过程正确，结果和理论不符，那么理论该修改的修改，该放弃的放弃。他们是现有科学最大的破坏者，是成功证伪科学命题最多的一群人。\n\n但同理，科学家又是对科学最坚定的一批人，他们在明知道一个科学命题可能在将来被修正的情况下，依然愿意把它当作前提，继续推理产出新的命题，并试图证伪。\n\n《三体》小说刚开始设置的一大悬疑，大量科学家因为自己正在进行的研究，产出了与理论完全不吻合的随机结果，因为所谓“物理学不存在了”而自杀，这个情节就很成问题。\n\n何况这种事情根本不需要书中的情节设定才会出现，物理学史上早就发生过。比如 β 衰变的质子的动能谱和动量角分布。玻尔想放弃能量守恒定律，泡利想假设一个探测器发现不了的新粒子，这在当时的实验条件下都是尚不能证伪的理论假设，但没听说俩人为这事寻死觅活的。\n\n所以改编成电视剧的时候，几乎重写整个人物关系的网飞版，把自杀改写成了球奸们伪装的他杀；就连以忠实于原作著称的腾讯版，也原创了一段主角主动重启科研装置，直面外星人恐吓的剧情，给原著做了点找补。\n\n## 那我缺的权威性这块谁给我补上啊\n\n坏了，碰到了不该碰的话题，那就先这样吧……\n\n## 报书名儿\n\n- William Lichten.《Data and Error Analysis》\n- 赵凯华《定性和半定量物理学》\n- 卡尔·波普尔《科学发现的逻辑》\n- 理查德·费曼《费曼物理学讲义》\n"},{"slug":"bayesian-equation-and-view-of-world","filename":"2024-09-27-bayesian-equation-and-view-of-world.md","date":"2024-09-27","title":".m | Bayesian 贝叶斯，从公式到世界观","layout":"post","keywords":["tex","m","phy","phi"],"excerpt":"我们老板真是太能吹了，Bro 居然跟隔壁真的在研究物理的课题组 brag abou 我会贝叶斯参数估计，yo know wat ur sayin? 赶紧来补课～","content":"\n\n## 公式\n\n我上学的时候，贝叶斯公式是概率论里面，少数高中完全不涉及，到了本科才第一次见的公式，所以我从来没背下来过。不过也用不着背，根据条件概率里面的一个平凡结果：\n\n$$\n\\Pr(A|B)\\ \\Pr(B) = \\Pr(B|A)\\ \\Pr(A)\n$$\n\n可以得到 $$\\Pr(A|B)$$ 和 $$\\Pr(B|A)$$ 之间的关系\n\n$$\n\\Pr(A|B) = \\frac{\\Pr(B|A)\\ \\Pr(A)}{\\Pr(B)}\n$$\n\n这就是贝叶斯公式本体。\n\n分母没什么意思，所以一般我们要用全概率公式替换，也就是把 $$A$$ 划分为全覆盖但是不相交的 $$\\{A_i | \\ A_i \\cap A_{j \\neq i}=\\varnothing,\\ \\bigcup_i A_i=A\\}$$\n\n$$\n\\Pr(A|B) = \\frac{\\Pr(B|A)\\ \\Pr(A)}{\\sum_i \\Pr(B|A_i) \\Pr(A_i)}\n$$\n\n其中任意一个子事件 $$A_j$$\n\n$$\n\\Pr(A_j|B) = \\frac{\\Pr(B|A_j)\\ \\Pr(A_j)}{\\sum_i \\Pr(B|A_i) \\Pr(A_i)}\n$$\n\n### 根据实验结果筛选理论模型\n\n以上是数学。在科学中，令\n\n- A 为一族理论模型的一组参数取值，记为 $$Param_k$$，下标可任意选取。\n- B 为实验观测数据，记为 *Ob*\n\n$$\n\\Pr(Param_j|Ob) = \\frac{\\Pr(Ob|Param_j)\\ \\Pr(Param_j)}{\\sum_i \\Pr(Ob|Param_i) \\Pr(Param_i)}\n$$\n\n其中 \n\n- $$\\Pr(Param_j)$$ 表示第 j 组参数是模型的正确参数的，未经实验验证，根据零假设计算的 **先验 (prior) 概率；**\n- $$\\Pr(Param_j|Ob)$$ 叫做经过实验观测修正之后的，第 j 组参数正确的 **后验 (posterior) 概率**。\n- $$\\Pr(Ob|Param_j)$$ 在之前的文章中讲过，是当前测量数据下，模型参数的 **似然性 (likelihood)**。\n\n$$\nposterior \\propto likelihood \\cdot prior\n$$\n\n### 举个例子\n\n隔壁组的问题可以简化为下图：\n\n![](/photos/2024-09-27-two-gaussian.png)\n\n- 有两组数据 (x, y1), (x, y2) 可以用同一族函数来拟合。（假设为两个高斯函数的叠加，$$y=f_{A_1,A_2,\\mu_1,\\mu_2,\\sigma_1,\\sigma_2}(x) = A_1e^{-\\frac{(x-\\mu_1)^2}{\\sigma_1^2}} + A_2e^{-\\frac{(x-\\mu_2)^2}{\\sigma_2^2}}$$\n- 两组数据的误差不同。（红色数据点显然比蓝色数据点，相对于理论值偏离得更远一些）\n- 问有没有一个数值，可以衡量每组数据的误差程度。\n\n我给他们的建议是\n\n- 根据自己的专业知识指定先验概率 $$\\Pr(param_j)=\\Pr(A_1,A_2,\\mu_1,\\mu_2,\\sigma_1,\\sigma_2)$$。比如选定一个参数空间的范围，范围之外概率为零，范围之内均匀分布。\n    - $$A_1,A_2 \\in \\left[\\min(\\{Y_1\\}\\cup \\{Y_2\\}),\\max(\\{Y_1\\}\\cup \\{Y_2\\}\\right]$$\n    - $$\\mu_1\\in[\\min\\{X\\},\\max\\{X\\}],\\ \\mu_2\\in[\\mu_1,\\max\\{X\\}]$$\n    - $$\\sigma_1,\\sigma_2\\in[0,\\ \\Sigma_i\\sqrt{|X_i-\\bar X|^2/N}]$$\n- 根据一些假设和统计规律计算 $$\\Pr(Ob|Param_j)$$\n    - 假设误差与 x 变量无关，服从期望为 0 的高斯分布，$$[y_i-f(x_i)]\\sim N(0,\\sigma^2)$$，标准差根据各数据点减去模型预测值的残差估计。\n    - 假设每个数据点的观测相互独立，$$\\Pr(Ob)=\\Pr(\\bigcap_i Ob_i)=\\prod_i\\Pr(Ob_i)$$\n    - 对于模型的每一组参数 ，$$\\Pr(Ob_i|param_j)$$ 取上述高斯分布的绝对值大于残差绝对值的部分，就是钟形曲线两侧尾巴的线下面积。\n- 对参数空间中的每一组值都算出一个后验概率之后，计算整个空间的信息熵（方法见之前的文章）。误差较大的一组数据，应当有更多组参数可以获得类似的拟合结果，从而信息熵更大。\n\n## 世界观\n\n对于概率，有三种理解：\n- 古典的 (classical)、\n- 频率学派的 (Frequentist)、\n- 贝叶斯的 (Bayesian).\n\n### 古典\n\n就是将古典概型推广，成为一种关于可能性的普遍观点——一个随机空间里的随机事件可以分解成若干子事件，子事件还可以再分，直到每个基本事件的概率相等，都等于基本事件总数的倒数，而要计算人们感兴趣的某一事件，只需要数出其包含的基本事件的数量就行了。\n\n让人联想到古希腊古典时代的原子论。时人认为物质世界也不是无限可分的，将任意一种材料打碎研磨，这一过程最终会有一个终点，最终的产物就是这种物质的“原子”。一块材料的大小，就是其所含原子数量的多少。\n\n有人批评这种观点用可能性去定义可能性，有循环论证谬误之嫌。但是看现代化了的概率论，概率被定义成了满足某些条件的函数，公理化是公理化了，逻辑链条是有了坚实的起点，但是那里的概率还能不能被当作可能性的度量，实在是不好说。\n\n有人批评这是机械唯物主义，这种人批判的武器一般是武器的批判，别争辩，先活下来再说。\n\n### 频率学派\n\n这种观点一言以蔽之：概率是频率在样本量趋于无穷时的极限。\n\n科学中（日常生活中也一样，只是人们通常没这么精确），测量误差不可避免，我们每一次的测量哪怕正确，互相之间也会有细微的差别，更不用说和待测的真实值不同了。\n\n解决方法在初中物理实验里学过：多次测量，把平均值当作真值（的估计量），根据标准差计算误差（置信区间、p 值等等……）。\n\n不同的人（假设有 M 个）可以对同一个可观测量进行 N 次测量，对于一个确定的 N，不论这个可观测量本身服从何种概率分布，这 N 个测量值的平均数 $$\\bar X_N$$ 都服从正态分布，这就是中心极限定理（注意不是大数定律）。\n\n当可观测量本身也服从正态分布的时候，就会导致标准差 (standard deviation) 和标准偏误 (standard error of the mean, 常简称为 standard error) 容易让初学者混淆。\n\n而按照这种世界观，所谓一个物理量的真值，就是所有可能的（所有已经发生过的+思想实验中可能发生的）测量的均值 $$\\bar X_\\infin$$。\n\n因为包括可能发生还未发生的测量，所以哪怕我们面对的问题是纯决定论的，客观存在一个确定的真值，无论我们已经进行过多少次测量，都无法保证得到真值。\n\n有人批评这是客观唯心主义，这种人批判的武器一般是武器的批判，别争辩，先活下来再说。\n\n### 贝叶斯\n\n前述世界观好歹还认为真值客观存在——\n\n贝叶斯世界观则直接不再对真值的客观存在下断言，不论先验还是后验，科学理论里的每一条命题，都不再孤单，而是要和所有可能的替代理论打包在一起；也不再“正确”，而是具有一个以概率衡量的可信程度。\n\n实验的作用不再是判断对错，而是在有限的先验知识（现存的科学理论）下，判断新取得的实验结果在多大程度上，更新了旧知识里每条命题的可信权重。\n\n而且每个人掌握的知识不同，先验概率不同，在同样的实验数据面前，所更新出来的知识体系也会不同。\n\n再者，如果先验概率为 0，任你实验数据如何显著，后验概率也一定为 0，所以对“未知的未知”无能为力。实践中，再离谱的先验假设，只要能想到，也要赋一个小而不为 0 的初值。\n\n有人批评这是主观唯心主义，这种人批判的武器一般是武器的批判，别争辩，先活下来再说。\n\n## 送分题\n\n已知本省不超过二十个地级行政单位。一中是本市最好的高中，本科过线人数年年创新高。\n\n已知本市报纸会公布喜报，上有全市前若干名学生的姓名、分数、录取学校等信息。省招办有根据成绩取得全省排名的服务。比如某年本市第十名，全省排名两千名开外。\n\n你能否据此评价母校和家乡的教学质量，以及本省各地区之间教育水平的平均程度？\n\n你该如何评价，从定义原假设和备择假设，到用何种概率分布对先验概率建模？\n\n你有资格评价吗？"},{"slug":"nick-lane-the-vital-question","filename":"2024-07-29-nick-lane-the-vital-question.md","date":"2024-07-29","title":".tex | Nick Lane \"The Vital Question\" 读书笔记","layout":"post","keywords":["tex","bio"],"excerpt":"顾简体中文名思义，这本书把真核细胞组成的多细胞生命体称为“复杂生命”，认为这种生命形式起源于一件“在历史上只发生过一次”的事件——线粒体“内共生”(endosymbiosis)——一个古菌 (archaea) 细胞吞噬了一个有氧呼吸的细菌，然后两者一起存活了下来。前者和后者一起演化成了现代真核细胞，其中后者演化成了真核细胞中的线粒体。","content":"\n往年暑假，隔壁理论组都会发起一个不务正业系列演讲，每周五抓壮丁讲点和科研本职工作无关的话题，例如美国央行的历史、自然利率、公园里的松鼠、以 stroad (street-road) 为代表的傻逼郊区规划……\n\n结果今年，隔壁组的老板度假去了，学弟参加暑期学校去了，本科学妹到德国交流去了，只剩下一个键政人，于是我们老板非常轻松地篡组夺权，把这个活动改造成了非不务正业的读书会。在股肱之臣（我）的坚持下，免费披萨和啤酒的共和传统保留了下来。\n\n![这已经不是一般的博士后了，必须要出重拳！](/photos/2024-07-29-coup.png)\n\n这已经不是一般的博士后了，必须要出重拳！\n\n---\n\n读的书目是 Nick Lane 的《The Vital Question》。名字里的 vital 很难翻译，字面意义上是重要的意思，但 vi- 前缀本身就有和生命相关的联想。以至于台版直接起名叫《生命之源》，陆版起名叫《复杂生命的起源》。\n\n顾简体中文名思义，这本书把真核细胞组成的多细胞生命体称为“复杂生命”，认为这种生命形式起源于一件“在历史上只发生过一次”的事件——线粒体“内共生”(endosymbiosis)——一个古菌 (archaea) 细胞吞噬了一个有氧呼吸的细菌，然后两者一起存活了下来。前者和后者一起演化成了现代真核细胞，其中后者演化成了真核细胞中的线粒体。\n\n但是此事毕竟发生于几十亿年前，并没有监控录像证明事情的始末经过，所以这种说法只能算是一种有证据支持的科学假说。\n\n所谓证据，包括数量相对少的古老岩石的成分、生物化石证据，和数量非常多的现存生物体的细胞结构、不同物种间的遗传信息的对比结果等等。\n\n所谓支持，按卡尔·波普尔的科学哲学观点，指的是这些事实能够证伪 { [ (原命题) 的否定 ] 的一个子集 }。即便自身言之成理，也无法否认其他可能性的成立。持不同观点的科学家以“谁主张谁举证”的原则立论，学界根据互相竞争的理论形成的不同预测构造实验，证伪错误的主张。\n\n作者认为这一内共生事件意义非凡，线粒体这样一个专门产生能量的结构，导致真核细胞有了远多于原核细胞的能量预算（以平均每个基因所能支配的能量来衡量），从而在结构和功能的复杂度上有了重大“进步”。\n\n不仅如此，作者更进一步认为很多并不直接和线粒体关联的现象，也是这一内共生事件的结果，比如核膜的出现、性别和有性生殖的出现、物种的产生、物种寿命的长短等等。不过这一部分的证据不太充足，就连作者对自己的论证也不很确信。\n\n---\n\n虽然作者的核心观点是关于真核生物的出现，但是我反而觉得全书写作的高光，出现在对细胞从无到有的过程的推断。其因果链条极长，但又有事实作为支撑，即便怀疑，也难以否认这一理论在智力上的精妙。\n\n作者根据以下事实：\n\n- 今天细胞生物系统普遍的能量通货——ATP ↔ ADP + Pi ；\n- 这一分子普遍的产生方式——ATP 合成酶在氢离子（质子）跨过生物膜上的过程中催化，也就需要在线粒体内膜两侧存在质子的浓度梯度；\n- 现存生物系统中梯度的来源是一系列氧化还原反应，电子从食物分子转移到以 Complex I 为代表的呼吸链分子上，最后结合到氧气分子，这些分子位于线粒体内膜上，电子转移的过程会将质子逆梯度跨膜输送；\n- 细菌和古菌两种原核生物的磷脂膜成分不同。\n- 生物出现前，古代地球大气中氧气浓度很低，二氧化碳浓度高。\n- 海底板块扩张中心附近的海床上，地壳运动从地幔中露出表面的岩石中富含橄榄石，橄榄石富含亚铁离子和镁。亚铁离子被水氧化是一个放热反应，伴随着大量氢气的释放，氢气溶于碱性海水。由此形成碱性热液喷口。\n- 碱性热液喷口，喷出60℃至90℃的温水，形状是布满互相通连、迷宫一样复杂的微孔结构。\n\n给出了细胞出现过程的推测：\n\n- 生物出现前，古代地球二氧化碳浓度高，海洋呈现酸性；碱性热液喷口附近海水为碱性，于是存在一个相对稳定的氢离子/质子梯度。\n- 热力学上：碱性环境中，氢气分子氧化的反应可以自发进行；酸性环境中，二氧化碳还原成有机分子的反应可以自发进行。（详见上一篇文章中的氧化还原电位问题。）两者合并，在质子梯度存在的前提下，氢气将二氧化碳还原为有机物的反应可以放能。\n- 动力学上：碱性热液喷口附近岩石表面的金属离子可以充当催化剂，显著降低活化能。这些金属离子的催化效率虽然低，但是随着进化，金属离子周围逐渐有蛋白质修饰，进而演化为今天的生物酶分子。\n- 碱性热液喷口的微孔结构存在热泳效应，可以让有机分子高度浓缩，有助于反应速率。微孔也可以让脂类分子自发形成囊泡，包裹各类有机分子，形成细胞的雏形。\n- （此时尚无遗传信息及其复制，此套理论也没有解释遗传信息分子的进化，但是可以推论，生化反应随着进化而不断复杂，出现了 DNA、RNA、核糖体等生物信息相关的物质和结构。）\n- 其中氢气还原二氧化碳的催化剂，进化成为了能量转换氢化酶 (energy-converting hydrogsase，简称*Ech*)，负责碳代谢。它嵌在原始细胞膜上，在质子从膜外进入膜内的过程中完成催化。这一机制直到今天的产甲烷细菌仍在使用。\n- 类似地，ATP 合成酶也镶嵌在原始细胞膜上，在质子从膜外进入膜内的过程中催化合成 ATP，负责能量代谢。\n- 此时的膜结构无法阻止质子跨膜，也就无法依靠自身维持质子梯度，于是只能存在于碱性热液喷口附近。\n- 后来早期细胞进化出了一个反向转运蛋白 (antiporter)，用一个钠离子交换一个水合氢离子跨膜。膜外的氢离子浓度高于膜内，于是氢离子从膜外进入膜内；钠离子离开原始细胞，并无法通过扩散透过原始细胞膜，但是他的尺寸类似水合氢离子，于是可以通过 Ech 和 ATP 合成酶回到膜内，驱动碳代谢和能量代谢。\n- 这种方式减弱了对外界氢离子浓度的依赖，扩大了原始细胞的生存范围。\n- 然后出现了细菌和古菌的分化：\n    - 产乙酸菌（细菌）的祖先倒转了 Ech 的方向，让其成为了维持质子浓度并消耗能量的质子泵，利用其他机制进行谈代谢。\n    - 产甲烷菌（古菌）的祖先演化出一个新的质子泵来运出质子，仍用质子通过原来的 Ech 来进行碳代谢。\n- 两种生物采取了不同的方式维持质子浓度，也就导致膜的选择性也是独立演化出来的，磷脂分子的结构不同，细胞壁的成分也不同。\n\n---\n\n看得出来，我们老板对这套理论应该是非常买账的。实验室刚创建还在施工，只能带着我们做 journal club 的时候就给我们安利过这套假说，之后也集中读过几篇反驳书中观点的论文，以及作者及其支持者们的回应，以及进一步的反驳。\n\n没错，学界存在很大的一批势力，对这一理论大加挞伐：\n\n首先是这个从原核细胞到真核细胞是一种进步的观点，已经不太符合现代生物学对 evolution 的理解了，甚至这个词现在都已经普遍不再翻译成“进化”，而代之以更中性的“演化”。\n\n同一环境下能够幸存的所有物种，在演化看来都是成功的，并不存在谁比谁更优越，即便有，衡量标准也是该种群遗传信息的拷贝数，从这个意义上看，细菌和古菌往往比同一环境中的单细胞真核生物还更成功些。（我对拷贝数这个衡量标准倒是也有些保留意见。）\n\n其次是这个“平均每个基因所能支配的能量”作为真核细胞优越性的标准，有点先射箭再画靶的嫌疑。\n\n虽然正方的主要观点结集在这本书里，而反方主要发表在学术期刊上，给人一种民科大战官科的观感，但是两方曾在一些顶级生物期刊上正面笔战，双方也都是生物学研究的业内人士。\n\n那些反驳论文是很久以前读的了，所以这里只是凭回忆概括一下，如果将来对这个问题还感兴趣的话，可能会再回去读一下。\n\n---\n\n由于内共生事件是本书的主角，而它在时间上位于生物演化中间，本书的行文顺序容易让人丧失时间观念。\n\n本来打算重新整理一下自己的笔记，写成一个编年史的体例，但是本书的很大篇幅用来讨论原因、机制、后果等等，试了一下发现效果也很不好。\n\n而且作者的行文思路也很跳脱，经常跑一段题，然后后面无缝接回刚才的话题……\n\n斗胆揣测一下原因，可能是是作者先写成了初稿，然后在编辑的威逼利诱下灌了水～\n\n![对于编辑第二重要的工作是……](/photos/2024-07-29-editor-job.png)\n\n对于编辑第二重要的工作是……\n\n这就导致在每周的讨论时间，很多时候他们提出的问题，都是作者在书里已经写过的，需要我对照自己读汉语版做的笔记，倒回去找到英文版的章节，指给他们看。\n\n当然了，也有可能是我个人的汉语阅读能力超出了其他人的英语阅读能力，但是这里的其他人包括了我老板，所以我觉得不太可能。\n\n# 读书笔记\n\n\n- Introduction: Why is life the way it is?\n- Part I: The Problem\n    - `1.` What is life?\n    - `2.` What is living?\n- Part II: The Origin of Life\n    - `3.` Energy at life’s origin\n    - `4.` The emergence of cells\n- Part III: Complexity\n    - `5.` The origin of complex cells\n    - `6.` Sex and the origins of death\n- Part IV: Predictions\n    - `7.` The power and the glory\n    - Epilogue: From the deep\n\n## Introduction: Why is life the way it is?\n\n开头，作者似乎甚至要把细菌开除生物户籍？好吧，是用 complex life 称呼以真核细胞为单元的复杂生命体。（那酵母呢？后文说单细胞阿米巴虫也算 complex life。）这个定义和 multicellularity 还不一样，也就是说，本书的内容范围远小于生物学研究对象的全体。\n\n细菌之于生物，类似原子之于物质——Kluyver’s students Cornelis van Niel and Roger Stanier: “Bacteria like atoms could not be broken down any further.”\n\n1. 线粒体和叶绿体起源的 endosymbiosis 假说。作者认为这两件事历史上各自只发生了一次，我有点怀疑。\n2. 核糖体基因在不同种生物间的基因差异推断出的演化距离，从而把古菌 archaea 从 bacteria 中独立出来。\n3. 基因对比表明内吞线粒体祖先的是一种 archaea. Bill Martin \u0026 Miklós Müller 认为真核生物的出现和线粒体祖先被内吞是一回事。\n\n作者的观点是：这一内吞给真核生物带来了能量上的优势。呼吸作用的能量基本用于质子泵产生跨膜质子梯度。\n\n## Part I: The Problem - 1. What is life?\n\n### ===\n\n人对于地外生物的想象，其实是反映了对地球生物特征的模糊总结，和无意识的刻板印象。\n\n物理学试图回答为什么物理定律是我们所知道的样子。（呵呵。）但是生物学很少有这方面的 predictability. （可预测性和为什么之间能划等号吗？）\n\nJacques Monod 在《Chance and Necessity》中认为，生命的产生来自于偶然。\n\n其他人反对道，（实际作者也持这种观点，）生命是宇宙化学的必然产物。一些物理定律可能会限制生物进化，导致趋同演化。\n\n但是由于只有一个地球，从统计学上来看，没办法判定到底有没有这样的定律，以及是哪些定律。进化论擅长总结历史，很难预测未来。作者认为能量的限制就是一个物理上的限制，也就是这本书的核心主张。\n\n薛定谔的《啥是生命》里认为，遗传物质是一种 aperiodic crystal，并非严格重复的结构，从而可以成为代码本。\n\n不同物种的基因量很反常识。洋葱、小麦、阿米巴虫的基因数量和DNA量比人类多。不同种类的两栖类的基因量跨度在两个数量级，其中能达到人类的 40 倍。\n\n基因组结构和尺寸没有明显的物理限制，其携带的信息就也不受限制。生物本应该可以在自然选择之下长成相应环境的最优解，但历史上的古生物显然不是这样的。所以这种限制不来自基因组。\n\n### A brief history of the first 2 billion years of life\n\n早期地球：\n\n诞生于 45 亿年前，开始的约 7 亿年不断受到小行星撞击，其中一个有火星的大小，并导致了月球的产生。早期岩石在地球上已找不到，但是在月球上可以找到。\n\n锆石晶体络合的分子表明：地球很早就出现了水，而且温度不高，和地球早期是到处都有沸腾岩浆的炼狱的传统观点不符；气体方面，传统观点认为空气中充满甲烷、氢气、氨气，新发现认为氧化物较多，如二氧化碳、水蒸气、氮气、二氧化硫，除了没有氧气和今天的大气层很像。结论：细菌可以在这样的环境中演化。\n\n早期生命：\n\n格陵兰西南的 Isua 和 Akilia 是已知最早的岩石，可追溯到 38 亿年前。生命的证据是岩石中碳颗粒中各同位素的比例，因为生物基本上只利用较轻的同位素。有人不同意这是生命的证据，作者认为即便不是，这种地球化学作用也是非生命到生命连续谱上的一环。\n\n澳大利亚和南非的古岩石包含了细胞化石，这是生物存在的确定证据。\n\n32 亿年前的地层出现了铁和碳的富集，这是原始光合作用的产物。\n\n29 — 24 亿年前，制氧的光合作用出现，至于 22 亿年前的大氧化事件，导致了地球的冰川化。至此生物的绝大多数生化反应都以出现，由细菌和古菌完成。\n\n（下一节中提到）16 —12 亿年前开始出现真核细胞的化石。7.5—6 亿年前，在一段地质活跃和冰球期之后，氧气含量再次快速上升，大到直径 1 米的生物化石开始出现，随后再一次大灭绝，5.41 亿年前寒武纪物种大爆发，大量动物开始出现。\n\n### The problem with genes and environment\n\n最传统观点认为氧气是个好东西，它的出现为进化松开了刹车，因为细胞有氧呼吸产生的能量（以能量而非功率衡量？按细胞平均还是按化学反应平均？）比其他呼吸方式高 1 个数量级。\n\n稍新一点的观点认为氧气不是好东西，过于活泼且对无氧呼吸的物种有毒。生命是靠共生和内共生才克服了氧气的存在，进化出了今天有氧呼吸的复杂物种。\n\n作者不同意后一种观点的潜台词——基因和环境构成了生物学所研究的一切，而氧气是环境的核心变量，塑造了进化图景。作者认为基因和环境之外，细胞和物理限制也是生物学的研究对象，而且后者很少能从前者直接得到答案。\n\n如果有无氧气是导致进化爆发与否的原因的话，作者认为应该看到今天的好氧生物进化自多种细菌祖先，即 polyphyletic radiation。而如果进化的瓶颈是生命本身的结构的话，我们就会看到有着“正确”结构的少数甚至单一谱系的后代占据大量生态位。\n\n这正是真核细胞的出现时的情况，而从单细胞到多细胞反而有大约 30 个独立的进化路径。\n\n### The black hole at the heart of biology\n\n前一节的 polyphyletic radiation 可以来自于自然选择，也可以是内共生的结果。但是现实并非如此。真核细胞生命体的细胞之间几乎看不到（内共生理论描述的现象之外的）区别。（那细胞壁呢？）\n\n内共生理论本身不排除 polyphyletic radiation. Margulis 的原教旨内共生理论甚至认为所有细胞器都是内共生而来。已知超过一千种的没有线粒体的真核生物似乎支持了这一理论，其中 Giardia 甚至有两个细胞核，Cavalier-Smith 把它们叫做 archezoa，主张这些物种是内吞线粒体之前的真核细胞，因此真核细胞在内共生之前就有真核细胞的各种特征，并非来自外部。（这个结论认为线粒体只能是第一种被内吞的细胞器，理由呢？其他细胞器不是内吞而来的，证据是基因吗，可靠吗？）总之这是现代真核细胞起源的教科书级观点。\n\n现代基因组研究发现，这些物钟并非原核生物和真核生物的过渡，而是曾有包括线粒体在内的真核生物的一切结构，然后在进化中把线粒体简化成了 hydrogenosomes 或 mitosomes。\n\n（解决了刚听说这本书的时候我的质疑：）That does not in itself mean that the origin of complex cells was a rare event. In principle, complex cells could have arisen on numerous occasions, but only one group persisted – all the rest died out for some reason. 但是作者又加强了自己的观点：这种其他合并事件的后代被淘汰的场景也没有发生。（为什么）\n\n据共同祖先，可以将真核细胞根据形态分为 5 组，各组组内的基因差异大于五组原始祖先的基因差异。符合 early radiation 尤其是 monophyletic radiation 的结果。除了叶绿体之外，原始祖先几乎啥都有了。\n\nAll are sexual, with a life cycle involving meiosis (reductive division) to form gametes like the sperm and egg, followed by the fusion of these gametes. The few eukaryotes that lose their sexuality tend to fall quickly extinct (quickly in this case meaning over a few million years). （等一下，酵母呢？）\n\n问题来到了几乎什么都有了的初代真核生物，它的各部件是哪里来的呢？细菌没有任何往这方向演化的迹象。\n\n### The missing steps to complexity\n\n作者也承认大量中性进化的积累本身会筛选掉进化的中间态，这符合原核生物和真核生物之间没有过渡物种的发现。（把我好闪，看了两遍才明白。）\n\n但是这解释不了内共生是一个一次性事件。（前面也说了可能是多次发生，但是其他事件的后代没有留存下来，主张了这种情况没有发生，但是至今未举证。）\n\n靶观点是两者之间的竞争，真核生物的适应性如此之强，以至于往真核生物某些特征发展的原核生物都被淘汰了。问题在于原核生物的数量众多，且基因可以横向转移，很难在演化中灭绝。所谓 oxygen holocaust，其实至今没有证据证明存在。\n\n而且之前提到的 archaezoa，虽然他们不是演化上的中间物种，但却是生态位上的中间物种，说明这些生态位不会被淘汰。真核细胞可以再简化，但是原核细胞无法复杂化，可能的解释来自于他们的结构。\n\n（作者说的细菌在有些情况下同时指真细菌 (bacteria, eubacteria) 和古菌 (archaea, archaebacteria)）两者基因和生化反应不同，但是形态相似。再次说明结构上的物理限制。\n\n### The wrong question\n\n要回答复杂生命产生的原因，其答案需要强到足够发生，但有没有强到会在演化历程中多次发生。\n\n答案并不能只从生物信息中寻找，需要演化的具体历史。\n\n扯了扯熵和自由能。说下一章要讲自由能里“自由”的意思。\n\n薛定谔的书名问错了问题，不应问 what is life, 而是 what is living.\n\n## Part I: The Problem - 2. What is living?\n\n### ===\n\n作者认为病毒不 alive，因为没有自有的 active metabolism。\n\n但随即自问，难道复杂生物不也是环境的寄生物吗。\n\n区别在于量的不同，病毒存活的环境里有他们需要的一切，而植物几乎不太需要从环境摄取有机物，我们处于两者之间。\n\nNASA 对生命的定义：a self-sustaining chemical system capable of Darwinian evolution\n\n而一个生命体的生死状态也可能是连续的，比如病毒、孢子、水熊虫的休眠状态。为什么他们的这些状态不会按热力学第二定律衰变呢？这说明生命和活着还有区别。生命是关于结构的，而活着是关于生长、繁殖，和环境互相关联。\n\n### Energy, entropy and structure\n\n反直觉的现象：孢子和把孢子磨碎到分子成分的水平，两者的熵变化不大。According to the careful measurements of the bioenergeticist Ted Battley, entropy barely changed.\n\n（~~我感觉这一节完全扯淡，事实错误。~~）像磷脂双分子膜可以在水中自发形成，因为其分子有疏水基团，所以双分子膜的自由能比这些分子散落在水中的自由能更低。（我依然感觉这一节完全扯淡，作者在谈自由能和熵的时候没指明自己所说的对象。如果说打碎的孢子是水油混合的，那就说明系统不在热力学平衡态，静置等待油分子再成膜，（这些成分不可能自发变回原来的孢子，）然后计算熵值呢？）\n\n（~~后面对于活化能的描述也很离谱，感觉连《细胞生物学精要》都没看过的水平。~~）\n\n（）\n\nEverything that happens in a living cell is spontaneous, and will take place on its own accord, given the right starting point.（这话不对吧）\n\n### The curiously narrow range of biological energy\n\n\u003e A single cell consumes around 10 million molecules of ATP every second! … There are about 40 trillion cells in the human body, giving a total turnover of ATP of around 60–100 kilograms per day. … In fact, we contain only about 60 grams of ATP  we know that every molecule of ATP is recharged once or twice a minute.\n\u003e \n\n\u003e We use about 2 milliwatts of energy per gram – or some 130 watts for an average person weighing 65 kg, a bit more than a standard 100 watt light bulb. That may not sound like a lot, but per gram it is a factor of 10,000 more than the sun.\n\u003e \n\n\u003e Bacteria such as E. coli can divide every 20 minutes. To fuel its growth E. coli consumes around 50 billion ATPs per cell division, some 50–100 times each cell’s mass.\n\u003e \n\n生物的单位体重功率极高，但不违背物理定律，因为洒在地球上的太阳能的总功率更高。（@钱院士）\n\n尽管如此，生物系统却及其受限于能量。\n\n（下面的基本上就是高中生物奥赛的分子生物学部分～）\n\n然后讲了讲 ATP 和 ADP，呼吸作用中的电子传递链，iron-sulphur cluster。就是说生物系统利用的是化学能，而不是热能、机械能、定向电流……（但是你说 UV radiation 就有点难绷。）另一个特殊之处在于 ATP 和 ADP 的转化是通过跨膜的质子泵完成的。\n\n然后作者开始水字数。ATP 大小比作人的话，Complex I 就有一座摩天大楼那么大。将质子泵过膜有两个结果：一是膜两侧的质子浓度差，二是电荷浓度差/电势差。\n\n\u003e The electric field you would experience in the vicinity of the membrane – the field strength – is 30 million volts per meter, equal to a bolt of lightning,\n\u003e \n\n电势差驱动 ATP 合成酶。\n\n\u003e For every ten protons that pass through the ATP synthase, the rotating head makes one complete turn, and three newly minted ATP molecules are released into the matrix.\n\u003e \n\n### A central puzzle in biology\n\nPeter Mitchell 的化学渗透假说 (chemiosmotic hypothesis): [https://pubmed.ncbi.nlm.nih.gov/13771349/](https://pubmed.ncbi.nlm.nih.gov/13771349/), 上文所说的过程叫做氧化磷酸化，1970s 才被解开。\n\n\u003e John Walker’s Nobel Prize in 1997 for the structure of the ATP synthase\n\u003e \n\nMitchell 给出一个更深刻的问题：生物是怎么让内部不同于外部的？\n\n不同于试管化学，生物体中的化学是向量化的，沿着膜的法线方向。\n\n生物使用氧化还原反应提供自由能，（电子的空间分布）；使用质子在膜两侧的渗透压储能，（质子的空间分布）。为什么要这么做？于是引出下两节的内容：\n\n### Life is all about electrons\n\n为什么要用氧化还原反应，答案比较简单，生物是碳基的，碳的最终来源主要是空气中的二氧化碳，正4价，而大分子有机物中的碳原子普遍低于 4 价，而改变化合价的反应自然就是氧化还原反应。\n\n之所以要以碳为骨架，是因为碳有 4 个成价电子，且比硅电负性更高；有气体形态的氧化物。\n\n但是现代物种中，碳代谢和能量代谢基本都独立开了，只由 ATP, thioesters 等少数几种分子链接，而且这些分子不需要生物通过氧化还原反应合成。\n\n而且也有其他的反应被认为是生物最初的反应，比如氮气和甲烷在紫外线下合成氰化物。这样的反应为什么没有在今天为生物供能呢？\n\n用电子传递链的好处是，这一链条的起始端并没限定反应物，不仅食物可以，一些细菌可以将矿物质和气体作为呼吸作用的反应物，而整个链条不会有太大变化。\n\n链条的终点也不一定是氧气，而可以是很多其他氧化物，甚至一些细菌可以呼吸产生矿物质。\n\n### Life is all about protons\n\n化学渗透现象的普遍性表明，这一机制在生物体中出现得非常早。与之同样普遍的是 DNA 转录成 RNA 再翻译成蛋白质的过程。\n\nArchaea 和 bacteria 的区别主要在于内部的生化反应种类，比如 DNA 复制用到的酶，细胞壁的化学成分，发酵作用的生化通路，细胞膜的化学成份。在如此基本的地方都有区别的前提下，化学渗透却是普遍的。\n\n这些异同对两者的共同祖先构成了疑问。\n\n考虑膜，立了一个靶子：可能原始祖先有细菌一样的膜，然后 archaea 替换成了自己的膜——以适应高温环境。（这个归因哪来的？）后面就是对高温的反驳——细菌也能生活在高温里，而绝大多数环境下两者没有演化优势。\n\n然后是，膜的具体成分区别（为什么不早说，是不是因为这会导致前面的疑问很傻逼？）是，细菌和古菌使用的是甘油的两种镜像异构体。但是两种合成甘油的酶在演化上并非很近的亲缘关系，也就是说 archaea 重新发明了一种酶，就为了利用同一种分子的异构体。因此祖先细胞膜成分应该和现代细胞很不一样。\n\n化学渗透出现得很早这一观点也有问题，因为 ATP 合成酶的结构十分精巧。现代细胞的 ATP 合成酶只在膜不允许质子透过的情况下才能运转正常，而古细胞膜是可以允许质子透过的。在没有质子浓度差的情况下，如何演化一种需要质子浓度差的酶，就成了一个鸡生蛋生鸡的问题。\n\n所以作者把这个问题和复杂生命的起源问题合并回答——化学渗透耦合将细菌和古菌困在他们的结构长达几十亿年的时间，直到一个偶然的内共生事件使得形成的真核细胞突破了能量限制。\n\n## Part II: The Origin of Life - 3. Energy at life’s origin\n\n### ===\n\n（相对来说读起来质疑最小的一章～）\n\n长链代谢路径+特异性的酶的作用是提高产物分子的特异性，从而避免生成不需要的副产物，节省能量。反过来说，在进化早期，产生同样的目标产物的过程会生产比现在多得多的副产物，对能量的需求反而更高。\n\n\u003e In 1953, Stanley Miller was an earnest young PhD student in the lab of Nobel laureate Harold Urey. In his iconic experiment, Miller passed electrical discharges, simulating lightning, through flasks containing water and a mixture of reduced (electron-rich) gases reminiscent of the atmosphere of Jupiter… Amazingly, Miller succeeded in synthesizing a number of **amino acids**… Miller, in contrast, featured on the cover of Time magazine in 1953.\n\u003e \n\n米勒-尤里实验和沃森-克里克解析 DNA 结构发生在同一年，但当时获得了更大的名声。两个发现其实都迷惑了生物学家：前者巩固了原始汤理论，后者让人将生命等同于信息，从而生命的起源等同于信息的起源，进一步编程信息分子自我复制的起源。\n\n因为结构的复杂度，最早的自我复制因子不太可能是 DNA，RNA 比较符合原始信息分子的设定。于是传统观点认为原始汤产生了 RNA 世界，然后逐渐演化成今天的复杂世界。\n\n但是能量上说不通，米勒-尤里实验即便在早期地球上能发生，闪电的能量和产物的数量很难满足生命产生的需要。紫外线更有希望一点，但是紫外线对现存的生物物质都是有害的，会导致其分解；且紫外线催化的有机反应的产物是氰化物，也是现存生物不利用的有毒物质。\n\n所以支持这一观点的科学家在地质学上寻找海洋超量蒸发的事件，认为这些事件导致的有机物浓度升高，从而使早期生命活动得以发生。但是这样的剧烈事件对生物本身却是灾难性的。\n\n生命是远离平衡态的 (equilibrium)，但也是稳态的 (steady state)，不断自我更新的同时维持不变的结构形式，这需要持续的能量流，来自俄裔比利时物理学家伊利亚·普利高金（Ilya Prigogine）提出的“耗散结构”（dissipative structure). （作者说这个过程和信息无关，我觉得不对，但是不影响其他结论。）\n\n### How to make a cell\n\n细胞的特点：\n\n1. 持续的活化碳供应，用来合成新的有机物\n2. 自由能供应，用来驱动代谢生化反应，包括新的蛋白质和DNA等物质的合成；\n3. 催化剂，用来加速和引导代谢反应；\n4. 泄废弃物，遵循热力学第二定律，驱使化学反应以正确的方向进行；\n5. 区隔化 (compartmentalization)，细胞式的结构，把内部和外部分隔开；\n6. 遗传物质，即RNA、DNA或同等物质，用来承载信息，规定具体的结构和功能。\n\n（然后他开始退回去谈上一节的信息分子复制了。~~感觉这本书的叙述顺序十分离谱，以至于各章节标题都有点问题，感觉像是被编辑无理裁切过一样。~~依然是在谈上面的六个特点，但不是从 1 到 6 的顺序～）（第 6 点：）新陈代谢和复制哪一个先出现？复制的指数性质使得代谢很难跟上遗传分子的增长速度。\n\n（第 6 点：）Graham Cairns-Smith 主张最早的信息分子是矿物质而不是有机分子。比较离谱，因为晶体的编码量太小。\n\n（第 1, 2 点：）脱水缩合反应在水溶液中为什么可行——与 ATP 水解耦合。\n\n（第 5 点：）囊泡可以自然形成，因为会增加总体熵。（不同意。）这样可以在有限空间里获得较高的分子浓度。表面积和体积的量纲不同也会导致自发分裂。\n\n\u003e 没有细胞壁的 L 型细菌 (L-form bacteria) 正是使用这种出芽生殖方式来分裂\n\u003e \n\n（第 4 点：）表面积与体积的比例问题，同时也限制了细胞的大小。这个问题的实质是反应物的供给与废物排泄的速度问题。原始汤理论对废物的移除又是一个根本的缺陷。因为在原始汤场景中，反应物和废物全都“腌”在一起。\n\n（第 3 点：）现代生物都以蛋白质（酶）为催化剂，但RNA也有一定的催化能力。最初的生化反应应该由金属离子催化，这些离子直到今天也是一些酶的配体 ligand。\n\n总而言之：大量的有机碳与能量流被引导流过无机催化剂。下一节是作者给出的具体答案：碱性深海热液喷口。这顺便也回答了上一章结尾的问题。\n\n### Hydrothermal vents as flow reactors\n\n深海热液喷口（黑烟囱）：\n\n水和岩浆的反应。\n\n微生物学家约翰·鲍罗什 (John Baross) 开始研究。\n\n金特·瓦赫特绍泽 (Günter Wächtershäuser) 提出了黄铁矿拉力反应机制来解释生命起源，但是问题很多：\n\n- 这一机制需要一氧化碳而非二氧化碳做反应物。\n- 温度太高，有机物会迅速降解为二氧化碳。\n- 强劲水流会让有机物快速扩散，且黑烟囱本身寿命也只有几十年。\n\n\u003e 黄铁矿常由亚铁离子（Fe^2+）和硫离子（S^2–）形成不断重复的晶格。直到今天，很多酶的核心仍然存在亚铁与硫化物形成的铁硫簇，包括一些呼吸蛋白都是这样。这些作为酶核心的铁硫簇结构，本质上与铁硫矿物的晶格结构完全一样.\n\u003e \n\n碱性热液喷口：\n\n迈克·拉塞尔（Mike Russell）在一篇1988年发表在《自然》上的短文章中提出碱性热液喷口与生命起源的关系。（预言性的学说，而非观测结果。）Bill Martin 以微生物学家的视角加入研究。\n\n（后面的内容就需要和下一节合并了，讲的都是这一学说的合理性，碱性只是合理性之一。）\n\n### The importance of being alkaline\n\n- 通过地壳运动从地幔中露出表面的岩石中富含橄榄石。橄榄石与水反应，会生成一种名为蛇纹岩（serpentinite）的水合矿物。\n- 大多发生在板块扩张中心附近的海床上，靠海水渗透到海床以下的岩层，有时深达数公里。60℃至90℃ 的温水。\n- 它们的形状不是大张口、直接向海洋喷出强力水流的烟囱，而是布满互相通连、迷宫一样复杂的微孔结构。\n    - 通过热泳（thermophoresis）过程，有机分子的浓度能达到起始浓度的数千甚至数百万倍。\n    - 它可以让脂肪酸自发形成囊泡，还有可能让氨基酸聚合成蛋白质，让……任何增加分子浓度的过程，都会促进分子之间的化学作用。\n- 它们不是酸性，而是强碱性。40亿年前的冥古宙，\n    - 水中没有氧气，铁会以亚铁离子的形式溶解在水中。\n    - 大气和海洋中的二氧化碳浓度比现在高得多。\n        - 这么多无机碳供应使得这些远古的喷口结构发展不受限制；\n        - 高浓度的二氧化碳使原始海洋酸化，让碳酸钙不容易沉淀\n- 2000年第一个海底碱性热液喷口区被发现，并命名为“失落之城”。\n\n高浓度的二氧化碳、微酸的海洋、碱性热液，再加上含有铁硫矿的喷口薄壁，这一组合——\n\n- 从热力学角度看，\n    - 二氧化碳会与氢气反应生成甲烷。这是一个放热反应，如果有机会，这个反应会自动发生。\n    - 这里的特定条件包括适中的温度，以及不能有氧气。\n    - 这些条件下，且环境温度在25℃至125℃之间，以二氧化碳和氢气为原料合成全套细胞生命物质（包括氨基酸、脂肪酸、碳水化合物、核苷酸，等等）是放热反应\n- 二氧化碳和氢气之间还有一道动力学障壁需要应对两道能量障壁。\n    - 第一道需要跨过去，达到甲醛和甲醇的还原程度；\n    - 第二道则绝不能跨过，细胞现在绝不能让反应进行到底，生成甲烷。\n\n\u003e 从氧化还原程度来看，生命大致相当于甲醛与甲醇的混合物。\n\u003e \n\n而让上述反应在动力学上可行的，就是质子浓度梯度，也就是下一节的内容。\n\n### Proton Power\n\n还原电位和电负性两个概念之间是什么关系？【Link】\n\n中性环境中，氢气的还原电位是 -414mV，甲酸盐的还原电位是 -430mV，甲醛还原电位是 -580mV——氢气不可能还原二氧化碳。\n\n分子的还原电位经常随着pH值改变，也就是随质子浓度的变化而变化。但是方向对于上述三者是一样的，单纯改变pH值没有任何作用，氢气仍然不可能还原二氧化碳。\n\n现在考虑隔着一层膜的质子梯度，膜两边的质子浓度（酸性）不同。在 pH 值为 10 的环境中，氢气的还原电位是 -584mV；在 pH 值为 6 的环境中，甲酸盐的还原电位是 -370mV，甲醛是 -520mV——氢气还原二氧化碳非常容易。\n\n碱性热液喷口就存在这样的现象。\n\n电子究竟是如何从氢气传递到二氧化碳的？答案就在薄壁结构中，那些嵌在微孔薄壁上的硫铁矿物质。它们虽然远没有铜导线那么好用，但还是会导电。\n\n（然后作者开始把命题加强，从“碱性热液喷口可以是生命起源”也就是充分性，试图加强为几乎只能是这一路径，甚至外星生命也是。）\n\n- 橄榄石是宇宙中最丰富的矿物之一，也是星际尘埃和吸积盘（accretion disc）的主要成分之一。包括地球在内的所有行星都是由吸积盘形成的。甚至太空中也可能发生橄榄石的蛇纹岩化作用，其实就是星际尘埃的水合。\n- 另一种丰富的物质是二氧化碳。太阳系大多数行星大气中，甚至在其他星系的行星大气中也探测到了二氧化碳。\n- 在几乎所有存在水的岩石行星上，我们都能找到它们。根据化学和地质学规律，它们会形成温暖的碱性热液喷口，会在催化性微孔系统的薄壁两侧形成质子梯度。\n\n## Part II: The Origin of Life - 4. The Emergence of Cells\n\n### ===\n\n种系发生学：phylogenetics\n\n生命树只是根据核糖体次基团 RNA 的编码基因序列的相似性进行的绘制的。而用其他 48 种保守基因对 50 种细菌和古菌进行绘制的话，每一种都会得到一个不同的树图，仅有的共性是源头细菌和古菌的分叉，以及末尾的分类。\n\n细菌和古菌的共性在于转录和翻译过程用到的酶、ATP 合成酶.\n\n差异在于 DNA 复制过程中用到的酶、细胞膜磷脂的手性。\n\n靶观点包括：\n\n- 共同祖先同时拥有两套，细菌和古菌各自丢弃了其中一套。\n- 细菌继承了共同祖先，古菌为了适应极端环境独立进化出一套，取代了共同祖先的。\n\n作者认为：共同祖先并不能执行不同之处所完成的生物功能，依赖环境提供，然后细菌和古菌各自进化出一套方案，取代从环境中获取。\n\n### The rocky road to LUCA\n\n乙酰辅酶A 是生物世界六条固定碳元素的化学反应途径种，唯一同时存在于细菌和古菌的。\n\n酶本身很复杂，但是活性基团只是乙酰基，而酶基团替换成甲基的硫代乙酸甲酯，可能在碱性热液喷口附近合成，催化剂是金属矿物，质子梯度由量转换氢化酶 (Ech) 驱动反应。\n\n### The problem of membrane permeability\n\n今天的细胞膜对质子来说几乎不可渗透。\n\n早期的细胞膜并非如此。这导致了古细胞无法自己生成质子梯度，但是古细胞直接利用碱性热液喷口的质子梯度。但是直接利用反而要求膜对质子的透过率要高，进化上不支持不可渗透膜的出现。\n\n解释利用到水合氢离子和钠离子的尺寸相似性，共同祖先有一个反向转运蛋白 (antiporter)，用一个钠离子交换一个水合氢离子跨膜，且钠离子也可以驱动质子泵。这使得古细胞可以扩大生存范围，在质子梯度不那么大的地方，自发产生质子梯度和膜的选择性就有了演化优势。\n\n### Why bacteria and archaea are fundamentally different\n\n产乙酸菌（细菌）与产甲烷菌（古菌）都利用氢气和二氧化碳制造乙酰辅酶A，通过电子歧化 (bifurcation) 机制。\n\n细菌和古菌的区别，就在于反向转运蛋白运输钠离子和质子的方向。\n\n- 始组细胞中，天然质子流从外向内通过 Ech，用来还原铁氧环蛋白，铁氧环蛋白还原二氧化碳。\n- 细菌也就是产乙酸菌的祖先，倒转了 Ech 的方向，铁氧环蛋白被用来泵出质子，借电子歧化还原二氧化碳。\n- 古菌也就是产甲烷菌的祖先，演化出一个新的质子泵来运出质子，仍用质子通过原来的 Ech 还原铁氧环蛋白，铁氧环蛋白还原二氧化碳。\n\n两个分类的生物独自维持质子梯度，导致膜的选择性也是独立演化出来的，磷脂分子的结构不同，细胞壁的成分也不同，（取决于细胞膜的性质。）\n\n## Part III: Complexity - 5. The Origin of Complex Cells\n\n### ===\n\n细菌和古菌的基因多样性高于真核生物，但是结构复杂性不如：体积、基因组尺寸\n\n具体的结构限制众说纷纭：\n\n- 失去细胞壁：卡瓦里耶·史密斯假说。但是蓝藻和蓝细菌都有细胞壁，复杂度依然差很多\n- 没有棒状染色体：环状 DNA 只能在一个地方开始复制，棒状有多个启动子平行工作。但是无法解释为什么没有演化出棒状，且有些细菌有棒状，依然结构简单。\n- 作者支持的观点：线粒体提供的能量\n\n### The chimeric origin of complexity\n\n大约 1/3 真核基因在细菌和古菌种都有等价基因，但是对应的物种在演化树中的位置却相距甚远。其中 3/4 来自细菌，1/4 来自古菌。\n\n而那些没有对应的基因，被称作真核生物的“识别基因”。\n\n- “古老真核假说”的解释：这些识别基因才是古老的基因，始于生命诞生之初，与其他分支之间的相似性随着时间流逝而磨灭。推论为真核细胞通过某些机制收纳了原核细胞的基因，比如内共生。然后是作者的反对：\n    - 先是诉诸动机，略幼稚。\n    - 化石记录中的原核生物比真核生物古老。现在没有找到内共生线粒体之前的真核生物。\n- 作者支持的解释，“嵌合起源假说”：\n    - 真核生物的识别基因还是来自细菌或古菌的某些祖先基因，只是因为它们演化得比其他基因快得多，对新功能的适应性演化完全抹去了过往历史，所以失去了与原核生物亲缘基因序列的相似性。\n    - 真核生物的细菌型基因来源远不仅限于 α-变形菌。根据已有研究的大致推测，至少有25种不同的现代细菌类群都为真核生物提供过基因。\n    - 在生命树的真核生物分支内部，所有这些细菌型和古菌型基因都一起分支、共同演化。很明显，真核生物在演化早期就获得了它们。排除了在真核生物的后续演化途中，持续的水平基因转移的影响。\n    - 一下子就从原核生物身上取走了好几千个基因，然后就再也没与原核生物进行过基因交流。对此最简单的解释，不是细菌式的水平基因转移，而是真核细胞式的内共生。\n\n引出“嵌合起源假说”的困难：曾经有25种不同的细菌和7～8种不同的古菌在演化早期全体参与了一场基因乱交派对，或者说一次单细胞生物的共生狂欢节，而在之后的整个真核生物史中，彼此都不再联系，这实在令人难以置信。\n\n- 很可能是真核生物最初是从某一群细菌中一次性获取了大量基因，而这群细菌后来慢慢分化了。剩下的仍然保持细菌的自由生活，在之后的15亿年间奉行水平基因转移，四处散播它们的基因。所以，它们古老的基因组合，现在会散布在很多现代细菌种群之中。\n\n### Why bacteria are still bacteria\n\n为什么它会永远保留在所有的细菌、古菌和真核生物身上。难道没有某种生物丢弃化学渗透偶联，代之以其他更好的能量机制？没有这种可能吗？\n\n有，发酵作用。速度较快，对原料的使用效率却较低。是生物除了化学渗透偶联之外唯一已知的产能方式。\n\n化学渗透偶联的好处：\n\n- 第二章\n- 灵活，它像是一个公共操作系统，支持多种电子供体和受体即插即用，还允许小范围的改装来产生更好的效果。\n- 相关基因可以通过水平基因转移在种群之间交流，就像把新的应用程序安装到其他兼容系统中。\n- 可以从任何环境中挤出最后一滴能量。它让细胞可以把能量“零钱”储存起来。如果需要10个质子才能合成1个ATP分子，而某个化学反应释放的能量只够泵出4个质子，那只需要把反应重复3次，泵出12个质子，再抽出其中10个就可以用来制造ATP分子。\n\n化学渗透偶联的其他作用\n\n- 摄取营养和排泄废物；\n- 转动细菌的鞭毛（一种可以旋转的外部推进结构），让细菌可以自由运动；\n- 被故意耗散，用来产生热量，褐色脂肪细胞就会这样做。\n- 质子梯度的崩解还被用来启动细菌种群突然的“程序性死亡”。\n\n### Energy per gene\n\n比较生物的能量水平的方法\n\n- 平均每克物质的能量。一克细菌和一克真核细胞的代谢率（测量标准是氧气的消耗速率，又称呼吸速率）：细菌的呼吸速率通常比单细胞真核生物更快，平均快3倍左右。\n- 单个细胞的代谢率：50种细菌和20种单细胞真核生物，这些真核生物的平均体积大约是细菌的15,000倍⑥。已知它们的呼吸速率是细菌的1/3，那么平均每个真核生物细胞每秒消耗的氧气是细菌细胞的5,000倍。真核细胞的体积比细菌大得多，DNA也多得多。即便如此，单个真核细胞还是比细菌细胞多了5,000倍的能量。\n- 每个基因平均分配到的能量：真核生物比原核生物基因多了1,200倍。\n- 把细菌基因组的大小（5,000个）比例放大到真核生物基因组的水平（20,000个），那么细菌每个基因的平均能量：就只有真核生物的1/5,000。\n    - 真核生物要么能够负担比细菌大5,000倍的基因组，\n    - 要么能为每个基因的表达提供比细菌多5,000倍的能量\n- 把细菌放大到真核细胞的大小，虽然ATP合成增加了625倍，但能量开销却增加了15,000倍，每份基因拷贝的平均能量反而减少为原来的1/25。另外，细菌和真核细胞每基因平均能量差距本来就有5,000倍之大（校正基因组大小之后的数字），5,000除以1/25，125,000倍！\n\n多出的能量的用途：\n\n- 单细胞生物复制DNA，大概只用到总能量的2%。\n- 80%的能量用于合成蛋白质。每连接一个肽键至少要花费5个ATP，5倍于聚合核苷酸生成DNA花费的能量。\n- 一个基因组中基因越多，合成蛋白质所需的能量就越多。这一结果可以通过简单统计核糖体数量来估算。（我有疑问）\n    - 大肠杆菌这样的普通细菌，平均有13,000个核糖体；而一个肝脏细胞至少有1,300万个核糖体，数量是细菌的1,000～10,000倍。\n    - 大肠杆菌这样的普通细菌，平均有13,000个核糖体；而一个肝脏细胞至少有1,300万个核糖体，数量是细菌的1,000～10,000倍。\n\n不只来自于尺寸的差异——把细菌的体积也增大到真核生物的平均尺寸，再来计算每个基因要花费多少能量。\n\n- 更大的细菌可以制造更多的ATP\n- 更大的体积也需要合成更多的蛋白质，消耗更多的ATP\n- 如果存在大如真核细胞的细菌，它每个基因的平均能量，比起同体积的真核细胞只有二十万分之一\n    - 表面积与体积之比：细菌放大到真核生物的尺寸，它的细胞半径会增加25倍，表面积则会增加625倍。\n    - 小小的基因组孤单地坐在细胞核里，现在要负责生产数量暴涨625倍的核糖体、蛋白质、RNA和脂质，还要在扩张到这么大的细胞空间内运送它们，成果仅仅是与从前一样的每单位面积ATP合成速率\n    - 要让蛋白质合成增长625倍，就需要625份完整的细菌基因组，而且每个基因组都以同样的方式运作。这样对每个细菌成员来说，每个基因的平均能量就跟原来一模一样。\n    - 真实情况比这还糟糕。细胞的内部空间现在可是放大了15,000倍。内部的代谢活动还没有明确；我们把它当成空白处理，即能量需求算成零。\n        - 这个放大的细菌就无法与真核细胞相提并论，毕竟真核细胞不只在体积上增加了15,000倍，其内部还充满了各种复杂的生化机器。\n        - 把细菌放大到真核细胞的大小，虽然ATP合成增加了625倍，但能量开销却增加了15,000倍，每份基因拷贝的平均能量反而减少为原来的1/25。另外，细菌和真核细胞每基因平均能量差距本来就有5,000倍之大（校正基因组大小之后的数字），5,000除以1/25，125,000倍！\n    - 例证：巨大的细菌\n        - 刺骨鱼菌（Epulopiscium）是一种厌氧菌，仅见于刺尾鱼（surgeonfish）后肠的无氧环境中。狭长身躯大约有半毫米长，肉眼可见。刺骨鱼菌的基因组拷贝多达20万份\n        - 嗜硫珠菌（Thiomargarita），还要更大，是直径接近一毫米的球菌，其绝大部分体积都由一个硕大的液泡占据。单独一个嗜硫珠菌可以长到果蝇的头那么大！嗜硫珠菌细胞虽然大部分都是液泡，也有18,000份基因组拷贝。\n        - 两种巨型细菌的基因组位置都很靠近细胞膜，分布在细胞膜内侧附近（图23）。细菌的中心位置则没有什么代谢活动。\n\n### How eukaryotes escaped\n\n细菌受到的严重能量限制倒是强有力的证据，可以证明嵌合起源是复杂生物诞生的必备条件。只有两种原核生物之间的内共生作用，才能打破加在细菌和古菌身上的能量桎梏。\n\n巨型细菌面临的问题是，要保持巨大的形态，它们就必须把整个基因组复制上几千次。但是一旦复制出来，基因组就无所事事了。细胞内所有的基因组是彼此完全一样的拷贝。即使有些细微的差异，也不受制于自然选择，因为它们不是自我复制的个体。同一细胞内的众多基因组即使出现变异，经过数代之后，也会像杂音一样消失。\n\n环境在不断改变，刚才那个无用的基因，现在可能又有用了。缺了它，细菌现在无法生长，除非通过水平基因转移再次获取。这种丢弃又重新获取基因的动态变化不断轮回，主导了细菌种群的构成。\n\n随着时间推移，细胞基因组的大小会逐渐稳定为一个最小可行的尺寸，而单个细菌可以随时从一个大得多的宏基因组（metagenome，整个种群的基因总和，另外还包括可以进行基因交流的亲缘种群）中获取基因。\n\n- 几代之后，快速分裂的细菌就会在种群中取得绝对优势。要获得生长速度上的微小优势，一个办法就是从基因组中丢弃一些DNA\n- 环境在不断改变，刚才那个无用的基因，现在可能又有用了。需要通过水平基因转移再次获取。\n- 一个大肠杆菌有4,000个基因，但大肠杆菌的宏基因组大概有18,000个基因。\n\n细菌类的内共生体，丢弃不必要基因的细菌会稍许提高复制速度，逐渐成为主流。关键差异在于环境的稳定性。不需要的基因就永远都不需要了。内共生体可以永久性地丢弃它们，基因组单向萎缩。\n\n内共生体的共同发展趋势，都是丢弃自己的基因。\n\n- 原核生物没有吞噬作用，无法吞噬其他细胞，所以它们之间的内共生作用很罕见。但是在细菌中，我们又确实知道几个现成的例子（图25）。这些例子的意义很清楚：原核生物的内共生确实会发生。\n- 几种真菌也有内共生体，虽然它们和细菌一样，也不会进行吞噬作用。\n- 而那些真正有吞噬作用的真核生物经常包含内共生体，已知的例子就有好几百个。\n\n最小的细菌基因组通常都发现于内共生体。\n\n- 比如立克次体，这种曾经毁灭了拿破仑侵俄大军的斑疹伤寒病原体，其基因组只有100万个碱基对，不到大肠杆菌基因组的1/4。\n- 另一种细菌Carsonella是寄生于木虱科昆虫的内共生体，它有目前已知最小的细菌基因组，只有大约20万对碱基，比某些植物的线粒体基因组还小。\n\n内共生体丢弃基因的好处：\n\n- 加快复制，还可以节省ATP。\n    - 考虑5% 的能量节省，理论上每秒省下的58万个ATP可以用来制造4.5微米长的肌动蛋白。\n    - 人类以及所有动物的线粒体，只保留了13个能够编码蛋白质的基因。经过漫长的演化，它们的基因组丢弃了99%以上的基因。\n- 这些多余的DNA片段，就成为真核生物演化的基因原材料。与细菌相比，最早的真核生物多了大约3,000个新的基因家族。新的基因可以被改造来执行各种各样的新功能，而且没有新增的能量开销。\n\n上述理论的问题：\n\n- 一些细菌（例如蓝细菌）会内化自己的生物能量膜，把细胞膜向内折叠成繁复的盘绕结构，这样可以大幅增加膜面积。为什么细菌不能通过这样的膜内化作用脱离化学渗透偶联的限制呢？\n- 为什么线粒体没有完全丢弃整个基因组，让能量收益达到极致呢？\n\n### Mitochondria — key to complexity\n\n线粒体为什么始终保留着一小群基因？\n\n- 也许并没有什么必然的生物物理原因，让这些基因非得留在线粒体中不可。它们之所以没有转移到细胞核中，并不是因为不行，只是因为演化史发展到现在，它们还没有转移。有些学者正努力尝试。\n- 也许如果没有保留这些基因，线粒体就不能存在。这些线粒体基因必须坚守现场，紧挨它们为之服务的生物能量膜。\n\n在战争中，“黄金控制”指中央政府，负责制定长远战略；“白银控制”指军队的指挥层，负责人员和武器调度；但战争的胜负是在战场上决定的，掌握在“青铜控制”的手中；他们是那些真正与敌军交锋的勇士。他们做出战术决定，激励手下部队，作为伟大的战士被历史铭记。线粒体基因就是青铜控制，现场的决策者。\n\n- 线粒体内膜的两侧有大约150～200毫伏的电位差，而膜的厚度只有5纳米。\n- 如果呼吸链不能好好地把电子传递给氧气（或其他任何电子受体），那就会导致类似短路的状况，即电子逃逸后，直接与氧气或氮气分子发生反应，形成反应性很强的自由基（free radicals）。\n- 如果线粒体基因被移到细胞核内，那么当发生氧气浓度改变、基质缺乏或自由基泄漏等严重情况时，线粒体几分钟内就会失去对膜电位的控制，细胞就会死亡。\n- 极少数真核生物丢掉了全部的线粒体基因，它们也失去了呼吸能力。比如氢酶体和纺锤剩体（发现于源真核生物体内、由线粒体特化形成的细胞器），一般都失去了所有基因，代价就是失去了化学渗透偶联的能力。\n- 前面讨论过的巨型细菌，总是把基因（应该说整个基因组）保留在生物能量膜旁边。\n- 拥有盘曲折叠内膜的蓝细菌。如果这些基因确实有必要留在现场才能控制呼吸作用，那么虽然蓝细菌小得多，也应该与巨型细菌一样，把整个基因组复制很多份，放在膜附近。它们的确是这样的。\n\n替代方案：利用细菌的质粒（plasmid）\n\n- 对原核生物来说，变大本身并没有优势，生产过剩的ATP也不会增加什么好处。\n- 单纯变大的第二个缺点，是细菌需要建立补给线来支持细胞内更远的代谢活动。\n\n真核生物如何突破尺寸的约束而演化出复杂的运输系统？他们的“氢气假说”认为，\n\n- 宿主和内共生体之间是一种新陈代谢的“互养”（syntrophy）关系，意义在于彼此交换生长所需的基质，而不只是能量。\n- 宿主体内的内共生体越多，就可以获得更多的基质，生长越快，内共生体的生存条件也越好。所以，在内共生作用的影响下，细胞越大越有好处\n- 内共生体开始丢弃基因时，它们对ATP的需求也随之降低。那么当所有的ADP都转换成ATP后，呼吸作用就会停止。呼吸链就会开始累积电子。演化提供了一个关键的蛋白质来救场：ADP-ATP转运蛋白（ADP-ATP transporter）\n\n## Part III: Complexity - 6. Sex and the Origins of Death\n\n### ===\n\n内共生作用促使真核生物崛起不是达尔文式的演化，因为它不是一系列逐代继承的微步改变，而是突然一跃进入未知领域，这种事件无法用标准的分支生命树图来展示。内共生作用是反向的树状图，它的树枝不是分叉，而是融合。\n\n但是内共生作用也是一次单一事件，发生于演化史中的一个时间节点，并不能一下子制造出细胞核，或者真核生物的任何其他主要特征。它的作用是触发了一系列后续事件，而这些事件的发展过程是标准的达尔文式演化。所以，并不是说真核生物的起源不符合达尔文式的演化理论。我认为，原核生物之间的单一内共生事件，使自然选择的整个场景彻底改观。\n\n理查德·戈尔德施密特（Richard Goldschmidt）提出的演化假说.\n\n以内共生作用为起点，立即对各个事件的发生顺序产生了一些限制。例如，细胞核与内膜系统必定出现在内共生作用之后；演化的实际发生速度也受到了一定限制。\n\n达尔文式的演化与渐变论（gradualism）经常被混为一谈，\n\n- 渐变的意思很简单：演化不会大跨步飞跃进入未知领域。所有的**适应性**变化，都应由微小而不连续的分步构成。\n- 纯粹的达尔文式演化。但这并不等于是说，这个过程在地质时间上一定会很慢。\n\n眼睛出现于寒武纪大爆发时期，在大约两百万年之内就演化出来了。数学模型曾经计算过，某种蠕虫身上原始的感光眼点演化成眼睛，假设平均生命周期为一年，每一代形态改变都不超过1%，答案是只需要50万年。\n\n演化史上的原核生物到真核生物的空白：\n\n- 要跨越原核生物与真核生物之间艰险的鸿沟，基因组合上可行的路径并不多，大部分探索者都中途而亡。\n- 这意味着最初的种群应该很小，最初的真核生物基因都不稳定，它们在一个很小的种群内快速演化，挣扎求生。\n- 所有的真核生物都拥有众多完全相同的特征。真核生物诞生之初，生殖隔离似乎并未发生，因为所有的真核生物都有一样的基本特征，很像是一个可以互相交配生殖的种群。有性生殖。\n\n有性生殖 vs. 无性生殖\n\n- 无性生殖（即“克隆”）会导致深远的发散演化，因为不同种群内的不同突变都会累积下来。这些突变在不同的环境中接受自然选择，面对的优势和劣势也迥然各异。\n- 有性生殖在种群内部形成基因池，不断地混合匹配各种特征，从而阻止分化。\n- 水平基因转移也涉及基因重组，也会变换不同的基因组合，造成“流动”的染色体。但并不是对等交换基因，也没有细胞融合或全基因组的系统性重组。水平基因转移是零敲碎打，而且是单向的，它无法对种群中的个体特征进行各种组合，反而会造成个体之间的分化。水平基因转移盛行的结果，是同一种细菌的不同菌株之间可能有多达一半的基因都不一样\n\n**两个原核生物之间的内共生作用是否有某种特殊效应，推动了有性生殖的演化？当然是的。**\n\n### The secret in the structure of our genes\n\n真核生物有着“破碎的基因”。\n\n- 我们一度被早期的细菌基因研究误导，认为人类染色体上的基因也应该像漂亮的珠串一般，按照有意义的顺序排列。\n- 它们由好几个较短的序列组成，每一段编码蛋白质的一部分；\n- 这些编码区域之间插入了长长的非编码DNA序列，我们称之为内含子（introns）。通常都比真正的编码序列长得多。\n- 每个基因中通常都插入了好几段内含子（基因通常的定义是，编码一整个蛋白质的DNA序列）\n- 内含子也会被转录到RNA上。不过在抵达核糖体之前，RNA上的内含子就已经被全部剪切掉。靠另一种精巧的纳米蛋白小机器——剪接体（spliceosome）来执行。\n\n好处：同一个基因可以通过不同的剪接方式拼出不同的蛋白质。比如抗体\n\n来源：\n\n- Ford Doolittle “内含子早现理论”：\n    - 早期的基因因为缺少现代基因复杂的修复机制，在复制过程中一定会迅速累积许多错误。DNA的长度会决定DNA上累积的突变数量，所以只有很短的基因组才可能避开突变熔毁的命运。\n    - 假说给出的最重要一项预测，就是“真核生物最先演化出现”，因为只有真核生物才有真正的内含子。当代的全基因组测序无可争辩地显示，真核生物起源于古菌宿主和细菌内共生体。\n- 内共生体。\n    - 细菌没有“真正的内含子”，但内含子的前身必定来自细菌，更准确地说，来自细菌的基因寄生物 (bacterial genetic parasites)，正式名称是移动II型自剪接内含子 (mobile group II self-splicing introns)。\n        - 这些移动内含子很可能非常古老，在三大域生物的基因组中都存在。\n        - 而它们又与逆转录病毒不同，从不需要离开宿主基因组这个安乐窝。\n        - 它们存在于基因之间的非编码区域，而且密度很低。\n    - 剪接体并非完全由蛋白质组成，其核心是一把RNA剪刀，和移动内含子的剪刀完全一样。它们剪切真核生物内含子的方式暴露了其来源，即细菌的自剪接内含子\n        - 内含子不会编码逆转录酶等蛋白质，也不能把自己切入或切出宿主DNA；\n        - 它们不是活动的基因寄生物，而是DNA序列上的赘疣，无所事事地待在那里。\n        - 但这些已经死亡的内含子，被累积的突变完全侵蚀，衰退得不成形状，却远比那些活着的寄生物更危险，因为它们再也无法剪切自己，宿主细胞必须主动移除它们。\n        - 宿主的办法就是从它们还活着的亲戚那里征用RNA剪刀。剪接体就是一种用细菌基因寄生物改造而成的真核生物机器。\n    - Eugene Koonin 和马丁：在真核生物诞生之初，内共生体在毫无防备的宿主体内放出了一群基因寄生物。内含子的入侵扩散到整个宿主基因组，塑造了真核生物基因组的基本结构，同时也推动了真核生物某些基本特征的形成，比如细胞核。我再补充一点：性。\n\n### Introns and the origin of the nucleus\n\n许多内含子在真核生物基因组中的位置都固定不变. 柠檬酸合酶（citrate synthase）基因总是含有2～3个内含子，而且插入位置几乎总是完全一样。\n\n- 内含子是各自独立插入这些位置的，出于某种原因，这些位置受到自然选择的青睐。\n- 过去某个时刻插入了共祖的基因组，只发生了一次，并传给了所有后代。\n\n直系同源基因 (ortholog) 和旁系同源基因 (paralog):\n\n- 直系同源基因基本是继承自共祖的共同基因，在不同物种的体内执行一模一样的功能。\n- 旁系同源基因，同样来自一个共同的祖先，但那个祖先基因却在同一个祖先细胞中经历了多次复制，形成了一个基因家族。\n\n旁系同源基因家族区分为“古老”和“近代”两类:\n\n- 古老：存在于所有真核生物中，但没有在任何原核生物中分化复制过的基因家族\n- 近代：只有在某些特定的真核生物种类中才有的基因家族，比如动物或植物。这种基因家族内的复制发生得较晚，是在那个特定生物种类的演化中发生的\n\n与近代旁系同源基因相比，古老旁系同源基因中内含子的位置应该更不规则。基因组测序的分析结果表明，库宁的预测非常准确。\n\n为什么这些内含子在细菌和古菌体内受到严格的控制，在真核生物细胞内却大肆扩散呢？\n\n- 最早的真核细胞（其实那时基本上还是个原核细胞，底子是一个古菌）基因组遭到了细菌内含子的大轰炸。如果宿主体内有很多内共生体，其中一个死亡并无大碍，但是这个已经死亡的内共生体，会把自己的DNA释放到胞质溶胶中。这些跳船的DNA很可能通过标准的水平基因转移方式，与宿主细胞的基因组发生重组。\n- 没有什么自然选择压力来限制早期内含子扩散。对细菌来说内含子在能量和基因方面是双重负担。真核生物的基因组可以自由扩充核基因组，正是因为内共生体的基因组不断缩小。宿主细胞并不会有计划地扩充基因组；之所以会扩充，只是因为更大的基因组也不会受到自然选择对细菌那样的惩罚。\n\n细胞核膜的出现\n\n- 剪接体还是要花几分钟时间才能切掉一段内含子。偏偏核糖体的工作速度奇快，每秒钟可以组装10个氨基酸，制造一个标准的细菌蛋白质（长度约为250个氨基酸）只需不到半分钟\n- 细胞核膜就是这道障碍，可以把转录和转译两个过程分开。\n- 这就解释了为什么真核生物需要细胞核，而原核生物不需要。原核生物根本没有内含子的麻烦。\n\n基因分析表明，宿主细胞是一个货真价实的古菌，所以它的细胞膜必然含有古菌脂质。但是，今天的真核生物膜却含有细菌脂质。\n\n- 可行性：各种细菌脂质和古菌脂质混合形成的嵌合膜，其实都是稳定的。\n- 动力：内共生体到宿主的混乱基因转移，一定包括了负责合成细菌脂质的基因。\n\n核膜产生的过程\n\n- 如果诱发某种突变，导致细菌的脂质合成速度加快，多余的脂质就会积聚形成内膜。脂质在自己合成之处附近积聚，结果是围绕着基因组形成了一堆堆脂质“小袋子”。\n- 一堆堆脂质小袋子也可以在DNA和核糖体之间临时拼凑起一道不太完美的障碍，减轻一点内含子带来的麻烦。\n- 这道障碍其实必须有缺口。完全封闭的膜反而会让RNA无法接触核糖体。\n- 核膜的形态非常符合这个假设。脂质小袋子就像塑料袋一样，可以被压扁。一个压扁的袋子，其横截面是两片紧贴且平行的膜，即双层膜结构。\n- 当细胞分裂时，核膜会散开，还原成分离的小囊泡；分裂完成后，这些小囊泡会生长并再次融合，重新形成两个子细胞的核膜。\n- 所有这些部件都由嵌合来源的蛋白质组成，一些蛋白质由细菌基因编码，少数由古菌基因编码，剩下的编码基因只有真核生物才有。除非细胞核是在获取线粒体后才演化出现的，是那次基因大规模混乱迁移的后续事件，否则我们根本无法解释这种基因组合模式。\n\n### The origin of sex\n\n染色体这种排列整齐进行基因重组的现象，在细菌和古菌的水平基因转移过程中也有出现，但一般不是对等进行的。细菌只是用这种机制修复受损的染色体，或是重新纳入以前丢弃的基因。两类基因重组用到的分子机器基本相同。\n\n有性生殖的特别之处在于重组的规模和对等基因交换。有性生殖可以打破原本固定的基因组合，让自然选择可以“看见”单独的基因，把我们的特质逐个分列出来。让真核生物拥有“流动”的染色体，组合中的基因版本不断变动（同一个基因的不同版本用专业术语说就是等位基因）。\n\n设想基因排列在一条染色体上，从不进行重组。\n\n- 自然选择只能鉴别整条染色体的适应能力。假设这条染色体上有几个非常重要的基因，稍有突变就会导致个体死亡。然而，对其他不太重要的基因突变，自然选择几乎无动于衷。轻微却有害的突变会在这些基因上逐渐积累，因为它们导致的小麻烦，会被保留几个关键基因带来的重大利益抵消。\n- 积极地作用于固定基因组合，结果可能更糟。这条染色体上的其他99个基因也会搭上优秀基因的便车\n\n缺点：\n\n- 打破在特定环境中已经获得成功的等位基因组合。\n- 无性生殖的种群每一代的数量都可以翻倍；而有性生殖的种群数量，还是和原来一样（从细胞的角度来计算）。\n- 必须先找到一个配偶：雄性代价、性病、基因寄生体\n\n有性生殖在真核生物中的完全普及，远远超过了“合理”的程度。原因很可能在于，真核生物的最后共祖已经是有性生殖。\n\n萨莉·奥托（Sally Otto）和尼克·巴顿（Nick Barton）：当基因突变率很高、自然选择压力很大，以及种群中充满基因多态性时，有性生殖的优势最大。\n\n- 无性生殖，高突变率意味着轻微有害的突变累积得更快，在发生选择性清除时损失也更大。基因组大到一定程度，突变熔毁就不可避免。基因组越大，就越难通过水平基因转移获取“正确”的基因。\n- 选择压力，来自寄生感染与变化的环境。只有全基因组范围内的重组才能有效增加基因多态性。会有一些细胞的内含子插在糟糕的位置上，也会有一些细胞的内含子插在较为无害的位置。接下去，自然选择会淘汰那些最不幸的细胞。\n- 基因多态，细菌和古菌通常都有单条环状染色体，而真核生物则有多条棒状染色体。\n    - 为什么是这样？最简单的答案是，当内含子切进切出基因组时，它们可能导致染色体形状出错。\n    - 这种通过有性生殖和基因重组来累积新基因的倾向，能很好地解释早期真核生物的基因组为什么会膨胀。\n\n如何\n\n- 分离染色体：普通细菌那样用细胞膜附着染色体。细菌分离大型质粒的机制 → 真核生物细胞分裂时使用的纺锤体。早期真核生物为了分开那些杂乱的染色体，很可能沿用了细菌的质粒分离机制。\n- 细胞融合：没有在有细胞壁的细菌中发现。L型细菌因为没有细胞壁就很容易融合。早期真核祖先很可能会积极主动地进行细胞融合。尼尔·布莱克斯通（Neil Blackstone）认为，早期的细胞融合可能由线粒体推动。（但是没说线粒体是怎么做到的。）\n\n### Two sexes\n\n为什么生物的性别总是两种\n\n不是两种的好处：\n\n- 同一种性别，那我们可以与任何人交配。我们选择伴侣的机会一下子增加了一倍\n- 三种、四种都比两种更好。就算限定只能与不同性别的个体交配，那我们可以和种群中2/3或3/4的人交配，而不仅仅是一半\n\n双方都不愿意承担充任“雌性”的代价。雌雄同体的生物，比如扁形虫，交配时会竭尽全力防止自己受精。\n\n最深刻的差异之一，在于线粒体的遗传。只有一种性别会把线粒体传给下一代，\n\n而单细胞藻类尽管会产生完全相同的配子（即同形配子，isogametes），结合时也只有一个配子可以把线粒体传下去；另一个配子的线粒体会从内部被消化掉。准确地说，只有线粒体DNA会被消化掉，看来问题在于容不下这些线粒体的基因.\n\n前面说线粒体煽动了有性生殖（并没给出很坚实的证据。），但是有性生殖并不会让线粒体传播得更快，有一半的线粒体传不下去。（然后是类似《自私的基因》一书的论证，但是这种因果关系的命题，有实验验证吗，什么样的实验构成验证呢？）（而且前面似乎暗示内吞线粒体活动实际上一次吞了好多个，那么当时线粒体祖先之间的竞争岂不是更加激烈吗？）\n\n作者的新观点：线粒体基因必须适应细胞核基因。线粒体的单亲遗传原因很可能在于改善两个基因组之间的相互适应。取样效应，将每一个线粒体弄进新细胞然后繁殖，结果是一群线粒体基因不同的细胞，如果一起弄进去，结果是一堆线粒体基因十分相似的细胞。多态性 (variance)\n\n数值模拟表明，单亲遗传的基因在种群中很难扩散，更不可能达到 fixed point。突变线粒体越多，维持品质的代价越高；突变线粒体越少，单亲遗传的好处越小。\n\n如果两种交配型（出于其他原因）已经存在，那么在某些条件下，单亲遗传突变基因会在种群中趋于固定。条件就是细胞内有大量线粒体，且线粒体基因的突变率很高。\n\n交配型 vs. “真正的性别”\n\n### Immortal germline, mortal body\n\n在很多代的时间跨度上，线粒体基因的演化速度比核基因快 10~50 倍。但最早出现的动物和我们大不一样：它们的形态类似于海绵或者珊瑚，是固着型的滤食动物，不会四处移动（至少在成年阶段不会）。它们的细胞没有很多线粒体也在意料之中，线粒体基因的突变率也很低，可能比核基因的突变率更低。\n\nArunas Radzvilavicius: 多细胞生物普通细胞的分裂效果与单亲遗传相似，它们都会增加细胞之间的多态性。因为普通分裂大概率没办法完美地均分线粒体的突变基因。\n\n两种策略：\n\n- 没有特化组织，全身到处都有干细胞，随机选择干细胞成为生殖细胞。这种策略下线粒体的多态性高，作为应对，此类生物线粒体数量少，突变率低，自然选择淘汰糟糕的生殖细胞糟糕个体的后代，但是个体的品质取决于最差的器官。\n- 一开始卵细胞就有更多线粒体，这样分给多个接收者时，差异比倍增复制再分给后代更小。而精子不遗传自己的线粒体，其多态性不减少，自然选择的效果依然明显。此类生物组织类型众多，且组织之间互相依赖。生殖细胞在胚胎发育之初就被藏匿起来，应对线粒体基因的高突变率。\n\n明确的、可以验证的预测，我们很想付诸实验。首选的实验生物就是海绵和珊瑚。二者都有精子与卵子，但都没有隔离的种系。如果我们不断选择线粒体突变率高的个体，它们会发展出隔离的种系吗？\n\n突变率为何升高：物活动增加，细胞和蛋白质的物质周转量随之增加。寒武纪大爆发前夕的海洋充氧事件，催生了活跃的两侧对称动物（Bilateria）\n\n“动物隔离出专门的种系。”中的“种系”到底是什么意思？\n\n死亡是身体预先计划好的、命中注定的终点。种系的不朽在于它们可以不断分裂下去，既不会衰老，也不会死去。这些特化的生殖细胞被藏匿起来，身体的其他部分就可以为了其他的专门用途而各自特化，首次出现了不能自我再生的组织。\n\n以这个视角看待整整40亿年的生命史，线粒体就处在真核生物演化的中心。\n\n（那这样看的话，非真核细胞就没有死亡的概念？也没有种系的概念？后者显然不能成立。）\n\n## Part IV: Predictions - 7. The Power and the Glory\n\n### ===\n\n呼吸蛋白具有独特的线粒体基因与核基因双重性质，互相完美镶嵌，犹如天作之合。线粒体表面上有自主性，好像随时想分裂就分裂，但这只是假象。事实上，它们功能的正常运转依赖于两个不同的基因组。\n\n（这种嵌合难道不和之前线粒体基因的“青铜控制”的功能相矛盾吗？）\n\n氧化还原两个中心之间的距离每增加1埃，电子的传递速率就会降低为原来的1/10。\n\n有性生殖对维持大型基因组中个体基因的正常运作是必需的，而两性有助于维持线粒体的质量。这导致的意外后果就是，两个基因组的演化方式完全不同。线粒体基因的演化速度比核基因快了10～50倍\n\n（这个说法似乎意味着两性首先是由是否将线粒体（尤其是其基因）传递给后代来定义的，而不是由性染色体的不同而定义的，但是这样不意味着性染色体的出现应当和线粒体的基因转移高度相关吗，事实上线粒体转移到细胞核的基因有这种分布的集中性吗？）（后文有提到，但是并没完全解决我的疑问。）\n\n大概是演化的短视最好的例子，最简单的原因就是繁殖速度，基因组越小的细菌繁殖得越快，长此以往就会成为主流。\n\n因为一些线粒体死亡后，其DNA释放到宿主胞质溶胶中，然后被细胞核纳入。这个过程会持续进行。一些迁移到细胞核的DNA后来获得了一段导向序列，就像一个地址代码。\n\n演化中必定有一个过渡阶段，在细胞核和幸存的线粒体中同时存在同一个基因的拷贝。除了我们线粒体中保留的13个蛋白质编码基因（只占原先基因组的不到1%），都是核基因组的拷贝被保留，线粒体的拷贝被丢弃。这种明确的趋势看起来不像是随机作用。\n\n一个可能的影响因素是雄性的品质。线粒体是母系遗传，从母亲传给女儿，所以不可能对有利于男性的线粒体基因变体进行选择。男性线粒体中即使突变出有利于男性的基因，也不会传下去。\n\n另一个可能的原因是，线粒体的基因很占空间，腾出来就可以放置进行呼吸作用或其他过程的结构。\n\n### On the origin of species\n\n直接互相关联的基因改变速度大致相同，比如编码呼吸链蛋白的各个基因；而其他核基因的改变（演化）速度则慢得多。线粒体基因的变化会导致与它们互动的核基因发生代偿性改变（compensatory change），反之亦然。\n\n如果线粒体基因组与核基因组配合不佳，就是细胞程序性死亡（或称细胞凋亡，apoptosis）的触发机制。\n\n- 各个氧化还原中心塞满了电子，前几个氧化还原中心都是铁硫簇，其中的铁在高还原态下会从Fe3+变成Fe2+（被还原）。\n- Fe2+可以直接与氧气反应，生成带负电的超氧自由基O2·–。\n- 自由基数量超过一定的阈值，就会氧化附近的内膜脂质，尤其是心磷脂。造成一种呼吸蛋白——细胞色素c脱离内膜。\n- 失去了细胞色素c，电子再也无法到达呼吸链的终点，电子流也就完全中断。\n- 不会简单地分解成碎片，而是会从内部释放出一大群蛋白质刽子手：半胱天冬酶（caspase enzyme）。它们会把细胞中的DNA、RNA、碳水化合物和蛋白质等大分子切成碎片。\n- 这些碎片会用小块的细胞膜包起来，形成一个个囊泡，再喂给周围的细胞。\n\n杂种衰退：线粒体基因组与核基因组的不兼容，是否造成了物种起源中更普遍的杂种衰退现象？分化（speciation）是真核生物不可避免的发展趋势。而且它的影响有时比其他机制更明显，原因就在于线粒体基因的演化速度。\n\n道格·华莱士（Doug Wallace）认为，线粒体处于生物适应的最前线。线粒体基因的快速演变，让动物能够迅速适应食物和气候变化。这是适应的第一步，之后才是更缓慢的形态适应。\n\n### Sex determination and Haldane’s rule\n\nHaldane’s rule：异种动物相互交配产生的杂种一代中，如果有一种性别缺失、稀少或者不育，那么这一性别就是杂合（heterozygous，也可称为异配heterogametic）性别。解释：性选择对雄性的影响更大，无法解释为什么雄鸟反而比雌鸟更不容易受到杂种衰退的影响。\n\n寄生虫、染色体数量、荷尔蒙、环境因素、压力、种群密度，甚至线粒体，都可能决定动物的性别。有更深刻的机制在发挥作用。决定性别的具体机制如此多样，而两种性别的发育又如此一致，这也意味着性别决定（即雄性发育或雌性发育的过程）应该有一个非常根本的共同基础，不同基因的作用只是表层的细节点缀而已。\n\n厄休拉·密特沃克（Ursula Mittwoch）提出性别由代谢率决定的假说。\n\n人类Y染色体上的SRY基因，能够提高这些生长因子活性的突变，就能诱发性别转换，让本来没有Y染色体（或SRY基因）的雌性胚胎发育成雄性。相反，降低生长因子活性的突变会有反向的效果，会让Y染色体功能正常的雄性胚胎发育成雌性。\n\nHaldane’s rule 和性别由代谢率决定的假说的关系：\n\n不育和无法存活都代表着某种功能缺陷。能量需求最高的细胞会最先发生能量短缺而死亡。这正是线粒体疾病的问题所在。假设两个细胞有相同的线粒体，如果两个细胞的代谢需求不同，那么结果也会不同。生物的大脑很大但不可替换能留下更多的健康后代，那当然它们就会繁衍兴盛。只有当生殖细胞与体细胞存在根本差异时，自然选择才能这样运作。但这也意味着肉体成了用完即弃的载体，寿命有限。最终，那些无法满足自身代谢需求的细胞会终结我们的生命。\n\n以上的探讨，为霍尔丹法则提供了一个简单明快的解释：代谢率最快的性别，最容易发生不育或者无法成活。\n\n例子：谷物害虫赤拟谷盗与近亲物种弗氏拟谷盗杂交、植物中细胞质雄性不育、果蝇胞质杂合细胞、雌鸟必须精心挑选交配对象\n\n### The threshold of death\n\n光靠堆积线粒体无法提升功率。有氧运动的肌肉中，最优化的空间分配大概是肌原纤维、线粒体和毛细血管各占1/3。拥有强大有氧代谢能力的代价，就是低生殖力。\n\n我把死亡的门槛提高，也就是说，我可以承受更多的自由基泄漏，而不至于启动凋亡。生殖力得到提升。问题是，我需要为此付出什么代价？后代只有很小的概率能拥有完美匹配的线粒体与核基因，这会直接导致另一种利弊平衡：适应性与疾病。\n\n作者怀疑，导致早期隐性流产中很大一部分是缘于线粒体－核不兼容。\n\n高死亡门槛会带来一个间接的，然而也是终极的代价：更快地衰老，以及更容易罹患各种老年病。\n\n### The free-radical theory of ageing\n\n自由基老化理论，源于20世纪50年代的辐射生物学（radiation biology）研究。电离辐射可以分解水分子，生成各种高反应性的“碎片”，带有一个不成对的电子，这就是氧自由基。\n\n蕾韦卡·格施曼（Rebeca Gerschman）、丹汉姆·哈曼（Danham Harman）等研究者都认识到：在线粒体中，同样的自由基可以直接从氧气产生，不需要电离辐射。\n\n一些著名的科学家，尤其是诺贝尔奖得主莱纳斯·鲍林（Linus Pauling），都相信抗氧化剂的神话；他们采取超量维生素C疗法，每天吃好几匙的剂量。\n\n《自由基生物学和医学》（*Free Radicals in Biology and Medicine*），这是哈利维尔（Barry Halliwell）和古特利基（John Gutteridge）编写的经典教科书：“到90年代已经很清楚，抗氧化剂绝不是抗衰老和疾病的万灵药。只有边缘保健产业还在推销这种观念。”\n\n研究人类的衰老过程时，我们没有测量到线粒体的自由基泄漏出现任何系统性的增加。线粒体的突变数量会有少许增加，但除了极少数组织区域，整体的突变比例低得惊人，远低于可能引发线粒体疾病的程度。促氧化剂（pro-oxidants）反而能够延长动物的寿命。\n\n安东尼奥·恩里格斯（Antonio Enriques）与同事通过细胞培养实验表明，使用抗氧化剂阻断自由基信号相当危险，可能会**抑制**ATP合成。看来，自由基信号可以通过增加呼吸蛋白复合体的数量来加强线粒体的呼吸能力，从而分别优化每个线粒体中的呼吸作用。\n\n自由基信号的根本意义在于：线粒体现在有问题，呼吸能力低于任务需求。\n\n雷蒙德·珀尔（Raymond Pearl）的“生命率理论”（rate-of-living theory）：代谢率较低的动物（通常是大象等大型物种），一般比代谢率较高的动物（比如大鼠和小鼠）寿命长\n\n\u003e 自由基老化理论原始的假设认为，自由基是呼吸作用不可避免的副产物，参与呼吸作用的氧气中大约有1%～5%一定会转化为自由基。\n\u003e \n\n所有传统实验测量的都是细胞或组织暴露在大气氧浓度下的情况，这个浓度远高于体内细胞接触到的实际氧气浓度。因此，实际的自由基泄漏速率可能比测量值低好几个数量级。\n\n其次，自由基泄漏**不是**呼吸作用中不可避免的副产物，而是故意释放的信号；而自由基的泄漏率，在不同物种、不同组织、每天的不同时间、不同的荷尔蒙状态、不同的热量摄取、不同的运动水平之间都存在天壤之别。\n\n真正的相关性，其实是在自由基泄漏与寿命长短之间。\n\n衰老过程中，一些线粒体确实会发生突变，让细胞中的线粒体种群成为不同类型的混杂，有些与核基因配合较好，有些较差。想想这会带来什么问题。最不兼容的线粒体通常会泄漏最多的自由基，因此自我复制的拷贝会多于其他线粒体。\n\n（自由基能在细胞内扩散吗？如果能的话，难道不是能促进所有的线粒体复制更多份吗？）\n\n细胞要么死于凋亡，比如脑细胞或心肌细胞的情况呢？这个组织会逐渐流失，这些变化都是衰老的表现。\n\n细胞没有死于凋亡，我们就能在“年迈”的细胞中发现线粒体突变的累积。经常引起慢性发炎和生长因子失调。刺激附近本来就有生长倾向的细胞，发展成癌症。\n\n一定程度的卡路里限制和低碳水化合物饮食，才能有效延缓衰老。它们都会促进生理压力反应（就像促氧化剂的作用），能够清除一些有缺陷的细胞和线粒体，短期内有利于生存，不过，代价通常是降低生育力。\n\n我们的祖先增加了有氧代谢能力、降低了自由基泄漏、降低了生育力，但同时延长了寿命。\n\n## Epilogue: From the Deep\n\n明神海丘附着在深海热液喷口附近的多毛纲蠕虫。蠕虫身上的微生物——其实只是其中一个细胞。明神海丘准核细胞（*Parakaryon myojinensis*）\n\n如果他们进行了全基因组测序，甚至只需要测定核糖体RNA的特征，就能更深刻地揭示这个细胞的真实身份，也能让这篇被人忽视的科学文献，变成《自然》级别的高影响力论文。但是他们把唯一的样本做成了电子显微镜切片。\n\n- 一个经过高度演变的真核细胞，为了适应不寻常的生活方式改变了正常的构造，才能寄生在深海热液喷口的蠕虫身上。但这种可能性不高。因为很多其他的细胞都在类似的环境中生存\n- 真的是一个中间型活化石，“真正的源真核生物”，不知如何能幸存至今。在稳定的深海环境中，它无法演化出现代真核生物的特征。它们并不是生活在从不改变的环境中，而是依附在多毛纲蠕虫的背上。蠕虫在真核生物演化初期显然尚不存在于世。\n- 这是一个原核细胞，获取了一些内共生体，正在变成一个类似于真核生物的细胞，重演演化之路。\n\n## 主线之外\n\n### 一些散碎知识\n\n核糖体在细胞器中算小的，人体肝脏细胞中约有千万量级，但从分子角度层面，这个结构又是一个大分子。\n\n核糖体的转录错误率大概在 1/10,000.\n\n古菌 archaea 并不比细菌更古老，反而比细菌更接近真核生物\n\nAllen Telescope Array 射电望远镜阵列在北加州的山地上，听命于旨在搜寻地外生命的 SETI\n\n人类 DNA 约有 30 亿字母，其中编码蛋白质的只有 2%，更大一部分用来进行基因调控，剩下的部分其功能科学家正在争论。\n\n洋葱、小麦、阿米巴虫的基因数量和DNA量比人类多。不同种类的两栖类的基因量跨度在两个数量级，其中能达到人类的 40 倍。\n\nLife, as biochemist Albert Szent-Györgyi observed, is nothing but an electron looking for a place to rest.\n\n从单细胞到多细胞反而有大约 30 个独立的进化路径。\n\nSuch ideas trace their roots back to the early twentieth century to Richard Altmann, Konstantin Mereschkowski, George Portier, Ivan Wallin and others, who argued that all complex cells arose through symbioses between simpler cells.\n\nmitochondria derive from α-proteobacteria; chloroplasts (the photosynthetic machinery of plants), deriving from cyanobacteria.\n\n端粒：telomeres\n\nEukaryotes have ‘genes in pieces’, in which short sections of DNA encoding proteins are interspersed by long non-coding regions, called introns.\n\n### 一些时间段/时间点\n\n- `5亿年`/`距今40亿年前`：地球产生 5 亿年后，生命出现：\n- `20亿年`：生命体的形态 (morphology) 进化停留在细菌水平\n- `距今15—20亿年前`：动、植物、单细胞原生生物的共同祖先出现\n- `5亿年前`：Cambrian explosion\n- `1670s`/`距今约350年前`：细胞被 Antony van Leeuwenhoek 发现\n- `1677`：Robert Hooke 也观察描述了细胞\n- `1944`：Schorödinger 发表 What is Life：1. 生命对抗熵增。2. 生命成功的关键在基因\n- `1953`：Crick \u0026 Watson 的第二篇 Nature 文章：\n- `1958`：Francis Crick 提出研究氨基酸序列信息 “protein taxonomy”\n- `1961`：Peter Mitchell 提出跨膜质子泵具有类似水库的储能作用。\n- `1967`：Lynn Margulis 提出线粒体和叶绿体的 endosymbiosis 假说\n- `1960s—`：Carl Woese 对比不同物种的编码同一核糖体结构的基因序列\n- `40年前`：Jacques Monod 写作了 Chance and Necessity\n- `1998`：Bill Martin 提出复杂生命来自 archaeon 宿主和后来成为线粒体的细菌的内共生合并。\n- `2011`：Lynn Margulis 去世\n\n\n# 讨论手记\n\n\n## June 7, 2024 ’s discussion: prologue to chapter 1\n\nmitochondrion 的基因组在内膜里面，我本以为在外面。在外面意味着假设今天的线粒体外膜是细胞膜，从而他自己有膜结构。在里面意味着内膜才是原始生物的细胞膜，线粒体外膜是胞吞时的囊泡膜。（维基百科有图）\n\n叶绿体 DNA 在什么位置？第 5 章有提及\n\nEndosymbiosis 在实验室里就能做。我问实验条件是否严格，是想问在自然环境下有多大概率发生，感觉老板答非所问。\n\nEndosymbiosis 之后的问题才更难，两个分裂周期不同的指数生长的生命体，怎么同步生长率，等等类似问题。\n\n老板推荐的文章：[https://www.cell.com/cell/pdf/S0092-8674(24)00182-X.pdf](https://www.cell.com/cell/pdf/S0092-8674(24)00182-X.pdf)\n\nSex: [https://www.pnas.org/doi/full/10.1073/pnas.1501725112](https://www.pnas.org/doi/full/10.1073/pnas.1501725112); 自然环境中的酵母细胞一般是二倍体，实验室用的单倍体是特殊处理后获得的。\n\ncell wall, 把老板难住了。（好像在第三章提到了）\n\n我们的呼吸作用前几步发生在细胞质，而这在需氧细菌中发生在它们的细胞质，等效于我们的线粒体内膜里面，把老板难住了\n\n被认为内吞线粒体的那种 archaea 最近在实验室养殖出来了。有 histone \n\n## June 14, 2024’s discussion: chapter 2 \u0026 3\n\nATP ↔ ADP+Pi 的好处已经说了，可以灵活地耦合在各种 ΔG \u003e 0 的化学反应里，使总反应的 ΔG \u003c 0，让他们在热力学和动力学上可行。\n\n为什么一定要用 ATP ↔ ADP+Pi？\n\n我们讨论的结果是理想状况下其实可以不一定用（比如燃料电池的电极），但是—— the ability to wait, and the tolerance to environmental noise\n\nGeometry problem：这些反应总需要一定的空间来发生，（还是说需要电极/天线来耦合不同的反应。）\n\n## June 21, 2024 discussion: chapter 2, 3 again\n\n老板的观点：DNA 或者遗传信息分子基本上充当了电池的作用，里面的信息转化成了自由能，驱动了细胞的能量变化的各种活动。\n\n我的问题：逆向的中心法则，在原始细胞里，先有原始的 DNA 分子，进行信息→物质的或称\n\nPURE system in a test tube\n\nError catastrophe of replicators in a German journal\n\n## July 5, 2024 discussion: chapter 4\n\nStarted with Figure 19.\n\nHuge debate of ocean mixing\n\na prediction: modern does not have a proton gradient, but would a sodium ion gradient compensate this lack of gradient?\n\npage 151 on english pdf\n\n## July 19, 2024 discussion: chapter 5\n\ncell cycle synchronization\n\npathways only exists in bacteria but not eukayrote, TSZ\n\ncytoskeleton are also found in bacteria\n\nescort machinery,  membrane shape in complex cells are also found in archae\n\ntriangle archae\n\nforward scattering to select vell size\n\nheteroplasmi\n\nlocalization tags onto pop tags\n"},{"slug":"reduction-potential-and-electronegativity","filename":"2024-06-16-reduction-potential-and-electronegativity.md","date":"2024-06-16","title":".tex | 还原电位和电负性","layout":"post","keywords":["tex","chem","bio"],"hasMath":true,"cover":"2024-06-16-chemical-battery.png","excerpt":"最近在看 Nick Lane 的《The Vital Question》，里面提到了“还原电位”的概念，就此复习一下高中学过的电化学知识。","content":"\n\u003e 最近在看 Nick Lane 的《The Vital Question》，里面 Part II, Chapter 3, proton power 一节讲地球上早期生命的生化反应的时候，提到了“[还原电位](https://zh.wikipedia.org/wiki/%E8%BF%98%E5%8E%9F%E7%94%B5%E4%BD%8D)”的概念，就此复习一下高中学过的电化学知识。\n\u003e \n\n### 还原电位 (Reduction Potential)\n\n维基百科：\n\n\u003e **还原电位**是**氧化还原电位 (Redox potential)** 的一种，指的是电活性物质发生电还原反应时的[电极电位](https://zh.wikipedia.org/w/index.php?title=%E7%94%B5%E6%9E%81%E7%94%B5%E4%BD%8D\u0026action=edit\u0026redlink=1)。\n\u003e \n\n（对应地，氧化电位就是电活性物质发生电氧化反应时的[电极电位](https://zh.wikipedia.org/w/index.php?title=%E7%94%B5%E6%9E%81%E7%94%B5%E4%BD%8D\u0026action=edit\u0026redlink=1)。）\n\n所以这一性质针对于一种反应物分子（个别情况下也可以是原子和离子，这些原子和离子也是化学反应的基本参与者，所以从语文上来讲也“是”分子）。\n\n这不是某一元素的性质。但是在一个还原反应中，真正化合价减少的一般也只有其中的一种元素的原子，所以也可以侧面体现出相应元素（高中好像管这个叫呈价元素是吧，记不清了）的性质。\n\n这也不是某一特定化学反应的性质。所以还原电位的测量是把待测的分子加入标准溶液中，在标准温度、标准气压下，测量标准电极的电位。\n\n\u003e Each species has its own intrinsic redox potential; the more positive the reduction potential (reduction potential is more often used due to general formalism in electrochemistry), the greater the species' affinity for electrons and tendency to be reduced.\n每种电活性物质有其特定的还原电位，还原电位值越正，代表该物质具有更强的得电子能力，即氧化性越强。\n\u003e \n\n于是就想起来高中的时候念“氧化剂被还原，得电子，化合价降低，发生还原反应，生成还原产物”的经，然后把氧化/还原、得/失、降低/升高对换再念一边，然后加上原电池/电解池、正极负极/阴极阳极再念两遍……\n\n然后老师就开始让你背，还说什么化学是理科中的文科，想想就头皮发麻。\n\n化学不该是这么学的。（如果你要高考还没高考的话当我没说，这么学确实是做题最快的。）\n\n拿书中的反应试着推理了一下，大约想通了还原电位和氧化性的关系。\n\n书中的反应是二氧化碳被氢气还原成甲烷，副产物是水，GPT 说这个反应叫 Sabatier reaction：\n\n$$\n\\text{CO}_2 + 4\\text{H}_2 \\rightarrow \\text{CH}_4 + 2\\text{H}_2\\text{O}\n$$\n\n还原反应：$$\\text{CO}_2 + 8\\text{H}^+ + 8\\text{e}^- \\rightarrow \\text{CH}_4 + 2\\text{H}_2\\text{O}$$，C 元素从 +4 价被还原成了 -4 价\n\n氧化反应：$$4\\text{H}_2 \\rightarrow 8\\text{H}^+ + 8\\text{e}^-$$，H 元素从 0 价被氧化成了 +1 价\n\n我把它画成了下图的形式。据书的作者说，这个反应可以在生命诞生的地方自发进行，所以这应该是一个原电池，导线中间的黑圈表示电流表。\n\n![原电池](/photos/2024-06-16-chemical-battery.png)\n\n左侧的电极周围发生的是氧化反应，会生成游离的电子。这些电子靠近电极时，进入导体的导带；右侧电极周围的还原反应正好需要电子作为反应物——电子的流向从左往右。\n\n电流方向和电子的流向相反，所以发生还原反应的右侧电极是原电池的正极，相对于氧化电极也就是电池负极，有一个正的电势差。此时反应自发进行，将化学能转化为电能。电势差正值越大，说明参与者的化学性质活泼，氧化剂的氧化性很强，被还原的趋势很大。\n\n这个例子作弊的地方在于反过来也能用，因为作者又说了，这个反应在今天的常见环境下是不能自发进行的，所以可以再画一个电解池的版本～\n\n![电解池](/photos/2024-06-16-electrolytical-cell.png)\n\n图中装置上唯一的变化就是把电流表换成了一个电源。电解液中发生的反应也不变，电子在两个的电极之间的流向也不变。\n\n但是这是一个电解池，发生还原反应电极的不再是电池的正极，而是电解池的阴极，相对于氧化电极也就是电解池阳极，有一个负的电势差。此时反应靠外来的电能驱动，电能转化成化学能。电势差的负值越大，说明参与者的化学性质越不活泼，氧化剂的氧化性很弱，被还原的趋势小。\n\n举这个化学反应做例子只是便于自己理解，还原电位的测量是在特定\n\n而且以上说法把电子当成了经典粒子，所以在量子力学的视角下不严格正确，但是图像也差不了太多～\n\n### 电负性 (Electronegativity)\n\n维基百科：\n\n\u003e 以一组数值的相对大小表示[元素](https://zh.wikipedia.org/wiki/%E5%85%83%E7%B4%A0)原子在分子中对成键电子的吸引能力，称为相对电负性，简称为电负性。元素电负性数值越大，原子在形成[化学键](https://zh.wikipedia.org/wiki/%E5%8C%96%E5%AD%A6%E9%94%AE)时对成键电子的吸引力越强。\n\u003e \n\n这个性质针对于某一种元素，不限于反应，不限于状态。\n\n好像不同的人给出了不同的定义和计算方法，感觉有点随意啊……\n"},{"slug":"information-entropy-kl-divergence-cross-entropy-mutual-information","filename":"2024-05-14-information-entropy-kl-divergence-cross-entropy-mutual-information.md","date":"2024-05-14","title":".tex | 比较两个概率分布/两条信息","layout":"post","keywords":["tex","phy","m"],"excerpt":"自信息、信息熵、KL Divergence、交叉熵、互信息","hasMath":true,"content":"\n\u003e 自鸣得意了半天，发现这篇文章基本就是维基百科 [Quantities of Information](https://en.wikipedia.org/wiki/Quantities_of_information) 词条英文版的翻译。但是对应的中文词条没有覆盖英文版那么多的内容，所以也不完全是无用功。\n\u003e \n\n## 信息和概率\n\n一条信息由一个命题来表达。（这一个命题可以是对多个命题进行逻辑演算的一个表达式。）\n\n而这个命题解答了人心中的某个疑问。既然这是个疑问，那么在得到确切的信息之前，有众多其他命题，和这条消息一样有可能是问题的答案。既然是有可能，那就是概率论可以派上用场的对方。所有这些可能成为答案的命题一起，构成一个随机变量空间。\n\n比如说一道有 ABCD 四个选项的选择题，如果是单选题，那么答案的随机变量空间就是 {A, B, C, D}，如果是多选题，则是 {A, B, C, D, AB, AC, AD, BC, BD, CD, ABC, ABD, ACD, BCD, ABCD}，如果是排序题、不定项排序题、答案出错了的题……\n\n## 描述一个概率分布的信息量\n\n### 自信息：Self Information\n\n自信息是一个随机事件的性质，也就是针对一个随机变量的**某一个可能取值**而言的。表达式为 \n\n$$\nI(m) = -\\log_n\\left(p(M=m)\\right)\n$$\n\n这是一个无量纲量，但是公式中指数的底数可以任意选择——\n\n- *n* = 2 的时候自信息的单位是 bit，也叫香农 (shannon), 这里的 bit 和二进制位 bit 不完全相同，一个香农是一个二进制位所能表示信息的**上限**：当一个二进制位完全取决于其它位时，这个位不包含任何额外信息，香农数为 0，但这个二进制位依然物理上存在；\n- *n* = *e* 的时候单位是 nat, 因为 $$\\log_e\\equiv\\ln$$ 叫做自然对数；\n- *n* = 10 的时候单位叫 hartley\n\n——单位之间的换算关系由对数的换底公式给出。\n\n这个量在信息论中的意义是，这条消息作为一个不方便问的问题的**答案**，**最少可以**用多少个 n 个选项的单选题套出答案。当 n=2 的时候，每个问题就是一个是非题，也就是一般疑问句。\n\n码农面试的时候经常问一类问题：一堆看起来相同的东西里面有一个不一样，你有一种不能直接测出答案的测量工具，最少需要测量几次才能辨别出来……但是自信息的计算不能提供具体的辨别方法，具体方法还是需要你自己去凑，而面试刷人很多都是在刷这种细枝末节。\n\n当然了，前提是你的面试官懂他自己在问什么，而不是相信美剧《硅谷》里压缩算法可以突破信息论极限的计算机民科～\n\n当 *p* = 0 时，自信息发散为无穷大。不过问题不大，原因在下一节。\n\n### 信息熵：Entropy\n\n信息熵是一个随机变量的概率分布的整体性质。\n\n算法很简单，就是自信息的概率期望，也就是按照随机变量每个取值的概率加权平均：\n\n$$\nS(p(M))=\\mathbb{E}_p[-\\log_n p(M)]=-\\sum_{m\\in M}p(m)\\log_n p(m)\n$$\n\n当 *p* = 0 时，自信息发散，但是概率为零，强行定义两者的积也为零，对信息熵不构成贡献。\n\n当我们只对某一特定的随机事件信息感兴趣，除此以外的所有事件合并为目标事件的补集，就得到二项信息熵 binary entropy:\n\n$$\nS_{binary} = -(1-p)\\log(1-p)-p\\log p = p\\log\\frac{1-p}{p}-\\log(1-p)\n$$\n\n沿着自信息的意义往下走，信息熵在信息论中的意义是，一个将众多信息/命题的集合作为备选答案的**问题**，**最少可以**用多少道 n 个选项的单选题的集合来等价替代。\n\n当这些最优的单选题确定之后，原问题的每一个选项，可以用单选题的答案序号来进行编码。指数的不同底数/信息量的不同单位就是数字的 n 进制，信息量就是相应进制下最大压缩编码后的位数。\n\n当然要讨论压缩的话，还需要另找地方记录各个单选题和选项，也就是压缩字典。\n\n## 比较两个概率分布的信息量\n\n而如何选择单选题，使之成为针对给定问题最优的问题集，会因为各个选项概率分布的不同而变化。即便是同一组信息/备选答案，两套不同的概率分布，各自会给出一套对自己最优的问题集，一套概率分布下的最优问题集不见得是另外一套概率分布下的最优问题集。\n\n\u003e 下面的表达式都只写出了离散变量的形式，连续随机变量需要将求和写成对应的积分。\n\u003e \n\n### 相对熵：Kullback–Leibler (K-L) Divergence\n\n英文里也叫 relative entropy 或者 I-divergence\n\n这里的两个概率分布映射自**同一个**随机变量空间。\n\n$$\nD_{KL}(p(X)|q(X))=\\sum_{x\\in X}p(x)\\log\\frac{p(x)}{q(x)}=-\\sum_{x\\in X}p(x)\\log\\frac{q(x)}{p(x)}\n$$\n\n这个量描述了当 *p*(*X*) 作为各选项的正确概率分布的情况下，用对 *q*(*X*) 最优的单选题去提问，**没问出来的信息**所需要的**额外的**单选题数目/编码数。\n\n在科学应用中，*p*(*X*) 一般是从实验中测量出来的概率分布，*q*(*X*) 是理论模型的预测。\n\n下面的例子计算了一个单选题，选 C、选 B、假想中一群学生的答案统计、胡猜四种概率分布 *p, q ,r , φ* 之间的 KL divergence。因为概率为零会出现发散问题，所以我们取 eps = 10^(-10) 把这些概率值截断：\n\n```python\nimport numpy as np\n\ndef kl_div(p,q,eps=1e-10):\n    p = np.clip(p,eps,1-eps)\n    q = np.clip(q,eps,1-eps)\n    return np.sum(p*np.log2(p/q))\n\np   = np.array([  0,   0,   1,   0])\nq   = np.array([  0,   1,   0,   0])\nr   = np.array([1/6, 1/6, 1/2, 1/6])\nphi = np.array([1/4, 1/4, 1/4, 1/4])\n\nresults = np.empty((4,4))\nfor i,v1 in enumerate([p,q,r,phi]):\n    for j,v2 in enumerate([p,q,r,phi]):\n        results[i,j] = kl_div(v1,v2)\n```\n\n| KL-div(行, 列)/bit | p | q | r | φ |\n| --- | --- | --- | --- | --- |\n| p = [0,0,1,0] | 0 | 33.219 | 1 | 2 |\n| q = [0,1,0,0] | 33.219 | 0 | 2.585 | 2 |\n| r = [1/6, 1/6, 1/2, 1/6] | 14.817 | 25.890 | 0 | 0.208 |\n| φ = [1/4,1/4,1/4,1/4] | 22.914 | 22.914 | 0.189 | 0 |\n\n从结果中我们可以看到：\n\n- 对角线为 0，符合其意义。\n- $$D_{KL}(p,q)$$ 和 $$D_{KL}(q,p)$$ 都应该是 +∞，这里的有限值是 eps 截断的结果\n- 除个别巧合，对称位置的值一般不相等。这个量不同于两点之间的距离。\n\n### 交叉熵：Cross Entropy\n\n这里的两个概率分布映射自**同一个**随机变量空间 X。\n\n概率分布 ***q* 相对于 *p*** 的交叉熵 cross entropy\n\n$$\nCE(p(X),q(X))=-\\sum_{x\\in X}p(x)\\log q(x)=S(p(X))+D_{KL}(p(X)|q(X))\n$$\n\n这个量描述了当 *p*(*X*) 作为各选项的正确概率分布的情况下，用对 *q*(*X*) 最优的单选题去提问，所需要的**总共的**单选题数目/编码数。\n\n类似于二项熵，*p* 和 *q* 之间的 binary cross entropy:\n\n$$\nBCE(p,q)=-p\\log q-(1-p)\\log(1-q)=p\\log\\frac{1-q}{q}-\\log(1-q)\n$$\n\n```python\ndef cross_entropy(p,q,eps=1e-10):\n    p = np.clip(p,eps,1-eps)\n    q = np.clip(q,eps,1-eps)\n    return -np.sum(p*np.log2(q))\n\nresults = np.empty((4,4))\nfor i,v1 in enumerate([p,q,r,phi]):\n    for j,v2 in enumerate([p,q,r,phi]):\n        results[i,j] = cross_entropy(v1,v2)\n```\n\n| Cross Entropy(行, 列)/bit | p      | q      | r   |   $$\\varphi$$   |\n| ---                      | ---    | ---    | ---   | --- |\n| p = [0,0,1,0]            | 0      | 33.219 | 1     | 2   |\n| q = [0,1,0,0]            | 33.219 | 0      | 2.585 |  2  |\n| r = [1/6, 1/6, 1/2, 1/6] | 16.610 | 27.683 | 1.792 | 2   |\n| $$\\varphi$$ = [1/4,1/4,1/4,1/4]    | 24.914 | 24.914 | 2.189 | 2   |\n\n- 对角线上不一定为零，而是自己的信息熵\n- 其他位置和 KL divergence 相差大约为第一个输入分布的信息熵，误差 eps 的截断\n\n### 互信息：Mutual Information\n\n这里的两个概率分布一般来说映射自**不同的**随机变量空间。\n\n$$\nMI(X,Y)=\\sum_{x,y}p(x,y)\\log\\frac{p(x,y)}{p(x)p(y)}=D_{KL}\\left(p(X,Y)|p(X)p(Y)\\right)\n$$\n\n从后一个等号可以看出，这一性质衡量的是 *X, Y* 两个随机变量的联合分布在多大程度上不同于“*X* 和 *Y* 相互独立”的零假设。两个随机变量相互独立时，互相不反映对方的信息，互信息 *MI* = 0。\n\n当从 *X* 所在的随机变量空间取样的难度比较大的时候，我们需要用容易取样的**另一个变量空间**的随机变量 *Y* 来推测 *X* 的情况，互信息就可以用来论证我们这种选择的合理性。\n\n## 扯点闲篇\n\n### PyTorch 中以此为基础的 loss functions\n\n`torch.nn` 中有如下几个和今天的文章相关的 loss functions：\n\n- `torch.[nn.KLDivLoss](https://pytorch.org/docs/stable/generated/torch.nn.KLDivLoss.html#torch.nn.KLDivLoss)`\n- `torch.[nn.CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss)`\n- `torch.[nn.BCELoss](https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html#torch.nn.BCELoss)`\n- `torch.[nn.BCEWithLogitsLoss](https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html#torch.nn.BCEWithLogitsLoss)`\n\n之所以没直接用这些函数计算上面的例子，是因为 `KLDivLoss` 是按元素计算，随后需要自己求和；`CrossEntropyLoss` 又是按类别的，还不需要归一化，而且文档的解释很复杂，我到现在也没看明白；而且还要注意这些函数的设计输入是不是 logit，这是机器学习里的概念，在此不展开了。\n\n### 玻尔兹曼的墓志铭\n\n$$\nS=k\\log W\n$$\n\n其中 *S* 是（微正则系综中的）热力学熵，*k* 是玻尔兹曼常数 $$k_B$$，*W* 是因为刻碑的师傅不会写 *Ω*。\n\nW 或 Ω 是处于相同能量的热力学状态的数量。因为你都需要统计物理了，显然是只知道能量，没办法知道所考虑的微观粒子究竟处于哪一个热力学状态。那此时的零假设就是处于所有状态的可能性相等，*p* = 1/Ω，信息熵 \n\n$$\nS =-\\sum_{m\\in M}p(m)\\log_n p(m)= -\\Omega\\cdot(\\frac{1}{\\Omega}\\log\\frac{1}{\\Omega})=\\log\\Omega\n$$\n\n和热力学熵只相差一个玻尔兹曼常数。这是因为信息熵是无量纲的，熵和温度的量纲相乘之后需要得到能量的量纲，只能由 $$k_B$$ 把量纲凑齐，而数值是自由能相关的实验里测出来的。\n\n好像这就是高中物理里熵的定义式是吧。\n\n上了大学以后，正则系综和巨正则系综中的熵也分别就是各自体系中各状态的概率分布的信息熵，乘上玻尔兹曼常数。~~（我也忘得差不多了，试图萌混过关）~~\n\n### 善卜者无先见之明\n\n公元 451 年，阿提拉 Attila 率领匈人攻入罗马领土，横扫有大量其他民族居住的高卢地区。西罗马帝国将军艾提乌斯 Aetius 联络了众多畏惧匈人的民族组成联军，其中包括西哥特人的王狄奥多里克 Theodoric，两军会战于卡塔隆 Catalaunian 平原。\n\n本来想用这个故事举例子来着，因为我记得阿提拉在战前找了个大师算了一卦，说是一位国王将战死，一个国家将崩塌。于是阿提拉很高兴，以为哥特人和狄奥多里克要玩完了，结果战斗打响，狄奥多里克确实死于乱军，但是罗马和哥特等族的联军击败了匈人，阿提拉的霸业雨打风吹去。\n\n于是试图说明算命的魅力就在于，用文字游戏表达一个自信息比较低的命题，同时误导对方相信一个自信息高得多的命题，在心理疏导之外，赚一个信息熵的差价。\n\n结果查证的时候发现好像不是这么回事，Barbarian Rising 故事片里的预言内容不一样；维基百科上没给出处，说算命的很准，于是阿提拉推迟到下午作战，方便晚上跑路；其他地方甚至压根没有算命的情节。但是写都写了，需要积累高考作文素材的小朋友们还是可以假装被我误导了~\n\n当然了，算命这个事还有一种情况，就是打着不确定的幌子，售卖确定但不方便承认自己确定的信息，那就是另一种生意，和另外的价格了~"},{"slug":"equivlance-between-diffusion-equation-and-random-walk","filename":"2024-04-25-equivlance-between-diffusion-equation-and-random-walk.md","date":"2024-04-25","title":".tex | 扩散方程和随机游走的等价","layout":"post","keywords":["tex","phy","m"],"excerpt":"之前 MCMC 讲错了","hasMath":true,"content":"\n\u003e 这些内容总结自美国研究生级别的《数学物理方法》两次课的笔记，大约两个小时。\n\u003cbr\u003e如果是中国大学本科的话，认真的老师半个小时庶几可以讲完;\n\u003cbr\u003e念 PPT 就算上课的话 15 分钟可以讲完，附赠一个段子;\n\u003cbr\u003e翻转课堂的话也就布置个作业，老师一句话可以讲完。\n\u003cbr\u003e以上数据除第一句外纯属揣测，没有黑任何人的意思，love and peace~\n\u003e \n\n### 扩散方程\n\n带有初值条件的扩散方程表述如下：\n\n$$\n\\begin{cases}\nu(x,t=0)=f(x) \\\\\n\\partial u(x,t)/\\partial t=\\sigma \\cdot \\partial^2 u(x,t)/ \\partial x^2\n\\end{cases}\n$$\n\n方程的解为：\n\n$$\nu(x,t) = \\frac{1}{\\sqrt{4\\pi \\sigma t}} \\int_{-\\infty}^{+\\infty} f(s)\\ e^{-\\frac{(x-s)^2}{4\\sigma t}}\\ ds\n$$\n\n解法是将 u(x,t) 对空间变量 x 作傅里叶变换为 U(k,t)，利用傅里叶变换的性质，变换后的方程将是关于时间 t 的一阶常微分方程。求解后作傅里叶逆变换 ~~即为上式。~~ ~~（完蛋，好久没做题了，那个 $$e^{-\\frac{(x-s)^2}{4\\sigma t}}$$ 是怎么凑出来的，为什么我直接给消掉了啊）~~ 凑出来了凑出来了，初值条件代入频域 k 空间里的通解来确定积分常数，可以看到结果 $$F(k) e^{-\\sigma t k^2}$$ 是两项之积，所以根据傅里叶变换的卷积定理，实空间 x 里的解是 f(x) 和 $$\\mathscr{F}_{k\\rightarrow x}^{-1}\\{e^{-\\sigma t k^2}\\}$$ 的卷积（所以上式的指数项以 (x-s) 为宗量），而计算后者的时候需要用到高斯积分～\n\n### 随机游走\n\n随机游走是一个离散过程，为了和连续时空中的扩散方程相对比，将空间变量 x 离散化为相隔 Δ 的格点 i，时间变量 t 离散化为相隔 δ 的 n。\n\n当一个粒子在 n 时刻位于格点 i 时，在下一个时刻 n+1, 它有 1/2 的概率移动到 i-1, 1/2 的概率移动到 i+1.\n\n所以，虽然每个进行随机游走的粒子在任意时刻都只有确定且唯一的位置，但是对于大量同样初始位置和运动规律的例子，n 时刻出现在 i 格点的概率 P(i,n) 有以下关系：\n\n$$\n\\begin{cases}\nP(i,0)= f_i \\\\\nP(i,n)=\\frac{1}{2}\\left[P(i-1,n-1)+P(i+1,n-1)\\right]\n\\end{cases}\n$$\n\n在初值条件为 $$f_i=\\delta_{i=0}$$ 时，递推结果如下：\n\n|  x 轴 — | i = -4 | i = -3 | i = -2 | i = -1 | O | i = 1 | i = 2 | i = 3 | i = 4 | → |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n| n = 0  | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 |  |\n| n = 1  | 0 | 0 | 0 | 1/2 | 0 | 1/2 | 0 | 0 | 0 |  |\n| n = 2  | 0 | 0 | 1/4 | 0 | 1/2 | 0 | 1/4 | 0 | 0 |  |\n| n = 3  | 0 | 1/8 | 0 | 3/8 | 0 | 3/8 | 0 | 1/8 | 0 |  |\n|**t 轴 ↓**|  |  |  |  |  |  |  |  |  |\n\n![](/photos/2024-04-25-random-walk-probabilities.png)\n\n离散的情况，很难对任意的初值条件写出解的表达式，但是对于上面的特殊情况，课上不加证明地给出了（可能是根据上图凑出来的）下面的解：\n\n$$\nP(i,n)=\\frac{1}{2^n}\\frac{n!}{\\left(\\frac{n+i}{2}\\right)!\\left(\\frac{n-i}{2}\\right)!}\n$$\n\n对的，以上关系只能表示 (n+i = 偶数) 的情况，但是康托尔告诉了我们，所有偶数和所有自然数的“数量”一样多，所以也没差太多～\n\n### 方程的等价\n\n概率的递推公式可以变换为：\n\n$$\n\\frac{1}{\\delta}\\left[P(i,n)-P(i,n-1)\\right]=\\frac{1}{2}\\left [\\frac{P(i-1,n-1)-2P(i,n-1)+P(i+1,n-1)}{\\Delta^2}\\right]\\frac{\\Delta^2}{\\delta}\n$$\n\n因为 $$x=i\\Delta,\\ t=n\\delta$$, 对两个变量的微分可以离散化成差分：\n\n$$\n\\frac{\\partial}{\\partial x}\\rightarrow \\frac{1}{\\Delta}\\left[()_i-()_{i-1}\\right],\\ \\frac{\\partial}{\\partial t}\\rightarrow \\frac{1}{\\delta}\\left[()_n-()_{n-1}\\right]\n$$\n\n直接就能看出扩散方程和随机游走的等价，且系数之间存在关系：$$\\sigma = \\frac{\\Delta^2}{2\\delta}$$\n\n### 解的等价\n\n只讨论一个 δ(x) 函数作为初值条件的情况，我们要证明此时扩散方程的解：\n\n$$\nu(x,t) = \\frac{1}{\\sqrt{4\\pi \\sigma t}} e^{-\\frac{x^2}{4\\sigma t}}\\ \\xleftarrow[{\\Delta,\\delta \\rightarrow 0;\\ i,n\\rightarrow \\infty}]{x=i\\Delta,\\ t=n\\delta} \\frac{1}{2^n}\\frac{n!}{\\left(\\frac{n+i}{2}\\right)!\\left(\\frac{n-i}{2}\\right)!} \\frac{1}{2\\Delta}\n$$\n\n只需讨论这一个情况，因为 δ(x-s) 函数可以看作将一个函数 f(x) 在自变量 x=s 时切片为 f(s)，而任何一个（性质比较“优美”的）函数都可以看作把它自己在定义域上的所有点切片后再重新叠加起来：\n\n$$\nf(x) = \\int_{-\\infty}^{+\\infty}f(s)\\delta(x-s)\\ ds\n$$\n\n过程需要用到 Sterling 公式对阶乘的近似：$$n! \\approx \\sqrt{2\\pi n}\\ n^n e^{-n}$$\n\n$$\n\\begin{array}{rcl}\n\\frac{P(i,n)}{2\\Delta} \u0026 \\approx \u0026 \\frac{1}{2\\Delta} \\frac{1}{2^n} \\frac{\\sqrt{2\\pi n}\\ n^n e^{-n}}{\\sqrt{\\frac{2\\pi (n-i)}{2}}\\ \\left(\\frac{n-i}{2}\\right)^{\\frac{n+i}{2}} e^{-\\frac{n+i}{2}}\\sqrt{\\frac{2\\pi (n+i)}{2}}\\ \\left(\\frac{n+i}{2}\\right)^{\\frac{n+i}{2}} e^{-\\frac{n+i}{2}}} \\\\\n\u0026 = \u0026 \\frac{1}{2\\Delta}\\frac{\\sqrt{2n}}{\\sqrt{\\pi(n^2-i^2)}}\\frac{n^n}{(n-i)^{\\frac{n}{2}-\\frac{i}{2}}(n+i)^{\\frac{n}{2}+\\frac{i}{2}}} \\\\\n\u0026 = \u0026 \\frac{1}{2\\Delta}\\frac{\\sqrt{2n}}{\\sqrt{\\pi(n^2-i^2)}}\\frac{n^n/n^n}{(n-i)^{\\frac{n}{2}}(n+i)^{\\frac{n}{2}}(n-i)^{-\\frac{i}{2}}(n+i)^{\\frac{i}{2}}/n^n} \\\\\n\u0026 = \u0026 \\frac{1}{2\\Delta}\\frac{\\sqrt{2n}}{\\sqrt{\\pi(n^2-i^2)}} \\frac{1}{\\left(1-\\frac{i}{n}\\right)^\\frac{n}{2}\\left(1+\\frac{i}{n}\\right)^\\frac{n}{2}\\left(1-\\frac{i}{n}\\right)^{-\\frac{i}{2}}\\left(1+\\frac{i}{n}\\right)^\\frac{i}{2}} \\\\\n\u0026 = \u0026 \\frac{1}{\\sqrt{2}\\Delta}\\frac{1}{\\sqrt{\\pi(n-\\frac{i^2}{n})}} \\frac{1}{\\left(1-\\frac{i^2}{n^2}\\right)^\\frac{n}{2}\\left(1-\\frac{i}{n}\\right)^{-\\frac{i}{2}}\\left(1+\\frac{i}{n}\\right)^\\frac{i}{2}} \\\\\n\u0026 \\xrightarrow[\\frac{i}{n}=\\frac{x\\Delta}{2\\sigma t},\\ \\frac{i^2}{n}=\\frac{x^2\\delta}{\\Delta^2 t}]{(1+a\\epsilon)^{1/\\epsilon}\\rightarrow e^a} \u0026 \\frac{1}{\\sqrt{4\\pi\\sigma t}}\\frac{1}{e^\\frac{x^2}{4\\sigma t} e^\\frac{x^2}{4\\sigma t} e^{-\\frac{x^2}{4\\sigma t}} } \\\\\n\u0026 = \u0026 \\frac{1}{\\sqrt{4\\pi\\sigma t}}e^{-\\frac{x^2}{4\\sigma t}}\n\\end{array}\n$$\n\n### 之前 MCMC 讲错了\n\n讲 Markov Chain Monte Carlo 模拟的时候举的例子是计算 $$\\int_{-\\infty}^{+\\infty}e^{-x^2}dx$$, 现在系数可以对上了：$$1=4\\sigma t=4 \\frac{\\Delta^2}{2\\delta} n\\delta = 2\\Delta^2n$$, 随机游走的步数和步长之间存在一个确定的关系，在步长确定的情况下，我们需要重复模拟大量粒子作相同步数的随机游走，然后统计这一确定步数走完之后的每个粒子的终末位置。\n\n所以，这并不是一个 Markov Chain Monte Carlo 模拟，只是一个普通的 Monte Carlo 模拟，我们拿到了想要知道的随机变量的原始概率分布，只不过取得符合这一概率分布的每一个样本的过程是一个 Markov 过程。\n\n正经的 MCMC，应该是只模拟一个粒子作随机行走，然后把它每一步的位置记录下来，统计到样本里去。这样的话时间 t 的信息就被抹去了，而且由于扩散方程描述的状态并不是热力学平衡态，并不能通过统计物理中的遍历性 (ergodicity) 来得到正确结果。\n\n采用了 Metropolis 算法的 MCMC, 一个粒子作随机行走只是其中的一个步骤，还要计算这一步之前和之后 $$e^{-x^2}$$ 的值，来决定这一步是否被加入样本，不成立的话要退回前一步继续走。\n"},{"slug":"mc-mcmc-markov-chain-monte-carlo-gibbs-sampling","filename":"2024-04-15-mc-mcmc-markov-chain-monte-carlo-gibbs-sampling.md","date":"2024-04-15","title":".tex | MC→MCMC 蒙特卡洛模拟，基于马尔科夫链采样","layout":"post","keywords":["tex","phy","m"],"excerpt":"蒙特卡洛模拟、马尔科夫链采样、Metropolis-Hastings 算法、吉布斯采样","hasMath":true,"content":"\nMonte Carlo 蒙特卡洛模拟，简称 MC. \n\nMarkov Chain Monte Carlo 是用马尔科夫链采样的蒙特卡洛模拟，简称 MCMC.\n\n## Monte Carlo 模拟\n\n这个比较简单了，举个例子，要计算 π 的近似值，可以在一块正方形板子里画一个内接圆，然后以均匀的概率往正方形里一粒一粒地扔沙子，每扔一粒，就判断并且记录这里沙子在圆内还是圆外，然后把沙子吹掉，如此往复。圆的面积是 πr²，正方形的面积是 4r²，所以落在圆内的概率（圆内沙子的数量和总数的比值）乘 4，就是所求。\n\n![](/photos/2024-04-15-monte-carlo-pi.png)\n\n归纳一下：当问题的解用一个随机变量的概率分布、期望值、二阶矩……等等来表示的时候，就生成一个符合该概率分布的随机样本，用样本的统计量去近似原概率分布。\n\n## Markov Chain Monte Carlo\n\n但是前述例子有一个步骤，就是我们往板子上扔完沙子要把沙子吹掉，每粒沙子，每次扔沙子之间也应该看不出区别，这是为了保证取样之间**相互独立且来自同一个概率分布**。\n\n但是很多取样过程无法满足这种条件，或者达成条件所需的成本很高。比如计算一个高斯积分 $$\\int_{-\\infty}^{+\\infty}e^{-x^2}dx$$，被积函数的取值范围涵盖整个实数集，想找一个在整个实数集上均匀分布的随机数发生器就比较难了。\n\n![](/photos/2024-04-15-monte-carlo-gaussian.png)\n\n但是学过物理的朋友应该知道，上面的被积函数是以狄拉克 δ(x) 函数为初值条件的一个扩散方程的解，在某一时刻的空间分布。（不想凑系数了，将就看吧）\n\n而扩散方程又是随机游走 (random walk) 在连续近似下的极限。\n\n所以我们直接模拟一堆粒子从原点出发作随机行走，向两个方向的概率相同，扩散系数以及积分里的常数对齐，统计粒子在整个过程中出现在不同 x 位置的频率，求和之后乘以步长就是积分结果。这个过程需要的随机数发生器容易获取得多，是一个以 0.5 为阈值的 [0,1) 的均匀分布，比如一个均匀硬币。\n\n而随机行走过程中走完每一步的位置，都只取决于前一步的位置，而与更久远的历史无关——这样的过程叫做马尔可夫过程。用这种方法取样获得随机样本的蒙特卡洛模拟，就是 MCMC.\n\n扩散方程和随机行走只是 MCMC 的一个很特殊很特殊的例子，而对于一般的 MCMC 模拟，有以下通用的 Markov Chain 采样的算法：\n\n### Metropolis-Hastings 算法\n\n已知一个随机变量 x, 和一个与目标概率分布 P(x) 成正比的函数 f(x)（不要求 f 归一化）\n\n1. 初始化\n    1. 选定初始采样点 $$x_0$$ \n    2. 选定一个采样函数 proposal function，也就是在已知当前 x 的取值时，下一个 x’ 取值的概率分布 $$g(x’\\vert x)$$；其中对于 Metropolis 算法，这个采样函数是对称的：$$g(x’\\vert x)=g(x\\vert x’)$$. 常用以两者之差为宗量的高斯函数。\n2. 在得出 t 时刻的 $$x_t$$ 之后：\n    1. 根据 $$g(x'\\vert x_t)$$ 抽样得到一个 x’\n    2. 计算 α = f(x’)/f(x) = P(x’)/P(x)\n    3. 决定是否将 x’ 加入样本\n        1. 如果 α ≥ 1, 直接加入\n        2. 如果 α \u003c 1, 以 α 为概率加入\n\n这种方法不保证采样的早期样本也符合目标概率分布，所以一般会抛弃最先加入的若干样本。\n\n### Gibbs 采样\n\n只是一种思路，不算是完整的算法。\n\n当被采样的随机变量是一个多维向量的情况，在不使用 Gibbs 采样的情况下，在迭代的某一步骤 t，每个分量都应该是前一步骤的函数：$$x_{i,t}=f(\\{x_{j,\\ t-1}\\})$$\n\n而 Gibbs 采样就是说，不必让每个维度 i 都根据前一个步骤的分量来取值，可以把当前 t 已经取样出来的分量直接带入到本回合后面的维度：$$x_{i,t}=f(\\{x_{j,\\ t}\\}_{j\u003ci}\\cup\\{x_{k,\\ t-1}\\}_{k\\ge i})$$"},{"slug":"note-consciousness-theories","filename":"2024-01-10-note-consciousness-theories.md","date":"2024-01-10","title":".tex | 意识理论笔记","layout":"post","keywords":["tex","bio"],"excerpt":"整合信息理论 (IIT) vs. 全局神经工作空间理论 (GNWH)","content":"\n\u003e 本文是《**[一场意识理论大混战，甚至“伪科学”帽子都飞出来了](https://mp.weixin.qq.com/s/S_nZFZD72Kq3sJmoXKplww)**》一文的读书笔记。\n\u003e \n\n## 名词解释\n\n- **发放**：对 spike 的翻译。发放率即 spike-count rate. [https://zh.wikipedia.org/wiki/神经编码](https://zh.wikipedia.org/wiki/%E7%A5%9E%E7%BB%8F%E7%BC%96%E7%A0%81)\n- **V1**：初级视皮层。[https://zh.wikipedia.org/wiki/视觉系统](https://zh.wikipedia.org/wiki/%E8%A7%86%E8%A7%89%E7%B3%BB%E7%BB%9F)\n\n## 事实陈述\n\n### 科赫 vs. 查默斯赌局：信息整合理论作者之一打的一个赌\n\n- 德裔美国神经科学家科赫 (Christof Koch) vs. 澳大利亚哲学家查默斯 (David Chalmers)\n- 内容：以下问题能否在25年内得到解决：\n    - 查默斯称为意识的“**困难问题**”（hard problem）：“主观的意识是怎样从客观的神经回路中涌现出来的”\n    - 等价于，科赫和其忘年交克里克所说的“**意识的神经相关集合**”（Neural correlates of consciousness）\n- 结果：没能解决，科赫认输，双方再约 25 年\n- 时间线\n    - 1990年，克里克和科赫《走向意识的神经生物学理论》（[Towards a Neurobiological Theory of Consciousness](https://profiles.nlm.nih.gov/spotlight/sc/catalog/nlm:nlmuid-101584582X469-doc)）\n        - 观点：研究意识也是一样，应该从研究脑中哪些神经活动和视知觉相关——意识的神经相关集合开始。\n        - 挑战：当主体受到视刺激后的脑活动变化，既可能是由于视知觉引起的，也可能是由于刺激变化本身引起的。\n    - 1996年，在德国工作的希腊神经科学家洛戈塞蒂斯（Nikos K. Logothetis）猴子双眼竞争实验\n        - **双眼竞争**：给主体的双眼分别看两个完全不同的景象时，主体看到的并非这两个景象的融合，而是轮流看到其中之一。\n        - 结果：\n            - 在初级视皮层和次级视皮层，绝大多数细胞的 **发放(?)** 率与知觉的反复变化无关。总体来说，只要一只眼睛有输入刺激，神经元的发放就会增强。这与猴子究竟看到了什么无关。\n            - 他们又发现在对猴子下颞叶（inferior temporal，IT）皮层及上颞叶沟（superior temporal sulcus，STS）的下侧（该区域与IT上部相邻）进行实验记录时，只有当猴子“看到”时才有发放。\n        - 推论：\n            - 一般认为 **V1(?)** 对意识贡献甚微。\n            - 克里克对此非常兴奋，他认为这一技术已使科学家找到了研究视知觉神经相关集合的钥匙，并宣称到20世纪末就能发现意识的神经相关集合。\n            - 像功能性核磁共振成像和光遗传学等新技术的出现，使科赫在当时认为，25年的时间去解决应该没问题。\n    - 科赫在1998年和查默斯打了这个赌。这个赌只与能否在2023年解决查默斯的“困难问题”有关，而与其他论点无关。\n\n### 意识的神经相关最小集合：信息整合理论的预备知识\n\n- 定义\n    - 意识的神经相关集合就是“神经元的某种机制或事件的集合。该集合是形成某个特定知觉或体验所需要的最小集合。”（[科赫2004](https://ajp.psychiatryonline.org/doi/10.1176/appi.ajp.162.2.407)）\n    - 他和托诺尼又引申出对所有可能意识内容的意识神经相关集合的总体，并称之为全意识神经相关集合（full neural correlates of consciousness）。（[2016](https://www.nature.com/articles/nrn.2016.22)）\n- 方法与相应结果\n    - 需要排除对脑涌现意识非必要的事件，比如报告信号这件事本身（通过检测眼动或瞳孔放大）\n        - 这种“无报告范式（no‑report paradigms）”所确定的有特定内容的神经相关集合比需要报告时得到的更局限于皮层后部。（[2016](https://www.nature.com/articles/nrn.2016.22)）\n    - 一种是“基于不同状态的方法”（state-based approaches）把清醒的健康受试者在不要求做任何任务而有意识时的脑活动和意识丧失时（如无梦睡眠、全身麻醉、昏迷或植物状态）的脑活动进行比较。\n        - 全意识神经相关集合往往包括额-顶叶网络，但是这里有些部分可能和受试者的警觉、注意等脑功能有关。\n    - 另一种“同样状态无任务范式”（within-state, no‑task paradigm）这主要是利用意识的自发波动，例如当受试者处于无快速眼动睡眠期时将其叫醒，有时受试者说是正在做梦，而有时则没有任何意识。把受试者在报告做梦或无意识前记录下来的脑电图进行比较，\n        - 全意识相关神经机制主要位于包括感觉区在内的后皮层热区（posterior cortical hot zone），也就是包括皮层后部颞-顶-枕叶交界处在内的脑区，这和根据有特定内容的意识神经相关集合所得的结果在总体上吻合得相当好，因此可以把后部皮层区看作意识神经相关集合的热门候选区。\n\n### 整合信息理论 vs. 全局神经工作空间理论\n\n- 整合信息理论 (IIT) 同一把大伞之下，有两个很不相同的内容\n    1. （根据“同样状态无任务范式”）全意识相关神经机制主要位于包括感觉区在内的后皮层热区（posterior cortical hot zone），也就是包括皮层后部颞-顶-枕叶交界处在内的脑区，这和根据有特定内容的意识神经相关集合所得的结果在总体上吻合得相当好，因此可以把后部皮层区看作意识神经相关集合的热门候选区。这一观点笔者称之为“后脑理论”。\n    2. 托诺尼早已因他制定了一个度量意识的指标Φ并冠名为“整合信息理论”而闻名，而科赫也曾称赞过这一理论是“有关意识的唯一有希望的基本理论”，因此他们的这一观点也被称为IIT\n        - 托诺尼、埃德尔曼（Gerald Edelman）曾经提出过“整体性”和“信息性”（或称“神经复杂性”）作为衡量意识程度的定量指标。托诺尼正是在这一基础上考虑了意识更多的基本性质（其核心依然是整体性和信息性，但是却略去了“主观性”或“私密性”这一意识的根本属性），并以此作为“公理”。\n        - 这些公理包括：內禀存在性（Intrinsic existence）、结构性（composition）、信息性、整体性（integration）、排他性（exclusion）。\n        - 在这些公理的基础之上，托诺尼认为如果一个物理系统要有意识的话，那么这个系统就必须有和上述公理相应的性质，它应该是一个有数量极大的可能状态的统一整体，为此在有关脑区之间必须有交互作用。意识的程度可用该系统超越其各组成部分所含信息量的总和的信息量来度量，他们把这称为“整合信息（integrated information）”，并用符号Φ来表示\n- 全局神经工作空间理论（global neuronal workspace hypothesis）\n    - 《[Consciousness and the Brain: Deciphering How the Brain Codes our Thoughts](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3971003/)》\n    - 法国认知神经科学家德阿纳（Stanislas Dehaene）提出。由于对意识仍然没有明确和普遍接受的定义，德阿纳将他的研究集中在他所谓的“进入意识（conscious access）”（受试者意识到了其所受刺激并可以向其他人报告的现象）上。\n    - 他们使用掩蔽、双眼竞争和其他方法表明，虽然刺激保持不变或几乎不变，但受试者的知觉却可能发生根本变化，例如从意识不到变成意识到，或正好相反，因此进入意识可以被视为唯一的变量，并可以通过实验对这一变量进行操控。\n    - 发现如下标记：\n        1. 刺激诱发的脑活动大大增强，扩大到多个脑区并突然引发前额叶皮层和顶叶皮层许多回路的活动；\n        2. 脑事件相关电位中的晚成分P3突然增强；\n        3. 在晚期突然爆发高频振荡；\n        4. 跨脑区域活动的同步化。\n    - 观点/结论\n        - 当有有意识的知觉时，神经元群以协调的方式开始发放，首先是在一些局部的特定区域，然后蔓延到皮层的广大范围。最终，它们侵入到许多前额叶和顶叶脑区，同时与前面的感觉区保持紧密同步。正是在这个时候，突然形成了一个协调一致的脑网络，有意识觉知也似乎由此产生。\n        - 意识是一种在全脑范围里的信息共享。人脑中有高效的长距离网络，特别是在前额叶皮层，以选择相关信息并将其扩播到整个脑。意识是一种演化装置，它使我们能够注意某个信息并在这一扩播系统中保持活跃。一旦这个信息被意识到了，根据我们当时的目标，它可以被灵活地传送到其他区域。\n\n### 邓普顿世界慈善基金会5年对抗合作\n\n- 权威杂志《科学》（*Science*）和《自然》（*Nature*）等载文称赞了这一对抗性合作\n- 有124位意识研究科学家联名发表公开信，把IIT谴责为“伪科学”\n- 六个独立实验室遵照双方预先商定的方案，并分别用功能性核磁共振、脑磁图和皮层电图技术对250名受试者测量其脑活动，以检验这两种理论对他们共同认同的两个实验方案中第一个方案的不同预测，双方自己并不参加实验。\n- 整合信息理论 IIT\n    - 有利：关于IIT，确实观察到，后皮层脑区持续有信息。\n    - 不利：并没有发现IIT所预测的脑区之间有持续的同步活动\n- 全局神经工作空间理论 GNWH\n    - 有利：意识的某些方面确实可以在前额叶皮层中表现出来\n    - 不利：\n        - 并非一切意识活动都可以在此有所反映\n        - 实验发现只有体验开始时才有信息扩布的证据，但未能发现在体验结束时也有扩布\n- 124位意识研究者，其中包括巴尔斯（Bernard J. Baars）、丹纳特（Daniel C. Dennett）和丘奇兰（Patricia S. Churchland）等著名学者，联名发布一封公开信，指责《科学》《自然》等媒体做了不实报道，并指责IIT是伪科学。抓住了和托诺尼的Φ值有关的问题全盘否定。\n\n## 作者观点\n\n- 解决意识的神经相关集合的问题呢，还是解决查默斯的困难问题，科赫似乎把这两个问题认为是同一个问题，作者不同意\n- 科赫/托诺尼和德阿纳的对抗性合作实际上是关公战秦琼。\n- 124人公开信故意回避了在对抗性合作中的IIT实际上是指后脑理论，而和Φ没有直接关系，因此他们对对抗性合作的批评就像是枪打稻草人，但是他们对Φ理论的批评却有其合理之处。\n    - 像意识这样复杂的对象能不能用公理化的方法来进行研究\n    - 在托诺尼的5条公理中也故意丢掉了对意识来说最关键的“主观性”，不完备的公理系统推导出来的Φ指标，只能度量意识作为神经系统超越其各组成部分所含信息量的总和的信息量这一个方面，而非意识本身。\n- 全局神经元工作空间假设对于进入意识标记的解释没问题，但对他的假设是否也能够解释意识或者即使只是进入意识本身持怀疑态度。\n    - “进入意识标记”并不是“进入意识”本身，就像某人的签名并不就是他自己一样。\n    - 除了用受试者的主观报告来判断他们是否意识到了什么之外，德阿纳的工作并没有触及意识的主观性问题。\n- 后脑理论的一个重要依据是采用“无报告”范式的研究方法，但是也有科学家根据一些报道称在采用这种方法时，也能在前额叶皮层检测到有活动，因此把这种方法贬之为“误导”。\n    - 不过被贬一方则坚称“从总体上来说，前额叶皮层对意识来说既非必要，也不充分，这和皮层后部完全不同。”\n- 动物行为学家戴维·培尼亚-古斯曼提出意识包括三个重要方面：主观意识、情感意识和元认知意识。\n    - 主观意识：主观存在感和具身的自我觉知感\n    - 元认知意识：自己知道自己是有认知能力"},{"slug":"probability-vs-likelihood","filename":"2023-11-21-probability-vs-likelihood.md","date":"2023-11-21","title":".tex | 概率 (probability) 和似然性 (likelihood)","layout":"post","keywords":["tex","m"],"hasMath":true,"excerpt":"如题","content":"\n一个随机变量 X 取值为 x 的概率 (probability)/概率密度，一般可以用一个有若干参数的函数来表示。这个函数的参数记作  $$\\theta$$：\n\n$$\nprob_X(x)=f(x|\\theta)\n$$\n\n而似然性 (likelihood) 就是把上式 f 看作以 $$\\theta$$ 为自变量，x 为参数的函数，从表达式上看不出区别：\n\n$$\nL(\\theta|x)=f(x|\\theta)=prob_X(x)\n$$\n\n最近处理一个数据集，整理完之后的直方图如下：\n\n![double-peak](/photos/2023-11-21-double-peak.png)\n\n比较明显，比起一个正态分布 $$f(x)=\\frac{1}{ \\sigma \\sqrt{2\\pi} } e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2}$$，这些数据更像是来自不同均值和方差的两个分布。那么对于每个数据点 $$x_0$$，它到底来自哪个分布呢？可以分别计算 $$L_1(\\mu_1,\\theta_1\\vert x_0) = f(x_0\\vert\\mu_1,\\theta_1)$$ 和 $$L_2(\\mu_2,\\theta_2\\vert x_0) = f(x_0\\vert\\mu_2,\\theta_2)$$，然后比较 $$L_1$$ 和 $$L_2$$ 的大小。\n"},{"slug":"what-is-intelligence-not-same-as-intelligence-is-what","filename":"2023-06-17-what-is-intelligence-not-same-as-intelligence-is-what.md","date":"2023-06-17","title":".tex | 什么是智能≠智能是什么","layout":"post","keywords":["tex","doc","md","ai"],"hasMath":true,"excerpt":"“什么是智能”的问题每每得不到回答，是因为它的逆问题“智能是什么”没有答案。","content":"\n## 0\n\n这是一篇酬和之作。\n\n徵文标题说的是：\n\n\u003e **機器會製造「內涵」嗎？**\n\u003e \n\n但是正文提出的问题是：\n\n\u003e AI透過程式組合出回答你問題的文字組合，有「涵義」嗎？\n\u003e \n\n可是，「內涵」和「涵義」两个词的——内涵/涵义——就不完全一样啊……\n\n“内涵”(connotation) 通常指词语或表达方式所隐含的情感、态度、暗示或附加的意义。它涉及到词语或表达方式所引起的情感、联想或隐含的观点。也就是弦外之音。\n\n而“涵义”(meaning) 一般指词语、表达方式或行为所传达的字面意义或字面上的定义。它强调的是直接的、明确的意义。\n\n看热闹不嫌事大，那我们再把问题搞复杂一点——在逻辑学里也有一个“内涵”(intension)，和“外延”(extension) 相对应。用**面向对象编程**的说法来理解，一个类里面定义的所有状态量和内部方法的集合，就构成这个类的“**内涵**”；所有（已经和将来能够）从这个类实例化出来的对象的集合，就构成这个类的“**外延**”。\n\n所以看起来，征文者想问的是日常“内涵”也就是言外之意，但是怕杠精（比如我）用有严格定义的逻辑“内涵”解构掉，所以换了“涵义”一词。\n\n这个问题很显然是因应最近大语言模型掀起的这一波 AI 浪潮。这个问题往前再问一句，就是“大语言模型是智能体/有智能吗？”\n\n- 前两年 DeepMind 的 AlphaGo/AlphaZero 系列 AI 在围棋中击败人类棋手时，人们也在问这个问题。\n- 上世纪四五十年代专家系统 (expert system) 刚刚开发的时候，人们也在问这个问题。\n- 从电子计算机往前追溯到机械计算机，甚至是巴比奇的差分机的时候，人们就已经开始问这样的问题了。\n\n这些问题求并集，然后在问题数量趋近于无穷下的极限，就是“什么是智能”。\n\n这样的问题每每得不到回答，是因为它的逆问题“智能是什么”没有答案。我们并没有智能的准确定义，只能一事一论。而之前的智能和非智能体的区别太明显，以至于作出判断也不能对智能的定义有所启发。\n\n## 1\n\n而对“智能是什么”的探究，哲学、逻辑学、计算机科学、生物学、管理学，不同领域的研究者有着不同的思路。\n\n### 古哲学·洞穴之壁与理念世界\n\n古希腊哲学家柏拉图在《理想国》里提到了“洞穴之壁”的寓言故事。\n\n有一群被囚禁在一个深洞的囚徒，从出生开始就被束缚在这个洞穴里，脖子和腿都被铁链锁住，没办法转身或离开。囚徒身后的洞穴入口处有一道火焰，火焰后有人持物体走过，物体的投射在洞穴内的墙壁上形成了影子。囚徒们就以为这些影子就是唯一的存在。\n\n这里的囚徒代表着人类，洞穴代表着世界，影子则代表着我们对于现象世界的感知和观念。人们的知识和信念往往受限于自己的经验和感知，就像囚徒们只看到了洞穴墙壁上的影子，而在影子之外还存在一个理想的理性世界。柏拉图用这个寓言故事表达了他对于人类认识和智慧的理解。所谓智慧，就是从洞穴的影子反过去推测火把前物体的能力。\n\n当然，这种思想被 Marx 主义定性为一种客观唯心主义、唯理论，是受其批判的。\n\n### 逻辑学·从命题到希尔伯特算符\n\n柏拉图的学生亚里士多德，今天在低年级的物理教科书里基本是个反面典型，但他对逻辑学进行了系统化和全面的研究，提出了许多逻辑学的基本概念和原理。这些成果后来成为了欧洲哲学和逻辑学的基石，对西方哲学和科学的发展产生了深远影响。\n\n所谓逻辑，就是研究命题的对错，以及如何判断命题对错的学问。而命题，就是能被判断对错的句子。但是句子显然可以再分成不同成分，于是就发明/发现了主体、客体、谓词、谓词的量词……等等概念，以及用这些概念构造命题的方法。\n\n但是要注意，虽然逻辑主要由语言来表达，但是逻辑还是和语言不同，主体、客体也不等于句子的主语、宾语。这两者的区别，基本可以类比于之前洞穴之壁寓言里的实体和影子。\n\n这种努力到目前为止的巅峰，基本上要数希尔伯特形式化逻辑系统了。感兴趣的朋友可以自行查阅戈得门特《代数学教程》的第一章，这玩意相当于思想界的引体向上，反正我是一个也拉不上去……\n\n### 计算机·从半导体到抽象语法树\n\n希尔伯特是德国的数学家，《代数学教程》也是数学而不是哲学教材。显而易见，逻辑虽然由哲学家奠基，但是主导权很快落到数学家，至少是哲学家兼数学家手里了。\n\n命题的“真”与“非真”同构于 {1, 0}，各种逻辑运算都可以分解成“或”与“非”两种基本逻辑运算的组合，这就是以数学家乔治·布尔 (George Boole) 命名的布尔代数。因为 {1, 0} 又可以同构于半导体电路的高低电位，和各种类似继电器的门电路组合，所以很容易用计算机在物理世界表示出这些逻辑运算。\n\n我们的电脑由上亿个这样的电位和逻辑门组成，一般的科普文章应该会去介绍芯片啊光刻机之类的东西，本文关注的是另一个方面：虽然生产电脑配件的厂商很多，不同的型号的元器件设计不同，组装出的成品应该千差万别，但是他们可以运行同样的程序，理想条件下（虽然实际工程中常常不理想）我们也可以期望他们跑出同样的结果。\n\n这说明所谓计算机科学，并不等同于研究计算机元件的电子科学和工程，这里电科和电子工程相当于洞穴岩壁上的影子，而计算机科学就相当于火光前的物体。这种超越物理的计算本质，一般用一种叫做“抽象语法树”的数据结构来表示。\n\n### 生物学·从神经元到神经网络\n\n人们发明计算机的时候，基本上还是把它当作工具，就没期望它有什么主体性和智慧。\n\n而随着生物学逐渐发现了神经系统及其作用，也随着物理学在二十世纪初的大发展之后的相对平静，很多物理学家开始插手其他学科。既然生命和非生命体的背后都服从同一套物理规律，既然物理学的众多成功经验说明，搞清楚构成系统的所有微观组成就可以理解宏观的系统，那么搞清楚人类的智力器官的基本单元以及相互作用，按理说也就能够理解什么是智慧。\n\n![a cartoon illustrating a neuron](/photos/2023-06-17-neuron.png)\n\n上图是一个神经细胞的结构示意图。从其他神经细胞释放出来的名为神经递质的化学物质，到达神经元左侧短且密集的树突之后，激活细胞膜表面的离子泵，主动运输离子跨过细胞膜，从而产生电信号。电信号沿细胞膜传导到右侧的树突，刺激凸触释放神经递质给下一个细胞。\n\n![a handdrawing style illustration of perceptron](/photos/2023-06-17-perceptron.png)\n\n上图就是根据神经元的工作原理抽象出的数学模型，名为 perceptron。一个 perceptron 就是一个函数，接受多个输入的自变量，加权求和之后套一个非线性的激活函数，得到一个输出。很多个这样的 perceptron 并连和串联，就构成下图，计算机算法中的神经网络。\n\n![a handdrawing style illustration of a neural network](/photos/2023-06-17-neural-network.png)\n\n而从实验方向研究神经系统，我们隔壁系就有，经常来我们系招人。基本上就是在小鼠的天灵盖上锯开一个天窗，然后给它带上个头盔，头盔上有能从天窗伸进去的电极，采集脑神经的电信号。以前头盔有网线伸到实验室天花板，实时传到数据中心的超算。现在好像进步了，改用 Wi-Fi 了。\n\n这实验怎么通过的伦理审查，咱也不知道，咱也不敢问……\n\n### 管理学·DIKW “数据-信息-知识-智慧”模型\n\n![a pyramid of DIKW model](/photos/2023-06-17-DIKW.png)\n\nDIKW 四个字母分别代表 data, information, knowledge, wisdom，即数据、信息、知识、智慧，是一种知识管理中的心智模型。\n\n四个层次，前一层都是后一层的基础，后一层都是对前一层的理解。\n\n如果是书面文字，数据就是笔画和字母；如果是语言，数据就是人声的响度、频率和音色。由笔画/字母/声音组成的有含义的字词就是信息。表示信息之间的关系的，可以判断对错的命题就是知识。包含和统摄各条知识的思想体系，就是智慧。\n\n反过来说，虽然智慧高于思想，但它仍需要通过把各条知识的表达汇总起来，才能被人感知。对知识的命题的理解依赖于构成名字的各个概念的涵义，属于信息水平的内容。而每个字都有不考虑其涵义的笔画字母构成。\n\n这层与层之间**看似**并没有插入额外的内容，智慧可以直接由笔画构成。但是我们一层层理解的深入，其实是不自觉地借用了我们当前社会约定俗成的解读方式。\n\n比如下面这个图片里的符号，对于现代人就只是数据，无法解读成信息。但是对于苏美尔人，这是用楔形文字表示的数字，是等腰直角三角形的腰和直角边的比值，也就是 $$\\sqrt{2}$$ 的近似值。\n\n![sumerian numerical approximation to square root of two](/photos/2023-06-17-ancient-root-2.png)\n\n约定俗成的数据解读方式，也就是关于**数据的数据**，根据西方的构词法，可以叫做“**元**数据”(meta-data)。\n\n数据和元数据一起构成信息，信息和元信息一起构成知识，知识和元知识一起构成智慧。俺坚持写博客的动机，就是用费曼学习法，把无意间使用的元知识显式地表达出来，而且记录下来，争取学而不退转。\n\n## 2\n\n回顾了这些，再来看大语言模型，就会发现它落在了各方努力的延长线的交点。\n\n大语言模型里有一个重要概念叫做“嵌入”(embedding)，就是把语言的基本字元 (token) 可逆地映射到一个超多维度的向量空间里。本来“国王”和“儿子”之间没办法加减乘除，但是嵌入后的向量空间里有加法和数乘，如果嵌入函数选得好，“国王”的向量 + “儿子”的向量，结果向量就约等于“王子”的向量。\n\n![illustration of vector addition from wikipedia](/photos/2023-06-17-vector-addition.png)\n\n生成式语言模型的核心就是一个超多元函数，接受前一个字嵌入后的向量作为输入，给出另一个向量作为输出，用嵌入函数的逆映射翻译成字元；再把旧的输出作为新的输入，直到输出结果是“语段结束”这样一个特殊字元为止。模型训练的过程，主要就是通过现成的语料，拟合这个超多元函数的参数。\n\n从 DIKW 模型来看，语言模型操作的是最基本的数据，它的输出究竟是什么信息，是不是正确的知识，体现了多少智慧，是人根据当下的社会文化来解读的。\n\n而实现 AI 的电子计算机，或是复杂生命的大脑，他们和智能之间的关系，应该就类似于具体的计算机电路和抽象语法树之间的关系。以此类比，未来的智能科学应该会成为一门独立的专业，它和计算机科学和神经生物学的区别，就像今天的电子科学与工程，和计算机科学之间的区别一样。当下神经生物学的热度，将来恐怕多半会被分流。\n\n这种对字符的计算不同于逻辑运算，语言模型不判断输出结果在逻辑上的正确与错误，这既给了他啥都能说几句的 feature，又给了它经常编假消息的 bug。\n\n想要改掉这种错误，引入对 AI 的纠错机制，治本之道恐怕还是诉诸于对世界的正确描述，与理论相关的还是要靠逻辑，与现实相关的还是要靠科学。\n\n只不过，大语言模型提供了一种数据结构，有希望把人类已知的真理储存在一起。对这种数据结构本身的研究，有可能反过来启发科学的发展。柏拉图的洞穴之壁可能不再是一个比喻，未来更大的语言模型的，亿万维度的参数空间有希望成为洞穴门口的那团火。\n\n只不过这一切都是“可能”，现在还只是 AI 的萌芽阶段，还没有足够的证据来证实或者证伪这种畅想。而且 AI 的参数量再大也是有限的，它所能表达的信息也就有限，而真理应当是无限的，就像科学一样，总要训练更新更大的模型，总要发现已知的未知，然后欣然接受更多未知的未知之存在。\n\n如果电子计算机实现的 AI 独立于人类产生了意识和超出人类的智慧，很难想象他们会继续用人类语言这种对他们来说很不方便的方式来交流。\n\n所以，哪怕是做个 AI 生成内容的质检员，科学家依然有事可做。这算是科学的堕落吗？当然不算，如果算的话，那从计算物理也被当作理论物理的那天起，人类就已经投降了（逃）\n\n## 3\n\n现在正面来回答问题：AI透過程式組合出回答你問題的文字組合，有「涵義」嗎？\n\n答：有。\n\n因为语言的「涵義」来自于语言的内容，和整个社会的文化，并不来自于这句话的作者的身份。即便是人与人之间的交流，诉诸身份也是一种非形式逻辑谬误，是理性不足的表现。只有在信息不足仍不得不下结论的时候才该使用，比如法律判决时的自由心证主义和/或法定证据主义。\n\n而鹿妈眼里真人鹿酱与 AI 鹿酱的区别，如果有的话，好像主要体现在动机的区别。动机这种东西，很多智慧不高的生物，比如小猫小狗都会有；而现在的 AI，似乎还没有展现出超出编程者设计的动机。编程写入的信息有限，现有 AI 的动机也就有限，鹿酱的赢面还是很大的。\n\n而动机是生物与非生物的区别吗？而什么是生物 ≠ 生物是什么，那就是另一个含混而复杂的问题了。\n\n## 4\n\n这篇博文发布的时候，高考应该已经结束了，马上该填报志愿了。\n\n那么，西元 2023 年，AI 来袭的当下，该选个啥专业在 AI 浪潮中幸存，或者选个啥专业给 AI 老爷带路呢？\n\n![a screenshot of a quotation from Three Body about attitudes towards aliens](/photos/2023-06-17-three-body-quotation.png)\n\n我的建议是，不要听别人的建议，按自己的兴趣来就好了。\n\n刚刚改开的时候，有一个超级热门的专业，叫科技英语。科技落下了好多年，对外开放需要语言交流，两者一结合应该是热门又稀缺了。结果呢，你现在还听说过这个专业吗？\n\n科技很重要是不错，语言很重要也不错，但是搞科技的人自己可以学英语，学英语的有几个搞得了科技？社会的进步主要靠创新，而创新的方向难以预测，不论这种预测分析听起来多有道理。\n\n如果真的找不到兴趣，那就在能力范围之内，找个难度最高的。如果想从事智力劳动，那数学含量是个不错的衡量标准；如果不排斥体力劳动，那训练时间越长越值得考虑。\n\n但这只是填志愿来不及时的权宜之计，发掘兴趣是人一生的课题。\n\n兴趣不是为了让你成功的时候更得意，毕竟成功的话不论做什么都很得意；\n\n兴趣是为了你不成功时也可以不失意，毕竟平凡才是人生的真谛。\n"},{"slug":"physics-based-neural-network-review-note","filename":"2023-03-20-physics-based-neural-network-review-note.md","date":"2023-03-20","title":".tex | 基于物理的神经网络 (PINN) 综述笔记","layout":"post","keywords":["tex","phy","md","ai"],"hasMath":true,"excerpt":"本文是《Scientific Machine Learning Through Physics–Informed Neural Networks: Where we are and What’s Next》这篇综述的读书笔记。","content":"\n\u003e 本文是《[Scientific Machine Learning Through Physics–Informed Neural Networks: Where we are and What’s Next](https://link.springer.com/article/10.1007/s10915-022-01939-z)》这篇综述的读书笔记。\n\u003e \n\n年前，今年新入职的天文学方面的一位老师给我们群发邮件，宣传某国家实验室超算的 GPU 编程马拉松活动，他可以担任指导老师。于是毫不意外地，我报了名。该编程马拉松项目还需要专门申请，申请材料里要写清楚打算干什么，于是报名的五六个人七嘴八舌地想创意。基于物理的神经网络 PINN 就是天文老师的点子。\n\n~~写到这里，我才意识到，老哥是不是想拿我们当免费劳动力啊~~~\n\n神经网络可以看作是一个复杂的非线性函数，接受一个（一般来说维度很高的）向量作为输入，一番计算后输出另一个向量。训练神经网络，就是找到这个函数的参数，绝大多数找参数的方法涉及计算网络输出对参数的偏导数，因此神经网络计算框架的核心功能就是自动微分 (auto-differentiation)。\n\n而很多物理问题，都可以用（偏）微分方程来描述，微分方程的解不是变量，而是函数，而且往往是复杂的非线性函数。所以基于物理的神经网络 (PINN) 就是以神经网络来表达这个函数，然后把这个函数带入到物理的微分方程中，把神经网络输出和真正的物理解之间的差距当作损失函数，反向传播回去来优化神经网络的参数。代入方程时的微分计算，正好可以利用现成框架的自动微分功能。\n\n在以 GPT 为代表的 transformer 类神经网络模型出现之前，自然语言处理类的机器学习项目，往往要在网络之外，利用人类的语法知识，对语段进行语义分割等等“中间任务”。Transformer 一出，算力出奇迹，中间任务逐渐变得没有必要了。\n\n在 GPT 崭露头角，并且越来越有迹象表明其将会涌现出通用人工智能的今天，这些基于物理的神经网络，会不会还未成熟就已过时？这种心情，就和《三体》第二卷开始，章北海和吴岳面对焊渍未漆的“唐”号航空母舰时差不多吧……\n\n\u003chr class=\"slender\"\u003e\n\n- Abstract\n    - PINNs are neural networks that encode model equations. a NN must fit observed data while reducing a PDE residual.\n\n1. Introduction\n    - The “curse of dimensionality” was first described by Bellman in the context of optimal control problems. (Bellman R.: Dynamic Programming. Sci. 153(3731), 34-37 (1966))\n    - Early work: MLP ([multilayer perceptron](https://en.wikipedia.org/wiki/Multilayer_perceptron)) with few hidden layers to solve PDEs. ([https://doi.org/10.1109/72.712178](https://doi.org/10.1109/72.712178))\n    - 感觉可能更全面的一篇综述：[https://doi.org/10.1007/s12206-021-0342-5](https://doi.org/10.1007/s12206-021-0342-5)。该文关注 what deep NN is used, how physical knowledge is represented, how physical information is integrated，本文只关于 PINN, a 2017 framework。\n\n    1. What the PINNs are\n        - PINNs solve problems involving PDEs:\n            - approximates PDE solutions by training a NN to minimize a loss function\n            - includes terms reflecting the initial and boundary conditions\n            - and PDE residual at selected points in the domain (called **collocation points**)\n            - given an input point in the integration domain, returns an estimated solution at that point.\n            - incorporates a [residual network](https://en.wikipedia.org/wiki/Residual_neural_network) that encodes the governing physical equations\n            - can be thought of as an **unsupervised strategy** when they are trained solely with physical equations in forward problems, but **supervised learning** when some properties are derived from data\n        - Advantages:\n            - [mesh-free](https://en.wikipedia.org/wiki/Meshfree_methods)? 但是我们给模型喂训练数据的时候往往已经暗含了 mesh 了吧\n            - on-demand computation after training\n            - forward and inverse problem using the same optimization, with minimal modification\n    2. What this Review is About\n        - 提到了一个做综述找文章的方法：本文涉及的文章可以在 Scopus 上进行高级搜索：`((physic* OR physical)) W/2 (informed OR constrained) W/2 “neural network”)`\n2. The Building Blocks of a PINN\n    - question:\n    \n    $$\n    F(u(z);\\gamma)=f(z),\\quad z\\ \\in\\ \\Omega \\\\ B(u(z))=g(z), \\quad z\\ \\in\\ \\partial \\Omega\n    $$\n    \n    - solution:\n    \n    $$\n    \\hat u_{\\theta}(z)\\approx u(z)\\\\ \\theta^* = \\arg\\min_{\\theta}\\left(\\omega_F L_F(\\theta)+\\omega_BL_B(\\theta)+\\omega_{data}L_{data}(\\theta)\\right)\n    $$\n    \n    1. Neural Network Architecture\n        - DNN (deep neural network) is an artificial neural network that is deeper than 2 layers.\n        \n        1. Feed-Forward Neural Network: \n            - $$u_{\\theta}(x) = C_{K} \\circ C_{k-1} ...\\alpha \\circ C_1(x),\\quad C_k(x) = W_k x_k + b_k$$\n            - Just change CNN from convolution to fully connected.\n            - Also known as multi-layer perceptrons (MLP)\n            \n            1. FFNN architectures \n                - Tartakovsky et al used 3 hidden layers, 50 units per layer,  and a hyperbolic tangent activation function. Other people use different numbers but of the same order of magnitude.\n                - A comparison paper: *Blechschmidt, J., Ernst, O.G.: Three ways to solve partial differential equations with neural networks –A review. GAMM-Mitteilungen 44(2), e202100,006 (2021).*\n            2. multiple FFNN: 2 phase [Stephan problem](https://en.wikipedia.org/wiki/Stefan_problem).\n            3. shallow networks: for training costs\n            4. activation function: the swish function in the paper has a learnable parameter, so — [how to add a learnable parameter in PyTorch](https://discuss.pytorch.org/t/how-could-i-create-a-module-with-learnable-parameters/28115)\n        2. Convolutional Neural Networks: \n            - I am most familiar with this one.\n            - $$f_i(x_i;W_i)=\\Phi_i(\\alpha_i(C_i(W_i,x_i)))$$\n            - performs well with multidimensional data such as images and speeches\n            \n            1. CNN architectures: \n                - `PhyGeoNet`: a physics-informed geometry-adaptive convolutional neural network. It uses a coordinate transformation to convert solution fields from irregular physical domains to rectangular reference domains.\n                - According to Fang ([https://doi.org/10.1109/TNNLS.2021.3070878](https://doi.org/10.1109/TNNLS.2021.3070878)), a Laplacian operator can be discretized using the finite volume approach, and the procedures are equivalent to convolution. Padding data can serve as boundary conditions.\n            2. convolutional encoder-decoder network\n        3. Recurrent Neural Network\n            - $$f_i(h_{i-1})=\\alpha\\left(W\\cdot h_{i-1}+U\\cdot x_i+b\\right)$$, where f is the layer-wise function, x is the input, h is the hidden vector state, W is a hidden-to-hidden weight matrix, U is an input-to-hidden matrix and b is a bias vector. 我认为等号左边的 $$h_{i-1}$$ 应当作为下标\n            - 感觉有点像 hidden Markov model，只不过 Markov 中间的 hidden layers 好像与序号无关（记不清了），~~RNN 看起来各个 W 和 H 似乎不同~~。**RNN cell is actually the exact same one and reused throughout.** (from [https://blog.floydhub.com/a-beginners-guide-on-recurrent-neural-networks-with-pytorch/](https://blog.floydhub.com/a-beginners-guide-on-recurrent-neural-networks-with-pytorch/)). Cartoon from Wikipedia:\n                \n                ![Untitled]({{ site.baseurl }}/assets/photos/2023-03-20-rnn-unit.png)\n                \n            - From [https://blog.floydhub.com/a-beginners-guide-on-recurrent-neural-networks-with-pytorch/](https://blog.floydhub.com/a-beginners-guide-on-recurrent-neural-networks-with-pytorch/):\n                \n                ![Untitled]({{ site.baseurl }}/assets/photos/2023-03-20-rnn-types.png)\n                \n            1. RNN architectures\n                - can be used to perform numerical Euler integration\n                - 基本上输出的第 i 项只与输入的第 i 和 i-1 项相关。\n            2. LSTM architectures\n                - 比 RNN 多更多中间隐变量，至于怎么做到整合长期记忆的，技术细节现在可以先略过\n        4. other architectures for PINN\n            1. Bayesian neural network: weights are distributions rather than deterministic values, and these distributions are learned using Bayesian inference. 只介绍了[一篇文章](https://doi.org/10.1016/j.jcp.2020.109913)\n            2. GAN architectures: \n                - two neural networks compete in a zero-sum game to deceive each other\n                - physics-informed GAN uses automatic differentiation to embed the governing physical laws in stochastic differential equations. The discriminator in PI–GAN is represented by a basic FFNN, while the generators are a combination of FFNNs and a NN induced by the SDE\n            3. multiple PINNs\n    2. Injection of Physical Laws\n        - 既然是要解常/偏微分方程，那么微分计算必不可少。四种方法：hand-coded, symbolic, numerical, auto-differentiation，最后一种显著胜出。所谓 auto-differentiation, 就是利用现成框架，框架自动给出原函数的导数的算法。\n        - Differential equation residual:\n            - $$r_F[\\hat u_\\theta](z)=r_\\theta(z):=F(\\hat u_\\theta(z);\\gamma)-f$$\n            - $$r_F[\\hat u_\\theta](z)=r_\\theta(x,t)=\\frac{\\partial}{\\partial t}\\hat u_\\theta(x,t)+F_x(\\hat u_\\theta(x,t))$$: 原文给出了来源，但是从字面上看不出来与前式的等价性\n        - Boundary condition residual: $$r_B[\\hat u_\\theta](z):=B(\\hat u_\\theta(z))-g(z)$$\n    3. Model Estimation by Learning Approaches\n        1. Observations about the Loss\n            - $$\\omega_F$$ accounts for the fidelity of the PDE model. Setting it to 0 trains the network without knowledge of underlying physics.\n            - In general, the number of $$\\theta$$ is more than the measurements, so regularization is needed.\n            - The number and position of residual points matter a lot.\n        2. Soft and Hard Constraints\n            - Soft: penalty terms. Bad:\n                - satisfying BC is not guaranteed\n                - assignment of the weight of BC affects learning efficiency, no theory for this.\n            - Hard: encoded into the network design. [Zhu et. al](https://doi.org/10.1007/s00466-020-01952-9)\n        3. Optimization methods\n            - minibatch sampling using the Adam algorithm\n            - increased sample size with L-BFGS (limited-memory Broyden-Fletcher-Goldfarb-Shanno)\n    4. Learning theory of PINN: roughly in DE, consistency + stability → convergence\n        1. convergence aspects: related to the number of parameters in NN\n        2. statistical learning error analysis: use *risk* to define *error*\n            - Empirical risk: $$\\hat R[u_\\theta]:=\\frac{1}{N}\\sum_{i=1}^N \\left\\|\\hat u_{\\theta}(z_i)-h_i\\right\\|^2$$\n            - Risk of using approximator: $$R[\\hat u_{\\theta}]:=\\int_{\\bar \\Omega}(\\hat u_{\\theta}(z)-u(z))^2dz$$\n            - Optimization error: the difference between the local and global minimum, is still an open question for PINN. $$E_O:=\\hat R[\\hat u_{\\theta}^*]-\\inf_{\\theta \\in \\Theta}\\hat R[u_\\theta]$$\n            - Generalization error: error when applied to unseen data. $$E_G:=\\sup_{\\theta \\in \\Theta}\\left\\|R[u_\\theta]-\\hat R[u_\\theta]\\right\\|$$\n            - Approximation error: $$E_A:=\\inf_{\\theta \\in \\Theta}R[u_\\theta]$$\n            - Global error between trained deep NN $$u^*_\\theta$$ and the correct solution is bounded: $$R[u^*_\\theta]\\le E_O+2E_G+E_A$$\n            - 有点乱，本来说 error 是误差，结果最后还是用 risk 作为误差\n        3. error analysis results for PINN\n3. Differential Problems Dealt with PINNs：读来感觉这一部分意义不大，将来遇到需要解决的问题时，回来看看之前有没有人做过就行了——另一方面看，一类方程就需要一类特殊构造的神经网络来解，那么说明神经网络解方程的通用性并不好~\n    1. Ordinary differential equations: \n        - Neural ODE as learners, a continuous representation of **ResNet**. [[Lai et al](https://doi.org/10.1016/j.jsv.2021.116196)], into 2 parts: a physics-informed term and an unknown discrepancy\n        - LSTM [[Zhang et al](https://doi.org/10.1016/j.cma.2020.113226)]\n        - [Directed graph models](https://doi.org/10.1016/j.compstruc.2020.106458) to implement ODE, and Euler RNN for numerical integration\n        - Symplectic Taylor neural networks in [Tong et al](https://doi.org/10.1016/j.jcp.2021.110325) use symplectic integrators\n    2. Partial differential equations: steady/unsteady的区别就是是否含时\n        1. steady-state PDEs\n        2. unsteady PDEs\n            1. Advection-diffusion-reaction problems\n                1. diffusion problems\n                2. advection problems\n            2. Flow problems\n                1. Navier-Stokes equations\n                2. hyperbolic equations\n            3. quantum problems\n    3. Other problems\n        1. Differential equations of fractional order\n            - automatic differentiation not applicable to fractional order → [L1 scheme](https://doi.org/10.1515/fca-2019-0086)\n            - [numerical discretization for fractional operators](https://doi.org/10.1137/18M1229845)\n            - [separate network to represent each fractional order](https://doi.org/10.1038/s43588-021-00158-0)\n        2. Uncertainty Estimation: [Bayesian](https://doi.org/10.1016/j.jcp.2020.109913)\n    4.  Solving a Differential Problem with PINN\n        - 1d non-linear Schrödinger equation\n        - dataset by simulation with MATLAB-based Chebfun open-source(?) software\n4. PINNs: Data, Applications, and Software\n    1. Data\n    2. Applications\n        1. Hemodynamics\n        2. Flows Problems\n        3. Optics and Electromagnetic Applications\n        4. Molecular Dynamics and Materials-Related Applications\n        5. Geoscience and Elastiostatic Problems\n        6. Industrial Application\n    3. Software\n        1. `DeepXDE`: initial library by one of the vanilla PINN authors\n        2. `NeuroDiffEq`: PyTorch based used at Harvard IACS\n        3. `Modulus`: previously known as Nvidia SimNet\n        4. `SciANN`: implementation of PINN as Keras wrapper\n        5. `PyDENs`: heat and wave equations\n        6. `NeuralPDE.jl`: part of SciML\n        7. `ADCME`: extending TensorFlow\n        8. `Nangs`: stopped updates, but faster than PyDENs\n        9. `TensorDiffEq`: TensorFlow for multi-worker distributed computing\n        10. `IDRLnet`: a python toolbox inspired by Nvidia SimNet\n        11. `Elvet`: coupled ODEs or PDEs, and variational problems about the minimization of a functional\n        12. Other Packages\n5. PINN Future Challenges and Directions\n    1. Overcoming Theoretical Difficulties in PINN\n    2. Improving Implementation Aspects in PINN\n    3. PINN in the SciML Framework\n    4. PINN in the AI Framework\n6. Conclusion\n"},{"slug":"logical-science-from-west","filename":"2022-08-22-logical-science-from-west.md","date":"2022-08-22","title":".doc | 也谈近代科学从西方起步","layout":"post","keywords":["doc","tex","phy","m","phi"],"excerpt":"为什么近代科学偏偏是在丢过一次古典传统的西方起步的呢？为什么那些成功继承了古典时代智慧的中古文明，比如伊斯兰文明或古中国文明，反而没有成功萌发近代科学思想呢？","content":"\n前不久在公众号转载过“海边的西塞罗”写的《**嗯！您关注的是一个早晚要“凉凉”的公众号**》，标题起得让人不知所云，但是文章内容讨论的是“近代科学为什么从西方起步”的问题，原文说：\n\n\u003e 既然你所讲述的，欧洲从古典时代到文艺复兴、科学曾经出现过一次“断层”，欧洲人是通过翻译阿拉伯人转译的古典时代文献才继承了希腊罗马先贤们的思想的。\n\u003e \n\u003e \n\u003e 那么**，为什么近代科学偏偏是在丢过一次古典传统的西方起步的呢？为什么那些成功继承了古典时代智慧的中古文明，比如伊斯兰文明或古中国文明，反而没有成功萌发近代科学思想呢？**\n\u003e \n\n作者立了一个靶子——\n\n\u003e 我之前听到的比较靠谱的解答，**是古希腊罗马有较好的数学思想，当定量的数学思想与定性的“自然哲学”发生结合，近代科学就诞生了。**\n但这种解释，其实也回答不了一个问题——你可以说古代东方离着希腊远，没有受到希腊某些思想的“药引”的启发。但特别奇怪的是，中世纪的中东却不是这样。\n伊斯兰文明的伍麦叶王朝在公元九世纪曾经掀起过一场声势浩大的“百年翻译运动”，……近代启发西方的那些古典思想典籍，阿拉伯人全有，且早获得了好几百年。\n\u003e \n\n给出的回答是所谓**“托勒密困境”**，即诸文明中的科学技术研究者因为要满足当权者/赞助者的功利性需要，将时间与精力耗费于附会科学（比如天文学）的非科学甚至伪科学（比如占星术）之上，而——\n\n\u003e 这种错误的职业拼接，锁死了天文学的进一步发展的通路，导致其无法实现向近代科学的飞跃——即便托勒密会数学、引入定量计算，也依然没用。\n\u003e \n\u003e **而这种“托勒密困境”，其实也是所有古典时代学者的困境——他们在研究学问时，必须回答“求用”的问题。**\n\u003e \n\u003e ……\n\u003e \n\u003e **于是从托勒密到哥白尼，我们会发现西方在这一轮对天文学的失而复得中，其实并没有增添什么，而是丢掉了一种东西——那就是“求用”的思维。**\n\u003e 欧洲知识分子们研究科学的正义性，来自于他们认定：自然作为一种上帝的造物，其本身就是美的。因此研究它、探索它本身，就是在赞美上帝，所以科学研究不必“求用”也有天然的正义性。\n\u003e \n\n作者写近代西方科学的不求用，是为了托物言志，检讨自己为了读者的关注不得不在历史写作之外“写时评、表达观点、带情绪”，预告自己将来可能会去写作崇高的钻研历史的题目。\n\n给蹭热点找理由这件事，我也做过嘛，感觉写的比这篇文章还简约隽永且立意高远呢～（文人相轻.jpg）\n\n但是科学革命发源于西方这个问题，我也很感兴趣，而且有自己的思考，而且思考的结果和上文不同。\n\n\u003chr class=\"slender\"\u003e\n\n学物理的孩子应该都听说过《费曼物理学讲义》的大名，没听说过的话建议听说一下，自主招生考试面试装逼的时候用的上。费曼先生在引言中也立了个靶子说——\n\n\u003e 你们可能会问，在讲述欧几里德几何时，先是陈述公理，然后作出各种各样的推论，那为什么在讲授物理学的时候不能先直截了当地列出基本规律，然后再就一切可能的情况说明定律的应用呢？\n\u003e \n\n然后上来就讲原子论，开篇问：\n\n\u003e 假如由于某种大灾难，所有的科学和知识都丢失了，只有一句话可传给下一代，那么怎样才能用最少的词汇来传达最多的信息呢？\n\u003e \n\n可惜这个问题仅仅是为了引出原子论，实在是大材小用。这说明费老先生浸淫于西方科学中，“不识庐山真面目，只缘身在此山中”。而我对近代科学起自西方的解释，正好就是这两句话串起来。下面就要兜一个大圈子，把两句话圆起来。\n\n\u003chr class=\"slender\"\u003e\n\n科学者，对世界之正确认知也。\n\n根据这个定义，把人们已知的，关于这个世界的所有知识罗列到一个集合里，这个集合就是科学。我们只谈到了一个集合，不涉及逻辑推演，也不涉及数学带来的定量优势，更不判断从事科学研究的人是否功利。\n\n但是，集合这种知识结构过于简单——\n\n- 集合里的各个元素都是平等的，要想表示出整个集合，除了全默写出来没别的办法；\n- 集合里的元素之间没有顺序，想取得其中的某一条科学命题，只能像抓阄一样，凭运气抽到为止。\n- 一旦由于天灾人祸，集合中的部分内容丢失，除了重新把当初发现它们时经历的艰难困苦重复一遍，也没有别的办法。（哦，也可以去隔壁文明的图书馆翻译。）\n\n所以，必须找到一种更复杂的结构，来组织这些信息，解决上述问题。\n\n\u003chr class=\"slender\"\u003e\n\n计算机专业有门基础课《数据结构与算法》，谈数据结构，最基础的两种就是数组和链表；谈算法，最基础的概念就是函数。注意，这里说的是数据结构，刚才说的是知识结构，两者可以类比，但并非同一概念。\n\n数组，和集合几乎一样，只不过给每个元素标记了一个序号。在计算机里，由于规定数组连续存放，每个元素占用内存长度相等，所以可以通过序号，从数组开头偏置指针，以 O(1) 的时间复杂度取得任意元素，快。\n\n类比到知识结构，语数外理化政史地生，一年级二年级三年级，第一章第二章第三章，第一第二第三个知识点，背吧。列表与列表之间井水不犯河水，你数学老师说你体育老师拉稀了不能上课，你体育老师说你数学老师放屁，两者完全可以在你的知识体系里共存。\n\n链表，和数组一样有顺序，但是并不给每个元素标号，而是在前一个元素的末尾，写上下一个元素的位置指针。找到一个元素需要从链表的开头一个一个往后捋，慢。好处是修改方便，在链表中间塞进去一个新元素，只需要把前面一个的指针指向新元素，新元素的指针指向后一个元素，删除一个旧元素也类似，只对增删点附近一个很小的区域进行改动，整个链表不会伤筋动骨。\n\n但是不论数组还是链表，都需要把所有的知识全写出来，随着时间的积累，科学的总量早晚要超越人脑的记忆力，超越笔记的厚度，对于个人，要皓首穷经，要韦编三绝，才有希望提出一点新内容；对于全人类，图书馆越造越大，一轮战乱，从头再来。\n\n于是函数登场。给定一个/一组输入，根据函数体描述的算法，返回确定的输出。那我们找到一种方法，写一个函数，接收链表的前一个元素作为输入，找到后一个元素输出。这样我们只需要存储第一个元素和这个函数，就可以恢复出整个链表，用计算换空间。\n\n\u003chr class=\"slender\"\u003e\n\n类比到知识结构，这个函数就是逻辑推演。\n\n科学内容中的每一条知识都是一个**命题**。\n\n从少数几条知识出发，这几条在逻辑上就称为**公理**，自然科学里也称之为**定律**。\n\n命题之间可以做**逻辑运算**，**或**、**且**、**非**、**蕴含**等等，运算的结果也是一条新的命题。命题的正确与否，取决于逻辑运算的规定。\n\n通过对公理和已经算出的真命题反复进行逻辑运算，产生的新的真命题，叫做**定理**。\n\n\u003chr class=\"slender\"\u003e\n\n欧几里德几何式的，也就是从有限多个命题出发，承认逻辑推演进行生成的新命题的正确性，这样的一种组织方式——\n\n- 对于学习，科学不再是一家之言，门户之见。一句话的正确性不再由说话者的身份决定，诉诸人身、诉诸权威成了谬误，“我爱吾师，但我更爱真理”一句话有了切实的落脚点。\n- 对于研究，降低了难度，后来者不必从头再来，而是站在前人的终点起跑。发现的新科学有办法整合进现有的科学，证伪的旧科学有办法剔除，而不会让科学整体伤筋动骨。愚弄黔首的矛盾和谬误，真理有办法与之势不两立。\n- 自带有容灾能力，科学得以在摧毁科学记录和科学家人身的重大灾难之后，在几百年的人才断档之后，依然有办法恢复。\n\n\u003chr class=\"slender\"\u003e\n\n刚才说数据结构和知识结构不同，知识管理界有个 DIKW 模型，也就是数据 (Data)、信息 (Information)、知识 (Knowledge)、智慧 (Wisdom)。\n\n纸张上的墨迹组成的字符只是数据，当这些单词按照语法理解为句段篇章之后才构成信息，这些篇章内容指代的概念、关系等等含义构成知识。如何理解知识与知识之间的关系需要智慧。\n\n“继承了古典时代智慧的中古文明，比如伊斯兰文明或古中国文明”——从各个文明没能演化出科学革命来看，**这些文明最多是有一部分学者继承了古典时代的知识，而没能认识到 *用逻辑组织知识* 这一智慧的价值**，而西方发掘出了这种智慧。至于这种发掘发生在西方，是偶然还是必然，由哪些条件促成，那是另一个很有趣的问题了。\n\n“我们会发现西方在这一轮对天文学的失而复得中，其实并没有增添什么，而是丢掉了一种东西——那就是‘求用’的思维。”——西方对天文学的失去，对应的是罗马统治下的和平结束时的战乱与社会崩溃，不论之后的文艺复兴如何光辉灿烂，**这种失落都是对科学乃至整个文明的威胁**，如果没有这种失落，科学革命想必会更早更容易发生。况且这种失落到复兴的整个过程中，对科学有影响的因素实在是太多了，既有正面又有负面，实在是难以分析归因。\n\n至于不求用的思维，有了逻辑推演，科学工作者的产出提高，高到了让社会愿意供养其全职研究的地步，那么不求用的思维，自然会建立起来；不求用对科研效率的提升，良性反哺科学的发展，自然会蔚然成风。反过来，**只有不求用的态度，研究者没有逻辑推演发展科学的能力，资助者没有逻辑推演评价成果的本事，不求用的态度只会鼓励灌水，产出真没用的水货。**\n\n\u003chr class=\"slender\"\u003e\n\n数学对自然科学的作用，定量化只是一个副产品。更重要的是作为逻辑科学的集大成者，发明/发现逻辑推演的规则，探索逻辑推演作为方法论的能力边界。一言以蔽之，欧几里德之后，数学已不只是“数字的学问”。\n\n至于费曼先生，他怎么可能不知道四大力学确实就是按照欧几里德式的，从基本定律出发的方式讲授的呢？面对一伙学普通物理的本科新生，说这种话实在有点骗小孩儿的嫌疑，怪不得那门课上到后来，本科生全都跑了。物理和数学的区别，在于理论和实验两条腿走路，但是理论的这条腿，实实在在地来自于超越了“数字的学问”的数学。\n"},{"slug":"essential-cell-biology-index","filename":"2022-04-29-essential-cell-biology-index.md","date":"2022-04-29","title":".pdf | 《细胞生物学精要 Essential Cell Biology》读书笔记","layout":"post","keywords":["pdf","tex","bio"],"excerpt":"捋一遍目录真的是读一次读不完的书很有效率的做法。","hasMath":true,"content":"\n\n\u003e 2024年1月29日更新：读“我们如何得知”第 3, 13, 15, 16, 17 节, “图版”第 3-1 节\n\n自从在个人博客上开启了[那本书](%7B%%20post_url%202022-03-05-great-cultural-revolution-ten-years-0%20%%7D)的读书笔记系列之后，发现捋一遍目录真的是读一次读不完的书很有效率的做法。\n\n上次发了一篇[《生物学知识提纲》](%7B%%20post_url%202022-02-25-barron-360-biology-contents%20%%7D)，其实我家里是有好几本大部头的本科程度的生物学教材的，这本《细胞生物学精要》就是其中之一，是爸妈来看我的时候用行李箱带过来的中译本。快要找工作了，如果决定更进一步地进入生物领域的话，是时候看点书了。\n\n这本书除了正常的章节之外，每一章都会对于一个科学命题，回答“我们如何得知这一命题”的问题，这一节插在正文中间，这次整理的时候单独摘了出来。另外部分章节末尾有图解部分概念的“图版”，也单独摘了出来。章节繁多，所以只展开了自己当下比较感兴趣的部分，并且将之标红加粗。\n\n## 篇章结构\n\n1. 介绍细胞\n2. 细胞的化学成分\n3. **能量、催化作用、生物合成**\n    1. 细胞中能量的应用\n    2. **自由能和催化作用**\n    3. 活化的载体分子与生物合成\n4. 蛋白质的结构和功能\n5. DNA 和染色体\n6. DNA 复制、修复和重组\n7. 从 DNA 到蛋白质：细胞如何阅读基因组\n8. **基因表达调控**\n    1. 基因表达概述\n    2. **转录是如何开启的**\n    3. 造成特异细胞类型的分子机制\n    4. **转录后调控**\n9. 基因和基因组如何进化\n10. 基因及基因组分析\n11. 膜的结构\n12. **膜转运**\n13. 细胞如何从食物中获得能量\n14. 线粒体和叶绿体中的能量生产\n15. 胞内区室及转运\n16. **细胞通讯**\n    1. 细胞信号传导的一般原理\n    2. G 蛋白偶联受体\n    3. 酶联受体\n17. 细胞骨架\n18. **细胞分裂周期概述**\n    1. 细胞分裂周期\n    2. **细胞周期控制系统**\n    3. S 期\n    4. M 期\n    5. 有丝分裂\n    6. 胞质分裂\n    7. 细胞数量和细胞大小的控制\n19. 性与遗传\n20. 细胞群落：组织、干细胞、癌\n\n## 我们如何得知\n\n1. 生命的共同机制\n2. 什么是大分子\n3. **使用动力学模拟和操作代谢途径**：基本都在 Machaelis-Menton 模型框架之内\n    - 一个酶对其催化反应的最大速率 $$V_{max}$$:\n        - 测量试管中不同底物浓度下的速率，找到其在底物浓度趋于无穷时的渐进行为。将浓度和速率都取倒数后作图，纵轴截距就是最大速率(!)\n        - 更快的反应（几微秒内），需要“停/流仪”([stopped flow](https://en.wikipedia.org/wiki/Stopped-flow))\n    - 调控：竞争性抑制剂不改变 $$V_{max}$$\n    - 设计：通过模拟，但是完全没提模拟的细节。可能需要看正文 10.3.3\n4. 探究蛋白质的结构\n5. 基因由 DNA 组成\n6. 复制的性质\n7. 破译遗传密码\n8. 基因调控——Eve 的故事\n9. 基因数目\n10. 人类基因组测序\n11. 测量膜流\n12. 乌贼为我们展示膜兴奋性的奥秘\n13. **发现柠檬酸循环**\n    - 当时的发现者 Krebs 等人：\n        - 肌肉切碎后的悬浮液中，特定的一群分子被快速氧化\n        - 这些分子形成两条通路：柠檬酸 → α-酮戊二酸 → 琥珀酸；琥珀酸 → 延胡索酸 → 苹果酸 → 草酰乙酸。（没有质谱技术，如何辨别这些小分子？）\n        - 小剂量的以上分子加入后可以导致大量氧气消耗\n        - 丙二酸实验\n            - 丙二酸可以特异性抑制琥珀酸脱氢酶的活性（如何知道？丙二酸结构上类似琥珀酸即丁二酸）\n            - 琥珀酸脱氢酶催化琥珀酸变为延胡索酸\n            - 将柠檬酸、异柠檬酸、α-酮戊二酸（之前通路的**上游**分子）加入含有丙二酸的肌肉细胞悬浮液后，琥珀酸会累积\n            - 将延胡索酸、苹果酸、草酰乙酸（之前通路的**下游**分子）加入含有丙二酸的肌肉细胞悬浮液后，琥珀酸**也**会累积\n        - 结论/假设：下游存在一个反应，使得最下游反应物变成最上游反应物\n            - 肌肉悬液和丙酮酸和草酰乙酸共培养时，形成了柠檬酸\n            - 假设：丙酮酸 + 草酰乙酸 → 柠檬酸或丙酮酸 + 草酰乙酸 → 柠檬酸\n            - 实际：乙酰 CoA 作为丙酮酸和草酰乙酸的中间体，十年之后才被发现\n    - 现有技术：放射性标记物 + 质谱法\n        - 放射性标记物\n        - 质谱法\n14. 化学渗透偶联如何驱动 ATP 合成\n15. **追踪蛋白质和囊泡运输**\n    - 定位到特定细胞器的蛋白含有特定“信号序列”，信号序列交换实验\n    1. in vitro, 将细胞器从细胞中分离出来，和携带信号序列的蛋白质置于同一试管中，放射性氨基酸标记和追踪；图15-29 (B) 没看懂汉语\n    2. 高温时分泌缺陷的酵母中发现了 25 种和胞吐有关的基因。25°C 转运正常，35°C 异常积累在内质网、高尔基体、囊泡中\n    3. 荧光蛋白视频\n16. **解析细胞信号通路**\n    1. 检测磷酸化：当信号通路接收到胞外信号分子时，多种蛋白被磷酸化\n        1. 破碎细胞、通过凝胶按大小分开、使用抗体检测磷酸化的蛋白\n        2. 在细胞暴露于信号分子时，提供放射性的 ATP；破碎细胞并通过凝胶；将凝胶在 X 光底片上曝光。\n    2. 鉴定相互作用蛋白：\n        1. 免疫共沉淀：用抗体抓住特定的蛋白，如果其正与其他蛋白结合，则被结合蛋白也会沉淀\n        2. DNA 重组技术：构建一系列突变蛋白，可以鉴定出用来结合的氨基酸位点。使用此种方法时，细胞内不能有对信号分子相应的正常受体（如何做到？）\n    3. 开关通路\n        1. 开：DNA 重组技术，编码不需外源信号也能持续激活的受体蛋白，例如人类癌症中的 Ras\n        2. 关：\n            1. DNA 重组技术，构建“显性失活”(dominant negative) 突变体\n            2. 小干扰 RNA (siRNA)，降解编码相应蛋白的 mRNA 或阻止 mRNA 翻译\n    4. 通路排序\n        1. 动物筛选：成千上万的实验动物用一种突变源处理，寻找突变所在的信号通路中哪一个没有正常工作，由此找到信号通路中编码蛋白的基因\n        2. 确定顺序：已于研究目标 X，选定一个基准蛋白 C，引入一种失活的 X 突变，再引入持续激活的 C，看通路是否仍能工作，能则 X 在 C 上游，反之则在下游。\n17. **寻找马达蛋白**\n    - 难点：在细胞外分离和研究相关蛋白质\n    - 方法：\n        - 显微镜技术发展，从特定种类的细胞中将胞内运输系统挤压出来。\n            - ~~光学显微镜时代：拥有巨大轴突的乌贼神经细胞，从轴突中挤出细胞质（轴浆），然后研究粒子跨细胞膜运动，轴浆被丢弃。~~\n            - 视频增强显微镜时代：空间分辨率 200nm，看到囊泡和被膜细胞器沿着细胞骨架移动，从 30~50nm 的颗粒到 5000nm 的线粒体。\n            - 这种运动需要 ATP，AMP-PNP 竞争性抑制细胞器转运\n        - 用纯化的缆绳、马达、货物从零开始装配一套正常运转的运输系统\n            - 抗体实验表明蛋白丝缆绳是 α-微管蛋白\n            - Ron Vale, Thomas Reese \u0026 Michael Sheetz 将纯化的缆绳（乌贼的视叶中纯化的微管）和货物（乌贼轴突中分离的细胞器）放在一起，寻找诱发运动的分子。乌贼轴突细胞质的提取物可以诱发运动\n            - AMP-PNP 虽抑制其运动，但仍能使组件结合。结合后提取微管，希望马达蛋白仍附着在微管上。加入 ATP 释放附着的蛋白，得到 110kDa 的多肽\n        - 细胞内观测：Steven Blcok et al, 1990\n            - 微小的硅珠包裹上低浓度的动力蛋白，能观察到硅珠沿微管行走\n            - 马达蛋白和荧光蛋白偶联，可以观察到单个动力蛋白分子\n            - 发现：每个分子从微管上掉落之前走大约 100 步，每步 8nm，约等于微管蛋白单体的长度。ATP 水解实验表明每步消耗一个 ATP\n            - 动力蛋白有两个头部，被认为以左右交替的方式前进。\n18. 细胞周期蛋白和 Cdk 的发现\n19. 利用 SNP 解开人类疾病的面纱\n20. 搞清楚那些对癌症有关键性影响的基因\n\n## 图版\n\n- 1-1. 显微镜\n- 1-2. 细胞结构\n- 2-1. 化学键和化学基团\n- 2-2. 水的化学性质\n- 2-3. 几种糖的类型概述\n- 2-4. 脂肪酸和其他脂类\n- 2-5. 蛋白质里的20种氨基酸\n- 2-6. 核苷酸概述\n- 2-7. 非共价键的主要类型\n- 3-1. **自由能与生物学反应**：写得不好，太多不加解释的事实陈述\n    - 反应的自由能变化和平衡常数之间存在函数关系，推导在 3.2.6 节\n    - 生物系统中，吉布斯自由焓为正的反应的发生，经常依靠反应耦合（多个反应共享一个或多个中间物）。总的自由能变化是各个反应自由能之和。\n- 4-1. 蛋白质功能的几个例子\n- **4-2. 描述小型 SH2 蛋白结构域的四种不同方式**\n- 4-3. 抗体的制备和使用\n- **4-4. 细胞的裂解和细胞抽提物的初步分离**\n    - “我们如何得知17”提到\n- **4-5. 用层析法分离蛋白质**\n    - “我们如何得知17”提到\n- 4-6. 用电泳法分离蛋白质\n- 13-1. 详解糖酵解的10个步骤\n- 13-2. 完整的柠檬酸循环\n- **14-1. 氧化还原电位**\n- 17-1. 三种蛋白丝的主要类型\n- 18-1. 动物细胞 M 期的主要阶段\n- 19-1. 经典遗传学的要义\n"},{"slug":"barron-360-biology-contents","filename":"2022-02-25-barron-360-biology-contents.md","date":"2022-02-25","title":".tex | 生物学知识提纲","layout":"post","keywords":["tex","bio"],"excerpt":"最近在学校书店看到一本书，是 BARRON’S 360 系列的《A Complete Study Guide to Biology with Online Practice》。把目录抄录下来，对照内容自己找资料自学。","content":"\n最近在学校书店看到一本书，是 BARRON’S 360 系列的《A Complete Study Guide to Biology with Online Practice》。（原本）觉得对半路出家的我来说很有用，但是美国的书实在是太TM贵了，于是把目录抄录下来，对照内容自己找资料自学。\n\n~~算是开了一个新坑？~~\n\n抄完之后才发现这基本上就是国内高中的生物学水平，对生物分类方面的介绍还是立足于向下一代传递信息，有趣但是对我的研究帮助不大；对分子生物学和细胞生物学方面的介绍少了点。“世界上最好的高中教育在美国，只不过是在美国大学。”段子诚不我欺。\n\n1. 总论/元信息 \u003cbr\u003e\nBIOLOGY: THE SCIENCE OF LIFE\n    1. 生物的科学\u003cbr\u003e\n    The Science of Biology\n    2. 生物学诸分支\u003cbr\u003e\n    Branches of Biology\n    3. 当代生物学家的工作\u003cbr\u003e\n    The Work of the Modern Biologist\n2. 生命的特征\u003cbr\u003e\n CHARACTERISTICS OF LIFE\n    1. 主要生命活动\u003cbr\u003e\n    Major Life Functions\n    2. 生物如何命名\u003cbr\u003e\n    How Living Things Are Named\n    3. 五界分类系统\u003cbr\u003e\n    Five-Kingdom System of Classifications\n    4. 六界分类系统\u003cbr\u003e\n    Six-Kingdom System of Classifications\n    5. 三域分类系统\u003cbr\u003e\n    Three-Domain System of Classifications\n3. 细胞：生命的基本单位 \u003cbr\u003e\nTHE CELL: BASIC UNIT OF LIFE\n    1. 作为基本单位的细胞\u003cbr\u003e\n    The Cell as a Basic Unit\n    2. 细胞的各部分\u003cbr\u003e\n    Parts of a Cell\n    3. 动植物细胞比较\u003cbr\u003e\n    Comparison of Plant and Animal Cells\n    4. 细胞和组织\u003cbr\u003e\n    Organization of Cells and Tissues\n    5. 细胞增殖\u003cbr\u003e\n    Cell Reproduction\n4. 生命的化学 \u003cbr\u003e\nTHE CHEMISTRY OF LIFE\n    1. 一些基础化学原则\u003cbr\u003e\n    Some Basic Principles of Chemistry\n    2. 化学键\u003cbr\u003e\n    Chemical Bonding\n    3. 化学反应\u003cbr\u003e\n    Chemical Reactions\n5. 细胞的基础化学 \u003cbr\u003e\nTHE BASIC CHEMISTRY OF CELLS\n    1. 作为化学工厂的细胞\u003cbr\u003e\n    The Cell as a Chemical Factory\n    2. 活体细胞中酶的作用\u003cbr\u003e\n    The Role of Enzyme in Living Cells\n    3. 细胞呼吸\u003cbr\u003e\n    Cellular Respiration\n6. 细菌和病毒 \u003cbr\u003e\nBACTERIA AND VIRUSES\n    1. 原核生物\u003cbr\u003e\n    Prokaryotes\n    2. 细菌的分类\u003cbr\u003e\n    Classification of Bacteria\n        1. 古菌界\u003cbr\u003e\n        Kingdom Archaeobacteria\n        2. 细菌界\u003cbr\u003e\n        Kingdom Eubacteria\n    3. 细菌的重要性\u003cbr\u003e\n    Importance of Bacteria\n    4. 病毒\u003cbr\u003e\n    Virus\n7. 原生生物界：原生动物、类真菌原生生物、类植物原生生物\u003cbr\u003e\nTHE PROTIST KINGDOM: PROTOZOA, FUNGUS-LIKE PROTISTS, PLANT-LIKE PROTISTS\n    1. 主要原生生物\u003cbr\u003e\n    Major Groups of Protists\n        1. 原生动物\u003cbr\u003e\n        Protozoa, the animal-like protists\n        2. 类真菌原生生物\u003cbr\u003e\n        Fungus-like protists\n        3. 类植物原生生物\u003cbr\u003e\n        plant-like protists\n    2. 原生生物对人类的重要性\u003cbr\u003e\n    Importance of Protists to Humans\n8. 真菌\u003cbr\u003e\nTHE FUNGI\n    1. 真菌的一般特性\u003cbr\u003e\n    General Features of Fungi\n    2. 主要真菌门\u003cbr\u003e\n    Major Divisions of Fungi\n    3. 特殊营养关系\u003cbr\u003e\n    Special Nutritional Relationships\n    4. 真菌对人类的重要性\u003cbr\u003e\n    Importance of Fungi to Humans\n9. 绿色植物\u003cbr\u003e\nTHE GREEN PLANTS\n    1. 植物的一般特点\u003cbr\u003e\n    General Characteristics of Plants\n    2. 主要植物门\u003cbr\u003e\n    Major Divisions of Plants\n    3. 光合作用\u003cbr\u003e\n    Photosynthesis\n    4. 植物激素\u003cbr\u003e\n    Plant Hermones\n    5. 光周期律\u003cbr\u003e\n    Photoperiodicity\n10. 无脊椎动物：从海绵到软体动物\u003cbr\u003e\nINVERTEBRATES: SPONGES TO MOLLUSKS\n    1. 动物体的基本组织结构\u003cbr\u003e\n    Basic Organization of the Animal Body\n    2. 无脊椎动物的一般特点\u003cbr\u003e\n    General Characteristics of Invertebrates\n    3. 海绵\u003cbr\u003e\n    Phylum Porifera — Sponges\n    4. 腔肠动物——水螅、水母、珊瑚、海葵及其近亲\u003cbr\u003e\n    Phylum Cnidaria (Coelenterates) — Hydrozoa, Jellyfish, Corals, Sea Anemones, and Their Relatives\n    5. 扁形动物门\u003cbr\u003e\n    Phylum Platyhelminthes—Flatworms\n    6. 线虫动物门\u003cbr\u003e\n    Phylum Nematoda—Roundworms\n    7. 环节动物门\u003cbr\u003e\n    Phylum Annelida—Segmented Worms\n    8. 软体动物门—— ~~蛤与蛤丝~~ \u003cbr\u003e\n    Phylum Mollusks—Clams and Their Relatives\n11. 无脊椎动物：节肢动物\u003cbr\u003e\nINVERTEBRATES: THE ANTHROPODA\n    1. 主要代表纲\u003cbr\u003e\n    Major Representative Classes\n    2. 蛛形钢\u003cbr\u003e\n    Class Arachnida—Spiders and Ticks\n    3. 软甲纲\u003cbr\u003e\n    Class Malacostraca—Lobsters and Their Relatives\n    4. 昆虫纲\u003cbr\u003e\n    Class Insecta\n    5. 节肢动物对人类的重要性\u003cbr\u003e\n    Importance of Arthropoda to Humans\n12. 后口动物、脊索动物、哺乳动物\u003cbr\u003e\nDEUTEROSTOMES, CHORDATES, AND MAMMALS\n    1. 后口动物总门\u003cbr\u003e\n    The Deuterostomes\n    2. 脊索动物门\u003cbr\u003e\n    Phylum Chordata—The Chordates\n    3. 脊椎动物亚门\u003cbr\u003e\n    Subphylum Vertebrata—The Vertebrates\n13. 人：一种特殊的有脊椎动物\u003cbr\u003e\nHOMO SAPIENS: A SPECIAL VERTEBRATE\n    1. 灵长类的特点\u003cbr\u003e\n    Characteristics of Primates\n    2. 骨骼系统\u003cbr\u003e\n    The Skeletal System\n    3. 肌肉系统\u003cbr\u003e\n    The Muscular System\n    4. 神经系统\u003cbr\u003e\n    The Nervous System\n    5. 内分泌系统\u003cbr\u003e\n    The Endocrine System\n    6. 呼吸系统\u003cbr\u003e\n    The Respiratory System\n    7. 循环系统\u003cbr\u003e\n    The Circulatory System\n    8. 淋巴系统\u003cbr\u003e\n    The Lymphatic System\n    9. 消化系统\u003cbr\u003e\n    The Digestive System\n    10. 排泄系统\u003cbr\u003e\n    The Excretory System\n    11. 感觉器官\u003cbr\u003e\n    Sense Organs\n    12. 生殖\u003cbr\u003e\n    Reproduction\n14. 营养：吃出健康\u003cbr\u003e\nNUTRITION: EATING FOR HEALTH\n    1. 大量营养物质\u003cbr\u003e\n    Macronutrients: Carbohydrates, Proteins, and Lipids\n    2. 微量营养物质\u003cbr\u003e\n    Micronutrients: Vitamins and Minerals\n    3. 进食障碍\u003cbr\u003e\n    Eating Disorders\n15. 人类疾病和免疫系统\u003cbr\u003e\nDISEASES OF HOMO SAPIENS AND THE IMMUNE SYS传染病TEM\n    1. 传染病\u003cbr\u003e\n    Infectious Diseases\n    2. 免疫系统\u003cbr\u003e\n    The Immune System\n    3. 非传染病\u003cbr\u003e\n    Noninfectious Diseases\n16. 遗传和基因学\u003cbr\u003e\nHEREDITY AND GENETICS\n    1. 经典遗传原理\u003cbr\u003e\n    Classical Principles of Heredity\n    2. 现代基因学\u003cbr\u003e\n    Modern Genetics\n    3. 基因技术\u003cbr\u003e\n    Genetic Technologies\n    4. 逆转录病毒——RNA中的基因编码\u003cbr\u003e\n    The Retrovirus of AIDS—Genetic Coding in RNA\n    5. 遗传对人类的重要性\u003cbr\u003e\n    Importance of Heredity to Humans\n17. 演化论\u003cbr\u003e\nPRINCIPLES OF EVOLUTION\n    1. 演化的证据\u003cbr\u003e\n    Evidence of Evolution\n    2. 演化的思想\u003cbr\u003e\n    Ideas About Evolution\n    3. 生命的起源\u003cbr\u003e\n    The Origin of Life\n    4. 人类的演化\u003cbr\u003e\n    Evolution of Humans\n18. 生态学\u003cbr\u003e\nECOLOGY\n    1. 生态系统的概念\u003cbr\u003e\n    The Concept of the Ecosystem\n    2. 生态系统中的能量流动\u003cbr\u003e\n    Energy Flow in an Ecosystem\n    3. 生物地球化学循环\u003cbr\u003e\n    Biogeochemical Cycles\n    4. 限制因素的概念\u003cbr\u003e\n    The Limiting Factor Concept\n    5. 温室效应\u003cbr\u003e\n    Greenhouse Effect\n    6. 全球变暖\u003cbr\u003e\n    Global Warming\n    7. 生态演化\u003cbr\u003e\n    Ecological Succession\n    8. 世界生物群系\u003cbr\u003e\n    World Biomes\n    9. 人类和生物圈\u003cbr\u003e\n    Humans and the Biosphere\n    10. 生态保护\u003cbr\u003e\n    Conservation"},{"slug":"lab-note","filename":"2020-09-21-lab-note.md","date":"2020-09-21","title":".tex | 整理一些关于实验记录的文章","layout":"post","keywords":["tex","phy","bio"],"excerpt":"摸鱼的方式有很多种，琢磨如何完美地进行实验记录就是个挺不错的由头。","content":"\n1. [微信公众号“BioArt植物”，原作者 Elisabeth Pain ，《实验记录到底怎么记？》](https://www.sciencemag.org/careers/2019/09/how-keep-lab-notebook)\n1. [Howard Kannare, 《Writing the Laboratory Notebook》](https://files.eric.ed.gov/fulltext/ED344734.pdf)\n1. [MIT Department of Mechanical Engineering, 《Instructions for Using Your Laboratory Notebook》](http://web.mit.edu/me-ugoffice/communication/labnotebooks.pdf)\n1. [微信公众号“生物学霸”，《颜宁：讲讲如何记实验记录》](https://xw.qq.com/partner/vivoscreen/20200820A00HZI00?vivoRcdMark=1)\n\n微信平台不允许添加指向微信之外的超链接，资源的获取方式见正文。作为报复，以上四条资源中有两条最早是在公众号里看到的，但是博文给出的链接都来自微信之外 :-)\n\n\u003chr class=\"slender\"\u003e\n\n## 0\n\n疫情依旧，摸鱼依旧。摸鱼的方式有很多种，其中比较高级的一种是打着完美主义的旗号，对着一个还没完成，或者根本不存在完成时的东西，疯狂输出时间和精力。琢磨如何完美地进行实验记录就是个挺不错的由头。\n\n说实话，学界对研究记录的要求实际上并不算严格，这一点在《Writing the Laboratory Notebook》里也有佐证。商业机构研发部门的研究记录会成为将来知识产权争端的主要证据，稍有不慎就是真金白银的经济损失，甚至关乎企业的生死存亡。而学界的工作在“科技”中偏重于”科“（即便是工程学科），在”研发“中偏重于”研“。（四者有什么联系和区别？科学认识世界，技术改造世界，研究把钱变成知识，开发把知识变成钱。）自由比起规范显然更有利于在未知领域的探索。\n\n所以，我们实验室对于实验记录并没有成文的规则，大家自己找本子自己记，格式和内容都跟随自己的喜好来，同实验室的同学也很少交流这个问题，仿佛说了就是承认自己的科研能力有问题。\n\n## 1\n\n越不谈越是心虚，于是在看到了《实验记录到底怎么记？》这篇文章之后，下决心要处理掉这个问题。这篇文章的作者访谈了几个科研人员，然后将他们的对话打碎，分到四个问题之下：\n\n- 为什么还要花时间精力去做实验记录？\n- 用传统纸质的记录本，还是电子版，还是都有？\n- 采取什么策略来保证实验记录有条理、完整并且实用？\n- 其他……\n\n并不推荐这篇文章，原因从这四个问题就能看出来：第一条属于幸存者偏见，一个觉得记录不重要的人压根不会认真记录，从而很难成为访谈对象；第二条属于典型的”有的人……有的人……“英式废话文套路；第三个问题太笼统，本应该细分为更明确的子问题；最后一个“其他”说明作者都不知道该怎么总结这些对话。明明是一篇文章，硬生生写出了微博一般的碎片感。\n\n## 2\n\n于是在网上继续找资源，机缘巧合之下，在一个知乎问题之下看到了一个还不错的答案，里面提到了 Howard Kannare 的《Writing the Laboratory Notebook》这本书。真的是“机缘巧合”，因为现在的我已经找不到原来的那个问题和答案了，哪怕专门为了这篇文章搜索了半天……这说明了网络资源的收藏和管理也是一个技术活（又可以水一篇文章了）。\n\n这本书在网上有英文的完整影印版，很容易就能搜到，实在不行的话在微信后台留言\"HowardKannare\"可以收到下载网址（注意回复的关键词没有空格）。\n\n本书各章的标题如下：\n\n1. The Reasons for Note keeping - An Overview\n2. The Hardware of Note keeping - Books, Pens, and Paper\n3. Legal and Ethical Aspects - Ownership, Rights, and Obligations\n4. Management of Notekeeping - Practices for Issuance, Use, and Storage of Notebooks\n5. Organizing and Writing the Notebook - Be Flexible\n6. Examples of Notebook Entries\n7. Patents and Invention Protection\n8. The Electronic Notebook\n9. Appendix A: Some Suggestions for Teaching Laboratory Notekeeping\n10. Appendix B: Photographs from the Historical Laboratory Notebooks of Famous Scientists\n\n书很长，有 150 多页，这导致内容涉及方方面面，包括了那些我们可能不是那么急需的方面；还有很多我们今天可能并不十分需要的冷知识，比如几十年前美国出产的纸张由于某种工艺导致保存期限比较短等等。好在多数章节最后都有总结，可以帮人省下不少时间。\n\n另一方面，这本书出版于1985年，那是一个什么年代呢？苹果在前一年才刚刚推出了 Macintosh 电脑，C++ 在当年才刚刚出版。所以对于电子实验记录，书中只在第8章和纸本笔记进行了一个简单的对比，而且有比较明显的时代局限性。\n\n总之，如果真要读这本书的话，抱着练习英语阅读的目标，要远比学记笔记要少些失望。\n\n## 3\n\n干了半天之后开始怀疑这件事从一开始是否有必要，这可是 PhD 的保留节目了。\n\n尤其是在读过《Writing the Laboratory Notebook》的第5章之后，读到单篇实验报告应该包括 introduction, experimental plan, observations and data, discussion of results 的时候，恍然发现，这不就是本科实验课要交的实验记录的写法吗，之所以没有老师教过我们怎么记实验记录，是不是因为他们觉得这个事情已经教过了？\n\n既然如此，那么第三份材料就是 MIT 机械系给本科生的实验报告要求，不长，只有 6 页，还包括了超过两页的模板笔记，可以当作《Writing the Laboratory Notebook》关于笔记内容部分的精华集锦来看。微信后台回复 \"MITlab\" 可以收到 PDF 下载地址。\n\n但这反而说明了，PI们散养研究生，不对实验记录进行更进一步的要求和培训是不对的。因为“实验记录本”≠“实验记录们”，研究生的工作不同于本科实验课，本科生做实验就只有在固定时间和固定时长的实验课上，一切超出规定范围内的动作大概率都是无用功甚至错误，每个实验要做什么，有哪些步骤，会观察到什么现象，都是事先设计好的，实验记录的各个部分会有哪些内容，大体上没跑。研究生的工作则不然，大到整个博士期间的所有工作都可以算作是一个项目（毕竟会写成一篇毕业论文），小到显微镜从开机到观测到关机的几个小时也可以整出一篇报告来，如何划分研究的基本单元？一个人可能同时在做相对独立的多项工作，是连续记录还是分开平行记录？以纸本为主，电子版主要用于备份，还是主要用电子设备，随手记在纸上的拍照作为附件？\n\n## 4\n\n对于这篇文章没有太多可说的，覆盖范围和《实验记录到底怎么记？》类似，感觉就相当于颜宁女士自己一个人对那篇文章中的问题的回答，由于是一个人的回答，所以不会有前一篇文章中不同观点混在一起的分裂感。看过了《Writing the Laboratory Notebook》之后，会发现文中的每一个点都在书中可以找到。\n"},{"slug":"yeast-cloth-food-house-travel","filename":"2019-12-29-yeast-cloth-food-house-travel.md","date":"2019-12-29","title":".tex | 实验室中酵母菌的衣食住行","layout":"post","keywords":["tex","bio"],"excerpt":"实验室里的酵母，吃得怎样，住在哪里，过得好吗？","content":"\n“酵母”其实是1500多种微生物的一个统称，但是在科学研究中最常用的一种是 _Saccharomyces cerevisiae_，下文中的酵母一词指的就是它们。作为一种单细胞生物，酵母在指数生长期，通过出芽生殖的方式，平均约90分钟就可以繁殖一代。同时作为真核生物，核膜以及各种具膜细胞器应有尽有。而且酵母对脂质的代谢路径和人类的代谢路径在进化上是同源的，也就是说有些治疗人类疾病的药物，也可以在酵母细胞中产生效果。结合酵母菌快速繁殖的特性，可以方便相关药物的筛选。再加上人们熟知的发酵、酿酒等应用，酵母就成了生物实验室中非常常见的一种模式生物。\n\n## “衣”\n\n~~全裸，下一题。~~\n\n这个问题其实还是可以再多说两句的。\n\n这里的“衣”显然是一种比喻，既然是比喻，那就要看说话的人想强调的是衣服的哪种性质。酵母的细胞膜外面还有一层细胞壁，对于一个细胞的定义来说，细胞壁并不是必需的。从这个角度讲，完全可以把细胞壁比作酵母的外套。这层外套的主要成分是糖类，既包括起到结构支撑作用的多糖，也包括传递信息，可以用来对酵母进行特异性识别的糖蛋白。\n\n即便有了这么一层外套，单个的酵母细胞在显微镜下依然是透明的，在普通的亮场 (bright field) 显微镜下，可以隐约看到酵母细胞内部比较巨大且明显的结构，比如液泡。要想看得更清楚，就得用上 DIC（微分干涉相差，differential interference contrast）显微技术了。要说清楚 DIC，需要一篇独立的文章，而文章的很大一部分都会是数学。简而言之，光在不同的介质中的传播速度不同，表征介质的这一性质的物理量叫做“折射率”，折射率 × 光传播的距离 = 光程。当我们让两束相同的光线，通过给定的距离，其中一束光线通过我们的酵母细胞，另一束经过一段折射率已知的均匀介质（比如说空气），两束光通过的光程不同，再次见面时就会有差别，用衍射的方法比较一下两束光，就能得出酵母细胞内不同位置的折射率信息，从而看出不同的结构。\n\nDIC 设备比普通的亮场显微镜要贵不少，而且有些不同的细胞器其实长得很像，即便用上 DIC 也不容易分清楚，这时候我们就要对酵母细胞本身动点手脚。有些荧光染色剂可以和某种细胞器特异性地结合，用特定波长的光照射细胞，被染色的细胞器就会发出另外一种波长的荧光，我们用滤光片把入射光过滤掉，就可以得到这种细胞器在细胞中的位置、形状和大小信息。除了用染色剂，还可以把荧光蛋白基因插入到某些细胞器蛋白质的基因旁边，这样就免去了每次观察细胞时染色的麻烦。~~（“所以你们实验室做转基因喽？那你告诉我，转基因食品能不能吃？”）~~\n\n之前两段说的是单个细胞，而在固态的培养基（“住”的部分会介绍）上面，酵母细胞不能大范围移动，单个酵母细胞作为祖先不断繁殖，“子又有孙，孙又有子”，久而久之（其实也就两三天）就可以形成一个菌落，就是培养基上一个肉眼可见的斑点。不论是黄色的 YPD 培养基还是白色的 SD 培养基，菌落的颜色肉眼观察不出区别，都是乳白色。\n\n## “食”\n\n酵母是一种异养生物，也就是说需要摄入营养物质，既要利用其中的化学能维持自己的生命活动，也要利用这些物质的原子和分子构建自己身体的组成成分。能够满足微生物细胞生长繁殖需求的营养物质，叫做培养基 (medium)。我们实验室中常用的培养基主要就两种，一是 YPD 培养基，二是 SD 选择培养基。\n\n### YPD\n\n这个名字直接来源于其成分——Yeast extract, Peptone, Dextrose——酵母提取物、胨、葡萄糖。用酵母提取物去喂酵母……不能多想，想多了就有点怪怪的……好处是真的好用，生长速度比下面要讲的 SD 培养基要快一些。而且由于我们实验室都是 SD 选择培养基，所以野生型酵母只能用 YPD。\n\n那么缺点呢？一个问题就是酵母提取物里面，究竟有什么物质，每种物质占比多少，都是一笔糊涂账，实验结果就不好分析。\n\n另外，YPD 培养基在很多波长的光照射下都会发出微弱的荧光，在之前说过的荧光显微观察中，这种荧光就会成为背景噪声。所以 YPD 培养的细胞不能直接进行荧光显微观察，要换到 SD 培养基养一段时间（一般半个小时以上），或者干脆就直接养在 SD 培养基里。\n\n### SD 选择培养基\n\nSD 的全称是 synthesis defined，意思是人工合成，而且成分是明确且固定的，配方在一般的科研网站上都能找到。而选择培养基的意思是，相比于普通的 SD 培养基，某种营养物质（一般是某种氨基酸）被去掉了，比如说 SD-His 培养基中就不含有组氨酸 (Histidine)。这些营养物质都是野生酵母没办法自己合成的，但是在用同源重组的方法进行基因编辑的时候，我们除了会导入目标基因之外，还会一起导入能表达合成这一营养物质的酶的基因。把被编辑过的细胞接种在选择培养基上，那些没能成功编辑的细胞将无法合成相关营养物质，只有编辑成功的细胞能够幸存，并形成菌落，这就是“选择”一词的由来。\n\n## “住”\n\n由于酵母既没有嘴，也不像草履虫一样拥有可以游动的鞭毛，所以只能和营养物质生活在一起。（当然也是和代谢废物生活在一起，不能多想……）所以说，培养基不仅是食物，还很类似于酵母菌的“家具”。\n\n前面说到了培养基，定义里说是营养物质，但是并没有强调其物质状态。培养基既可以按照配方配置成溶液，这叫做液态培养基。在溶液中按比例（一般是 2% 的质量分数）加入琼脂 (agar)，先加热溶解，然后稍微冷却之后倒入培养皿 (petri dish)，继续冷却就形成了凝胶，这样就制成了一个半固态培养基。\n\n在培养基中加入了细胞之后，此时的混合物应该叫做培养液 (culture)，如果把 medium 和 culture 弄混了的话，有些人的脸上会浮现出微妙的笑容。屠格涅夫说过：“有教养不是吃饭的时候不洒汤，而是别人洒汤的时候你不去看他。”为了避免别人没有教养，我们还是应该勉为其难地区分一下这两个概念。:-)\n\n说完家具来说房子，也就是 culture 的容器。\n\n在液态培养液中，酵母的密度要略微大于培养基的密度，所以长时间的静置会使得细胞在容器的底部沉积，影响生长速度。所以我们需要把容器固定在恒温的机械平台上不断摇晃，为了让离心力尽可能地大，容器要选用底面积比较大的锥形瓶，用普通试管的话需要将其斜放。~~（我看看有哪个高中生蹦出来说不存在离心力这回事，哼）~~\n\n对于琼脂培养基，培养皿直接放在恒温箱里静置。前面说过，单个细胞会繁殖，然后无法移动，形成一个菌落。没有基因突变的话，这一个菌落中细胞的基因型都是相同的。如果两个空间上相邻的菌落不断长大，就会连在一起，进而长成一个大菌落（突然想到了光学课本里瑞利判据那张连环画），此时就不好确定基因型的纯洁了。在长到这种情况之前，就要把培养皿用 3M 胶带封起来，然后放到低温下暂存，等待日后实验时取出单个菌落继续培养，或是冷冻长期保存。\n\n说到了温度，酵母菌的正常生长需要 30 摄氏度。刚才说到要保存菌落已经足够大的培养皿，这时的温度一般是 4 摄氏度，也就是冰箱冷藏室的温度。而长期保存细胞的话需要放入 -70 ~ -80 摄氏度的低温冷柜中。\n\n## “行”\n\n我们实验室还并没有厉害到需要给其他实验室寄送我们培养的菌株的地步，我们实验室里的菌株是怎么来的我也忘了……所以这一节所说的“行”，指的是酵母菌在不同的培养基之间的转移。\n\n网上的教程里比较讲究，啥高科技工具都有，就跟吃螃蟹的蟹八件似的。我们实验室比较莽，不论是冷冻的细胞，还是液态或者琼脂培养基中的细胞，一律都是用消毒杀菌过的细木棍来取细胞，如果不是要转移到琼脂培养皿的话，甚至还可以用一次性移液枪头。琼脂培养基的话要仔细蘸一蘸，确保取到的细胞来自同一个菌落，冷冻细胞的话就刮一刮容器里的冰沙，液态培养基的话就随便蘸一蘸。\n\n如果目的地是液态培养基的话就很简单，木棍伸进去，搅合搅合，完事儿。\n\n如果是琼脂培养基的话，要轻轻地在琼脂表面来回涂擦，要小心不能刮坏琼脂层。来回涂擦几次之后，把木棍换到之前没接触细胞的一面，在刚才涂擦的区域边缘，往没涂过的地方来回涂擦几次，如是反复者二三。这样做是因为刚开始的细胞数量可能比较多，长出来的菌落从一开始就连成一片，后来几次涂擦时细胞的密度就会越来越低，某个区域就会出现足够数量又相距足够距离的一群菌落。总之是个技术活还是个经验活。\n\n如果是新制备长期保存的细胞的话，需要先在液态培养基中将细胞养到合适的浓度，然后在专用的容器中按照 1:1 的比例混合培养液和甘油，然后就可以放进冷柜了。"}]],["phy",[{"slug":"error-propagation-and-philosophy-of-science","filename":"2024-10-21-error-propagation-and-philosophy-of-science.md","date":"2024-10-21","title":".tex | 误差的传递，科学之所以科学","layout":"post","keywords":["tex","phy","phi"],"excerpt":"多变量测量的误差传递，及其在科学哲学中的作用。","content":"\n如果一个物理量需要用多个直接观测量计算出来：$$y=f(x_1,x_2,...,x_n)$$，这样的量叫做因变量，直接观测量叫做自变量。（比如用直尺测量长方形面积时，长、宽是自变量，面积是因变量，通过长和宽相乘计算面积的方法是一个函数。）\n\n因为中小学减负，因变量的说法不教了，改叫函数值，为了少学一个知识点。\n\n但是学过 C/C++ 的应该知道，对于 $$y=f(x_1,x_2,...)$$\n\n- 因变量 $$y$$ 是一个左值，指向 $$y$$ 的指针 `float *p = \u0026y;` 拿到的地址，位于内存的数据区；\n- 函数值 $$f(\\cdot)$$ 是一个右值，$$f$$ 本身就是一个指针，`void *fp = f;` 拿到的地址，位于内存的指令区。\n\n## 多变量测量的误差传递\n\n先跳过单变量误差的部分（大致原理在《贝叶斯，从公式到世界观》一文频率学派的部分，具体细节以后再写），不论是测量仪器的说明书给出的误差，还是测量者通过独立重复实验取得的统计误差，我们先假设已经拿到了观测量 $$x$$ 的测量值 $$\\bar x$$、误差 $$\\Delta x$$\n\n因为全微分公式，对于 $$y=f(x_1,x_2,...,x_n)$$\n\n$$\n\\mathrm{d}y = \\frac{\\partial f}{\\partial x_1}\\mathrm{d}x_1 + \n\\frac{\\partial f}{\\partial x_2}\\mathrm{d}x_2 + ... + \\frac{\\partial f}{\\partial x_n}\\mathrm{d}x_n = \\sum_i^n \\frac{\\partial f}{\\partial x_i}\\mathrm{d}x_i\n$$\n\n又因为误差相对于真值往往小几个数量级，所以我们把误差看作是真值的微分，用 $$\\Delta$$ 取代 $$\\mathrm{d}$$. （有人问真值为 0 怎么办，绝大多数情况下可以通过平移零点定义的办法来几乎任意地改变测量值的数量级，而误差不会因为这种变换而出现数量级变化。）\n\n还因为对多个自变量的测量是相互独立的，每个自变量 $$(x_1,x_2,...,x_n)$$ 占据相空间中的一个维度，维度之间互相正交。\n\n所以物理上，因变量的误差就是上述 ~~微分~~ 微差向量的“长度”，以  L2 范数 (norm) 来衡量：\n\n$$\n\\begin{array}{rcl}\\Delta y \u0026 = \u0026 \\sqrt{ \\left(\\frac{\\partial f}{\\partial x_1}\\bigg|_{\\vec x}\\right)^2\\Delta x_1^2 + \\left(\\frac{\\partial f}{\\partial x_2}\\bigg|_{\\vec x}\\right)^2\\Delta x_2^2 +...+ \\left(\\frac{\\partial f}{\\partial x_n}\\bigg|_{\\vec x}\\right)^2\\Delta x_n^2 } \\\\ \u0026 = \u0026 \\sqrt{\\sum_i^n{\\left(\\frac{\\partial f}{\\partial x_i}\\bigg|_{\\vec x}\\right)^2\\Delta x_i^2}}\\end{array}\n$$\n\n物理学家因此不害怕误差——理论物理的模型哪怕非常复杂，在数学上往往依然“性质优美”，只要理论的自变量可以在实验上测量，误差明确且有限，那理论给出的预测值的误差就同样明确且有限，依然可以指导实践。\n\n## 误差与可证伪性\n\n而根据卡尔·波普尔的科学哲学，具体来说就是可证伪性的划界标准，科学就不只是不害怕误差了，简直是依赖误差而生，靠误差来和伪科学划清界限。\n\n所谓科学的可证伪性，《[科学是什么？——兼谈“非科学、伪科学、反科学”和一些常见谬误](https://program-think.blogspot.com/2015/10/What-is-Science.html)》一文概括为：\n\n- 科学理论是一个相互关联的**命题**的**集合**。\n- 科学理论必须是基于**演绎**法建立整个理论体系的。也就是从不证自明的**定律**出发，依据**逻辑规则**，推论出各种各样的**定理**。\n- 理论中的命题必须是**客观**陈述，也就是能由不同的主体进行独立检验。\n- 检验的方式是**证伪**，也就是寻找现实中的一个现象，说明从理论中某个命题是错的。经过证伪程序，且没能被证伪的命题，就被验证为真。（根据逆否命题的等价性，演绎推论如果被证伪，它的逻辑前提也会连锁被证伪。那科学几百年来靠什么幸存，我们以后再狡辩～）\n- （既然一个存在命题就能否定一个学科理论中的待验证命题，）科学理论中的命题应该是**全称命题**，此即科学的普世性 (universality)。\n- 对全称命题的**特设性修正**（比如把“所有的天鹅都是白色的”修正成“所有北半球的天鹅都是白色的”），应该要提高理论的可证伪程度，否则就是伪科学。\n- 以上各个要求，单独只构成必要条件。\n\n所以定量科学的科学性就体现在，\n\n- 只要理论的自变量和因变量可以在实验上测量，\n- 自变量的误差明确且有限，那理论给出的预测值的误差就同样明确且有限，\n- 因变量的误差同样明确且有限，\n- 将理论预测值和因变量测量值摆在一起，只要差距不大于两者的误差，（技术细节在统计学中的假设检验部分。）\n- 我们就认为对“理论预测和因变量真值相等”的证伪失败了，从而接受他们相等。\n\n因为实验仪器和方法的进步可以缩小误差范围，增加科学理论的可证伪性，一条无限延展且随时可以投入精力的赛道从此出现，科学从相隔几代的少数天才之间的思维接力，变成了夙兴夜寐前赴后继的竞赛。\n\n科学家之间的竞争催生了对制造仪器之工程技术的巨大需求，需求大到部分科学家亲自下场改进甚至发明仪器，科学由此反哺技术；进步的技术使得科学得以产出更高质量的数据，支持更复杂的理论的检验。科学和技术，合成了“科技”一个词。\n\n试想一下，如果科学号称自己绝对精确，不存在误差，要么在弱小之时就被证伪，无法赢得人们的信任；要么任由实验精度低到看不出误差的程度，然后用其他手段维持自己的光辉形象。\n\n之前《也谈近代科学从西方起步》一文中说，“物理和数学的区别，在于理论和实验两条腿走路”。如今算是把实验这另一条腿简单介绍完了。\n\n最后需要注意，这里说的是某个哲学理论能够解释科学实践，而不是科学实践必须服从某套哲学理论。\n\n科学只对客观现实负责，不需要对哲学信条负责，不应该对哲学王兼英雄王负责。\n\n## 科学还正确吗？\n\n如果承认可证伪性作为科学与非科学的划界标准，也就意味着，现在包含在科学中的每条知识，都有在未来被更加精确的实验推翻的可能。\n\n这种事也不是没发生过。比如材料的电阻，在相当大的数值范围内，都和温度成线性关系，而且这条线向左延拓到绝对零度时基本为 0。在那个时代，认为电阻来自无规则的热运动，和绝对温标成正比才是符合奥卡姆剃刀原则的理论。但是 1908 年，昂内斯用液氦将汞的温度降到 4.15 K 时，发现汞的电阻突变降低为 0，这就是超导研究的开端。\n\n那还能说科学知识是正确的吗？《费曼物理学讲义》的回答是，不谈科学是不是正确的，只保证科学 (science)是科学的 (scientific)。也就是保证程序的正确，把正确程序获得的结果交给工程技术，用工程技术上的成就取信于社会，反过来为科学的正确性背书。\n\n所以说，科学家是对科学最不迷信的一批人，一旦实验过程正确，结果和理论不符，那么理论该修改的修改，该放弃的放弃。他们是现有科学最大的破坏者，是成功证伪科学命题最多的一群人。\n\n但同理，科学家又是对科学最坚定的一批人，他们在明知道一个科学命题可能在将来被修正的情况下，依然愿意把它当作前提，继续推理产出新的命题，并试图证伪。\n\n《三体》小说刚开始设置的一大悬疑，大量科学家因为自己正在进行的研究，产出了与理论完全不吻合的随机结果，因为所谓“物理学不存在了”而自杀，这个情节就很成问题。\n\n何况这种事情根本不需要书中的情节设定才会出现，物理学史上早就发生过。比如 β 衰变的质子的动能谱和动量角分布。玻尔想放弃能量守恒定律，泡利想假设一个探测器发现不了的新粒子，这在当时的实验条件下都是尚不能证伪的理论假设，但没听说俩人为这事寻死觅活的。\n\n所以改编成电视剧的时候，几乎重写整个人物关系的网飞版，把自杀改写成了球奸们伪装的他杀；就连以忠实于原作著称的腾讯版，也原创了一段主角主动重启科研装置，直面外星人恐吓的剧情，给原著做了点找补。\n\n## 那我缺的权威性这块谁给我补上啊\n\n坏了，碰到了不该碰的话题，那就先这样吧……\n\n## 报书名儿\n\n- William Lichten.《Data and Error Analysis》\n- 赵凯华《定性和半定量物理学》\n- 卡尔·波普尔《科学发现的逻辑》\n- 理查德·费曼《费曼物理学讲义》\n"},{"slug":"bayesian-equation-and-view-of-world","filename":"2024-09-27-bayesian-equation-and-view-of-world.md","date":"2024-09-27","title":".m | Bayesian 贝叶斯，从公式到世界观","layout":"post","keywords":["tex","m","phy","phi"],"excerpt":"我们老板真是太能吹了，Bro 居然跟隔壁真的在研究物理的课题组 brag abou 我会贝叶斯参数估计，yo know wat ur sayin? 赶紧来补课～","content":"\n\n## 公式\n\n我上学的时候，贝叶斯公式是概率论里面，少数高中完全不涉及，到了本科才第一次见的公式，所以我从来没背下来过。不过也用不着背，根据条件概率里面的一个平凡结果：\n\n$$\n\\Pr(A|B)\\ \\Pr(B) = \\Pr(B|A)\\ \\Pr(A)\n$$\n\n可以得到 $$\\Pr(A|B)$$ 和 $$\\Pr(B|A)$$ 之间的关系\n\n$$\n\\Pr(A|B) = \\frac{\\Pr(B|A)\\ \\Pr(A)}{\\Pr(B)}\n$$\n\n这就是贝叶斯公式本体。\n\n分母没什么意思，所以一般我们要用全概率公式替换，也就是把 $$A$$ 划分为全覆盖但是不相交的 $$\\{A_i | \\ A_i \\cap A_{j \\neq i}=\\varnothing,\\ \\bigcup_i A_i=A\\}$$\n\n$$\n\\Pr(A|B) = \\frac{\\Pr(B|A)\\ \\Pr(A)}{\\sum_i \\Pr(B|A_i) \\Pr(A_i)}\n$$\n\n其中任意一个子事件 $$A_j$$\n\n$$\n\\Pr(A_j|B) = \\frac{\\Pr(B|A_j)\\ \\Pr(A_j)}{\\sum_i \\Pr(B|A_i) \\Pr(A_i)}\n$$\n\n### 根据实验结果筛选理论模型\n\n以上是数学。在科学中，令\n\n- A 为一族理论模型的一组参数取值，记为 $$Param_k$$，下标可任意选取。\n- B 为实验观测数据，记为 *Ob*\n\n$$\n\\Pr(Param_j|Ob) = \\frac{\\Pr(Ob|Param_j)\\ \\Pr(Param_j)}{\\sum_i \\Pr(Ob|Param_i) \\Pr(Param_i)}\n$$\n\n其中 \n\n- $$\\Pr(Param_j)$$ 表示第 j 组参数是模型的正确参数的，未经实验验证，根据零假设计算的 **先验 (prior) 概率；**\n- $$\\Pr(Param_j|Ob)$$ 叫做经过实验观测修正之后的，第 j 组参数正确的 **后验 (posterior) 概率**。\n- $$\\Pr(Ob|Param_j)$$ 在之前的文章中讲过，是当前测量数据下，模型参数的 **似然性 (likelihood)**。\n\n$$\nposterior \\propto likelihood \\cdot prior\n$$\n\n### 举个例子\n\n隔壁组的问题可以简化为下图：\n\n![](/photos/2024-09-27-two-gaussian.png)\n\n- 有两组数据 (x, y1), (x, y2) 可以用同一族函数来拟合。（假设为两个高斯函数的叠加，$$y=f_{A_1,A_2,\\mu_1,\\mu_2,\\sigma_1,\\sigma_2}(x) = A_1e^{-\\frac{(x-\\mu_1)^2}{\\sigma_1^2}} + A_2e^{-\\frac{(x-\\mu_2)^2}{\\sigma_2^2}}$$\n- 两组数据的误差不同。（红色数据点显然比蓝色数据点，相对于理论值偏离得更远一些）\n- 问有没有一个数值，可以衡量每组数据的误差程度。\n\n我给他们的建议是\n\n- 根据自己的专业知识指定先验概率 $$\\Pr(param_j)=\\Pr(A_1,A_2,\\mu_1,\\mu_2,\\sigma_1,\\sigma_2)$$。比如选定一个参数空间的范围，范围之外概率为零，范围之内均匀分布。\n    - $$A_1,A_2 \\in \\left[\\min(\\{Y_1\\}\\cup \\{Y_2\\}),\\max(\\{Y_1\\}\\cup \\{Y_2\\}\\right]$$\n    - $$\\mu_1\\in[\\min\\{X\\},\\max\\{X\\}],\\ \\mu_2\\in[\\mu_1,\\max\\{X\\}]$$\n    - $$\\sigma_1,\\sigma_2\\in[0,\\ \\Sigma_i\\sqrt{|X_i-\\bar X|^2/N}]$$\n- 根据一些假设和统计规律计算 $$\\Pr(Ob|Param_j)$$\n    - 假设误差与 x 变量无关，服从期望为 0 的高斯分布，$$[y_i-f(x_i)]\\sim N(0,\\sigma^2)$$，标准差根据各数据点减去模型预测值的残差估计。\n    - 假设每个数据点的观测相互独立，$$\\Pr(Ob)=\\Pr(\\bigcap_i Ob_i)=\\prod_i\\Pr(Ob_i)$$\n    - 对于模型的每一组参数 ，$$\\Pr(Ob_i|param_j)$$ 取上述高斯分布的绝对值大于残差绝对值的部分，就是钟形曲线两侧尾巴的线下面积。\n- 对参数空间中的每一组值都算出一个后验概率之后，计算整个空间的信息熵（方法见之前的文章）。误差较大的一组数据，应当有更多组参数可以获得类似的拟合结果，从而信息熵更大。\n\n## 世界观\n\n对于概率，有三种理解：\n- 古典的 (classical)、\n- 频率学派的 (Frequentist)、\n- 贝叶斯的 (Bayesian).\n\n### 古典\n\n就是将古典概型推广，成为一种关于可能性的普遍观点——一个随机空间里的随机事件可以分解成若干子事件，子事件还可以再分，直到每个基本事件的概率相等，都等于基本事件总数的倒数，而要计算人们感兴趣的某一事件，只需要数出其包含的基本事件的数量就行了。\n\n让人联想到古希腊古典时代的原子论。时人认为物质世界也不是无限可分的，将任意一种材料打碎研磨，这一过程最终会有一个终点，最终的产物就是这种物质的“原子”。一块材料的大小，就是其所含原子数量的多少。\n\n有人批评这种观点用可能性去定义可能性，有循环论证谬误之嫌。但是看现代化了的概率论，概率被定义成了满足某些条件的函数，公理化是公理化了，逻辑链条是有了坚实的起点，但是那里的概率还能不能被当作可能性的度量，实在是不好说。\n\n有人批评这是机械唯物主义，这种人批判的武器一般是武器的批判，别争辩，先活下来再说。\n\n### 频率学派\n\n这种观点一言以蔽之：概率是频率在样本量趋于无穷时的极限。\n\n科学中（日常生活中也一样，只是人们通常没这么精确），测量误差不可避免，我们每一次的测量哪怕正确，互相之间也会有细微的差别，更不用说和待测的真实值不同了。\n\n解决方法在初中物理实验里学过：多次测量，把平均值当作真值（的估计量），根据标准差计算误差（置信区间、p 值等等……）。\n\n不同的人（假设有 M 个）可以对同一个可观测量进行 N 次测量，对于一个确定的 N，不论这个可观测量本身服从何种概率分布，这 N 个测量值的平均数 $$\\bar X_N$$ 都服从正态分布，这就是中心极限定理（注意不是大数定律）。\n\n当可观测量本身也服从正态分布的时候，就会导致标准差 (standard deviation) 和标准偏误 (standard error of the mean, 常简称为 standard error) 容易让初学者混淆。\n\n而按照这种世界观，所谓一个物理量的真值，就是所有可能的（所有已经发生过的+思想实验中可能发生的）测量的均值 $$\\bar X_\\infin$$。\n\n因为包括可能发生还未发生的测量，所以哪怕我们面对的问题是纯决定论的，客观存在一个确定的真值，无论我们已经进行过多少次测量，都无法保证得到真值。\n\n有人批评这是客观唯心主义，这种人批判的武器一般是武器的批判，别争辩，先活下来再说。\n\n### 贝叶斯\n\n前述世界观好歹还认为真值客观存在——\n\n贝叶斯世界观则直接不再对真值的客观存在下断言，不论先验还是后验，科学理论里的每一条命题，都不再孤单，而是要和所有可能的替代理论打包在一起；也不再“正确”，而是具有一个以概率衡量的可信程度。\n\n实验的作用不再是判断对错，而是在有限的先验知识（现存的科学理论）下，判断新取得的实验结果在多大程度上，更新了旧知识里每条命题的可信权重。\n\n而且每个人掌握的知识不同，先验概率不同，在同样的实验数据面前，所更新出来的知识体系也会不同。\n\n再者，如果先验概率为 0，任你实验数据如何显著，后验概率也一定为 0，所以对“未知的未知”无能为力。实践中，再离谱的先验假设，只要能想到，也要赋一个小而不为 0 的初值。\n\n有人批评这是主观唯心主义，这种人批判的武器一般是武器的批判，别争辩，先活下来再说。\n\n## 送分题\n\n已知本省不超过二十个地级行政单位。一中是本市最好的高中，本科过线人数年年创新高。\n\n已知本市报纸会公布喜报，上有全市前若干名学生的姓名、分数、录取学校等信息。省招办有根据成绩取得全省排名的服务。比如某年本市第十名，全省排名两千名开外。\n\n你能否据此评价母校和家乡的教学质量，以及本省各地区之间教育水平的平均程度？\n\n你该如何评价，从定义原假设和备择假设，到用何种概率分布对先验概率建模？\n\n你有资格评价吗？"},{"slug":"information-entropy-kl-divergence-cross-entropy-mutual-information","filename":"2024-05-14-information-entropy-kl-divergence-cross-entropy-mutual-information.md","date":"2024-05-14","title":".tex | 比较两个概率分布/两条信息","layout":"post","keywords":["tex","phy","m"],"excerpt":"自信息、信息熵、KL Divergence、交叉熵、互信息","hasMath":true,"content":"\n\u003e 自鸣得意了半天，发现这篇文章基本就是维基百科 [Quantities of Information](https://en.wikipedia.org/wiki/Quantities_of_information) 词条英文版的翻译。但是对应的中文词条没有覆盖英文版那么多的内容，所以也不完全是无用功。\n\u003e \n\n## 信息和概率\n\n一条信息由一个命题来表达。（这一个命题可以是对多个命题进行逻辑演算的一个表达式。）\n\n而这个命题解答了人心中的某个疑问。既然这是个疑问，那么在得到确切的信息之前，有众多其他命题，和这条消息一样有可能是问题的答案。既然是有可能，那就是概率论可以派上用场的对方。所有这些可能成为答案的命题一起，构成一个随机变量空间。\n\n比如说一道有 ABCD 四个选项的选择题，如果是单选题，那么答案的随机变量空间就是 {A, B, C, D}，如果是多选题，则是 {A, B, C, D, AB, AC, AD, BC, BD, CD, ABC, ABD, ACD, BCD, ABCD}，如果是排序题、不定项排序题、答案出错了的题……\n\n## 描述一个概率分布的信息量\n\n### 自信息：Self Information\n\n自信息是一个随机事件的性质，也就是针对一个随机变量的**某一个可能取值**而言的。表达式为 \n\n$$\nI(m) = -\\log_n\\left(p(M=m)\\right)\n$$\n\n这是一个无量纲量，但是公式中指数的底数可以任意选择——\n\n- *n* = 2 的时候自信息的单位是 bit，也叫香农 (shannon), 这里的 bit 和二进制位 bit 不完全相同，一个香农是一个二进制位所能表示信息的**上限**：当一个二进制位完全取决于其它位时，这个位不包含任何额外信息，香农数为 0，但这个二进制位依然物理上存在；\n- *n* = *e* 的时候单位是 nat, 因为 $$\\log_e\\equiv\\ln$$ 叫做自然对数；\n- *n* = 10 的时候单位叫 hartley\n\n——单位之间的换算关系由对数的换底公式给出。\n\n这个量在信息论中的意义是，这条消息作为一个不方便问的问题的**答案**，**最少可以**用多少个 n 个选项的单选题套出答案。当 n=2 的时候，每个问题就是一个是非题，也就是一般疑问句。\n\n码农面试的时候经常问一类问题：一堆看起来相同的东西里面有一个不一样，你有一种不能直接测出答案的测量工具，最少需要测量几次才能辨别出来……但是自信息的计算不能提供具体的辨别方法，具体方法还是需要你自己去凑，而面试刷人很多都是在刷这种细枝末节。\n\n当然了，前提是你的面试官懂他自己在问什么，而不是相信美剧《硅谷》里压缩算法可以突破信息论极限的计算机民科～\n\n当 *p* = 0 时，自信息发散为无穷大。不过问题不大，原因在下一节。\n\n### 信息熵：Entropy\n\n信息熵是一个随机变量的概率分布的整体性质。\n\n算法很简单，就是自信息的概率期望，也就是按照随机变量每个取值的概率加权平均：\n\n$$\nS(p(M))=\\mathbb{E}_p[-\\log_n p(M)]=-\\sum_{m\\in M}p(m)\\log_n p(m)\n$$\n\n当 *p* = 0 时，自信息发散，但是概率为零，强行定义两者的积也为零，对信息熵不构成贡献。\n\n当我们只对某一特定的随机事件信息感兴趣，除此以外的所有事件合并为目标事件的补集，就得到二项信息熵 binary entropy:\n\n$$\nS_{binary} = -(1-p)\\log(1-p)-p\\log p = p\\log\\frac{1-p}{p}-\\log(1-p)\n$$\n\n沿着自信息的意义往下走，信息熵在信息论中的意义是，一个将众多信息/命题的集合作为备选答案的**问题**，**最少可以**用多少道 n 个选项的单选题的集合来等价替代。\n\n当这些最优的单选题确定之后，原问题的每一个选项，可以用单选题的答案序号来进行编码。指数的不同底数/信息量的不同单位就是数字的 n 进制，信息量就是相应进制下最大压缩编码后的位数。\n\n当然要讨论压缩的话，还需要另找地方记录各个单选题和选项，也就是压缩字典。\n\n## 比较两个概率分布的信息量\n\n而如何选择单选题，使之成为针对给定问题最优的问题集，会因为各个选项概率分布的不同而变化。即便是同一组信息/备选答案，两套不同的概率分布，各自会给出一套对自己最优的问题集，一套概率分布下的最优问题集不见得是另外一套概率分布下的最优问题集。\n\n\u003e 下面的表达式都只写出了离散变量的形式，连续随机变量需要将求和写成对应的积分。\n\u003e \n\n### 相对熵：Kullback–Leibler (K-L) Divergence\n\n英文里也叫 relative entropy 或者 I-divergence\n\n这里的两个概率分布映射自**同一个**随机变量空间。\n\n$$\nD_{KL}(p(X)|q(X))=\\sum_{x\\in X}p(x)\\log\\frac{p(x)}{q(x)}=-\\sum_{x\\in X}p(x)\\log\\frac{q(x)}{p(x)}\n$$\n\n这个量描述了当 *p*(*X*) 作为各选项的正确概率分布的情况下，用对 *q*(*X*) 最优的单选题去提问，**没问出来的信息**所需要的**额外的**单选题数目/编码数。\n\n在科学应用中，*p*(*X*) 一般是从实验中测量出来的概率分布，*q*(*X*) 是理论模型的预测。\n\n下面的例子计算了一个单选题，选 C、选 B、假想中一群学生的答案统计、胡猜四种概率分布 *p, q ,r , φ* 之间的 KL divergence。因为概率为零会出现发散问题，所以我们取 eps = 10^(-10) 把这些概率值截断：\n\n```python\nimport numpy as np\n\ndef kl_div(p,q,eps=1e-10):\n    p = np.clip(p,eps,1-eps)\n    q = np.clip(q,eps,1-eps)\n    return np.sum(p*np.log2(p/q))\n\np   = np.array([  0,   0,   1,   0])\nq   = np.array([  0,   1,   0,   0])\nr   = np.array([1/6, 1/6, 1/2, 1/6])\nphi = np.array([1/4, 1/4, 1/4, 1/4])\n\nresults = np.empty((4,4))\nfor i,v1 in enumerate([p,q,r,phi]):\n    for j,v2 in enumerate([p,q,r,phi]):\n        results[i,j] = kl_div(v1,v2)\n```\n\n| KL-div(行, 列)/bit | p | q | r | φ |\n| --- | --- | --- | --- | --- |\n| p = [0,0,1,0] | 0 | 33.219 | 1 | 2 |\n| q = [0,1,0,0] | 33.219 | 0 | 2.585 | 2 |\n| r = [1/6, 1/6, 1/2, 1/6] | 14.817 | 25.890 | 0 | 0.208 |\n| φ = [1/4,1/4,1/4,1/4] | 22.914 | 22.914 | 0.189 | 0 |\n\n从结果中我们可以看到：\n\n- 对角线为 0，符合其意义。\n- $$D_{KL}(p,q)$$ 和 $$D_{KL}(q,p)$$ 都应该是 +∞，这里的有限值是 eps 截断的结果\n- 除个别巧合，对称位置的值一般不相等。这个量不同于两点之间的距离。\n\n### 交叉熵：Cross Entropy\n\n这里的两个概率分布映射自**同一个**随机变量空间 X。\n\n概率分布 ***q* 相对于 *p*** 的交叉熵 cross entropy\n\n$$\nCE(p(X),q(X))=-\\sum_{x\\in X}p(x)\\log q(x)=S(p(X))+D_{KL}(p(X)|q(X))\n$$\n\n这个量描述了当 *p*(*X*) 作为各选项的正确概率分布的情况下，用对 *q*(*X*) 最优的单选题去提问，所需要的**总共的**单选题数目/编码数。\n\n类似于二项熵，*p* 和 *q* 之间的 binary cross entropy:\n\n$$\nBCE(p,q)=-p\\log q-(1-p)\\log(1-q)=p\\log\\frac{1-q}{q}-\\log(1-q)\n$$\n\n```python\ndef cross_entropy(p,q,eps=1e-10):\n    p = np.clip(p,eps,1-eps)\n    q = np.clip(q,eps,1-eps)\n    return -np.sum(p*np.log2(q))\n\nresults = np.empty((4,4))\nfor i,v1 in enumerate([p,q,r,phi]):\n    for j,v2 in enumerate([p,q,r,phi]):\n        results[i,j] = cross_entropy(v1,v2)\n```\n\n| Cross Entropy(行, 列)/bit | p      | q      | r   |   $$\\varphi$$   |\n| ---                      | ---    | ---    | ---   | --- |\n| p = [0,0,1,0]            | 0      | 33.219 | 1     | 2   |\n| q = [0,1,0,0]            | 33.219 | 0      | 2.585 |  2  |\n| r = [1/6, 1/6, 1/2, 1/6] | 16.610 | 27.683 | 1.792 | 2   |\n| $$\\varphi$$ = [1/4,1/4,1/4,1/4]    | 24.914 | 24.914 | 2.189 | 2   |\n\n- 对角线上不一定为零，而是自己的信息熵\n- 其他位置和 KL divergence 相差大约为第一个输入分布的信息熵，误差 eps 的截断\n\n### 互信息：Mutual Information\n\n这里的两个概率分布一般来说映射自**不同的**随机变量空间。\n\n$$\nMI(X,Y)=\\sum_{x,y}p(x,y)\\log\\frac{p(x,y)}{p(x)p(y)}=D_{KL}\\left(p(X,Y)|p(X)p(Y)\\right)\n$$\n\n从后一个等号可以看出，这一性质衡量的是 *X, Y* 两个随机变量的联合分布在多大程度上不同于“*X* 和 *Y* 相互独立”的零假设。两个随机变量相互独立时，互相不反映对方的信息，互信息 *MI* = 0。\n\n当从 *X* 所在的随机变量空间取样的难度比较大的时候，我们需要用容易取样的**另一个变量空间**的随机变量 *Y* 来推测 *X* 的情况，互信息就可以用来论证我们这种选择的合理性。\n\n## 扯点闲篇\n\n### PyTorch 中以此为基础的 loss functions\n\n`torch.nn` 中有如下几个和今天的文章相关的 loss functions：\n\n- `torch.[nn.KLDivLoss](https://pytorch.org/docs/stable/generated/torch.nn.KLDivLoss.html#torch.nn.KLDivLoss)`\n- `torch.[nn.CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss)`\n- `torch.[nn.BCELoss](https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html#torch.nn.BCELoss)`\n- `torch.[nn.BCEWithLogitsLoss](https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html#torch.nn.BCEWithLogitsLoss)`\n\n之所以没直接用这些函数计算上面的例子，是因为 `KLDivLoss` 是按元素计算，随后需要自己求和；`CrossEntropyLoss` 又是按类别的，还不需要归一化，而且文档的解释很复杂，我到现在也没看明白；而且还要注意这些函数的设计输入是不是 logit，这是机器学习里的概念，在此不展开了。\n\n### 玻尔兹曼的墓志铭\n\n$$\nS=k\\log W\n$$\n\n其中 *S* 是（微正则系综中的）热力学熵，*k* 是玻尔兹曼常数 $$k_B$$，*W* 是因为刻碑的师傅不会写 *Ω*。\n\nW 或 Ω 是处于相同能量的热力学状态的数量。因为你都需要统计物理了，显然是只知道能量，没办法知道所考虑的微观粒子究竟处于哪一个热力学状态。那此时的零假设就是处于所有状态的可能性相等，*p* = 1/Ω，信息熵 \n\n$$\nS =-\\sum_{m\\in M}p(m)\\log_n p(m)= -\\Omega\\cdot(\\frac{1}{\\Omega}\\log\\frac{1}{\\Omega})=\\log\\Omega\n$$\n\n和热力学熵只相差一个玻尔兹曼常数。这是因为信息熵是无量纲的，熵和温度的量纲相乘之后需要得到能量的量纲，只能由 $$k_B$$ 把量纲凑齐，而数值是自由能相关的实验里测出来的。\n\n好像这就是高中物理里熵的定义式是吧。\n\n上了大学以后，正则系综和巨正则系综中的熵也分别就是各自体系中各状态的概率分布的信息熵，乘上玻尔兹曼常数。~~（我也忘得差不多了，试图萌混过关）~~\n\n### 善卜者无先见之明\n\n公元 451 年，阿提拉 Attila 率领匈人攻入罗马领土，横扫有大量其他民族居住的高卢地区。西罗马帝国将军艾提乌斯 Aetius 联络了众多畏惧匈人的民族组成联军，其中包括西哥特人的王狄奥多里克 Theodoric，两军会战于卡塔隆 Catalaunian 平原。\n\n本来想用这个故事举例子来着，因为我记得阿提拉在战前找了个大师算了一卦，说是一位国王将战死，一个国家将崩塌。于是阿提拉很高兴，以为哥特人和狄奥多里克要玩完了，结果战斗打响，狄奥多里克确实死于乱军，但是罗马和哥特等族的联军击败了匈人，阿提拉的霸业雨打风吹去。\n\n于是试图说明算命的魅力就在于，用文字游戏表达一个自信息比较低的命题，同时误导对方相信一个自信息高得多的命题，在心理疏导之外，赚一个信息熵的差价。\n\n结果查证的时候发现好像不是这么回事，Barbarian Rising 故事片里的预言内容不一样；维基百科上没给出处，说算命的很准，于是阿提拉推迟到下午作战，方便晚上跑路；其他地方甚至压根没有算命的情节。但是写都写了，需要积累高考作文素材的小朋友们还是可以假装被我误导了~\n\n当然了，算命这个事还有一种情况，就是打着不确定的幌子，售卖确定但不方便承认自己确定的信息，那就是另一种生意，和另外的价格了~"},{"slug":"equivlance-between-diffusion-equation-and-random-walk","filename":"2024-04-25-equivlance-between-diffusion-equation-and-random-walk.md","date":"2024-04-25","title":".tex | 扩散方程和随机游走的等价","layout":"post","keywords":["tex","phy","m"],"excerpt":"之前 MCMC 讲错了","hasMath":true,"content":"\n\u003e 这些内容总结自美国研究生级别的《数学物理方法》两次课的笔记，大约两个小时。\n\u003cbr\u003e如果是中国大学本科的话，认真的老师半个小时庶几可以讲完;\n\u003cbr\u003e念 PPT 就算上课的话 15 分钟可以讲完，附赠一个段子;\n\u003cbr\u003e翻转课堂的话也就布置个作业，老师一句话可以讲完。\n\u003cbr\u003e以上数据除第一句外纯属揣测，没有黑任何人的意思，love and peace~\n\u003e \n\n### 扩散方程\n\n带有初值条件的扩散方程表述如下：\n\n$$\n\\begin{cases}\nu(x,t=0)=f(x) \\\\\n\\partial u(x,t)/\\partial t=\\sigma \\cdot \\partial^2 u(x,t)/ \\partial x^2\n\\end{cases}\n$$\n\n方程的解为：\n\n$$\nu(x,t) = \\frac{1}{\\sqrt{4\\pi \\sigma t}} \\int_{-\\infty}^{+\\infty} f(s)\\ e^{-\\frac{(x-s)^2}{4\\sigma t}}\\ ds\n$$\n\n解法是将 u(x,t) 对空间变量 x 作傅里叶变换为 U(k,t)，利用傅里叶变换的性质，变换后的方程将是关于时间 t 的一阶常微分方程。求解后作傅里叶逆变换 ~~即为上式。~~ ~~（完蛋，好久没做题了，那个 $$e^{-\\frac{(x-s)^2}{4\\sigma t}}$$ 是怎么凑出来的，为什么我直接给消掉了啊）~~ 凑出来了凑出来了，初值条件代入频域 k 空间里的通解来确定积分常数，可以看到结果 $$F(k) e^{-\\sigma t k^2}$$ 是两项之积，所以根据傅里叶变换的卷积定理，实空间 x 里的解是 f(x) 和 $$\\mathscr{F}_{k\\rightarrow x}^{-1}\\{e^{-\\sigma t k^2}\\}$$ 的卷积（所以上式的指数项以 (x-s) 为宗量），而计算后者的时候需要用到高斯积分～\n\n### 随机游走\n\n随机游走是一个离散过程，为了和连续时空中的扩散方程相对比，将空间变量 x 离散化为相隔 Δ 的格点 i，时间变量 t 离散化为相隔 δ 的 n。\n\n当一个粒子在 n 时刻位于格点 i 时，在下一个时刻 n+1, 它有 1/2 的概率移动到 i-1, 1/2 的概率移动到 i+1.\n\n所以，虽然每个进行随机游走的粒子在任意时刻都只有确定且唯一的位置，但是对于大量同样初始位置和运动规律的例子，n 时刻出现在 i 格点的概率 P(i,n) 有以下关系：\n\n$$\n\\begin{cases}\nP(i,0)= f_i \\\\\nP(i,n)=\\frac{1}{2}\\left[P(i-1,n-1)+P(i+1,n-1)\\right]\n\\end{cases}\n$$\n\n在初值条件为 $$f_i=\\delta_{i=0}$$ 时，递推结果如下：\n\n|  x 轴 — | i = -4 | i = -3 | i = -2 | i = -1 | O | i = 1 | i = 2 | i = 3 | i = 4 | → |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n| n = 0  | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 |  |\n| n = 1  | 0 | 0 | 0 | 1/2 | 0 | 1/2 | 0 | 0 | 0 |  |\n| n = 2  | 0 | 0 | 1/4 | 0 | 1/2 | 0 | 1/4 | 0 | 0 |  |\n| n = 3  | 0 | 1/8 | 0 | 3/8 | 0 | 3/8 | 0 | 1/8 | 0 |  |\n|**t 轴 ↓**|  |  |  |  |  |  |  |  |  |\n\n![](/photos/2024-04-25-random-walk-probabilities.png)\n\n离散的情况，很难对任意的初值条件写出解的表达式，但是对于上面的特殊情况，课上不加证明地给出了（可能是根据上图凑出来的）下面的解：\n\n$$\nP(i,n)=\\frac{1}{2^n}\\frac{n!}{\\left(\\frac{n+i}{2}\\right)!\\left(\\frac{n-i}{2}\\right)!}\n$$\n\n对的，以上关系只能表示 (n+i = 偶数) 的情况，但是康托尔告诉了我们，所有偶数和所有自然数的“数量”一样多，所以也没差太多～\n\n### 方程的等价\n\n概率的递推公式可以变换为：\n\n$$\n\\frac{1}{\\delta}\\left[P(i,n)-P(i,n-1)\\right]=\\frac{1}{2}\\left [\\frac{P(i-1,n-1)-2P(i,n-1)+P(i+1,n-1)}{\\Delta^2}\\right]\\frac{\\Delta^2}{\\delta}\n$$\n\n因为 $$x=i\\Delta,\\ t=n\\delta$$, 对两个变量的微分可以离散化成差分：\n\n$$\n\\frac{\\partial}{\\partial x}\\rightarrow \\frac{1}{\\Delta}\\left[()_i-()_{i-1}\\right],\\ \\frac{\\partial}{\\partial t}\\rightarrow \\frac{1}{\\delta}\\left[()_n-()_{n-1}\\right]\n$$\n\n直接就能看出扩散方程和随机游走的等价，且系数之间存在关系：$$\\sigma = \\frac{\\Delta^2}{2\\delta}$$\n\n### 解的等价\n\n只讨论一个 δ(x) 函数作为初值条件的情况，我们要证明此时扩散方程的解：\n\n$$\nu(x,t) = \\frac{1}{\\sqrt{4\\pi \\sigma t}} e^{-\\frac{x^2}{4\\sigma t}}\\ \\xleftarrow[{\\Delta,\\delta \\rightarrow 0;\\ i,n\\rightarrow \\infty}]{x=i\\Delta,\\ t=n\\delta} \\frac{1}{2^n}\\frac{n!}{\\left(\\frac{n+i}{2}\\right)!\\left(\\frac{n-i}{2}\\right)!} \\frac{1}{2\\Delta}\n$$\n\n只需讨论这一个情况，因为 δ(x-s) 函数可以看作将一个函数 f(x) 在自变量 x=s 时切片为 f(s)，而任何一个（性质比较“优美”的）函数都可以看作把它自己在定义域上的所有点切片后再重新叠加起来：\n\n$$\nf(x) = \\int_{-\\infty}^{+\\infty}f(s)\\delta(x-s)\\ ds\n$$\n\n过程需要用到 Sterling 公式对阶乘的近似：$$n! \\approx \\sqrt{2\\pi n}\\ n^n e^{-n}$$\n\n$$\n\\begin{array}{rcl}\n\\frac{P(i,n)}{2\\Delta} \u0026 \\approx \u0026 \\frac{1}{2\\Delta} \\frac{1}{2^n} \\frac{\\sqrt{2\\pi n}\\ n^n e^{-n}}{\\sqrt{\\frac{2\\pi (n-i)}{2}}\\ \\left(\\frac{n-i}{2}\\right)^{\\frac{n+i}{2}} e^{-\\frac{n+i}{2}}\\sqrt{\\frac{2\\pi (n+i)}{2}}\\ \\left(\\frac{n+i}{2}\\right)^{\\frac{n+i}{2}} e^{-\\frac{n+i}{2}}} \\\\\n\u0026 = \u0026 \\frac{1}{2\\Delta}\\frac{\\sqrt{2n}}{\\sqrt{\\pi(n^2-i^2)}}\\frac{n^n}{(n-i)^{\\frac{n}{2}-\\frac{i}{2}}(n+i)^{\\frac{n}{2}+\\frac{i}{2}}} \\\\\n\u0026 = \u0026 \\frac{1}{2\\Delta}\\frac{\\sqrt{2n}}{\\sqrt{\\pi(n^2-i^2)}}\\frac{n^n/n^n}{(n-i)^{\\frac{n}{2}}(n+i)^{\\frac{n}{2}}(n-i)^{-\\frac{i}{2}}(n+i)^{\\frac{i}{2}}/n^n} \\\\\n\u0026 = \u0026 \\frac{1}{2\\Delta}\\frac{\\sqrt{2n}}{\\sqrt{\\pi(n^2-i^2)}} \\frac{1}{\\left(1-\\frac{i}{n}\\right)^\\frac{n}{2}\\left(1+\\frac{i}{n}\\right)^\\frac{n}{2}\\left(1-\\frac{i}{n}\\right)^{-\\frac{i}{2}}\\left(1+\\frac{i}{n}\\right)^\\frac{i}{2}} \\\\\n\u0026 = \u0026 \\frac{1}{\\sqrt{2}\\Delta}\\frac{1}{\\sqrt{\\pi(n-\\frac{i^2}{n})}} \\frac{1}{\\left(1-\\frac{i^2}{n^2}\\right)^\\frac{n}{2}\\left(1-\\frac{i}{n}\\right)^{-\\frac{i}{2}}\\left(1+\\frac{i}{n}\\right)^\\frac{i}{2}} \\\\\n\u0026 \\xrightarrow[\\frac{i}{n}=\\frac{x\\Delta}{2\\sigma t},\\ \\frac{i^2}{n}=\\frac{x^2\\delta}{\\Delta^2 t}]{(1+a\\epsilon)^{1/\\epsilon}\\rightarrow e^a} \u0026 \\frac{1}{\\sqrt{4\\pi\\sigma t}}\\frac{1}{e^\\frac{x^2}{4\\sigma t} e^\\frac{x^2}{4\\sigma t} e^{-\\frac{x^2}{4\\sigma t}} } \\\\\n\u0026 = \u0026 \\frac{1}{\\sqrt{4\\pi\\sigma t}}e^{-\\frac{x^2}{4\\sigma t}}\n\\end{array}\n$$\n\n### 之前 MCMC 讲错了\n\n讲 Markov Chain Monte Carlo 模拟的时候举的例子是计算 $$\\int_{-\\infty}^{+\\infty}e^{-x^2}dx$$, 现在系数可以对上了：$$1=4\\sigma t=4 \\frac{\\Delta^2}{2\\delta} n\\delta = 2\\Delta^2n$$, 随机游走的步数和步长之间存在一个确定的关系，在步长确定的情况下，我们需要重复模拟大量粒子作相同步数的随机游走，然后统计这一确定步数走完之后的每个粒子的终末位置。\n\n所以，这并不是一个 Markov Chain Monte Carlo 模拟，只是一个普通的 Monte Carlo 模拟，我们拿到了想要知道的随机变量的原始概率分布，只不过取得符合这一概率分布的每一个样本的过程是一个 Markov 过程。\n\n正经的 MCMC，应该是只模拟一个粒子作随机行走，然后把它每一步的位置记录下来，统计到样本里去。这样的话时间 t 的信息就被抹去了，而且由于扩散方程描述的状态并不是热力学平衡态，并不能通过统计物理中的遍历性 (ergodicity) 来得到正确结果。\n\n采用了 Metropolis 算法的 MCMC, 一个粒子作随机行走只是其中的一个步骤，还要计算这一步之前和之后 $$e^{-x^2}$$ 的值，来决定这一步是否被加入样本，不成立的话要退回前一步继续走。\n"},{"slug":"mc-mcmc-markov-chain-monte-carlo-gibbs-sampling","filename":"2024-04-15-mc-mcmc-markov-chain-monte-carlo-gibbs-sampling.md","date":"2024-04-15","title":".tex | MC→MCMC 蒙特卡洛模拟，基于马尔科夫链采样","layout":"post","keywords":["tex","phy","m"],"excerpt":"蒙特卡洛模拟、马尔科夫链采样、Metropolis-Hastings 算法、吉布斯采样","hasMath":true,"content":"\nMonte Carlo 蒙特卡洛模拟，简称 MC. \n\nMarkov Chain Monte Carlo 是用马尔科夫链采样的蒙特卡洛模拟，简称 MCMC.\n\n## Monte Carlo 模拟\n\n这个比较简单了，举个例子，要计算 π 的近似值，可以在一块正方形板子里画一个内接圆，然后以均匀的概率往正方形里一粒一粒地扔沙子，每扔一粒，就判断并且记录这里沙子在圆内还是圆外，然后把沙子吹掉，如此往复。圆的面积是 πr²，正方形的面积是 4r²，所以落在圆内的概率（圆内沙子的数量和总数的比值）乘 4，就是所求。\n\n![](/photos/2024-04-15-monte-carlo-pi.png)\n\n归纳一下：当问题的解用一个随机变量的概率分布、期望值、二阶矩……等等来表示的时候，就生成一个符合该概率分布的随机样本，用样本的统计量去近似原概率分布。\n\n## Markov Chain Monte Carlo\n\n但是前述例子有一个步骤，就是我们往板子上扔完沙子要把沙子吹掉，每粒沙子，每次扔沙子之间也应该看不出区别，这是为了保证取样之间**相互独立且来自同一个概率分布**。\n\n但是很多取样过程无法满足这种条件，或者达成条件所需的成本很高。比如计算一个高斯积分 $$\\int_{-\\infty}^{+\\infty}e^{-x^2}dx$$，被积函数的取值范围涵盖整个实数集，想找一个在整个实数集上均匀分布的随机数发生器就比较难了。\n\n![](/photos/2024-04-15-monte-carlo-gaussian.png)\n\n但是学过物理的朋友应该知道，上面的被积函数是以狄拉克 δ(x) 函数为初值条件的一个扩散方程的解，在某一时刻的空间分布。（不想凑系数了，将就看吧）\n\n而扩散方程又是随机游走 (random walk) 在连续近似下的极限。\n\n所以我们直接模拟一堆粒子从原点出发作随机行走，向两个方向的概率相同，扩散系数以及积分里的常数对齐，统计粒子在整个过程中出现在不同 x 位置的频率，求和之后乘以步长就是积分结果。这个过程需要的随机数发生器容易获取得多，是一个以 0.5 为阈值的 [0,1) 的均匀分布，比如一个均匀硬币。\n\n而随机行走过程中走完每一步的位置，都只取决于前一步的位置，而与更久远的历史无关——这样的过程叫做马尔可夫过程。用这种方法取样获得随机样本的蒙特卡洛模拟，就是 MCMC.\n\n扩散方程和随机行走只是 MCMC 的一个很特殊很特殊的例子，而对于一般的 MCMC 模拟，有以下通用的 Markov Chain 采样的算法：\n\n### Metropolis-Hastings 算法\n\n已知一个随机变量 x, 和一个与目标概率分布 P(x) 成正比的函数 f(x)（不要求 f 归一化）\n\n1. 初始化\n    1. 选定初始采样点 $$x_0$$ \n    2. 选定一个采样函数 proposal function，也就是在已知当前 x 的取值时，下一个 x’ 取值的概率分布 $$g(x’\\vert x)$$；其中对于 Metropolis 算法，这个采样函数是对称的：$$g(x’\\vert x)=g(x\\vert x’)$$. 常用以两者之差为宗量的高斯函数。\n2. 在得出 t 时刻的 $$x_t$$ 之后：\n    1. 根据 $$g(x'\\vert x_t)$$ 抽样得到一个 x’\n    2. 计算 α = f(x’)/f(x) = P(x’)/P(x)\n    3. 决定是否将 x’ 加入样本\n        1. 如果 α ≥ 1, 直接加入\n        2. 如果 α \u003c 1, 以 α 为概率加入\n\n这种方法不保证采样的早期样本也符合目标概率分布，所以一般会抛弃最先加入的若干样本。\n\n### Gibbs 采样\n\n只是一种思路，不算是完整的算法。\n\n当被采样的随机变量是一个多维向量的情况，在不使用 Gibbs 采样的情况下，在迭代的某一步骤 t，每个分量都应该是前一步骤的函数：$$x_{i,t}=f(\\{x_{j,\\ t-1}\\})$$\n\n而 Gibbs 采样就是说，不必让每个维度 i 都根据前一个步骤的分量来取值，可以把当前 t 已经取样出来的分量直接带入到本回合后面的维度：$$x_{i,t}=f(\\{x_{j,\\ t}\\}_{j\u003ci}\\cup\\{x_{k,\\ t-1}\\}_{k\\ge i})$$"},{"slug":"physics-based-neural-network-review-note","filename":"2023-03-20-physics-based-neural-network-review-note.md","date":"2023-03-20","title":".tex | 基于物理的神经网络 (PINN) 综述笔记","layout":"post","keywords":["tex","phy","md","ai"],"hasMath":true,"excerpt":"本文是《Scientific Machine Learning Through Physics–Informed Neural Networks: Where we are and What’s Next》这篇综述的读书笔记。","content":"\n\u003e 本文是《[Scientific Machine Learning Through Physics–Informed Neural Networks: Where we are and What’s Next](https://link.springer.com/article/10.1007/s10915-022-01939-z)》这篇综述的读书笔记。\n\u003e \n\n年前，今年新入职的天文学方面的一位老师给我们群发邮件，宣传某国家实验室超算的 GPU 编程马拉松活动，他可以担任指导老师。于是毫不意外地，我报了名。该编程马拉松项目还需要专门申请，申请材料里要写清楚打算干什么，于是报名的五六个人七嘴八舌地想创意。基于物理的神经网络 PINN 就是天文老师的点子。\n\n~~写到这里，我才意识到，老哥是不是想拿我们当免费劳动力啊~~~\n\n神经网络可以看作是一个复杂的非线性函数，接受一个（一般来说维度很高的）向量作为输入，一番计算后输出另一个向量。训练神经网络，就是找到这个函数的参数，绝大多数找参数的方法涉及计算网络输出对参数的偏导数，因此神经网络计算框架的核心功能就是自动微分 (auto-differentiation)。\n\n而很多物理问题，都可以用（偏）微分方程来描述，微分方程的解不是变量，而是函数，而且往往是复杂的非线性函数。所以基于物理的神经网络 (PINN) 就是以神经网络来表达这个函数，然后把这个函数带入到物理的微分方程中，把神经网络输出和真正的物理解之间的差距当作损失函数，反向传播回去来优化神经网络的参数。代入方程时的微分计算，正好可以利用现成框架的自动微分功能。\n\n在以 GPT 为代表的 transformer 类神经网络模型出现之前，自然语言处理类的机器学习项目，往往要在网络之外，利用人类的语法知识，对语段进行语义分割等等“中间任务”。Transformer 一出，算力出奇迹，中间任务逐渐变得没有必要了。\n\n在 GPT 崭露头角，并且越来越有迹象表明其将会涌现出通用人工智能的今天，这些基于物理的神经网络，会不会还未成熟就已过时？这种心情，就和《三体》第二卷开始，章北海和吴岳面对焊渍未漆的“唐”号航空母舰时差不多吧……\n\n\u003chr class=\"slender\"\u003e\n\n- Abstract\n    - PINNs are neural networks that encode model equations. a NN must fit observed data while reducing a PDE residual.\n\n1. Introduction\n    - The “curse of dimensionality” was first described by Bellman in the context of optimal control problems. (Bellman R.: Dynamic Programming. Sci. 153(3731), 34-37 (1966))\n    - Early work: MLP ([multilayer perceptron](https://en.wikipedia.org/wiki/Multilayer_perceptron)) with few hidden layers to solve PDEs. ([https://doi.org/10.1109/72.712178](https://doi.org/10.1109/72.712178))\n    - 感觉可能更全面的一篇综述：[https://doi.org/10.1007/s12206-021-0342-5](https://doi.org/10.1007/s12206-021-0342-5)。该文关注 what deep NN is used, how physical knowledge is represented, how physical information is integrated，本文只关于 PINN, a 2017 framework。\n\n    1. What the PINNs are\n        - PINNs solve problems involving PDEs:\n            - approximates PDE solutions by training a NN to minimize a loss function\n            - includes terms reflecting the initial and boundary conditions\n            - and PDE residual at selected points in the domain (called **collocation points**)\n            - given an input point in the integration domain, returns an estimated solution at that point.\n            - incorporates a [residual network](https://en.wikipedia.org/wiki/Residual_neural_network) that encodes the governing physical equations\n            - can be thought of as an **unsupervised strategy** when they are trained solely with physical equations in forward problems, but **supervised learning** when some properties are derived from data\n        - Advantages:\n            - [mesh-free](https://en.wikipedia.org/wiki/Meshfree_methods)? 但是我们给模型喂训练数据的时候往往已经暗含了 mesh 了吧\n            - on-demand computation after training\n            - forward and inverse problem using the same optimization, with minimal modification\n    2. What this Review is About\n        - 提到了一个做综述找文章的方法：本文涉及的文章可以在 Scopus 上进行高级搜索：`((physic* OR physical)) W/2 (informed OR constrained) W/2 “neural network”)`\n2. The Building Blocks of a PINN\n    - question:\n    \n    $$\n    F(u(z);\\gamma)=f(z),\\quad z\\ \\in\\ \\Omega \\\\ B(u(z))=g(z), \\quad z\\ \\in\\ \\partial \\Omega\n    $$\n    \n    - solution:\n    \n    $$\n    \\hat u_{\\theta}(z)\\approx u(z)\\\\ \\theta^* = \\arg\\min_{\\theta}\\left(\\omega_F L_F(\\theta)+\\omega_BL_B(\\theta)+\\omega_{data}L_{data}(\\theta)\\right)\n    $$\n    \n    1. Neural Network Architecture\n        - DNN (deep neural network) is an artificial neural network that is deeper than 2 layers.\n        \n        1. Feed-Forward Neural Network: \n            - $$u_{\\theta}(x) = C_{K} \\circ C_{k-1} ...\\alpha \\circ C_1(x),\\quad C_k(x) = W_k x_k + b_k$$\n            - Just change CNN from convolution to fully connected.\n            - Also known as multi-layer perceptrons (MLP)\n            \n            1. FFNN architectures \n                - Tartakovsky et al used 3 hidden layers, 50 units per layer,  and a hyperbolic tangent activation function. Other people use different numbers but of the same order of magnitude.\n                - A comparison paper: *Blechschmidt, J., Ernst, O.G.: Three ways to solve partial differential equations with neural networks –A review. GAMM-Mitteilungen 44(2), e202100,006 (2021).*\n            2. multiple FFNN: 2 phase [Stephan problem](https://en.wikipedia.org/wiki/Stefan_problem).\n            3. shallow networks: for training costs\n            4. activation function: the swish function in the paper has a learnable parameter, so — [how to add a learnable parameter in PyTorch](https://discuss.pytorch.org/t/how-could-i-create-a-module-with-learnable-parameters/28115)\n        2. Convolutional Neural Networks: \n            - I am most familiar with this one.\n            - $$f_i(x_i;W_i)=\\Phi_i(\\alpha_i(C_i(W_i,x_i)))$$\n            - performs well with multidimensional data such as images and speeches\n            \n            1. CNN architectures: \n                - `PhyGeoNet`: a physics-informed geometry-adaptive convolutional neural network. It uses a coordinate transformation to convert solution fields from irregular physical domains to rectangular reference domains.\n                - According to Fang ([https://doi.org/10.1109/TNNLS.2021.3070878](https://doi.org/10.1109/TNNLS.2021.3070878)), a Laplacian operator can be discretized using the finite volume approach, and the procedures are equivalent to convolution. Padding data can serve as boundary conditions.\n            2. convolutional encoder-decoder network\n        3. Recurrent Neural Network\n            - $$f_i(h_{i-1})=\\alpha\\left(W\\cdot h_{i-1}+U\\cdot x_i+b\\right)$$, where f is the layer-wise function, x is the input, h is the hidden vector state, W is a hidden-to-hidden weight matrix, U is an input-to-hidden matrix and b is a bias vector. 我认为等号左边的 $$h_{i-1}$$ 应当作为下标\n            - 感觉有点像 hidden Markov model，只不过 Markov 中间的 hidden layers 好像与序号无关（记不清了），~~RNN 看起来各个 W 和 H 似乎不同~~。**RNN cell is actually the exact same one and reused throughout.** (from [https://blog.floydhub.com/a-beginners-guide-on-recurrent-neural-networks-with-pytorch/](https://blog.floydhub.com/a-beginners-guide-on-recurrent-neural-networks-with-pytorch/)). Cartoon from Wikipedia:\n                \n                ![Untitled]({{ site.baseurl }}/assets/photos/2023-03-20-rnn-unit.png)\n                \n            - From [https://blog.floydhub.com/a-beginners-guide-on-recurrent-neural-networks-with-pytorch/](https://blog.floydhub.com/a-beginners-guide-on-recurrent-neural-networks-with-pytorch/):\n                \n                ![Untitled]({{ site.baseurl }}/assets/photos/2023-03-20-rnn-types.png)\n                \n            1. RNN architectures\n                - can be used to perform numerical Euler integration\n                - 基本上输出的第 i 项只与输入的第 i 和 i-1 项相关。\n            2. LSTM architectures\n                - 比 RNN 多更多中间隐变量，至于怎么做到整合长期记忆的，技术细节现在可以先略过\n        4. other architectures for PINN\n            1. Bayesian neural network: weights are distributions rather than deterministic values, and these distributions are learned using Bayesian inference. 只介绍了[一篇文章](https://doi.org/10.1016/j.jcp.2020.109913)\n            2. GAN architectures: \n                - two neural networks compete in a zero-sum game to deceive each other\n                - physics-informed GAN uses automatic differentiation to embed the governing physical laws in stochastic differential equations. The discriminator in PI–GAN is represented by a basic FFNN, while the generators are a combination of FFNNs and a NN induced by the SDE\n            3. multiple PINNs\n    2. Injection of Physical Laws\n        - 既然是要解常/偏微分方程，那么微分计算必不可少。四种方法：hand-coded, symbolic, numerical, auto-differentiation，最后一种显著胜出。所谓 auto-differentiation, 就是利用现成框架，框架自动给出原函数的导数的算法。\n        - Differential equation residual:\n            - $$r_F[\\hat u_\\theta](z)=r_\\theta(z):=F(\\hat u_\\theta(z);\\gamma)-f$$\n            - $$r_F[\\hat u_\\theta](z)=r_\\theta(x,t)=\\frac{\\partial}{\\partial t}\\hat u_\\theta(x,t)+F_x(\\hat u_\\theta(x,t))$$: 原文给出了来源，但是从字面上看不出来与前式的等价性\n        - Boundary condition residual: $$r_B[\\hat u_\\theta](z):=B(\\hat u_\\theta(z))-g(z)$$\n    3. Model Estimation by Learning Approaches\n        1. Observations about the Loss\n            - $$\\omega_F$$ accounts for the fidelity of the PDE model. Setting it to 0 trains the network without knowledge of underlying physics.\n            - In general, the number of $$\\theta$$ is more than the measurements, so regularization is needed.\n            - The number and position of residual points matter a lot.\n        2. Soft and Hard Constraints\n            - Soft: penalty terms. Bad:\n                - satisfying BC is not guaranteed\n                - assignment of the weight of BC affects learning efficiency, no theory for this.\n            - Hard: encoded into the network design. [Zhu et. al](https://doi.org/10.1007/s00466-020-01952-9)\n        3. Optimization methods\n            - minibatch sampling using the Adam algorithm\n            - increased sample size with L-BFGS (limited-memory Broyden-Fletcher-Goldfarb-Shanno)\n    4. Learning theory of PINN: roughly in DE, consistency + stability → convergence\n        1. convergence aspects: related to the number of parameters in NN\n        2. statistical learning error analysis: use *risk* to define *error*\n            - Empirical risk: $$\\hat R[u_\\theta]:=\\frac{1}{N}\\sum_{i=1}^N \\left\\|\\hat u_{\\theta}(z_i)-h_i\\right\\|^2$$\n            - Risk of using approximator: $$R[\\hat u_{\\theta}]:=\\int_{\\bar \\Omega}(\\hat u_{\\theta}(z)-u(z))^2dz$$\n            - Optimization error: the difference between the local and global minimum, is still an open question for PINN. $$E_O:=\\hat R[\\hat u_{\\theta}^*]-\\inf_{\\theta \\in \\Theta}\\hat R[u_\\theta]$$\n            - Generalization error: error when applied to unseen data. $$E_G:=\\sup_{\\theta \\in \\Theta}\\left\\|R[u_\\theta]-\\hat R[u_\\theta]\\right\\|$$\n            - Approximation error: $$E_A:=\\inf_{\\theta \\in \\Theta}R[u_\\theta]$$\n            - Global error between trained deep NN $$u^*_\\theta$$ and the correct solution is bounded: $$R[u^*_\\theta]\\le E_O+2E_G+E_A$$\n            - 有点乱，本来说 error 是误差，结果最后还是用 risk 作为误差\n        3. error analysis results for PINN\n3. Differential Problems Dealt with PINNs：读来感觉这一部分意义不大，将来遇到需要解决的问题时，回来看看之前有没有人做过就行了——另一方面看，一类方程就需要一类特殊构造的神经网络来解，那么说明神经网络解方程的通用性并不好~\n    1. Ordinary differential equations: \n        - Neural ODE as learners, a continuous representation of **ResNet**. [[Lai et al](https://doi.org/10.1016/j.jsv.2021.116196)], into 2 parts: a physics-informed term and an unknown discrepancy\n        - LSTM [[Zhang et al](https://doi.org/10.1016/j.cma.2020.113226)]\n        - [Directed graph models](https://doi.org/10.1016/j.compstruc.2020.106458) to implement ODE, and Euler RNN for numerical integration\n        - Symplectic Taylor neural networks in [Tong et al](https://doi.org/10.1016/j.jcp.2021.110325) use symplectic integrators\n    2. Partial differential equations: steady/unsteady的区别就是是否含时\n        1. steady-state PDEs\n        2. unsteady PDEs\n            1. Advection-diffusion-reaction problems\n                1. diffusion problems\n                2. advection problems\n            2. Flow problems\n                1. Navier-Stokes equations\n                2. hyperbolic equations\n            3. quantum problems\n    3. Other problems\n        1. Differential equations of fractional order\n            - automatic differentiation not applicable to fractional order → [L1 scheme](https://doi.org/10.1515/fca-2019-0086)\n            - [numerical discretization for fractional operators](https://doi.org/10.1137/18M1229845)\n            - [separate network to represent each fractional order](https://doi.org/10.1038/s43588-021-00158-0)\n        2. Uncertainty Estimation: [Bayesian](https://doi.org/10.1016/j.jcp.2020.109913)\n    4.  Solving a Differential Problem with PINN\n        - 1d non-linear Schrödinger equation\n        - dataset by simulation with MATLAB-based Chebfun open-source(?) software\n4. PINNs: Data, Applications, and Software\n    1. Data\n    2. Applications\n        1. Hemodynamics\n        2. Flows Problems\n        3. Optics and Electromagnetic Applications\n        4. Molecular Dynamics and Materials-Related Applications\n        5. Geoscience and Elastiostatic Problems\n        6. Industrial Application\n    3. Software\n        1. `DeepXDE`: initial library by one of the vanilla PINN authors\n        2. `NeuroDiffEq`: PyTorch based used at Harvard IACS\n        3. `Modulus`: previously known as Nvidia SimNet\n        4. `SciANN`: implementation of PINN as Keras wrapper\n        5. `PyDENs`: heat and wave equations\n        6. `NeuralPDE.jl`: part of SciML\n        7. `ADCME`: extending TensorFlow\n        8. `Nangs`: stopped updates, but faster than PyDENs\n        9. `TensorDiffEq`: TensorFlow for multi-worker distributed computing\n        10. `IDRLnet`: a python toolbox inspired by Nvidia SimNet\n        11. `Elvet`: coupled ODEs or PDEs, and variational problems about the minimization of a functional\n        12. Other Packages\n5. PINN Future Challenges and Directions\n    1. Overcoming Theoretical Difficulties in PINN\n    2. Improving Implementation Aspects in PINN\n    3. PINN in the SciML Framework\n    4. PINN in the AI Framework\n6. Conclusion\n"},{"slug":"logical-science-from-west","filename":"2022-08-22-logical-science-from-west.md","date":"2022-08-22","title":".doc | 也谈近代科学从西方起步","layout":"post","keywords":["doc","tex","phy","m","phi"],"excerpt":"为什么近代科学偏偏是在丢过一次古典传统的西方起步的呢？为什么那些成功继承了古典时代智慧的中古文明，比如伊斯兰文明或古中国文明，反而没有成功萌发近代科学思想呢？","content":"\n前不久在公众号转载过“海边的西塞罗”写的《**嗯！您关注的是一个早晚要“凉凉”的公众号**》，标题起得让人不知所云，但是文章内容讨论的是“近代科学为什么从西方起步”的问题，原文说：\n\n\u003e 既然你所讲述的，欧洲从古典时代到文艺复兴、科学曾经出现过一次“断层”，欧洲人是通过翻译阿拉伯人转译的古典时代文献才继承了希腊罗马先贤们的思想的。\n\u003e \n\u003e \n\u003e 那么**，为什么近代科学偏偏是在丢过一次古典传统的西方起步的呢？为什么那些成功继承了古典时代智慧的中古文明，比如伊斯兰文明或古中国文明，反而没有成功萌发近代科学思想呢？**\n\u003e \n\n作者立了一个靶子——\n\n\u003e 我之前听到的比较靠谱的解答，**是古希腊罗马有较好的数学思想，当定量的数学思想与定性的“自然哲学”发生结合，近代科学就诞生了。**\n但这种解释，其实也回答不了一个问题——你可以说古代东方离着希腊远，没有受到希腊某些思想的“药引”的启发。但特别奇怪的是，中世纪的中东却不是这样。\n伊斯兰文明的伍麦叶王朝在公元九世纪曾经掀起过一场声势浩大的“百年翻译运动”，……近代启发西方的那些古典思想典籍，阿拉伯人全有，且早获得了好几百年。\n\u003e \n\n给出的回答是所谓**“托勒密困境”**，即诸文明中的科学技术研究者因为要满足当权者/赞助者的功利性需要，将时间与精力耗费于附会科学（比如天文学）的非科学甚至伪科学（比如占星术）之上，而——\n\n\u003e 这种错误的职业拼接，锁死了天文学的进一步发展的通路，导致其无法实现向近代科学的飞跃——即便托勒密会数学、引入定量计算，也依然没用。\n\u003e \n\u003e **而这种“托勒密困境”，其实也是所有古典时代学者的困境——他们在研究学问时，必须回答“求用”的问题。**\n\u003e \n\u003e ……\n\u003e \n\u003e **于是从托勒密到哥白尼，我们会发现西方在这一轮对天文学的失而复得中，其实并没有增添什么，而是丢掉了一种东西——那就是“求用”的思维。**\n\u003e 欧洲知识分子们研究科学的正义性，来自于他们认定：自然作为一种上帝的造物，其本身就是美的。因此研究它、探索它本身，就是在赞美上帝，所以科学研究不必“求用”也有天然的正义性。\n\u003e \n\n作者写近代西方科学的不求用，是为了托物言志，检讨自己为了读者的关注不得不在历史写作之外“写时评、表达观点、带情绪”，预告自己将来可能会去写作崇高的钻研历史的题目。\n\n给蹭热点找理由这件事，我也做过嘛，感觉写的比这篇文章还简约隽永且立意高远呢～（文人相轻.jpg）\n\n但是科学革命发源于西方这个问题，我也很感兴趣，而且有自己的思考，而且思考的结果和上文不同。\n\n\u003chr class=\"slender\"\u003e\n\n学物理的孩子应该都听说过《费曼物理学讲义》的大名，没听说过的话建议听说一下，自主招生考试面试装逼的时候用的上。费曼先生在引言中也立了个靶子说——\n\n\u003e 你们可能会问，在讲述欧几里德几何时，先是陈述公理，然后作出各种各样的推论，那为什么在讲授物理学的时候不能先直截了当地列出基本规律，然后再就一切可能的情况说明定律的应用呢？\n\u003e \n\n然后上来就讲原子论，开篇问：\n\n\u003e 假如由于某种大灾难，所有的科学和知识都丢失了，只有一句话可传给下一代，那么怎样才能用最少的词汇来传达最多的信息呢？\n\u003e \n\n可惜这个问题仅仅是为了引出原子论，实在是大材小用。这说明费老先生浸淫于西方科学中，“不识庐山真面目，只缘身在此山中”。而我对近代科学起自西方的解释，正好就是这两句话串起来。下面就要兜一个大圈子，把两句话圆起来。\n\n\u003chr class=\"slender\"\u003e\n\n科学者，对世界之正确认知也。\n\n根据这个定义，把人们已知的，关于这个世界的所有知识罗列到一个集合里，这个集合就是科学。我们只谈到了一个集合，不涉及逻辑推演，也不涉及数学带来的定量优势，更不判断从事科学研究的人是否功利。\n\n但是，集合这种知识结构过于简单——\n\n- 集合里的各个元素都是平等的，要想表示出整个集合，除了全默写出来没别的办法；\n- 集合里的元素之间没有顺序，想取得其中的某一条科学命题，只能像抓阄一样，凭运气抽到为止。\n- 一旦由于天灾人祸，集合中的部分内容丢失，除了重新把当初发现它们时经历的艰难困苦重复一遍，也没有别的办法。（哦，也可以去隔壁文明的图书馆翻译。）\n\n所以，必须找到一种更复杂的结构，来组织这些信息，解决上述问题。\n\n\u003chr class=\"slender\"\u003e\n\n计算机专业有门基础课《数据结构与算法》，谈数据结构，最基础的两种就是数组和链表；谈算法，最基础的概念就是函数。注意，这里说的是数据结构，刚才说的是知识结构，两者可以类比，但并非同一概念。\n\n数组，和集合几乎一样，只不过给每个元素标记了一个序号。在计算机里，由于规定数组连续存放，每个元素占用内存长度相等，所以可以通过序号，从数组开头偏置指针，以 O(1) 的时间复杂度取得任意元素，快。\n\n类比到知识结构，语数外理化政史地生，一年级二年级三年级，第一章第二章第三章，第一第二第三个知识点，背吧。列表与列表之间井水不犯河水，你数学老师说你体育老师拉稀了不能上课，你体育老师说你数学老师放屁，两者完全可以在你的知识体系里共存。\n\n链表，和数组一样有顺序，但是并不给每个元素标号，而是在前一个元素的末尾，写上下一个元素的位置指针。找到一个元素需要从链表的开头一个一个往后捋，慢。好处是修改方便，在链表中间塞进去一个新元素，只需要把前面一个的指针指向新元素，新元素的指针指向后一个元素，删除一个旧元素也类似，只对增删点附近一个很小的区域进行改动，整个链表不会伤筋动骨。\n\n但是不论数组还是链表，都需要把所有的知识全写出来，随着时间的积累，科学的总量早晚要超越人脑的记忆力，超越笔记的厚度，对于个人，要皓首穷经，要韦编三绝，才有希望提出一点新内容；对于全人类，图书馆越造越大，一轮战乱，从头再来。\n\n于是函数登场。给定一个/一组输入，根据函数体描述的算法，返回确定的输出。那我们找到一种方法，写一个函数，接收链表的前一个元素作为输入，找到后一个元素输出。这样我们只需要存储第一个元素和这个函数，就可以恢复出整个链表，用计算换空间。\n\n\u003chr class=\"slender\"\u003e\n\n类比到知识结构，这个函数就是逻辑推演。\n\n科学内容中的每一条知识都是一个**命题**。\n\n从少数几条知识出发，这几条在逻辑上就称为**公理**，自然科学里也称之为**定律**。\n\n命题之间可以做**逻辑运算**，**或**、**且**、**非**、**蕴含**等等，运算的结果也是一条新的命题。命题的正确与否，取决于逻辑运算的规定。\n\n通过对公理和已经算出的真命题反复进行逻辑运算，产生的新的真命题，叫做**定理**。\n\n\u003chr class=\"slender\"\u003e\n\n欧几里德几何式的，也就是从有限多个命题出发，承认逻辑推演进行生成的新命题的正确性，这样的一种组织方式——\n\n- 对于学习，科学不再是一家之言，门户之见。一句话的正确性不再由说话者的身份决定，诉诸人身、诉诸权威成了谬误，“我爱吾师，但我更爱真理”一句话有了切实的落脚点。\n- 对于研究，降低了难度，后来者不必从头再来，而是站在前人的终点起跑。发现的新科学有办法整合进现有的科学，证伪的旧科学有办法剔除，而不会让科学整体伤筋动骨。愚弄黔首的矛盾和谬误，真理有办法与之势不两立。\n- 自带有容灾能力，科学得以在摧毁科学记录和科学家人身的重大灾难之后，在几百年的人才断档之后，依然有办法恢复。\n\n\u003chr class=\"slender\"\u003e\n\n刚才说数据结构和知识结构不同，知识管理界有个 DIKW 模型，也就是数据 (Data)、信息 (Information)、知识 (Knowledge)、智慧 (Wisdom)。\n\n纸张上的墨迹组成的字符只是数据，当这些单词按照语法理解为句段篇章之后才构成信息，这些篇章内容指代的概念、关系等等含义构成知识。如何理解知识与知识之间的关系需要智慧。\n\n“继承了古典时代智慧的中古文明，比如伊斯兰文明或古中国文明”——从各个文明没能演化出科学革命来看，**这些文明最多是有一部分学者继承了古典时代的知识，而没能认识到 *用逻辑组织知识* 这一智慧的价值**，而西方发掘出了这种智慧。至于这种发掘发生在西方，是偶然还是必然，由哪些条件促成，那是另一个很有趣的问题了。\n\n“我们会发现西方在这一轮对天文学的失而复得中，其实并没有增添什么，而是丢掉了一种东西——那就是‘求用’的思维。”——西方对天文学的失去，对应的是罗马统治下的和平结束时的战乱与社会崩溃，不论之后的文艺复兴如何光辉灿烂，**这种失落都是对科学乃至整个文明的威胁**，如果没有这种失落，科学革命想必会更早更容易发生。况且这种失落到复兴的整个过程中，对科学有影响的因素实在是太多了，既有正面又有负面，实在是难以分析归因。\n\n至于不求用的思维，有了逻辑推演，科学工作者的产出提高，高到了让社会愿意供养其全职研究的地步，那么不求用的思维，自然会建立起来；不求用对科研效率的提升，良性反哺科学的发展，自然会蔚然成风。反过来，**只有不求用的态度，研究者没有逻辑推演发展科学的能力，资助者没有逻辑推演评价成果的本事，不求用的态度只会鼓励灌水，产出真没用的水货。**\n\n\u003chr class=\"slender\"\u003e\n\n数学对自然科学的作用，定量化只是一个副产品。更重要的是作为逻辑科学的集大成者，发明/发现逻辑推演的规则，探索逻辑推演作为方法论的能力边界。一言以蔽之，欧几里德之后，数学已不只是“数字的学问”。\n\n至于费曼先生，他怎么可能不知道四大力学确实就是按照欧几里德式的，从基本定律出发的方式讲授的呢？面对一伙学普通物理的本科新生，说这种话实在有点骗小孩儿的嫌疑，怪不得那门课上到后来，本科生全都跑了。物理和数学的区别，在于理论和实验两条腿走路，但是理论的这条腿，实实在在地来自于超越了“数字的学问”的数学。\n"},{"slug":"qc-hackathon-write-up","filename":"2021-04-22-qc-hackathon-write-up.md","date":"2021-04-22","title":".qs | QC Hack 量子编程马拉松","layout":"post","keywords":["md","phy"],"hasMath":true,"excerpt":"4月初的时候，系秘书转发了一封邮件，耶鲁和斯坦福有两个关于量子计算的学生社团，打算举办为期一周的线上训练营,然后在周末举办一个24小时的编程马拉松","content":"\n\n## 一\n\n4月初的时候，系秘书转发了一封邮件，耶鲁和斯坦福有两个关于量子计算的学生社团，打算举办为期一周的[线上训练营](https://www.quantumcoalition.io/)，然后在周末举办一个24小时的编程马拉松 ([hackathon](https://en.wikipedia.org/wiki/Hackathon)) 的活动。只要年满18岁就可以参加，并不限定本科生。\n\n整个活动由几家从事量子计算的科技公司赞助，前面的线上训练营基本就是各家轮流上来介绍一下自己家的量子计算平台的使用方法，最后的编程马拉松也由他们每家出一套题，所以这个活动也有在学生和公司之间搭桥，给参与者争取实习机会的目的在里面。参与者可以自由组队，但是在项目提交的的时候每个人只能属于一支队伍。虽然参与者可以参加任意数量的题目，但是每一名参与者最终只能成为一家公司的优胜者。如果预感到自己在某一个项目的赢面比较大，可以在提交之前通知自己参加的其他队伍把自己除名。24小时的时间限制还是比较紧迫的，所以基本上认准一家答题就可以了。\n\n女朋友也收到了一样的邮件，所以理所当然地一起组队。我之前在本科阶段上过一门一学期的量子信息和量子通信课程，内容约等于在量子力学之后再上一个学期的习题课，以及在不讲群论的情况下应用 SU(2) 群，并没有接触过这个活动中会用到的编程语言。女朋友没有上过这门课，基本就是物理专业普通研究生的量子力学水平。周中的线上训练营，我只参加了第一天的，是 Microsoft 的 Quantum Development Kit (QDK) 和 Q# 编程语言相关的，顺便介绍了一下量子计算中很有名的 Deutsch 算法。剩下的讲座我基本上都没有参加，一方面是知道前面的规则之后就懒下来了，另一方面是实验室的工作仍然需要继续，再有就是线上活动实在是太容易摸鱼了没有效率。周五的晚上，女朋友看了一晚上我的量子信息笔记，我看了看 Q# 的语法规则，在台式机上安装了开发环境。以上就是我们参加编程马拉松之前的基础和准备。\n\n\n## 二\n\nHackathon 美东时间周六上午10点开始，周日上午10点结束。因为我们只看了 Microsoft 相关的内容，所以直奔[相关题目](https://github.com/quantumcoalition/qchack-microsoft-challenge)。\n\n题目一共分为两部分。\n\n第一部分一共四道题，就像是一般的计算机课程的作业一样，参赛者只需要在举办方写好的主程序文件里的指定区域填入代码，然后运行主办方写好的测试文件检查结果，测试通过即可得分。四道题目要求如下：\n\n1. 判断一个3-5位的2进制数能否被4整除。\n2. 判断一个3比特位当中是否至少有两位不同。\n3. 同第2题，但是要求量子比特门最多只能使用 3-比特，而且 3-比特门最多使用一次。\n4. 给定一个有两种颜色的无向图，判断图当中不含有任何单一颜色的三角形。\n\n第二部分内容比较自由，要求用 Grover's 算法解决一个自己感兴趣的问题，打哪指哪，然后写一篇文章介绍自己的这个项目，并提交相关的代码。根据问题深度(6分)、工具使用(5分)、创新性(4分)、教育价值(5分) 四方面进行评分。\n\n\n## 三\n\n### I.1.\n\n第一道题最简单，但是我们当时约等于0基础，所以做起来也颇费了一些时间。不过由于我听过第一天的课，知道 `oracle` 在 Q# 编程语言中是一个很重要的概念，所以在题目给出的参考教程 [Quantum Kantas](https://github.com/Microsoft/QuantumKatas/) 里找到了[oracle 相关的教程](https://github.com/microsoft/QuantumKatas/tree/main/tutorials/Oracles)。里面有个名为 `ControlledOnBitString` 的 function，可以根据一串量子比特的取值是否等于一个特定的二值串而对另外一个比特做一个特定的操作。前一天晚上又知道了 `Microsoft.Quantum.Convert` 的 namespace 里有各种数据类型转换的函数，搭配 `IntAsBoolArray`，就做出了第一题的初版。后来看到了更简单的 `ControlledOnInt` 函数，就直接用上了。\n\n### I.2.\n\n第二题的初版是女朋友做的。题目要求是找出是否至少两位不同，这一判断的否定就是三位比特全部相同，所以同样用 `ControlledOnBitString` 函数，然后判断一次全 `true` 一次全 `false`，再把最终结果取反就可以了。但是在做第三题的时候，因为两个题目长得太像了，中间不小心把一个能通过第二题测试但是通不过第三题测试的答案直接覆盖在了第二题上面，懒的改回去了，于是就成了最后提交的版本。\n\n### I.3.\n\n第三题和第二题非常不同。第二题的解决思路中，判断全 `true` 和全 `false`有3个控制位1个输出位，这里用了两次 4-量子比特门，所以第三问需要全新的思路。另外我曾经试过在一个 `operation` 里申请一个新的 `Qubit()` 结果测试报错，因为误解了报错信息，所以误认为除了程序的主 operation 之外不能创建新的 qubit，于是被卡住了。这时候已经来到了下午，实在想不出来又很困，于是去床上躺了一会。半睡半醒之间想到，题目虽然要求输入的量子比特不变，但是我们仍然可以直接改动输入，只要在函数结束之前把对输入的改动全部复原就可以了。于是用 CNOT 门分别作用在 1-2, 1-3 对输入的量子比特上，两个门分别以第2、3号比特为输出。然后用一个 3-bit 门判断2、3号比特是否相同，并输出到结果位上。为了复原第2、3号比特，只需要把 CNOT 在两对比特上分别再用一次就行了。\n\n但是这个结果还是无法通过测试（后来成为了第二题的提交版本），报错的提示信息是使用了超过一次 3-量子比特门——这不是开玩笑吗？于是打开了官方提供的测试文件，发现测试代码计算 3-量子比特门的使用次数的时候，会把用户定义的 3-量子比特门的数量，和 `CCNOT` 门的数量做加法，于是看文档，我们定义的那个 “用一个 3-bit 门判断2、3号比特是否相同，并输出到结果位上” 的操作和 `CCNOT` 门是等价的，于是直接换用 `CCNOT` 门，问题解决。\n\n### I.4.\n\n第四题看起来复杂，但是可以分成三个部分：\n\n1. 找出图中所有的三角形，确定每个三角形的三条边，这一步完全可以用经典算法完成；\n2. 创建一个和三角形相同数量的量子比特数列，对每个三角形，把三条边直接带入第二/三题的操作里，结果输入创建的量子比特列中；\n3. 判断量子比特列是否全为 `true`，结果输出到整个程序的结果位上。\n\n第一步由女朋友来想我来写（毕竟只有一台电脑有开发环境），难点在于：\n\n1. Q# 语法改变数列值的语法十分难受\n    \u003cbr\u003e`mutable points = [-1,-1,-1,-1,-1,-1];`\n    \u003cbr\u003e`set points w/=0..1 \u003c- [0,1];`；\n2. 作为一种强类型语言对元组和数列的区分让我这个 python 选手十分蛋疼\n    \u003cbr\u003e`(Int,Int)`/`Int[]`；\n3. 求数列中不重复的值居然不排序不能给出正确结果。\n    \u003cbr\u003e`let uniquePoints = Arrays.Unique(EqualI,Arrays.Sorted(LessThanI,points));`\n\n这也是唯一一段用上了 `Message()` 函数来 debug 的部分。\n\n第三步就重新回到了第三题暂时敷衍掉的问题：对于在操作中创建的 `Qubit()`/`Qubit[]`，`Reset()`/`ResetAll()` 函数相当于测量，会破坏操作的 adjoint 性质，不测量则（当时的我）没有办法将这个量子比特列复原。\n\n此时已经午夜，我来解决这个问题，女朋友去看第二部分，后来她看完 Grover‘s 算法的教程去睡了，我还在想这个问题。直接把报错信息复制到 Google，找到了一个[论坛里的问答]()，好像是去年微软在其他地方举办的类似活动的。里面只是提到要“uncompute the qubits”，给出的例子用的是旧版本 Q# 的语法，~~没法直接抄~~ 。最终不抱希望地把之前对那个 `Qubit[]` 做过的循环顺序倒过来重做了一遍，诶，您猜怎么着，还是没通过！绝望了！正序重做一边，诶，通过了！为什么为什么为什么？到现在也没弄清楚。\n\n### II.\n\n然后把女朋友叫醒，让她来讲一讲 Grover's 算法。听完之后我的理解是，对于一个 $$f:(0,1)^N \\rarr (0,1)$$ 的函数，这个算法可以大概率地找到一个解 $$S\\in(0,1)^N$$ 满足 $$f(S)=1$$. \n\n至于这个函数 $$f$$，之前每一道题都是这样一个函数，当时已经夜里两三点了，实在是没时间再想一个新函数了，于是我们直接就拿复杂度最高的第4题来换个皮。换个什么皮呢？为了这个活动翘掉了这周的[《文明6》联机游戏](barrier-forward-keyboard-mouse-to-another-computer)，然后之前看 YouTuber [\"PotatoMcWhiskey\"](https://www.youtube.com/user/PotatoMcWhiskey)介绍过[一个 Mod](https://steamcommunity.com/sharedfiles/filedetails/?id=1753346735\u0026searchtext=diplomacy)，里面可以将文明之间的外交关系可视化为无向图，所以，诶嘿嘿嘿……\n\n女朋友写完文稿就睡了，我把文稿改了改，然后和官方对 Grover's 算法的实现缝合了一下。提交的时候，距离截止时间大约还有一个小时。\n\n\n## 四\n\n之后的周五的时候收到了消息，我们得奖了。优胜者一共6支队伍。从活动结束之后公布的结果看，要想成为优胜，第一部分的4道题必须全部正确，然后第二部分得分在 8-20 分之间。\n\n这个成绩是个什么水平？截止到写这篇文章的此刻，官方题目的 Github 仓库有 80 份 fork，有少数几份 fork 是针对已有的 fork，有可能来自同一队伍，再考虑到可能有些队伍的不同成员分别 fork 了主项目，所以估测 60 支队伍应该是有的，官方给出 6 组优胜者这么一个不零不整的数字，个人猜测是取了前 10%？据主办方在 discord 提供的消息，有一支队伍的第二题成绩高于8分，但是前面没有全对，所以没有得奖；其余队伍的第二题都不超过6分；并不清楚有多少队伍第一题全对，主办方也不打算公布各队的详细成绩。\n\n这大约说明活动的参与者，其成绩基本上符合二八原理——少数人得到的分数，占据了所有参赛者全部得分的大多数。\n\n参加过这个活动之后，我们一下子就从量子计算小白摇身一变，成了优秀人才了？实际上，直到现在，我还是搞不太清楚 oracle 到底是个什么东西，女朋友对量子计算的理解估计比我还差（逃）。美国哲学教授约翰·希尔勒提出过一个叫做[“中文房间”](https://zh.wikipedia.org/wiki/%E4%B8%AD%E6%96%87%E6%88%BF%E9%97%B4)的思想实验，说一个只会说英语的人被关在一间满是汉字字块的房间里，不断从房间外收到写着中文问题的纸条。房间里有一本英文写成的手册，指示如何对输入的汉字进行回复。凭借这个手册，房中人可以在完全不会中文的情况下，与外界进行交流。希尔勒类比外人、房中人、手册，与程序员、计算机、计算机程序，认为房中人不会中文，进而论证计算机不可能通过程序来获得理解力。\n\n希尔勒教授想论证啥是他的事，我倒是对这个类比的本体很感兴趣——如果一个人已经能够熟练运用那个英文写成的汉字使用手册了，我们还能不能，能在多大程度上说他不懂中文呢？就说一般的程序员，工作时间能保证不看 stack overflow 的有几个，所以他们都不会编程？反对中文房间思想实验结论的人，很多都支持用图灵测试超过某一阈值来作为有智能的标志，但是我觉得，智能本身就不是一个非有即无的性质，而是一个连续分布，没有上限的谱。\n\n另一方面，得分名列前茅，和能力名列前茅，又是两回事。本科的时候做建模美赛，我们学校数理金融的一个学神前一年成绩“略有不佳”，没拿到 M 奖，于是我们那年找到了我和风神俩学物理的，准备再次冲击荣誉。巧了这一年的题目正好有一道浴缸放热水的问题，这不就是物理中的扩散方程嘛，那得奖还不是手拿把掐的？结果呢，H 奖，丢人丢到姥姥家去了。合着我们两个成绩还都不错的物理专业学生，在自己的专业里，打不过那么多同龄的非物理专业本科生？\n\n两相对照之下，我想起了很久之前看过的一篇博客文章，文章以一个问题开头——“熟练”的反义词是什么？当然说“生疏”这文章就写不下去了，作者给出的答案是——“应变”。熟练意味着，你对于问题、选项、最优解已经有了充分且完备的了解，只需要重复自己的经验就可以了，但是在自己不了解的战场上，经验至少不能直接派上用场，这时候，脱离具体环境的应变能力就成了生存和取胜的关键，我们当时的专业水平高不成低不就，反而成了掣肘我们的桎梏。\n\n读到这篇文章的时候，我被这种剑走偏锋的观点击中了，从那以后，一直都在注意培养自己的应变能力——如果明天我所研究的这个领域消失了，我还有没有谋生的能力？如果自己正在解决的问题被上帝或者 Matrix 作弊修改成一个新问题，我能不能看到连作弊都改动不了的题眼，然后一击命中？在凌晨两三点的时候，我也没有放弃解决第一题第 4 问的 Qubit 复位问题，虽然当时我并不知道评分标准，但是内心非常确定，这个问题必须解决。\n\n以上两次活动的成绩差别，也可以从得奖难度来看。建模美赛的 M 奖，得奖率应该远小于 10%，即便考虑到二八原理中绝大多数参赛者都只是凑数，而且样本越大凑数者越多，这个差距也还是无法忽略。我们能够得奖，和量子计算领域才刚刚萌芽，连“方兴未艾”都算不上，因此竞争并不激烈也有很大关系，应变能力是切入这些蓝海领域的必要条件，是躲避内卷的利器。我们现在对“内卷”人人喊打，但是培养应变能力是需要牺牲相当多本可以精进专业的时间和精力的。当社会中的大多数人向往着逃离内卷的时候，真的不需要有人咬定一个领域不断深耕？我现在的选择真的正确吗？我不知道。我是打算留在当前的领域继续熟练，还是换个领域应变，抑或是虚掷 PhD 光阴换一张工作签证？我也不知道。\n\n## 五\n\n哦对了，我有女朋友了，而且在 hackathon 的过程中把女朋友惹哭了……问题是我现在已经不记得具体是怎么把人家惹哭的了，连道歉都显得很不诚恳……我确实是一个不擅长合作的人，或者说跟别人说话的我，和想问题的我并不是同一个人，之前本科 CUPT 和建模的时候也一样，需要和人打交道的时候就几乎干不了活儿，严重的时候自己就退化成了鼓励师……总之一切错误在我，希望她不要记仇…… \u003cbr\u003e（。・＿・。）ﾉ\n"},{"slug":"lab-note","filename":"2020-09-21-lab-note.md","date":"2020-09-21","title":".tex | 整理一些关于实验记录的文章","layout":"post","keywords":["tex","phy","bio"],"excerpt":"摸鱼的方式有很多种，琢磨如何完美地进行实验记录就是个挺不错的由头。","content":"\n1. [微信公众号“BioArt植物”，原作者 Elisabeth Pain ，《实验记录到底怎么记？》](https://www.sciencemag.org/careers/2019/09/how-keep-lab-notebook)\n1. [Howard Kannare, 《Writing the Laboratory Notebook》](https://files.eric.ed.gov/fulltext/ED344734.pdf)\n1. [MIT Department of Mechanical Engineering, 《Instructions for Using Your Laboratory Notebook》](http://web.mit.edu/me-ugoffice/communication/labnotebooks.pdf)\n1. [微信公众号“生物学霸”，《颜宁：讲讲如何记实验记录》](https://xw.qq.com/partner/vivoscreen/20200820A00HZI00?vivoRcdMark=1)\n\n微信平台不允许添加指向微信之外的超链接，资源的获取方式见正文。作为报复，以上四条资源中有两条最早是在公众号里看到的，但是博文给出的链接都来自微信之外 :-)\n\n\u003chr class=\"slender\"\u003e\n\n## 0\n\n疫情依旧，摸鱼依旧。摸鱼的方式有很多种，其中比较高级的一种是打着完美主义的旗号，对着一个还没完成，或者根本不存在完成时的东西，疯狂输出时间和精力。琢磨如何完美地进行实验记录就是个挺不错的由头。\n\n说实话，学界对研究记录的要求实际上并不算严格，这一点在《Writing the Laboratory Notebook》里也有佐证。商业机构研发部门的研究记录会成为将来知识产权争端的主要证据，稍有不慎就是真金白银的经济损失，甚至关乎企业的生死存亡。而学界的工作在“科技”中偏重于”科“（即便是工程学科），在”研发“中偏重于”研“。（四者有什么联系和区别？科学认识世界，技术改造世界，研究把钱变成知识，开发把知识变成钱。）自由比起规范显然更有利于在未知领域的探索。\n\n所以，我们实验室对于实验记录并没有成文的规则，大家自己找本子自己记，格式和内容都跟随自己的喜好来，同实验室的同学也很少交流这个问题，仿佛说了就是承认自己的科研能力有问题。\n\n## 1\n\n越不谈越是心虚，于是在看到了《实验记录到底怎么记？》这篇文章之后，下决心要处理掉这个问题。这篇文章的作者访谈了几个科研人员，然后将他们的对话打碎，分到四个问题之下：\n\n- 为什么还要花时间精力去做实验记录？\n- 用传统纸质的记录本，还是电子版，还是都有？\n- 采取什么策略来保证实验记录有条理、完整并且实用？\n- 其他……\n\n并不推荐这篇文章，原因从这四个问题就能看出来：第一条属于幸存者偏见，一个觉得记录不重要的人压根不会认真记录，从而很难成为访谈对象；第二条属于典型的”有的人……有的人……“英式废话文套路；第三个问题太笼统，本应该细分为更明确的子问题；最后一个“其他”说明作者都不知道该怎么总结这些对话。明明是一篇文章，硬生生写出了微博一般的碎片感。\n\n## 2\n\n于是在网上继续找资源，机缘巧合之下，在一个知乎问题之下看到了一个还不错的答案，里面提到了 Howard Kannare 的《Writing the Laboratory Notebook》这本书。真的是“机缘巧合”，因为现在的我已经找不到原来的那个问题和答案了，哪怕专门为了这篇文章搜索了半天……这说明了网络资源的收藏和管理也是一个技术活（又可以水一篇文章了）。\n\n这本书在网上有英文的完整影印版，很容易就能搜到，实在不行的话在微信后台留言\"HowardKannare\"可以收到下载网址（注意回复的关键词没有空格）。\n\n本书各章的标题如下：\n\n1. The Reasons for Note keeping - An Overview\n2. The Hardware of Note keeping - Books, Pens, and Paper\n3. Legal and Ethical Aspects - Ownership, Rights, and Obligations\n4. Management of Notekeeping - Practices for Issuance, Use, and Storage of Notebooks\n5. Organizing and Writing the Notebook - Be Flexible\n6. Examples of Notebook Entries\n7. Patents and Invention Protection\n8. The Electronic Notebook\n9. Appendix A: Some Suggestions for Teaching Laboratory Notekeeping\n10. Appendix B: Photographs from the Historical Laboratory Notebooks of Famous Scientists\n\n书很长，有 150 多页，这导致内容涉及方方面面，包括了那些我们可能不是那么急需的方面；还有很多我们今天可能并不十分需要的冷知识，比如几十年前美国出产的纸张由于某种工艺导致保存期限比较短等等。好在多数章节最后都有总结，可以帮人省下不少时间。\n\n另一方面，这本书出版于1985年，那是一个什么年代呢？苹果在前一年才刚刚推出了 Macintosh 电脑，C++ 在当年才刚刚出版。所以对于电子实验记录，书中只在第8章和纸本笔记进行了一个简单的对比，而且有比较明显的时代局限性。\n\n总之，如果真要读这本书的话，抱着练习英语阅读的目标，要远比学记笔记要少些失望。\n\n## 3\n\n干了半天之后开始怀疑这件事从一开始是否有必要，这可是 PhD 的保留节目了。\n\n尤其是在读过《Writing the Laboratory Notebook》的第5章之后，读到单篇实验报告应该包括 introduction, experimental plan, observations and data, discussion of results 的时候，恍然发现，这不就是本科实验课要交的实验记录的写法吗，之所以没有老师教过我们怎么记实验记录，是不是因为他们觉得这个事情已经教过了？\n\n既然如此，那么第三份材料就是 MIT 机械系给本科生的实验报告要求，不长，只有 6 页，还包括了超过两页的模板笔记，可以当作《Writing the Laboratory Notebook》关于笔记内容部分的精华集锦来看。微信后台回复 \"MITlab\" 可以收到 PDF 下载地址。\n\n但这反而说明了，PI们散养研究生，不对实验记录进行更进一步的要求和培训是不对的。因为“实验记录本”≠“实验记录们”，研究生的工作不同于本科实验课，本科生做实验就只有在固定时间和固定时长的实验课上，一切超出规定范围内的动作大概率都是无用功甚至错误，每个实验要做什么，有哪些步骤，会观察到什么现象，都是事先设计好的，实验记录的各个部分会有哪些内容，大体上没跑。研究生的工作则不然，大到整个博士期间的所有工作都可以算作是一个项目（毕竟会写成一篇毕业论文），小到显微镜从开机到观测到关机的几个小时也可以整出一篇报告来，如何划分研究的基本单元？一个人可能同时在做相对独立的多项工作，是连续记录还是分开平行记录？以纸本为主，电子版主要用于备份，还是主要用电子设备，随手记在纸上的拍照作为附件？\n\n## 4\n\n对于这篇文章没有太多可说的，覆盖范围和《实验记录到底怎么记？》类似，感觉就相当于颜宁女士自己一个人对那篇文章中的问题的回答，由于是一个人的回答，所以不会有前一篇文章中不同观点混在一起的分裂感。看过了《Writing the Laboratory Notebook》之后，会发现文中的每一个点都在书中可以找到。\n"}]],["m",[{"slug":"bayesian-equation-and-view-of-world","filename":"2024-09-27-bayesian-equation-and-view-of-world.md","date":"2024-09-27","title":".m | Bayesian 贝叶斯，从公式到世界观","layout":"post","keywords":["tex","m","phy","phi"],"excerpt":"我们老板真是太能吹了，Bro 居然跟隔壁真的在研究物理的课题组 brag abou 我会贝叶斯参数估计，yo know wat ur sayin? 赶紧来补课～","content":"\n\n## 公式\n\n我上学的时候，贝叶斯公式是概率论里面，少数高中完全不涉及，到了本科才第一次见的公式，所以我从来没背下来过。不过也用不着背，根据条件概率里面的一个平凡结果：\n\n$$\n\\Pr(A|B)\\ \\Pr(B) = \\Pr(B|A)\\ \\Pr(A)\n$$\n\n可以得到 $$\\Pr(A|B)$$ 和 $$\\Pr(B|A)$$ 之间的关系\n\n$$\n\\Pr(A|B) = \\frac{\\Pr(B|A)\\ \\Pr(A)}{\\Pr(B)}\n$$\n\n这就是贝叶斯公式本体。\n\n分母没什么意思，所以一般我们要用全概率公式替换，也就是把 $$A$$ 划分为全覆盖但是不相交的 $$\\{A_i | \\ A_i \\cap A_{j \\neq i}=\\varnothing,\\ \\bigcup_i A_i=A\\}$$\n\n$$\n\\Pr(A|B) = \\frac{\\Pr(B|A)\\ \\Pr(A)}{\\sum_i \\Pr(B|A_i) \\Pr(A_i)}\n$$\n\n其中任意一个子事件 $$A_j$$\n\n$$\n\\Pr(A_j|B) = \\frac{\\Pr(B|A_j)\\ \\Pr(A_j)}{\\sum_i \\Pr(B|A_i) \\Pr(A_i)}\n$$\n\n### 根据实验结果筛选理论模型\n\n以上是数学。在科学中，令\n\n- A 为一族理论模型的一组参数取值，记为 $$Param_k$$，下标可任意选取。\n- B 为实验观测数据，记为 *Ob*\n\n$$\n\\Pr(Param_j|Ob) = \\frac{\\Pr(Ob|Param_j)\\ \\Pr(Param_j)}{\\sum_i \\Pr(Ob|Param_i) \\Pr(Param_i)}\n$$\n\n其中 \n\n- $$\\Pr(Param_j)$$ 表示第 j 组参数是模型的正确参数的，未经实验验证，根据零假设计算的 **先验 (prior) 概率；**\n- $$\\Pr(Param_j|Ob)$$ 叫做经过实验观测修正之后的，第 j 组参数正确的 **后验 (posterior) 概率**。\n- $$\\Pr(Ob|Param_j)$$ 在之前的文章中讲过，是当前测量数据下，模型参数的 **似然性 (likelihood)**。\n\n$$\nposterior \\propto likelihood \\cdot prior\n$$\n\n### 举个例子\n\n隔壁组的问题可以简化为下图：\n\n![](/photos/2024-09-27-two-gaussian.png)\n\n- 有两组数据 (x, y1), (x, y2) 可以用同一族函数来拟合。（假设为两个高斯函数的叠加，$$y=f_{A_1,A_2,\\mu_1,\\mu_2,\\sigma_1,\\sigma_2}(x) = A_1e^{-\\frac{(x-\\mu_1)^2}{\\sigma_1^2}} + A_2e^{-\\frac{(x-\\mu_2)^2}{\\sigma_2^2}}$$\n- 两组数据的误差不同。（红色数据点显然比蓝色数据点，相对于理论值偏离得更远一些）\n- 问有没有一个数值，可以衡量每组数据的误差程度。\n\n我给他们的建议是\n\n- 根据自己的专业知识指定先验概率 $$\\Pr(param_j)=\\Pr(A_1,A_2,\\mu_1,\\mu_2,\\sigma_1,\\sigma_2)$$。比如选定一个参数空间的范围，范围之外概率为零，范围之内均匀分布。\n    - $$A_1,A_2 \\in \\left[\\min(\\{Y_1\\}\\cup \\{Y_2\\}),\\max(\\{Y_1\\}\\cup \\{Y_2\\}\\right]$$\n    - $$\\mu_1\\in[\\min\\{X\\},\\max\\{X\\}],\\ \\mu_2\\in[\\mu_1,\\max\\{X\\}]$$\n    - $$\\sigma_1,\\sigma_2\\in[0,\\ \\Sigma_i\\sqrt{|X_i-\\bar X|^2/N}]$$\n- 根据一些假设和统计规律计算 $$\\Pr(Ob|Param_j)$$\n    - 假设误差与 x 变量无关，服从期望为 0 的高斯分布，$$[y_i-f(x_i)]\\sim N(0,\\sigma^2)$$，标准差根据各数据点减去模型预测值的残差估计。\n    - 假设每个数据点的观测相互独立，$$\\Pr(Ob)=\\Pr(\\bigcap_i Ob_i)=\\prod_i\\Pr(Ob_i)$$\n    - 对于模型的每一组参数 ，$$\\Pr(Ob_i|param_j)$$ 取上述高斯分布的绝对值大于残差绝对值的部分，就是钟形曲线两侧尾巴的线下面积。\n- 对参数空间中的每一组值都算出一个后验概率之后，计算整个空间的信息熵（方法见之前的文章）。误差较大的一组数据，应当有更多组参数可以获得类似的拟合结果，从而信息熵更大。\n\n## 世界观\n\n对于概率，有三种理解：\n- 古典的 (classical)、\n- 频率学派的 (Frequentist)、\n- 贝叶斯的 (Bayesian).\n\n### 古典\n\n就是将古典概型推广，成为一种关于可能性的普遍观点——一个随机空间里的随机事件可以分解成若干子事件，子事件还可以再分，直到每个基本事件的概率相等，都等于基本事件总数的倒数，而要计算人们感兴趣的某一事件，只需要数出其包含的基本事件的数量就行了。\n\n让人联想到古希腊古典时代的原子论。时人认为物质世界也不是无限可分的，将任意一种材料打碎研磨，这一过程最终会有一个终点，最终的产物就是这种物质的“原子”。一块材料的大小，就是其所含原子数量的多少。\n\n有人批评这种观点用可能性去定义可能性，有循环论证谬误之嫌。但是看现代化了的概率论，概率被定义成了满足某些条件的函数，公理化是公理化了，逻辑链条是有了坚实的起点，但是那里的概率还能不能被当作可能性的度量，实在是不好说。\n\n有人批评这是机械唯物主义，这种人批判的武器一般是武器的批判，别争辩，先活下来再说。\n\n### 频率学派\n\n这种观点一言以蔽之：概率是频率在样本量趋于无穷时的极限。\n\n科学中（日常生活中也一样，只是人们通常没这么精确），测量误差不可避免，我们每一次的测量哪怕正确，互相之间也会有细微的差别，更不用说和待测的真实值不同了。\n\n解决方法在初中物理实验里学过：多次测量，把平均值当作真值（的估计量），根据标准差计算误差（置信区间、p 值等等……）。\n\n不同的人（假设有 M 个）可以对同一个可观测量进行 N 次测量，对于一个确定的 N，不论这个可观测量本身服从何种概率分布，这 N 个测量值的平均数 $$\\bar X_N$$ 都服从正态分布，这就是中心极限定理（注意不是大数定律）。\n\n当可观测量本身也服从正态分布的时候，就会导致标准差 (standard deviation) 和标准偏误 (standard error of the mean, 常简称为 standard error) 容易让初学者混淆。\n\n而按照这种世界观，所谓一个物理量的真值，就是所有可能的（所有已经发生过的+思想实验中可能发生的）测量的均值 $$\\bar X_\\infin$$。\n\n因为包括可能发生还未发生的测量，所以哪怕我们面对的问题是纯决定论的，客观存在一个确定的真值，无论我们已经进行过多少次测量，都无法保证得到真值。\n\n有人批评这是客观唯心主义，这种人批判的武器一般是武器的批判，别争辩，先活下来再说。\n\n### 贝叶斯\n\n前述世界观好歹还认为真值客观存在——\n\n贝叶斯世界观则直接不再对真值的客观存在下断言，不论先验还是后验，科学理论里的每一条命题，都不再孤单，而是要和所有可能的替代理论打包在一起；也不再“正确”，而是具有一个以概率衡量的可信程度。\n\n实验的作用不再是判断对错，而是在有限的先验知识（现存的科学理论）下，判断新取得的实验结果在多大程度上，更新了旧知识里每条命题的可信权重。\n\n而且每个人掌握的知识不同，先验概率不同，在同样的实验数据面前，所更新出来的知识体系也会不同。\n\n再者，如果先验概率为 0，任你实验数据如何显著，后验概率也一定为 0，所以对“未知的未知”无能为力。实践中，再离谱的先验假设，只要能想到，也要赋一个小而不为 0 的初值。\n\n有人批评这是主观唯心主义，这种人批判的武器一般是武器的批判，别争辩，先活下来再说。\n\n## 送分题\n\n已知本省不超过二十个地级行政单位。一中是本市最好的高中，本科过线人数年年创新高。\n\n已知本市报纸会公布喜报，上有全市前若干名学生的姓名、分数、录取学校等信息。省招办有根据成绩取得全省排名的服务。比如某年本市第十名，全省排名两千名开外。\n\n你能否据此评价母校和家乡的教学质量，以及本省各地区之间教育水平的平均程度？\n\n你该如何评价，从定义原假设和备择假设，到用何种概率分布对先验概率建模？\n\n你有资格评价吗？"},{"slug":"information-entropy-kl-divergence-cross-entropy-mutual-information","filename":"2024-05-14-information-entropy-kl-divergence-cross-entropy-mutual-information.md","date":"2024-05-14","title":".tex | 比较两个概率分布/两条信息","layout":"post","keywords":["tex","phy","m"],"excerpt":"自信息、信息熵、KL Divergence、交叉熵、互信息","hasMath":true,"content":"\n\u003e 自鸣得意了半天，发现这篇文章基本就是维基百科 [Quantities of Information](https://en.wikipedia.org/wiki/Quantities_of_information) 词条英文版的翻译。但是对应的中文词条没有覆盖英文版那么多的内容，所以也不完全是无用功。\n\u003e \n\n## 信息和概率\n\n一条信息由一个命题来表达。（这一个命题可以是对多个命题进行逻辑演算的一个表达式。）\n\n而这个命题解答了人心中的某个疑问。既然这是个疑问，那么在得到确切的信息之前，有众多其他命题，和这条消息一样有可能是问题的答案。既然是有可能，那就是概率论可以派上用场的对方。所有这些可能成为答案的命题一起，构成一个随机变量空间。\n\n比如说一道有 ABCD 四个选项的选择题，如果是单选题，那么答案的随机变量空间就是 {A, B, C, D}，如果是多选题，则是 {A, B, C, D, AB, AC, AD, BC, BD, CD, ABC, ABD, ACD, BCD, ABCD}，如果是排序题、不定项排序题、答案出错了的题……\n\n## 描述一个概率分布的信息量\n\n### 自信息：Self Information\n\n自信息是一个随机事件的性质，也就是针对一个随机变量的**某一个可能取值**而言的。表达式为 \n\n$$\nI(m) = -\\log_n\\left(p(M=m)\\right)\n$$\n\n这是一个无量纲量，但是公式中指数的底数可以任意选择——\n\n- *n* = 2 的时候自信息的单位是 bit，也叫香农 (shannon), 这里的 bit 和二进制位 bit 不完全相同，一个香农是一个二进制位所能表示信息的**上限**：当一个二进制位完全取决于其它位时，这个位不包含任何额外信息，香农数为 0，但这个二进制位依然物理上存在；\n- *n* = *e* 的时候单位是 nat, 因为 $$\\log_e\\equiv\\ln$$ 叫做自然对数；\n- *n* = 10 的时候单位叫 hartley\n\n——单位之间的换算关系由对数的换底公式给出。\n\n这个量在信息论中的意义是，这条消息作为一个不方便问的问题的**答案**，**最少可以**用多少个 n 个选项的单选题套出答案。当 n=2 的时候，每个问题就是一个是非题，也就是一般疑问句。\n\n码农面试的时候经常问一类问题：一堆看起来相同的东西里面有一个不一样，你有一种不能直接测出答案的测量工具，最少需要测量几次才能辨别出来……但是自信息的计算不能提供具体的辨别方法，具体方法还是需要你自己去凑，而面试刷人很多都是在刷这种细枝末节。\n\n当然了，前提是你的面试官懂他自己在问什么，而不是相信美剧《硅谷》里压缩算法可以突破信息论极限的计算机民科～\n\n当 *p* = 0 时，自信息发散为无穷大。不过问题不大，原因在下一节。\n\n### 信息熵：Entropy\n\n信息熵是一个随机变量的概率分布的整体性质。\n\n算法很简单，就是自信息的概率期望，也就是按照随机变量每个取值的概率加权平均：\n\n$$\nS(p(M))=\\mathbb{E}_p[-\\log_n p(M)]=-\\sum_{m\\in M}p(m)\\log_n p(m)\n$$\n\n当 *p* = 0 时，自信息发散，但是概率为零，强行定义两者的积也为零，对信息熵不构成贡献。\n\n当我们只对某一特定的随机事件信息感兴趣，除此以外的所有事件合并为目标事件的补集，就得到二项信息熵 binary entropy:\n\n$$\nS_{binary} = -(1-p)\\log(1-p)-p\\log p = p\\log\\frac{1-p}{p}-\\log(1-p)\n$$\n\n沿着自信息的意义往下走，信息熵在信息论中的意义是，一个将众多信息/命题的集合作为备选答案的**问题**，**最少可以**用多少道 n 个选项的单选题的集合来等价替代。\n\n当这些最优的单选题确定之后，原问题的每一个选项，可以用单选题的答案序号来进行编码。指数的不同底数/信息量的不同单位就是数字的 n 进制，信息量就是相应进制下最大压缩编码后的位数。\n\n当然要讨论压缩的话，还需要另找地方记录各个单选题和选项，也就是压缩字典。\n\n## 比较两个概率分布的信息量\n\n而如何选择单选题，使之成为针对给定问题最优的问题集，会因为各个选项概率分布的不同而变化。即便是同一组信息/备选答案，两套不同的概率分布，各自会给出一套对自己最优的问题集，一套概率分布下的最优问题集不见得是另外一套概率分布下的最优问题集。\n\n\u003e 下面的表达式都只写出了离散变量的形式，连续随机变量需要将求和写成对应的积分。\n\u003e \n\n### 相对熵：Kullback–Leibler (K-L) Divergence\n\n英文里也叫 relative entropy 或者 I-divergence\n\n这里的两个概率分布映射自**同一个**随机变量空间。\n\n$$\nD_{KL}(p(X)|q(X))=\\sum_{x\\in X}p(x)\\log\\frac{p(x)}{q(x)}=-\\sum_{x\\in X}p(x)\\log\\frac{q(x)}{p(x)}\n$$\n\n这个量描述了当 *p*(*X*) 作为各选项的正确概率分布的情况下，用对 *q*(*X*) 最优的单选题去提问，**没问出来的信息**所需要的**额外的**单选题数目/编码数。\n\n在科学应用中，*p*(*X*) 一般是从实验中测量出来的概率分布，*q*(*X*) 是理论模型的预测。\n\n下面的例子计算了一个单选题，选 C、选 B、假想中一群学生的答案统计、胡猜四种概率分布 *p, q ,r , φ* 之间的 KL divergence。因为概率为零会出现发散问题，所以我们取 eps = 10^(-10) 把这些概率值截断：\n\n```python\nimport numpy as np\n\ndef kl_div(p,q,eps=1e-10):\n    p = np.clip(p,eps,1-eps)\n    q = np.clip(q,eps,1-eps)\n    return np.sum(p*np.log2(p/q))\n\np   = np.array([  0,   0,   1,   0])\nq   = np.array([  0,   1,   0,   0])\nr   = np.array([1/6, 1/6, 1/2, 1/6])\nphi = np.array([1/4, 1/4, 1/4, 1/4])\n\nresults = np.empty((4,4))\nfor i,v1 in enumerate([p,q,r,phi]):\n    for j,v2 in enumerate([p,q,r,phi]):\n        results[i,j] = kl_div(v1,v2)\n```\n\n| KL-div(行, 列)/bit | p | q | r | φ |\n| --- | --- | --- | --- | --- |\n| p = [0,0,1,0] | 0 | 33.219 | 1 | 2 |\n| q = [0,1,0,0] | 33.219 | 0 | 2.585 | 2 |\n| r = [1/6, 1/6, 1/2, 1/6] | 14.817 | 25.890 | 0 | 0.208 |\n| φ = [1/4,1/4,1/4,1/4] | 22.914 | 22.914 | 0.189 | 0 |\n\n从结果中我们可以看到：\n\n- 对角线为 0，符合其意义。\n- $$D_{KL}(p,q)$$ 和 $$D_{KL}(q,p)$$ 都应该是 +∞，这里的有限值是 eps 截断的结果\n- 除个别巧合，对称位置的值一般不相等。这个量不同于两点之间的距离。\n\n### 交叉熵：Cross Entropy\n\n这里的两个概率分布映射自**同一个**随机变量空间 X。\n\n概率分布 ***q* 相对于 *p*** 的交叉熵 cross entropy\n\n$$\nCE(p(X),q(X))=-\\sum_{x\\in X}p(x)\\log q(x)=S(p(X))+D_{KL}(p(X)|q(X))\n$$\n\n这个量描述了当 *p*(*X*) 作为各选项的正确概率分布的情况下，用对 *q*(*X*) 最优的单选题去提问，所需要的**总共的**单选题数目/编码数。\n\n类似于二项熵，*p* 和 *q* 之间的 binary cross entropy:\n\n$$\nBCE(p,q)=-p\\log q-(1-p)\\log(1-q)=p\\log\\frac{1-q}{q}-\\log(1-q)\n$$\n\n```python\ndef cross_entropy(p,q,eps=1e-10):\n    p = np.clip(p,eps,1-eps)\n    q = np.clip(q,eps,1-eps)\n    return -np.sum(p*np.log2(q))\n\nresults = np.empty((4,4))\nfor i,v1 in enumerate([p,q,r,phi]):\n    for j,v2 in enumerate([p,q,r,phi]):\n        results[i,j] = cross_entropy(v1,v2)\n```\n\n| Cross Entropy(行, 列)/bit | p      | q      | r   |   $$\\varphi$$   |\n| ---                      | ---    | ---    | ---   | --- |\n| p = [0,0,1,0]            | 0      | 33.219 | 1     | 2   |\n| q = [0,1,0,0]            | 33.219 | 0      | 2.585 |  2  |\n| r = [1/6, 1/6, 1/2, 1/6] | 16.610 | 27.683 | 1.792 | 2   |\n| $$\\varphi$$ = [1/4,1/4,1/4,1/4]    | 24.914 | 24.914 | 2.189 | 2   |\n\n- 对角线上不一定为零，而是自己的信息熵\n- 其他位置和 KL divergence 相差大约为第一个输入分布的信息熵，误差 eps 的截断\n\n### 互信息：Mutual Information\n\n这里的两个概率分布一般来说映射自**不同的**随机变量空间。\n\n$$\nMI(X,Y)=\\sum_{x,y}p(x,y)\\log\\frac{p(x,y)}{p(x)p(y)}=D_{KL}\\left(p(X,Y)|p(X)p(Y)\\right)\n$$\n\n从后一个等号可以看出，这一性质衡量的是 *X, Y* 两个随机变量的联合分布在多大程度上不同于“*X* 和 *Y* 相互独立”的零假设。两个随机变量相互独立时，互相不反映对方的信息，互信息 *MI* = 0。\n\n当从 *X* 所在的随机变量空间取样的难度比较大的时候，我们需要用容易取样的**另一个变量空间**的随机变量 *Y* 来推测 *X* 的情况，互信息就可以用来论证我们这种选择的合理性。\n\n## 扯点闲篇\n\n### PyTorch 中以此为基础的 loss functions\n\n`torch.nn` 中有如下几个和今天的文章相关的 loss functions：\n\n- `torch.[nn.KLDivLoss](https://pytorch.org/docs/stable/generated/torch.nn.KLDivLoss.html#torch.nn.KLDivLoss)`\n- `torch.[nn.CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss)`\n- `torch.[nn.BCELoss](https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html#torch.nn.BCELoss)`\n- `torch.[nn.BCEWithLogitsLoss](https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html#torch.nn.BCEWithLogitsLoss)`\n\n之所以没直接用这些函数计算上面的例子，是因为 `KLDivLoss` 是按元素计算，随后需要自己求和；`CrossEntropyLoss` 又是按类别的，还不需要归一化，而且文档的解释很复杂，我到现在也没看明白；而且还要注意这些函数的设计输入是不是 logit，这是机器学习里的概念，在此不展开了。\n\n### 玻尔兹曼的墓志铭\n\n$$\nS=k\\log W\n$$\n\n其中 *S* 是（微正则系综中的）热力学熵，*k* 是玻尔兹曼常数 $$k_B$$，*W* 是因为刻碑的师傅不会写 *Ω*。\n\nW 或 Ω 是处于相同能量的热力学状态的数量。因为你都需要统计物理了，显然是只知道能量，没办法知道所考虑的微观粒子究竟处于哪一个热力学状态。那此时的零假设就是处于所有状态的可能性相等，*p* = 1/Ω，信息熵 \n\n$$\nS =-\\sum_{m\\in M}p(m)\\log_n p(m)= -\\Omega\\cdot(\\frac{1}{\\Omega}\\log\\frac{1}{\\Omega})=\\log\\Omega\n$$\n\n和热力学熵只相差一个玻尔兹曼常数。这是因为信息熵是无量纲的，熵和温度的量纲相乘之后需要得到能量的量纲，只能由 $$k_B$$ 把量纲凑齐，而数值是自由能相关的实验里测出来的。\n\n好像这就是高中物理里熵的定义式是吧。\n\n上了大学以后，正则系综和巨正则系综中的熵也分别就是各自体系中各状态的概率分布的信息熵，乘上玻尔兹曼常数。~~（我也忘得差不多了，试图萌混过关）~~\n\n### 善卜者无先见之明\n\n公元 451 年，阿提拉 Attila 率领匈人攻入罗马领土，横扫有大量其他民族居住的高卢地区。西罗马帝国将军艾提乌斯 Aetius 联络了众多畏惧匈人的民族组成联军，其中包括西哥特人的王狄奥多里克 Theodoric，两军会战于卡塔隆 Catalaunian 平原。\n\n本来想用这个故事举例子来着，因为我记得阿提拉在战前找了个大师算了一卦，说是一位国王将战死，一个国家将崩塌。于是阿提拉很高兴，以为哥特人和狄奥多里克要玩完了，结果战斗打响，狄奥多里克确实死于乱军，但是罗马和哥特等族的联军击败了匈人，阿提拉的霸业雨打风吹去。\n\n于是试图说明算命的魅力就在于，用文字游戏表达一个自信息比较低的命题，同时误导对方相信一个自信息高得多的命题，在心理疏导之外，赚一个信息熵的差价。\n\n结果查证的时候发现好像不是这么回事，Barbarian Rising 故事片里的预言内容不一样；维基百科上没给出处，说算命的很准，于是阿提拉推迟到下午作战，方便晚上跑路；其他地方甚至压根没有算命的情节。但是写都写了，需要积累高考作文素材的小朋友们还是可以假装被我误导了~\n\n当然了，算命这个事还有一种情况，就是打着不确定的幌子，售卖确定但不方便承认自己确定的信息，那就是另一种生意，和另外的价格了~"},{"slug":"equivlance-between-diffusion-equation-and-random-walk","filename":"2024-04-25-equivlance-between-diffusion-equation-and-random-walk.md","date":"2024-04-25","title":".tex | 扩散方程和随机游走的等价","layout":"post","keywords":["tex","phy","m"],"excerpt":"之前 MCMC 讲错了","hasMath":true,"content":"\n\u003e 这些内容总结自美国研究生级别的《数学物理方法》两次课的笔记，大约两个小时。\n\u003cbr\u003e如果是中国大学本科的话，认真的老师半个小时庶几可以讲完;\n\u003cbr\u003e念 PPT 就算上课的话 15 分钟可以讲完，附赠一个段子;\n\u003cbr\u003e翻转课堂的话也就布置个作业，老师一句话可以讲完。\n\u003cbr\u003e以上数据除第一句外纯属揣测，没有黑任何人的意思，love and peace~\n\u003e \n\n### 扩散方程\n\n带有初值条件的扩散方程表述如下：\n\n$$\n\\begin{cases}\nu(x,t=0)=f(x) \\\\\n\\partial u(x,t)/\\partial t=\\sigma \\cdot \\partial^2 u(x,t)/ \\partial x^2\n\\end{cases}\n$$\n\n方程的解为：\n\n$$\nu(x,t) = \\frac{1}{\\sqrt{4\\pi \\sigma t}} \\int_{-\\infty}^{+\\infty} f(s)\\ e^{-\\frac{(x-s)^2}{4\\sigma t}}\\ ds\n$$\n\n解法是将 u(x,t) 对空间变量 x 作傅里叶变换为 U(k,t)，利用傅里叶变换的性质，变换后的方程将是关于时间 t 的一阶常微分方程。求解后作傅里叶逆变换 ~~即为上式。~~ ~~（完蛋，好久没做题了，那个 $$e^{-\\frac{(x-s)^2}{4\\sigma t}}$$ 是怎么凑出来的，为什么我直接给消掉了啊）~~ 凑出来了凑出来了，初值条件代入频域 k 空间里的通解来确定积分常数，可以看到结果 $$F(k) e^{-\\sigma t k^2}$$ 是两项之积，所以根据傅里叶变换的卷积定理，实空间 x 里的解是 f(x) 和 $$\\mathscr{F}_{k\\rightarrow x}^{-1}\\{e^{-\\sigma t k^2}\\}$$ 的卷积（所以上式的指数项以 (x-s) 为宗量），而计算后者的时候需要用到高斯积分～\n\n### 随机游走\n\n随机游走是一个离散过程，为了和连续时空中的扩散方程相对比，将空间变量 x 离散化为相隔 Δ 的格点 i，时间变量 t 离散化为相隔 δ 的 n。\n\n当一个粒子在 n 时刻位于格点 i 时，在下一个时刻 n+1, 它有 1/2 的概率移动到 i-1, 1/2 的概率移动到 i+1.\n\n所以，虽然每个进行随机游走的粒子在任意时刻都只有确定且唯一的位置，但是对于大量同样初始位置和运动规律的例子，n 时刻出现在 i 格点的概率 P(i,n) 有以下关系：\n\n$$\n\\begin{cases}\nP(i,0)= f_i \\\\\nP(i,n)=\\frac{1}{2}\\left[P(i-1,n-1)+P(i+1,n-1)\\right]\n\\end{cases}\n$$\n\n在初值条件为 $$f_i=\\delta_{i=0}$$ 时，递推结果如下：\n\n|  x 轴 — | i = -4 | i = -3 | i = -2 | i = -1 | O | i = 1 | i = 2 | i = 3 | i = 4 | → |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n| n = 0  | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 |  |\n| n = 1  | 0 | 0 | 0 | 1/2 | 0 | 1/2 | 0 | 0 | 0 |  |\n| n = 2  | 0 | 0 | 1/4 | 0 | 1/2 | 0 | 1/4 | 0 | 0 |  |\n| n = 3  | 0 | 1/8 | 0 | 3/8 | 0 | 3/8 | 0 | 1/8 | 0 |  |\n|**t 轴 ↓**|  |  |  |  |  |  |  |  |  |\n\n![](/photos/2024-04-25-random-walk-probabilities.png)\n\n离散的情况，很难对任意的初值条件写出解的表达式，但是对于上面的特殊情况，课上不加证明地给出了（可能是根据上图凑出来的）下面的解：\n\n$$\nP(i,n)=\\frac{1}{2^n}\\frac{n!}{\\left(\\frac{n+i}{2}\\right)!\\left(\\frac{n-i}{2}\\right)!}\n$$\n\n对的，以上关系只能表示 (n+i = 偶数) 的情况，但是康托尔告诉了我们，所有偶数和所有自然数的“数量”一样多，所以也没差太多～\n\n### 方程的等价\n\n概率的递推公式可以变换为：\n\n$$\n\\frac{1}{\\delta}\\left[P(i,n)-P(i,n-1)\\right]=\\frac{1}{2}\\left [\\frac{P(i-1,n-1)-2P(i,n-1)+P(i+1,n-1)}{\\Delta^2}\\right]\\frac{\\Delta^2}{\\delta}\n$$\n\n因为 $$x=i\\Delta,\\ t=n\\delta$$, 对两个变量的微分可以离散化成差分：\n\n$$\n\\frac{\\partial}{\\partial x}\\rightarrow \\frac{1}{\\Delta}\\left[()_i-()_{i-1}\\right],\\ \\frac{\\partial}{\\partial t}\\rightarrow \\frac{1}{\\delta}\\left[()_n-()_{n-1}\\right]\n$$\n\n直接就能看出扩散方程和随机游走的等价，且系数之间存在关系：$$\\sigma = \\frac{\\Delta^2}{2\\delta}$$\n\n### 解的等价\n\n只讨论一个 δ(x) 函数作为初值条件的情况，我们要证明此时扩散方程的解：\n\n$$\nu(x,t) = \\frac{1}{\\sqrt{4\\pi \\sigma t}} e^{-\\frac{x^2}{4\\sigma t}}\\ \\xleftarrow[{\\Delta,\\delta \\rightarrow 0;\\ i,n\\rightarrow \\infty}]{x=i\\Delta,\\ t=n\\delta} \\frac{1}{2^n}\\frac{n!}{\\left(\\frac{n+i}{2}\\right)!\\left(\\frac{n-i}{2}\\right)!} \\frac{1}{2\\Delta}\n$$\n\n只需讨论这一个情况，因为 δ(x-s) 函数可以看作将一个函数 f(x) 在自变量 x=s 时切片为 f(s)，而任何一个（性质比较“优美”的）函数都可以看作把它自己在定义域上的所有点切片后再重新叠加起来：\n\n$$\nf(x) = \\int_{-\\infty}^{+\\infty}f(s)\\delta(x-s)\\ ds\n$$\n\n过程需要用到 Sterling 公式对阶乘的近似：$$n! \\approx \\sqrt{2\\pi n}\\ n^n e^{-n}$$\n\n$$\n\\begin{array}{rcl}\n\\frac{P(i,n)}{2\\Delta} \u0026 \\approx \u0026 \\frac{1}{2\\Delta} \\frac{1}{2^n} \\frac{\\sqrt{2\\pi n}\\ n^n e^{-n}}{\\sqrt{\\frac{2\\pi (n-i)}{2}}\\ \\left(\\frac{n-i}{2}\\right)^{\\frac{n+i}{2}} e^{-\\frac{n+i}{2}}\\sqrt{\\frac{2\\pi (n+i)}{2}}\\ \\left(\\frac{n+i}{2}\\right)^{\\frac{n+i}{2}} e^{-\\frac{n+i}{2}}} \\\\\n\u0026 = \u0026 \\frac{1}{2\\Delta}\\frac{\\sqrt{2n}}{\\sqrt{\\pi(n^2-i^2)}}\\frac{n^n}{(n-i)^{\\frac{n}{2}-\\frac{i}{2}}(n+i)^{\\frac{n}{2}+\\frac{i}{2}}} \\\\\n\u0026 = \u0026 \\frac{1}{2\\Delta}\\frac{\\sqrt{2n}}{\\sqrt{\\pi(n^2-i^2)}}\\frac{n^n/n^n}{(n-i)^{\\frac{n}{2}}(n+i)^{\\frac{n}{2}}(n-i)^{-\\frac{i}{2}}(n+i)^{\\frac{i}{2}}/n^n} \\\\\n\u0026 = \u0026 \\frac{1}{2\\Delta}\\frac{\\sqrt{2n}}{\\sqrt{\\pi(n^2-i^2)}} \\frac{1}{\\left(1-\\frac{i}{n}\\right)^\\frac{n}{2}\\left(1+\\frac{i}{n}\\right)^\\frac{n}{2}\\left(1-\\frac{i}{n}\\right)^{-\\frac{i}{2}}\\left(1+\\frac{i}{n}\\right)^\\frac{i}{2}} \\\\\n\u0026 = \u0026 \\frac{1}{\\sqrt{2}\\Delta}\\frac{1}{\\sqrt{\\pi(n-\\frac{i^2}{n})}} \\frac{1}{\\left(1-\\frac{i^2}{n^2}\\right)^\\frac{n}{2}\\left(1-\\frac{i}{n}\\right)^{-\\frac{i}{2}}\\left(1+\\frac{i}{n}\\right)^\\frac{i}{2}} \\\\\n\u0026 \\xrightarrow[\\frac{i}{n}=\\frac{x\\Delta}{2\\sigma t},\\ \\frac{i^2}{n}=\\frac{x^2\\delta}{\\Delta^2 t}]{(1+a\\epsilon)^{1/\\epsilon}\\rightarrow e^a} \u0026 \\frac{1}{\\sqrt{4\\pi\\sigma t}}\\frac{1}{e^\\frac{x^2}{4\\sigma t} e^\\frac{x^2}{4\\sigma t} e^{-\\frac{x^2}{4\\sigma t}} } \\\\\n\u0026 = \u0026 \\frac{1}{\\sqrt{4\\pi\\sigma t}}e^{-\\frac{x^2}{4\\sigma t}}\n\\end{array}\n$$\n\n### 之前 MCMC 讲错了\n\n讲 Markov Chain Monte Carlo 模拟的时候举的例子是计算 $$\\int_{-\\infty}^{+\\infty}e^{-x^2}dx$$, 现在系数可以对上了：$$1=4\\sigma t=4 \\frac{\\Delta^2}{2\\delta} n\\delta = 2\\Delta^2n$$, 随机游走的步数和步长之间存在一个确定的关系，在步长确定的情况下，我们需要重复模拟大量粒子作相同步数的随机游走，然后统计这一确定步数走完之后的每个粒子的终末位置。\n\n所以，这并不是一个 Markov Chain Monte Carlo 模拟，只是一个普通的 Monte Carlo 模拟，我们拿到了想要知道的随机变量的原始概率分布，只不过取得符合这一概率分布的每一个样本的过程是一个 Markov 过程。\n\n正经的 MCMC，应该是只模拟一个粒子作随机行走，然后把它每一步的位置记录下来，统计到样本里去。这样的话时间 t 的信息就被抹去了，而且由于扩散方程描述的状态并不是热力学平衡态，并不能通过统计物理中的遍历性 (ergodicity) 来得到正确结果。\n\n采用了 Metropolis 算法的 MCMC, 一个粒子作随机行走只是其中的一个步骤，还要计算这一步之前和之后 $$e^{-x^2}$$ 的值，来决定这一步是否被加入样本，不成立的话要退回前一步继续走。\n"},{"slug":"mc-mcmc-markov-chain-monte-carlo-gibbs-sampling","filename":"2024-04-15-mc-mcmc-markov-chain-monte-carlo-gibbs-sampling.md","date":"2024-04-15","title":".tex | MC→MCMC 蒙特卡洛模拟，基于马尔科夫链采样","layout":"post","keywords":["tex","phy","m"],"excerpt":"蒙特卡洛模拟、马尔科夫链采样、Metropolis-Hastings 算法、吉布斯采样","hasMath":true,"content":"\nMonte Carlo 蒙特卡洛模拟，简称 MC. \n\nMarkov Chain Monte Carlo 是用马尔科夫链采样的蒙特卡洛模拟，简称 MCMC.\n\n## Monte Carlo 模拟\n\n这个比较简单了，举个例子，要计算 π 的近似值，可以在一块正方形板子里画一个内接圆，然后以均匀的概率往正方形里一粒一粒地扔沙子，每扔一粒，就判断并且记录这里沙子在圆内还是圆外，然后把沙子吹掉，如此往复。圆的面积是 πr²，正方形的面积是 4r²，所以落在圆内的概率（圆内沙子的数量和总数的比值）乘 4，就是所求。\n\n![](/photos/2024-04-15-monte-carlo-pi.png)\n\n归纳一下：当问题的解用一个随机变量的概率分布、期望值、二阶矩……等等来表示的时候，就生成一个符合该概率分布的随机样本，用样本的统计量去近似原概率分布。\n\n## Markov Chain Monte Carlo\n\n但是前述例子有一个步骤，就是我们往板子上扔完沙子要把沙子吹掉，每粒沙子，每次扔沙子之间也应该看不出区别，这是为了保证取样之间**相互独立且来自同一个概率分布**。\n\n但是很多取样过程无法满足这种条件，或者达成条件所需的成本很高。比如计算一个高斯积分 $$\\int_{-\\infty}^{+\\infty}e^{-x^2}dx$$，被积函数的取值范围涵盖整个实数集，想找一个在整个实数集上均匀分布的随机数发生器就比较难了。\n\n![](/photos/2024-04-15-monte-carlo-gaussian.png)\n\n但是学过物理的朋友应该知道，上面的被积函数是以狄拉克 δ(x) 函数为初值条件的一个扩散方程的解，在某一时刻的空间分布。（不想凑系数了，将就看吧）\n\n而扩散方程又是随机游走 (random walk) 在连续近似下的极限。\n\n所以我们直接模拟一堆粒子从原点出发作随机行走，向两个方向的概率相同，扩散系数以及积分里的常数对齐，统计粒子在整个过程中出现在不同 x 位置的频率，求和之后乘以步长就是积分结果。这个过程需要的随机数发生器容易获取得多，是一个以 0.5 为阈值的 [0,1) 的均匀分布，比如一个均匀硬币。\n\n而随机行走过程中走完每一步的位置，都只取决于前一步的位置，而与更久远的历史无关——这样的过程叫做马尔可夫过程。用这种方法取样获得随机样本的蒙特卡洛模拟，就是 MCMC.\n\n扩散方程和随机行走只是 MCMC 的一个很特殊很特殊的例子，而对于一般的 MCMC 模拟，有以下通用的 Markov Chain 采样的算法：\n\n### Metropolis-Hastings 算法\n\n已知一个随机变量 x, 和一个与目标概率分布 P(x) 成正比的函数 f(x)（不要求 f 归一化）\n\n1. 初始化\n    1. 选定初始采样点 $$x_0$$ \n    2. 选定一个采样函数 proposal function，也就是在已知当前 x 的取值时，下一个 x’ 取值的概率分布 $$g(x’\\vert x)$$；其中对于 Metropolis 算法，这个采样函数是对称的：$$g(x’\\vert x)=g(x\\vert x’)$$. 常用以两者之差为宗量的高斯函数。\n2. 在得出 t 时刻的 $$x_t$$ 之后：\n    1. 根据 $$g(x'\\vert x_t)$$ 抽样得到一个 x’\n    2. 计算 α = f(x’)/f(x) = P(x’)/P(x)\n    3. 决定是否将 x’ 加入样本\n        1. 如果 α ≥ 1, 直接加入\n        2. 如果 α \u003c 1, 以 α 为概率加入\n\n这种方法不保证采样的早期样本也符合目标概率分布，所以一般会抛弃最先加入的若干样本。\n\n### Gibbs 采样\n\n只是一种思路，不算是完整的算法。\n\n当被采样的随机变量是一个多维向量的情况，在不使用 Gibbs 采样的情况下，在迭代的某一步骤 t，每个分量都应该是前一步骤的函数：$$x_{i,t}=f(\\{x_{j,\\ t-1}\\})$$\n\n而 Gibbs 采样就是说，不必让每个维度 i 都根据前一个步骤的分量来取值，可以把当前 t 已经取样出来的分量直接带入到本回合后面的维度：$$x_{i,t}=f(\\{x_{j,\\ t}\\}_{j\u003ci}\\cup\\{x_{k,\\ t-1}\\}_{k\\ge i})$$"},{"slug":"probability-vs-likelihood","filename":"2023-11-21-probability-vs-likelihood.md","date":"2023-11-21","title":".tex | 概率 (probability) 和似然性 (likelihood)","layout":"post","keywords":["tex","m"],"hasMath":true,"excerpt":"如题","content":"\n一个随机变量 X 取值为 x 的概率 (probability)/概率密度，一般可以用一个有若干参数的函数来表示。这个函数的参数记作  $$\\theta$$：\n\n$$\nprob_X(x)=f(x|\\theta)\n$$\n\n而似然性 (likelihood) 就是把上式 f 看作以 $$\\theta$$ 为自变量，x 为参数的函数，从表达式上看不出区别：\n\n$$\nL(\\theta|x)=f(x|\\theta)=prob_X(x)\n$$\n\n最近处理一个数据集，整理完之后的直方图如下：\n\n![double-peak](/photos/2023-11-21-double-peak.png)\n\n比较明显，比起一个正态分布 $$f(x)=\\frac{1}{ \\sigma \\sqrt{2\\pi} } e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2}$$，这些数据更像是来自不同均值和方差的两个分布。那么对于每个数据点 $$x_0$$，它到底来自哪个分布呢？可以分别计算 $$L_1(\\mu_1,\\theta_1\\vert x_0) = f(x_0\\vert\\mu_1,\\theta_1)$$ 和 $$L_2(\\mu_2,\\theta_2\\vert x_0) = f(x_0\\vert\\mu_2,\\theta_2)$$，然后比较 $$L_1$$ 和 $$L_2$$ 的大小。\n"},{"slug":"parameters-in-convolution-in-neural-network-and-transposeconv","filename":"2022-12-29-parameters-in-convolution-in-neural-network-and-transposeconv.md","date":"2022-12-29","title":".ai | 神经网络中的卷积及其参数","layout":"post","keywords":["ai","py","md","m"],"hasMath":true,"excerpt":"在读 PyTorch 的文档和源码的时候，发现写文档的人也不怎么解释啥是卷积，卷积的各个参数是什么意思，只在文档里扔了个链接就完事了……","content":"\n在读 PyTorch 的文档和源码的时候，发现写文档的人也不怎么解释啥是卷积，卷积的各个参数是什么意思，只在文档里扔了个链接就完事了，链接那头是一个 GitHub 上的动图演示仓库，是一篇论文《A guide to convolution arithmetic for deep learning》（链接在文末）的附件。于是这篇文章，基本上就是论文的读书笔记了。\n\n## 数学的卷积：连续 vs. 离散\n\n### 定义\n\n连续的情况，两个单变量函数 $$f(\\cdot)$$ 和 $$g(\\cdot)$$ 的卷积，定义为：\n\n$$\n\\left(f*g\\right)(x):=\\int_{-\\infty}^{\\infty}f(\\tau)g(x-\\tau)d\\tau\n$$\n\n离散的情况，两个向量（也就是一阶张量） $$\\vec f$$ 和 $$\\vec g$$ 的卷积，定义为：\n\n$$\n\\left(\\vec f * \\vec g\\right)_i := \\sum_{j=-\\infty}^{\\infty} f_j g_{i-j}\n$$\n\n多变量函数/高阶张量的情况，只需要多加几重积分/求和号就可以类推了。\n\n看这两个定义——\n\n只看等号左边的话，可以把卷积看作是一种特殊的乘法，也就是一种**运算。**f 和 g 的地位是平等的，卷积甚至还满足交换律，你甚至可以把两者的顺序变一变；\n\n但是看等号右边的话，卷积就应该被看作是一种**变换**。f 和 g 的地位不再平等，f 是被变换的函数/向量，g 是变换的核 (kernel)。函数的情况里，g 把定义在 $$\\tau$$ 空间里的函数 f 变换成了 x 空间里的另一个函数；向量的情况里，g 把一个 J (j 所有可能取值的数量) 维向量 f 变换成了一个 I (i 所有可能取值的数量) 维向量。\n\n神经网络中的卷积，**借用**的主要是第二种**理解**。\n\n### 手算一个例子\n\n例如 $$\\vec f = (1,2,3,4)$$, $$\\vec g = (1,2,3)$$，而且约定下标从 0 开始的话——\n\n\u0026nbsp; $$(\\vec f*\\vec g)_0 = f_0g_0 = 1$$\n\n\u0026nbsp; $$(\\vec f*\\vec g)_1 = f_0g_1 + f_1g_0  = 4$$\n\n\u0026nbsp; $$(\\vec f*\\vec g)_2 = f_0g_2 + f_1g_1 + f_2g_0 = 10$$\n\n\u0026nbsp; $$(\\vec f*\\vec g)_3 = f_1g_2 + f_2g_1 + f_3g_0 = 16$$\n\n\u0026nbsp; $$(\\vec f*\\vec g)_4 = f_2g_2 + f_3g_1 = 17$$\n\n\u0026nbsp; $$(\\vec f*\\vec g)_5 = f_3g_2 = 12$$\n\n不想手算？\n\n```python\nimport numpy as np\nfrom scipy import signal\nsignal.convolve(np.array([1,2,3,4]),np.array([1,2,3]))\n```\n\n### 形象化表示\n\n上面的计算过程，可以看作是——\n\n1. 把 g 向量的**顺序反过来；**\n2. 把 g 的最右一个元素和 f 的最左元素对齐，\n3. 上下两行都有数字的列相乘（也就是把没有数字的地方看作 0），然后把所有乘积相加，得到 f*g 的第一项；\n4. 把 g 向右移动一格\n5. 重复第3、4步\n6. 直到 g 的最左项移动到 f 的最右一个元素。\n\n形如下列各表：\n\n| --- | --- | --- | --- | --- | --- | --- |\n| f |  |  | 1 | 2 | 3 | 4 |\n| g | 3 | 2 | 1 |  |  |  |\n| (f*g)(0) = 1 |  |  | 1 |  |  |  |\n\n\u003chr class=\"slender\"\u003e\n\n| --- | --- | --- | --- | --- | --- |\n| f |  | 1 | 2 | 3 | 4 |\n| g | 3 | 2 | 1 |  |  |\n| (f*g)(1) = 4 |  | 2 | 2 |  |  |\n\n\u003chr class=\"slender\"\u003e\n\n| --- | --- | --- | --- | --- |\n| f | 1 | 2 | 3 | 4 |\n| g | 3 | 2 | 1 |  |\n| (f*g)(2) = 10 | 3 | 4 | 3 |  |\n\n\u003chr class=\"slender\"\u003e\n\n| --- | --- | --- | --- | --- |\n| f | 1 | 2 | 3 | 4 |\n| g |  | 3 | 2 | 1 |\n| (f*g)(3) = 16 |  | 6 | 6 | 4 |\n\n\u003chr class=\"slender\"\u003e\n\n| --- | --- | --- | --- | --- | --- |\n| f | 1 | 2 | 3 | 4 |  |\n| g |  |  | 3 | 2 | 1 |\n| (f*g)(4) = 17 |  |  | 9 | 8 |  |\n\n\u003chr class=\"slender\"\u003e\n\n| --- | --- | --- | --- | --- | --- | --- |\n| f | 1 | 2 | 3 | 4 |  |  |\n| g |  |  |  | 3 | 2 | 1 |\n| (f*g)(5) = 12 |  |  |  | 12 |  |  |\n\n## 机器学习的卷积，是卷积吗？\n\n看论文给出的图 Figure 1.1，在卷积核是灰色 3\\*3 矩阵的情况下，对蓝色 5\\*5 矩阵的卷积就是直接把核对齐到蓝色矩阵上，**并没有把核的元素顺序颠倒过来**。\n\n这玩意能叫卷积吗？\n\n![convolution](/photos/2022-12-29-convolution.png)\n\n有人强行挽尊，说我们画图示的时候已经把核给颠倒过来了，想知道卷积核就把灰色小矩阵再颠倒回去——\n\n但是，不颠倒就对齐相乘的运算也是有名字的，叫 cross correlation。核有没有颠倒，convolution 还是 cross correlation 一组合，可以带来升维打击般的混乱，堪比高中化学的“还原剂被氧化，氧化剂被还原”……所以，对于计算机专业的数学水平，不予置评～\n\n\n## 卷积`torch.nn.Conv` 及其各个参数\n\n### `in_channels` \u0026 `out_channels`\n\n“卷积”的意义在于用一种比较省内存的方式，考虑输入张量中各个元素，和空间上相近的邻居元素之间的关系。所以只需要在真的存在空间关系的维度做卷积，其他维度可以留着不动。\n\n比如一张彩色图片，是一个 (颜色*高度*宽度) 的 3 阶张量，我们只需要对高度和宽度两个维度做卷积，颜色就是不参与“卷积”的 channel。\n\n`in_channel` 就是被“卷积”的张量的 channel 数，`out_channel` 是“卷积”结果的 channel 数。比如我们想从一张 RGB 三色图片中分辨出前景和背景两种不同区域，`in_channel=3`, `out_channel=2`。\n\n而 `in_channel` 如何能够与 `out_channel` 取值不同，原理见 Figure 1.3。我们使用 `out_channel` 个不含 channel 维度的“卷积”核，每一个核都与每一个 in channel 做卷积，得到图中的蓝、紫色小矩阵，然后直接把不同的 in channel 暴力求和，得到的结果分别作为卷积结果的 out channel。（这个暴力求和与我以前想得不一样，我以为是什么每一元素都做了一个`in_channel`*`out_channel` 的全联通层）\n\n![channels](/photos/2022-12-29-channels.png)\n\n PyTorch 的习惯，对于一个 N 阶“卷积”，参与卷积的是张量的最后 N 阶，`in_channel` 和 `out_channel` 也就是被卷张量和卷积结果的 `Tensor.shape[-(N+1)]`\n\n后面图示的例子都没有考虑 `in_channel` 和 `out_channel` 的数量，也就是都当作 1 了。\n\n### `kernel_size`\n\n就是灰色矩阵“卷积”核，每边有几个数字。如果不同方向的边长不一，该参数就需要用一个 tuple 来表示。Figure 1.1 的灰色卷积核，`kernel_size=(3,3)`\n\n![kernel](/photos/2022-12-29-kernel.png)\n\n### `padding` \u0026 `padding_mode`\n\n前面手算例子的时候很鸡贼地把 0 作为向量下标的起点。如果采用日常 1 开头的下标来算，第 1 项结果为零，整个卷积结果的长度会长很多，而且多出来的后面几项也都是零。\n\n而且在这个过程中，我们实际上是把一个有限长度的向量，看作了一个以所有整数 $$\\Z$$ 为定义域的函数，除了那有限的几项之外，其余地方都定义函数值为 0。\n\n用计算机计算的话显然没法如此奢侈地谈“无限多个”，例子中实际用到的，在 $$\\vec f$$ 左右两边各需要 2 个 0，也就是说 `padding=2`, `padding_mode='zeros'`\n\nFigure 1.2 表示的就是 `padding=(1,1)` 的情况（蓝色是被卷张量，白色是 padding，灰色是卷积核，绿色是卷积结果）：\n\n![padding](/photos/2022-12-29-padding.png)\n\n既然神经网络中的卷积并不是真正的卷积，所以他们索性不装了——\n\n正常卷积的结果往往比被卷张量大一圈（具体大多少取决于  `kernel_size`, `padding`, `stride` 多个参数），但是图像处理的时候经常希望输出图片和输入图片一样大，此时可以用字符串 `“same”` 作为 `padding` 的参数，自动计算 padding 的大小。`“strict”` 则表示 `padding=0`, 这样输出图片尺寸会变小，但是没有 padding，也就没有往图片里掺杂研究者对图片边缘以外信息的臆测。\n\n同时 `padding_mode` 参数表示往被卷张量四周填充的数字也不一定是 0。比如对于图片，0 往往表示纯黑，而绝大多数图片的视野之外，往往是和图片边缘像素值相差不大的值。所以 `padding_mode` 除了 `zeros` 之外，还接受以下取值：\n\n- `reflect`: 以图片边缘为镜面，把边缘附近的像素值对陈反射出去；\n- `replicate`: 只取边缘的像素值作为常数，直接向外延拓；\n- `circular`: 类似于物理中的周期性边界条件，取对边附近的像素值作为 padding 内容。\n\n### `stride`\n\n前面手算卷积的第4步，把卷积核向右移动了1格，如果每次移动超过1格，就需要这个参数指定移动步长。如果不同方向的步长不同，也是用 tuple 来表示。\n\nFigure 1.4 表示的就是 `stride=(2,2)` 的情况（蓝色是被卷张量，蓝色中的深色块是卷积核，绿色是卷积结果）：\n\n![stride](/photos/2022-12-29-stride.png)\n\n### `dilation`\n\n这个参数把“卷积”核撑开，也就相当于在“卷积”核的相邻元素之间加 0。Figure 1.5 表示的就是 `dilation=(1,1)` 的情况（蓝色是被卷张量，蓝色中的深色块是卷积核，绿色是卷积结果）：\n\n![dilation](/photos/2022-12-29-dilation.png)\n\n比如 `dilation=1` 时，(1,2,3) 的卷积核就相当于 (1,0,2,0,3)\n\n比如 `dilation=2` 时，(1,2,3) 的卷积核就相当于 (1,0,0,2,0,0,3)\n\n这样可以让卷积核在尺寸比较小的情况下，覆盖到更大面积的被卷张量。当然具体实现时，不可能直接补 0 这么浪费内存。\n\n### `groups`\n\n该参数必须是 `in_channel` 和 `out_channel` 的公约数，当其不为 1 时，就相当于同时做 `groups` 个卷积，其中每个卷积的 `in_channel=in_channel/groups`, `out_channel=out_channel/groups`\n\n### `bias`\n\n该参数是一个布尔值，卷积类似于一种高维空间里的乘法，这个参数就决定是否要拟合 `y=kx+b` 中的 `b`\n\n## “卷积”的“逆运算”： `TransposeConv`\n\n卷积的结果比 padding 之后的被卷张量要小。尤其当“卷积”的 `stride` 约等于 `kernel_size` 时，卷积的就变成了某些池化 (pooling)（求最大值不是一种线性算子，所以最大值池化不能用卷积表示，但是平均值池化可以）。\n\n那么在类似 U-net 这样的模型里，右半边的数据升维（下图中的绿箭头），就需要一种“卷积”的“逆运算”。有人把这种运算叫做 deconvolution，有人叫做 transposed convolution，还有人叫做 convolution with fractional strides。\n\n![Unet](/photos/2022-12-29-unet.png)\n\nPyTorch 取的是第二种名字。论文解释了为什么这么取名字，笔记以后有时间再补上把……\n\n因为这个与运算本身就是作为“卷积”的逆运算出现的，所以 PyTorch 的文档里这么说：\n\n\u003e This is set so that when a `Conv2d` and a `ConvTranspose2d` are initialized with same parameters, they are inverses of each other in regard to the input and output shapes.\n\u003e \n\n也就是说，把 `ConvTranspose` 的输入和输出反过来，然后按照 `Conv` 的规则确定各个参数，填入 `ConvTranspose` 的括号里就可以了，除了 `output_padding`\n\n### `output_padding`\n\n`ConvTranspose` 的输出就是对应 `Conv` 的输入。看 Figure 2.7：\n\n![padding_output](/photos/2022-12-29-output-padding.png)\n\n当 $$(input+2*padding)/stride$$ 不能整除的时候，最右的几列最下的几行就被卷积核忽略掉了。那么在逆运算 `TransposeConv` 中，这就意味着同一个输入可能对应着 $$stride-1$$ 种可能的输出。`output_padding`参数就可以消除这种歧义，调整 `TransposeConv` 输出张量的尺寸。\n\n## 参考链接\n\n- 给卷积正名: [https://www.kaggle.com/general/225375](https://www.kaggle.com/general/225375)\n- PyTorch Conv2d 源码: [https://pytorch.org/docs/stable/_modules/torch/nn/modules/conv.html#_ConvNd](https://pytorch.org/docs/stable/_modules/torch/nn/modules/conv.html#_ConvNd)\n- 论文: [https://arxiv.org/abs/1603.07285](https://arxiv.org/abs/1603.07285)\n- 动图演示: [https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md](https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md)\n- U-net: [https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/](https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/)\n- PyTorch TransposeConv 文档: [https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html](https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html)\n"},{"slug":"logical-science-from-west","filename":"2022-08-22-logical-science-from-west.md","date":"2022-08-22","title":".doc | 也谈近代科学从西方起步","layout":"post","keywords":["doc","tex","phy","m","phi"],"excerpt":"为什么近代科学偏偏是在丢过一次古典传统的西方起步的呢？为什么那些成功继承了古典时代智慧的中古文明，比如伊斯兰文明或古中国文明，反而没有成功萌发近代科学思想呢？","content":"\n前不久在公众号转载过“海边的西塞罗”写的《**嗯！您关注的是一个早晚要“凉凉”的公众号**》，标题起得让人不知所云，但是文章内容讨论的是“近代科学为什么从西方起步”的问题，原文说：\n\n\u003e 既然你所讲述的，欧洲从古典时代到文艺复兴、科学曾经出现过一次“断层”，欧洲人是通过翻译阿拉伯人转译的古典时代文献才继承了希腊罗马先贤们的思想的。\n\u003e \n\u003e \n\u003e 那么**，为什么近代科学偏偏是在丢过一次古典传统的西方起步的呢？为什么那些成功继承了古典时代智慧的中古文明，比如伊斯兰文明或古中国文明，反而没有成功萌发近代科学思想呢？**\n\u003e \n\n作者立了一个靶子——\n\n\u003e 我之前听到的比较靠谱的解答，**是古希腊罗马有较好的数学思想，当定量的数学思想与定性的“自然哲学”发生结合，近代科学就诞生了。**\n但这种解释，其实也回答不了一个问题——你可以说古代东方离着希腊远，没有受到希腊某些思想的“药引”的启发。但特别奇怪的是，中世纪的中东却不是这样。\n伊斯兰文明的伍麦叶王朝在公元九世纪曾经掀起过一场声势浩大的“百年翻译运动”，……近代启发西方的那些古典思想典籍，阿拉伯人全有，且早获得了好几百年。\n\u003e \n\n给出的回答是所谓**“托勒密困境”**，即诸文明中的科学技术研究者因为要满足当权者/赞助者的功利性需要，将时间与精力耗费于附会科学（比如天文学）的非科学甚至伪科学（比如占星术）之上，而——\n\n\u003e 这种错误的职业拼接，锁死了天文学的进一步发展的通路，导致其无法实现向近代科学的飞跃——即便托勒密会数学、引入定量计算，也依然没用。\n\u003e \n\u003e **而这种“托勒密困境”，其实也是所有古典时代学者的困境——他们在研究学问时，必须回答“求用”的问题。**\n\u003e \n\u003e ……\n\u003e \n\u003e **于是从托勒密到哥白尼，我们会发现西方在这一轮对天文学的失而复得中，其实并没有增添什么，而是丢掉了一种东西——那就是“求用”的思维。**\n\u003e 欧洲知识分子们研究科学的正义性，来自于他们认定：自然作为一种上帝的造物，其本身就是美的。因此研究它、探索它本身，就是在赞美上帝，所以科学研究不必“求用”也有天然的正义性。\n\u003e \n\n作者写近代西方科学的不求用，是为了托物言志，检讨自己为了读者的关注不得不在历史写作之外“写时评、表达观点、带情绪”，预告自己将来可能会去写作崇高的钻研历史的题目。\n\n给蹭热点找理由这件事，我也做过嘛，感觉写的比这篇文章还简约隽永且立意高远呢～（文人相轻.jpg）\n\n但是科学革命发源于西方这个问题，我也很感兴趣，而且有自己的思考，而且思考的结果和上文不同。\n\n\u003chr class=\"slender\"\u003e\n\n学物理的孩子应该都听说过《费曼物理学讲义》的大名，没听说过的话建议听说一下，自主招生考试面试装逼的时候用的上。费曼先生在引言中也立了个靶子说——\n\n\u003e 你们可能会问，在讲述欧几里德几何时，先是陈述公理，然后作出各种各样的推论，那为什么在讲授物理学的时候不能先直截了当地列出基本规律，然后再就一切可能的情况说明定律的应用呢？\n\u003e \n\n然后上来就讲原子论，开篇问：\n\n\u003e 假如由于某种大灾难，所有的科学和知识都丢失了，只有一句话可传给下一代，那么怎样才能用最少的词汇来传达最多的信息呢？\n\u003e \n\n可惜这个问题仅仅是为了引出原子论，实在是大材小用。这说明费老先生浸淫于西方科学中，“不识庐山真面目，只缘身在此山中”。而我对近代科学起自西方的解释，正好就是这两句话串起来。下面就要兜一个大圈子，把两句话圆起来。\n\n\u003chr class=\"slender\"\u003e\n\n科学者，对世界之正确认知也。\n\n根据这个定义，把人们已知的，关于这个世界的所有知识罗列到一个集合里，这个集合就是科学。我们只谈到了一个集合，不涉及逻辑推演，也不涉及数学带来的定量优势，更不判断从事科学研究的人是否功利。\n\n但是，集合这种知识结构过于简单——\n\n- 集合里的各个元素都是平等的，要想表示出整个集合，除了全默写出来没别的办法；\n- 集合里的元素之间没有顺序，想取得其中的某一条科学命题，只能像抓阄一样，凭运气抽到为止。\n- 一旦由于天灾人祸，集合中的部分内容丢失，除了重新把当初发现它们时经历的艰难困苦重复一遍，也没有别的办法。（哦，也可以去隔壁文明的图书馆翻译。）\n\n所以，必须找到一种更复杂的结构，来组织这些信息，解决上述问题。\n\n\u003chr class=\"slender\"\u003e\n\n计算机专业有门基础课《数据结构与算法》，谈数据结构，最基础的两种就是数组和链表；谈算法，最基础的概念就是函数。注意，这里说的是数据结构，刚才说的是知识结构，两者可以类比，但并非同一概念。\n\n数组，和集合几乎一样，只不过给每个元素标记了一个序号。在计算机里，由于规定数组连续存放，每个元素占用内存长度相等，所以可以通过序号，从数组开头偏置指针，以 O(1) 的时间复杂度取得任意元素，快。\n\n类比到知识结构，语数外理化政史地生，一年级二年级三年级，第一章第二章第三章，第一第二第三个知识点，背吧。列表与列表之间井水不犯河水，你数学老师说你体育老师拉稀了不能上课，你体育老师说你数学老师放屁，两者完全可以在你的知识体系里共存。\n\n链表，和数组一样有顺序，但是并不给每个元素标号，而是在前一个元素的末尾，写上下一个元素的位置指针。找到一个元素需要从链表的开头一个一个往后捋，慢。好处是修改方便，在链表中间塞进去一个新元素，只需要把前面一个的指针指向新元素，新元素的指针指向后一个元素，删除一个旧元素也类似，只对增删点附近一个很小的区域进行改动，整个链表不会伤筋动骨。\n\n但是不论数组还是链表，都需要把所有的知识全写出来，随着时间的积累，科学的总量早晚要超越人脑的记忆力，超越笔记的厚度，对于个人，要皓首穷经，要韦编三绝，才有希望提出一点新内容；对于全人类，图书馆越造越大，一轮战乱，从头再来。\n\n于是函数登场。给定一个/一组输入，根据函数体描述的算法，返回确定的输出。那我们找到一种方法，写一个函数，接收链表的前一个元素作为输入，找到后一个元素输出。这样我们只需要存储第一个元素和这个函数，就可以恢复出整个链表，用计算换空间。\n\n\u003chr class=\"slender\"\u003e\n\n类比到知识结构，这个函数就是逻辑推演。\n\n科学内容中的每一条知识都是一个**命题**。\n\n从少数几条知识出发，这几条在逻辑上就称为**公理**，自然科学里也称之为**定律**。\n\n命题之间可以做**逻辑运算**，**或**、**且**、**非**、**蕴含**等等，运算的结果也是一条新的命题。命题的正确与否，取决于逻辑运算的规定。\n\n通过对公理和已经算出的真命题反复进行逻辑运算，产生的新的真命题，叫做**定理**。\n\n\u003chr class=\"slender\"\u003e\n\n欧几里德几何式的，也就是从有限多个命题出发，承认逻辑推演进行生成的新命题的正确性，这样的一种组织方式——\n\n- 对于学习，科学不再是一家之言，门户之见。一句话的正确性不再由说话者的身份决定，诉诸人身、诉诸权威成了谬误，“我爱吾师，但我更爱真理”一句话有了切实的落脚点。\n- 对于研究，降低了难度，后来者不必从头再来，而是站在前人的终点起跑。发现的新科学有办法整合进现有的科学，证伪的旧科学有办法剔除，而不会让科学整体伤筋动骨。愚弄黔首的矛盾和谬误，真理有办法与之势不两立。\n- 自带有容灾能力，科学得以在摧毁科学记录和科学家人身的重大灾难之后，在几百年的人才断档之后，依然有办法恢复。\n\n\u003chr class=\"slender\"\u003e\n\n刚才说数据结构和知识结构不同，知识管理界有个 DIKW 模型，也就是数据 (Data)、信息 (Information)、知识 (Knowledge)、智慧 (Wisdom)。\n\n纸张上的墨迹组成的字符只是数据，当这些单词按照语法理解为句段篇章之后才构成信息，这些篇章内容指代的概念、关系等等含义构成知识。如何理解知识与知识之间的关系需要智慧。\n\n“继承了古典时代智慧的中古文明，比如伊斯兰文明或古中国文明”——从各个文明没能演化出科学革命来看，**这些文明最多是有一部分学者继承了古典时代的知识，而没能认识到 *用逻辑组织知识* 这一智慧的价值**，而西方发掘出了这种智慧。至于这种发掘发生在西方，是偶然还是必然，由哪些条件促成，那是另一个很有趣的问题了。\n\n“我们会发现西方在这一轮对天文学的失而复得中，其实并没有增添什么，而是丢掉了一种东西——那就是‘求用’的思维。”——西方对天文学的失去，对应的是罗马统治下的和平结束时的战乱与社会崩溃，不论之后的文艺复兴如何光辉灿烂，**这种失落都是对科学乃至整个文明的威胁**，如果没有这种失落，科学革命想必会更早更容易发生。况且这种失落到复兴的整个过程中，对科学有影响的因素实在是太多了，既有正面又有负面，实在是难以分析归因。\n\n至于不求用的思维，有了逻辑推演，科学工作者的产出提高，高到了让社会愿意供养其全职研究的地步，那么不求用的思维，自然会建立起来；不求用对科研效率的提升，良性反哺科学的发展，自然会蔚然成风。反过来，**只有不求用的态度，研究者没有逻辑推演发展科学的能力，资助者没有逻辑推演评价成果的本事，不求用的态度只会鼓励灌水，产出真没用的水货。**\n\n\u003chr class=\"slender\"\u003e\n\n数学对自然科学的作用，定量化只是一个副产品。更重要的是作为逻辑科学的集大成者，发明/发现逻辑推演的规则，探索逻辑推演作为方法论的能力边界。一言以蔽之，欧几里德之后，数学已不只是“数字的学问”。\n\n至于费曼先生，他怎么可能不知道四大力学确实就是按照欧几里德式的，从基本定律出发的方式讲授的呢？面对一伙学普通物理的本科新生，说这种话实在有点骗小孩儿的嫌疑，怪不得那门课上到后来，本科生全都跑了。物理和数学的区别，在于理论和实验两条腿走路，但是理论的这条腿，实实在在地来自于超越了“数字的学问”的数学。\n"},{"slug":"mimic-mathematica-with-wolfram-engine-and-vscode","filename":"2022-06-19-mimic-mathematica-with-wolfram-engine-and-vscode.md","date":"2022-06-19","title":".nb | Mathematica 入门：免费正版、vscode、近似原生体验","layout":"post","keywords":["md","nb","m"],"excerpt":"不用算号器，完全合法的免费手段搭建一个免费的 Wolfram Languange 运行环境，效果尽可能贴近 Mathematica。","content":"\n## Mathematica 的原生体验暨山寨目标\n\n本科的时候老师总是跟我们念叨，让我们学点科学计算软件，可学的不多，不过 MatLab, Mathematica, Maple, Origin 和 Labview.  ~~作为编程语言的 MatLab 是世界上语法最垃圾的（没有之一）~~ ，Maple 实在是太小众了，Origin 和 Labview 不仅应用场景有限而且繁琐还有版权问题，于是 MMA 就成了我主要的折腾对象。不敢说拿手，起码是略懂。\n\nWolfram 和 python 一样也是[动态语言和解释型语言](python-interpreter-editor-virtualenv)，而且默认的新建文件类型就是 `.nb` 笔记本文件，命令按块执行，输出结果直接显示在代码块下方，强烈怀疑 Jupyter Notebook 就是山寨了 Mathematica。当然另有一种 `.m` 文件，用于执行文件内的所有命令，适用于比较大型的独立应用。\n\n![]({{ site.baseurl }}/assets/photos/2022-06-19-mathematica-notebook.gif)\n\n虽然现在的学校给学生买了正版许可证，但是只能用在一台电脑上，所以笔记本上就安装不了。虽然百度贴吧的精华帖里有传统艺能算号器教程，但是现在 Wolfram 开放了免费的 Wolfram Engine，所以我们还是来点正大光明的，用完全合法的免费手段搭建一个免费的 Wolfram Languange 运行环境，效果尽可能贴近 Mathematica。\n\n需要用到的工具有：\n\n- Wolfram Engine\n- Wolfram Script\n- Wolfram Engine For Jupyter\n- jupyter\n- vscode\n\n## Wolfram Engine 和 Wolfram Script 下载和安装\n\n《How do I install Mathematica on Linux?》：[https://support.wolfram.com/12453](https://support.wolfram.com/12453)\n\n在 Google 上搜索“Wolfram Engine”后可以找到官网的下载地址：[https://www.wolfram.com/engine/](https://www.wolfram.com/engine/)\n\n根据自己的操作系统点击下载之后会弹出获取许可证的页面 ([https://account.wolfram.com/access/wolfram-engine/free](https://account.wolfram.com/access/wolfram-engine/free))，没有 Wolfram 账号的需要注册一个账号。\n\n完成之后在下载文件夹打开 terminal, 输入以下命令，其中 xyz 是下载文件名中的版本号：\n\n```python\nsudo bash WolframEngine_xxx.yy.zz_LINUX\n```\n\n强烈建议按照默认设置完成安装，不做要任何个性化的调整，理由见下方引用块。\n\n\u003e 我把第二个选项，也就是 Wolfram Engine 可执行文件的路径设置成了自己 home 下安放一般独立软件的文件夹，结果激活过程出现了问题：输入 `wolframscript` 之后说找不到 WoflramEngine，填入自己的路径之后提示 Wolfram Engine 尚未激活，手动启动 `WolframEngine` 之后提示输入激活密钥 activation key，但是各处遍寻不得。\n解决方法来自以下 StackOverflow 回答：[https://mathematica.stackexchange.com/questions/198822/the-wolfram-kernel-must-be-activated-for-wolframscript-to-use-it](https://mathematica.stackexchange.com/questions/198822/the-wolfram-kernel-must-be-activated-for-wolframscript-to-use-it)\n在wolfram官网登陆自己的账号之后，在一个新的标签页输入以下网址 [https://www.wolframcloud.com/users/user-current/activationkeys](https://www.wolframcloud.com/users/user-current/activationkeys)，即可看到自己的 activation key，在 terminal 中打开 Wolfram Engine，根据提示把 activation key 复制粘贴到指定位置，即可完成激活。\n但是第二天配置好 vscode 和 Jupyter 之后，再次在命令行打开 WolframScript 的时候提示激活失败，重新按照上述方法操作后，显示 activation key 已被使用。即便是删除后按照默认设置重装，也依然会提示超过许可证限制。\n后来在官网给出的联系方式给客服发了消息，客服回信给了新的激活码。\n\u003e \n\n再下载 [WolframScript](https://account.wolfram.com/products/downloads/wolframscript)，这是 Wolfram 的前端。然后按照各个操作系统自己的规矩安装 WolframScript，我的 fedora 就是双击 rpm 文件然后根据提示操作。完成后在命令行输入 `wolframscript`, 根据提示输入 Wolfram 账号和密码，Wolfram Engine 就会联网激活自己。\n\n激活成功之后，在 terminal 输入 `wolframscript`, 显示的结果如下，即说明 Wolfram Engine 和 Wolfram Script 配置成功。\n\n```bash\nWolfram Language 13.0.1 Engine for Linux x86 (64-bit)\nCopyright 1988-2022 Wolfram Research, Inc.\n\nIn[1]:=\n```\n\n在 `In[1]:=` 处输入 `Exit[]` 并按回车，即可退出 Wolfram 回到命令行。\n\n## 将 Wolfram Engine 设为 Jupyter 的后端\n\n[https://github.com/WolframResearch/WolframLanguageForJupyter](https://github.com/WolframResearch/WolframLanguageForJupyter)\n\n根据上面网址的指示，将官方 repo 克隆到本地，因为我们的 python 分隔成了多个[虚拟环境](https://virtual.env)，所以比官网教程多一步 `workon base`:\n\n```bash\n[me@my_computer dev]$ git clone https://github.com/WolframResearch/WolframLanguageForJupyter.git\n# Cloning into 'WolframLanguageForJupyter'...\n# remote: Enumerating objects: 649, done.\n# remote: Counting objects: 100% (140/140), done.\n# remote: Compressing objects: 100% (52/52), done.\n# remote: Total 649 (delta 93), reused 126 (delta 88), pack-reused 509\n# Receiving objects: 100% (649/649), 321.55 KiB | 2.23 MiB/s, done.\n# Resolving deltas: 100% (411/411), done.\n[me@my_computer dev]$ cd WolframLanguageForJupyter\n[me@my_computer WolframLanguageForJupyter]$ workon base\n(base) [me@my_computer WolframLanguageForJupyter]$ ./configure-jupyter.wls add\n(base) [me@my_computer WolframLanguageForJupyter]$\n```\n\n## 用法和效果\n\n打开 vscode，按 `Ctrl+Shift+P` 呼出命令搜索框，找到 \"Jupyter: Create Interactive Window\":\n\n![]({{ site.baseurl }}/assets/photos/2022-06-19-jupyter-start.png)\n\n单击 Jupyter 后端内核的图表（下图中右上角的\"base(Python 3.9.12)\"字样），把内核切换为 \"Wolfram Language ##\"\n\n![]({{ site.baseurl }}/assets/photos/2022-06-19-switch-kernel.png)\n\n等待后端内核切换完成，就可以输入 Mathematica 命令查看效果了：\n\n![]({{ site.baseurl }}/assets/photos/2022-06-19-final-result.png)\n\n完成！\n"}]]]},"__N_SSG":true},"page":"/articles/[id]","query":{"id":"mc-mcmc-markov-chain-monte-carlo-gibbs-sampling"},"buildId":"buildID","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>