<!DOCTYPE html><html lang="zh-CN"><head><title>.tex | 什么是智能≠智能是什么 | 阿掖山·博客</title><meta property="og:title" content=".tex | 什么是智能≠智能是什么 | 阿掖山·博客"/><meta name="twitter:title" content=".tex | 什么是智能≠智能是什么 | 阿掖山·博客"/><meta name="description" content="“什么是智能”的问题每每得不到回答，是因为它的逆问题“智能是什么”没有答案。"/><meta property="og:description" content="“什么是智能”的问题每每得不到回答，是因为它的逆问题“智能是什么”没有答案。"/><meta name="twitter:description" content="“什么是智能”的问题每每得不到回答，是因为它的逆问题“智能是什么”没有答案。"/><meta property="og:image" content="https://blog.mountaye.com/assets/img/before_h2.png"/><meta name="twitter:image" content="https://blog.mountaye.com/assets/img/before_h2.png"/><meta charSet="utf-8"/><meta http-equiv="X-UA-Compatible" content="IE=edge"/><meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no"/><meta name="generator" content="Next.js v14.2.6"/><link rel="canonical" href="https://blog.mountaye.com/articles/what-is-intelligence-not-same-as-intelligence-is-what"/><link rel="icon" href="/favicon.ico" type="image/x-icon"/><meta property="og:locale" content="zh-CN"/><meta property="og:url" content="https://blog.mountaye.com/articles/what-is-intelligence-not-same-as-intelligence-is-what"/><meta name="twitter:card" content="summary_large_image"/><meta name="next-head-count" content="17"/><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css"/><link rel="preload" href="/_next/static/css/70a087ca0a2aa809.css" as="style"/><link rel="stylesheet" href="/_next/static/css/70a087ca0a2aa809.css" data-n-g=""/><link rel="preload" href="/_next/static/css/c01db217da1c4d5f.css" as="style"/><link rel="stylesheet" href="/_next/static/css/c01db217da1c4d5f.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-42372ed130431b0a.js"></script><script src="/_next/static/chunks/webpack-2df7a8d27de1794c.js" defer=""></script><script src="/_next/static/chunks/framework-64ad27b21261a9ce.js" defer=""></script><script src="/_next/static/chunks/main-e618b1edbb350739.js" defer=""></script><script src="/_next/static/chunks/pages/_app-82a2279cdebeb61d.js" defer=""></script><script src="/_next/static/chunks/24-03c6412f1cc555e5.js" defer=""></script><script src="/_next/static/chunks/923-e362c0f2482feb7b.js" defer=""></script><script src="/_next/static/chunks/pages/articles/%5Bid%5D-9564c34343a7e645.js" defer=""></script><script src="/_next/static/buildID/_buildManifest.js" defer=""></script><script src="/_next/static/buildID/_ssgManifest.js" defer=""></script></head><body><div id="__next"><div id="all" class="w-full h-max"><nav id="top" class="w-full h-11 sm:h-12 lg:h-14 fixed top-0 flex flex-row justify-between items-center z-50 bg-white dark:bg-black shadow-lg dark:shadow-gray-700 transition-transform"><a class="whitespace-nowrap rounded-md text-sm font-medium ring-offset-background transition-colors focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:pointer-events-none disabled:opacity-50 dark:text-slate-400 text-primary underline-offset-4 hover:underline h-10 py-2 w-max px-2 flex flex-row justify-end items-center grow-0 shrink-0" href="/"><span class="relative flex shrink-0 overflow-hidden rounded-full h-9 w-9 sm:h-10 sm:w-10 md:h-12 md:w-12"><span class="flex h-full w-full items-center justify-center rounded-full bg-muted">Aye</span></span><div id="site title" class="pl-2 hidden sm:block"><span class="text-2xl font-serif dark:font-sans">阿掖山</span><span class="ml-[0.5em] text-base font-sans hidden lg:inline">博客</span></div></a><div class=" grow-0 shrink flex w-[25%]"><input type="text" class="flex h-10 w-full rounded-md border border-input px-3 py-2 text-sm ring-offset-background file:border-0 file:bg-transparent file:text-sm file:font-medium placeholder:text-muted-foreground focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:cursor-not-allowed disabled:opacity-50 rounded-r-none bg-inherit dark:border-gray-700" id="search-input" placeholder="Search"/><button class="inline-flex items-center justify-center whitespace-nowrap text-sm font-medium ring-offset-background transition-colors focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:pointer-events-none disabled:opacity-50 dark:text-slate-400 border border-input hover:bg-accent hover:text-accent-foreground h-10 w-10 rounded-none border-x-0 bg-inherit dark:border-gray-700" type="button" id="radix-:R19km:" aria-haspopup="menu" aria-expanded="false" data-state="closed"><span class="relative flex shrink-0 overflow-hidden h-5 w-5 rounded-none"><span class="flex h-full w-full items-center justify-center rounded-full bg-muted">Aye</span></span></button><button class="inline-flex items-center justify-center whitespace-nowrap rounded-md text-sm font-medium ring-offset-background transition-colors focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:pointer-events-none disabled:opacity-50 dark:text-slate-400 border border-input hover:bg-accent hover:text-accent-foreground h-10 w-10 rounded-l-none bg-inherit dark:border-gray-700"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-search h-4 w-4"><circle cx="11" cy="11" r="8"></circle><path d="m21 21-4.3-4.3"></path></svg></button></div><div class="flex flex-row"><button class="inline-flex items-center justify-center whitespace-nowrap rounded-md text-sm font-medium ring-offset-background transition-colors focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:pointer-events-none disabled:opacity-50 dark:text-slate-400 hover:bg-accent hover:text-accent-foreground h-10 py-2 md:hidden px-1 md:px-2 lg:px-3" type="button" id="radix-:R2tkm:" aria-haspopup="menu" aria-expanded="false" data-state="closed"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-layers"><path d="m12.83 2.18a2 2 0 0 0-1.66 0L2.6 6.08a1 1 0 0 0 0 1.83l8.58 3.91a2 2 0 0 0 1.66 0l8.58-3.9a1 1 0 0 0 0-1.83Z"></path><path d="m22 17.65-9.17 4.16a2 2 0 0 1-1.66 0L2 17.65"></path><path d="m22 12.65-9.17 4.16a2 2 0 0 1-1.66 0L2 12.65"></path></svg></button><div class="hidden md:block "><a class="inline-flex items-center justify-center whitespace-nowrap rounded-md text-sm font-medium ring-offset-background transition-colors focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:pointer-events-none disabled:opacity-50 dark:text-slate-400 hover:bg-accent hover:text-accent-foreground h-10 py-2 px-1 md:px-2 lg:px-3" href="/history"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-history"><path d="M3 12a9 9 0 1 0 9-9 9.75 9.75 0 0 0-6.74 2.74L3 8"></path><path d="M3 3v5h5"></path><path d="M12 7v5l4 2"></path></svg></a><a class="inline-flex items-center justify-center whitespace-nowrap rounded-md text-sm font-medium ring-offset-background transition-colors focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:pointer-events-none disabled:opacity-50 dark:text-slate-400 hover:bg-accent hover:text-accent-foreground h-10 py-2 px-1 md:px-2 lg:px-3" href="/topics"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-tags"><path d="m15 5 6.3 6.3a2.4 2.4 0 0 1 0 3.4L17 19"></path><path d="M9.586 5.586A2 2 0 0 0 8.172 5H3a1 1 0 0 0-1 1v5.172a2 2 0 0 0 .586 1.414L8.29 18.29a2.426 2.426 0 0 0 3.42 0l3.58-3.58a2.426 2.426 0 0 0 0-3.42z"></path><circle cx="6.5" cy="9.5" r=".5" fill="currentColor"></circle></svg></a><a class="inline-flex items-center justify-center whitespace-nowrap rounded-md text-sm font-medium ring-offset-background transition-colors focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:pointer-events-none disabled:opacity-50 dark:text-slate-400 hover:bg-accent hover:text-accent-foreground h-10 py-2 px-1 md:px-2 lg:px-3" href="/discuss"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-messages-square"><path d="M14 9a2 2 0 0 1-2 2H6l-4 4V4c0-1.1.9-2 2-2h8a2 2 0 0 1 2 2z"></path><path d="M18 9h2a2 2 0 0 1 2 2v11l-4-4h-6a2 2 0 0 1-2-2v-1"></path></svg></a></div><button class="inline-flex items-center justify-center whitespace-nowrap rounded-md text-sm font-medium ring-offset-background transition-colors focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:pointer-events-none disabled:opacity-50 dark:text-slate-400 hover:bg-accent hover:text-accent-foreground h-10 py-2 px-1 md:px-2 lg:px-3" type="button" id="radix-:R1dkm:" aria-haspopup="menu" aria-expanded="false" data-state="closed"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-languages w-5 h-5"><path d="m5 8 6 6"></path><path d="m4 14 6-6 2-3"></path><path d="M2 5h12"></path><path d="M7 2h1"></path><path d="m22 22-5-10-5 10"></path><path d="M14 18h6"></path></svg><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-chevron-down w-4 h-4 hidden md:block"><path d="m6 9 6 6 6-6"></path></svg></button><div class="inline-flex items-center justify-center whitespace-nowrap rounded-md text-sm font-medium ring-offset-background transition-colors focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:pointer-events-none disabled:opacity-50 dark:text-slate-400 hover:bg-accent hover:text-accent-foreground h-10 py-2 px-1 md:px-2 lg:px-3 flex items-center space-x-1"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-sun"><circle cx="12" cy="12" r="4"></circle><path d="M12 2v2"></path><path d="M12 20v2"></path><path d="m4.93 4.93 1.41 1.41"></path><path d="m17.66 17.66 1.41 1.41"></path><path d="M2 12h2"></path><path d="M20 12h2"></path><path d="m6.34 17.66-1.41 1.41"></path><path d="m19.07 4.93-1.41 1.41"></path></svg><button type="button" role="switch" aria-checked="false" data-state="unchecked" value="off" class="peer inline-flex h-6 w-11 shrink-0 cursor-pointer items-center rounded-full border-2 border-transparent transition-colors focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 focus-visible:ring-offset-background disabled:cursor-not-allowed disabled:opacity-50 data-[state=checked]:bg-primary data-[state=unchecked]:bg-input" id="night-mode"><span data-state="unchecked" class="pointer-events-none block h-5 w-5 rounded-full bg-background shadow-lg ring-0 transition-transform data-[state=checked]:translate-x-5 data-[state=unchecked]:translate-x-0 dark:bg-slate-500"></span></button><input type="checkbox" aria-hidden="true" style="transform:translateX(-100%);position:absolute;pointer-events:none;opacity:0;margin:0" tabindex="-1" value="off"/><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-moon text-sm font-medium leading-none peer-disabled:cursor-not-allowed peer-disabled:opacity-70 hidden md:block" for="night-mode"><path d="M12 3a6 6 0 0 0 9 9 9 9 0 1 1-9-9Z"></path></svg></div></div></nav><aside id="side" class="
                        hidden md:flex flex-col 
                        w-36 h-screen
                        fixed left-0 bottom-0 
                        z-40 bg-white dark:bg-black shadow-2xl dark:shadow-gray-700         
             "><div class="h-16 grow-0"></div><div class="grow py-20 px-4 text-base font-serif dark:font-sans text-wrap overflow-x-auto utils_right2left__q5Pl7 utils_verticalLines__s6Q8n"><p id="quote-line">智力活动是一种生活态度</p><p>——<span id="quote-author">阿掖山，一个博客</span></p></div><footer class="py-1 grow-0 shrink-0 flex flex-col items-center"><div class="text-sm"><p>阿掖山<!-- --> © <!-- -->2024<br/>作者保留版权</p></div><div class="flex flex-row"><a class="inline-flex items-center justify-center whitespace-nowrap rounded-md text-sm font-medium ring-offset-background transition-colors focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:pointer-events-none disabled:opacity-50 dark:text-slate-400 hover:bg-accent hover:text-accent-foreground h-10 py-2 px-1" href="/feed.xml"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-rss stroke-[#3c70c6]"><path d="M4 11a9 9 0 0 1 9 9"></path><path d="M4 4a16 16 0 0 1 16 16"></path><circle cx="5" cy="19" r="1"></circle></svg></a><a class="inline-flex items-center justify-center whitespace-nowrap rounded-md text-sm font-medium ring-offset-background transition-colors focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:pointer-events-none disabled:opacity-50 dark:text-slate-400 hover:bg-accent hover:text-accent-foreground h-10 py-2 px-1" href="https://www.github.com/MountAye"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-github stroke-[#3c70c6]"><path d="M15 22v-4a4.8 4.8 0 0 0-1-3.5c3 0 6-2 6-5.5.08-1.25-.27-2.48-1-3.5.28-1.15.28-2.35 0-3.5 0 0-1 0-3 1.5-2.64-.5-5.36-.5-8 0C6 2 5 2 5 2c-.3 1.15-.3 2.35 0 3.5A5.403 5.403 0 0 0 4 9c0 3.5 3 5.5 6 5.5-.39.49-.68 1.05-.85 1.65-.17.6-.22 1.23-.15 1.85v4"></path><path d="M9 18c-4.51 2-5-2-7-2"></path></svg></a><a class="inline-flex items-center justify-center whitespace-nowrap rounded-md text-sm font-medium ring-offset-background transition-colors focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:pointer-events-none disabled:opacity-50 dark:text-slate-400 hover:bg-accent hover:text-accent-foreground h-10 py-2 px-1" href="https://www.twitter.com/MountAye"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-twitter stroke-[#3c70c6]"><path d="M22 4s-.7 2.1-2 3.4c1.6 10-9.4 17.3-18 11.6 2.2.1 4.4-.6 6-2C3 15.5.5 9.6 3 5c2.2 2.6 5.6 4.1 9 4-.9-4.2 4-6.6 7-3.8 1.1 0 3-1.2 3-1.2z"></path></svg></a><a class="inline-flex items-center justify-center whitespace-nowrap rounded-md text-sm font-medium ring-offset-background transition-colors focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:pointer-events-none disabled:opacity-50 dark:text-slate-400 hover:bg-accent hover:text-accent-foreground h-10 py-2 px-1" href="https://github.com/MountAye/comments/discussions/categories/blog"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-mail stroke-[#3c70c6]"><rect width="20" height="16" x="2" y="4" rx="2"></rect><path d="m22 7-8.97 5.7a1.94 1.94 0 0 1-2.06 0L2 7"></path></svg></a></div></footer></aside><div id="main" class="
                    block h-full w-screen pt-16
                    sm:w-[640px] sm:ml-auto sm:mr-0 
                    md:w-[calc(100vw-144px)]
         "><div class="rounded-lg bg-card text-card-foreground w-full md:m-auto lg:w-[800px] shadow-2xl relative top-5 border-0 dark:shadow-gray-700"><div class="rounded-lg bg-card text-card-foreground w-full h-full border-0 shadow-lg dark:shadow-gray-700" id="top" style="background-image:none;background-size:cover"><div class="p-6 pt-0 backdrop-blur-sm bg-gray-50/70 dark:bg-black/70"><div data-orientation="vertical"><div data-state="closed" data-orientation="vertical" class="border-b border-none"><h3 data-orientation="vertical" data-state="closed" class="flex"><button type="button" aria-controls="radix-:R4nkm:" aria-expanded="false" data-state="closed" data-orientation="vertical" id="radix-:Rnkm:" class="flex flex-1 items-center justify-between py-4 font-medium transition-all hover:underline [&amp;[data-state=open]&gt;svg]:rotate-180" data-radix-collection-item=""><div style="position:relative;width:100%;padding-bottom:25%" data-radix-aspect-ratio-wrapper=""><div class="h-full flex flex-col justify-end items-start" style="position:absolute;top:0;right:0;bottom:0;left:0"><p id="date">2023 年 6 月 17 日</p><h1 id="title" class="text-start text-lg sm:text-2xl md:text-4xl text-[#3c70c6] font-semibold font-serif dark:font-sans">.tex | 什么是智能≠智能是什么</h1><div id="tags" class="mt-1"><div class="inline-flex items-center rounded-full border px-2.5 py-0.5 font-semibold transition-colors focus:outline-none focus:ring-2 focus:ring-ring focus:ring-offset-2 border-transparent bg-primary hover:bg-primary/80 text-white dark:text-slate-400 text-sm mr-1 font-mono" style="background-color:#329894"><a alt="科学类说明文" href="/articles/what-is-intelligence-not-same-as-intelligence-is-what#collection-tex">tex</a></div><div class="inline-flex items-center rounded-full border px-2.5 py-0.5 font-semibold transition-colors focus:outline-none focus:ring-2 focus:ring-ring focus:ring-offset-2 border-transparent bg-primary hover:bg-primary/80 text-white dark:text-slate-400 text-sm mr-1 font-mono" style="background-color:#285191"><a alt="随笔散文" href="/articles/what-is-intelligence-not-same-as-intelligence-is-what#collection-doc">doc</a></div><div class="inline-flex items-center rounded-full border px-2.5 py-0.5 font-semibold transition-colors focus:outline-none focus:ring-2 focus:ring-ring focus:ring-offset-2 border-transparent bg-primary hover:bg-primary/80 text-white dark:text-slate-400 text-sm mr-1 font-mono" style="background-color:#ffbf00"><a alt="IT类业余笔记" href="/articles/what-is-intelligence-not-same-as-intelligence-is-what#collection-md">md</a></div><div class="inline-flex items-center rounded-full border px-2.5 py-0.5 font-semibold transition-colors focus:outline-none focus:ring-2 focus:ring-ring focus:ring-offset-2 border-transparent bg-primary hover:bg-primary/80 text-white dark:text-slate-400 text-sm mr-1 font-mono" style="background-color:#708090"><a alt="机器学习和人工智能" href="/articles/what-is-intelligence-not-same-as-intelligence-is-what#collection-ai">ai</a></div></div></div></div><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-chevron-down h-4 w-4 shrink-0 transition-transform duration-200"><path d="m6 9 6 6 6-6"></path></svg></button></h3><div data-state="closed" id="radix-:R4nkm:" hidden="" role="region" aria-labelledby="radix-:Rnkm:" data-orientation="vertical" class="overflow-hidden text-sm transition-all data-[state=closed]:animate-accordion-up data-[state=open]:animate-accordion-down" style="--radix-accordion-content-height:var(--radix-collapsible-content-height);--radix-accordion-content-width:var(--radix-collapsible-content-width)"></div></div></div></div></div><div class="hidden xl:block absolute -left-48 top-0 h-full"><div class="rounded-lg bg-card text-card-foreground sticky top-20 w-48 border-0 shadow-xl"><div class="p-6 py-4 pl-4 pr-0 text-nowrap overflow-clip text-ellipsis font-serif dark:font-sans"><div data-orientation="vertical"><div data-state="open" data-orientation="vertical" class="border-b border-none"><h3 data-orientation="vertical" data-state="open" class="flex"><button type="button" aria-controls="radix-:R57km:" aria-expanded="true" data-state="open" data-orientation="vertical" id="radix-:R17km:" class="flex flex-1 items-center justify-between font-medium transition-all hover:underline [&amp;[data-state=open]&gt;svg]:rotate-180 py-0" data-radix-collection-item=""><a href="#">目录</a><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-chevron-down h-4 w-4 shrink-0 transition-transform duration-200"><path d="m6 9 6 6 6-6"></path></svg></button></h3><div data-state="open" id="radix-:R57km:" role="region" aria-labelledby="radix-:R17km:" data-orientation="vertical" class="overflow-hidden text-sm transition-all data-[state=closed]:animate-accordion-up data-[state=open]:animate-accordion-down" style="--radix-accordion-content-height:var(--radix-collapsible-content-height);--radix-accordion-content-width:var(--radix-collapsible-content-width)"><div class="pt-0 pl-4 pb-0"><ul><li><a href="#0">0</a></li><li><div data-orientation="vertical"><div data-state="closed" data-orientation="vertical" class="border-b border-none"><h3 data-orientation="vertical" data-state="closed" class="flex"><button type="button" aria-controls="radix-:Rld7km:" aria-expanded="false" data-state="closed" data-orientation="vertical" id="radix-:R5d7km:" class="flex flex-1 items-center justify-between font-medium transition-all hover:underline [&amp;[data-state=open]&gt;svg]:rotate-180 py-0" data-radix-collection-item=""><a href="#1">1</a><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-chevron-down h-4 w-4 shrink-0 transition-transform duration-200"><path d="m6 9 6 6 6-6"></path></svg></button></h3><div data-state="closed" id="radix-:Rld7km:" hidden="" role="region" aria-labelledby="radix-:R5d7km:" data-orientation="vertical" class="overflow-hidden text-sm transition-all data-[state=closed]:animate-accordion-up data-[state=open]:animate-accordion-down" style="--radix-accordion-content-height:var(--radix-collapsible-content-height);--radix-accordion-content-width:var(--radix-collapsible-content-width)"></div></div></div></li><li><a href="#2">2</a></li><li><a href="#3">3</a></li><li><a href="#4">4</a></li></ul></div></div></div></div></div></div></div><div class="p-6 typeset_written___p4iY pt-10 pb-20"><h2 id="0"><a href="#0">0</a></h2>
<p>这是一篇酬和之作。</p>
<p>徵文标题说的是：</p>
<blockquote>
<p><strong>機器會製造「內涵」嗎？</strong></p>
</blockquote>
<p>但是正文提出的问题是：</p>
<blockquote>
<p>AI透過程式組合出回答你問題的文字組合，有「涵義」嗎？</p>
</blockquote>
<p>可是，「內涵」和「涵義」两个词的——内涵/涵义——就不完全一样啊……</p>
<p>“内涵”(connotation) 通常指词语或表达方式所隐含的情感、态度、暗示或附加的意义。它涉及到词语或表达方式所引起的情感、联想或隐含的观点。也就是弦外之音。</p>
<p>而“涵义”(meaning) 一般指词语、表达方式或行为所传达的字面意义或字面上的定义。它强调的是直接的、明确的意义。</p>
<p>看热闹不嫌事大，那我们再把问题搞复杂一点——在逻辑学里也有一个“内涵”(intension)，和“外延”(extension) 相对应。用<strong>面向对象编程</strong>的说法来理解，一个类里面定义的所有状态量和内部方法的集合，就构成这个类的“<strong>内涵</strong>”；所有（已经和将来能够）从这个类实例化出来的对象的集合，就构成这个类的“<strong>外延</strong>”。</p>
<p>所以看起来，征文者想问的是日常“内涵”也就是言外之意，但是怕杠精（比如我）用有严格定义的逻辑“内涵”解构掉，所以换了“涵义”一词。</p>
<p>这个问题很显然是因应最近大语言模型掀起的这一波 AI 浪潮。这个问题往前再问一句，就是“大语言模型是智能体/有智能吗？”</p>
<ul>
<li>前两年 DeepMind 的 AlphaGo/AlphaZero 系列 AI 在围棋中击败人类棋手时，人们也在问这个问题。</li>
<li>上世纪四五十年代专家系统 (expert system) 刚刚开发的时候，人们也在问这个问题。</li>
<li>从电子计算机往前追溯到机械计算机，甚至是巴比奇的差分机的时候，人们就已经开始问这样的问题了。</li>
</ul>
<p>这些问题求并集，然后在问题数量趋近于无穷下的极限，就是“什么是智能”。</p>
<p>这样的问题每每得不到回答，是因为它的逆问题“智能是什么”没有答案。我们并没有智能的准确定义，只能一事一论。而之前的智能和非智能体的区别太明显，以至于作出判断也不能对智能的定义有所启发。</p>
<h2 id="1"><a href="#1">1</a></h2>
<p>而对“智能是什么”的探究，哲学、逻辑学、计算机科学、生物学、管理学，不同领域的研究者有着不同的思路。</p>
<h3 id="古哲学洞穴之壁与理念世界"><a href="#古哲学洞穴之壁与理念世界">古哲学·洞穴之壁与理念世界</a></h3>
<p>古希腊哲学家柏拉图在《理想国》里提到了“洞穴之壁”的寓言故事。</p>
<p>有一群被囚禁在一个深洞的囚徒，从出生开始就被束缚在这个洞穴里，脖子和腿都被铁链锁住，没办法转身或离开。囚徒身后的洞穴入口处有一道火焰，火焰后有人持物体走过，物体的投射在洞穴内的墙壁上形成了影子。囚徒们就以为这些影子就是唯一的存在。</p>
<p>这里的囚徒代表着人类，洞穴代表着世界，影子则代表着我们对于现象世界的感知和观念。人们的知识和信念往往受限于自己的经验和感知，就像囚徒们只看到了洞穴墙壁上的影子，而在影子之外还存在一个理想的理性世界。柏拉图用这个寓言故事表达了他对于人类认识和智慧的理解。所谓智慧，就是从洞穴的影子反过去推测火把前物体的能力。</p>
<p>当然，这种思想被 Marx 主义定性为一种客观唯心主义、唯理论，是受其批判的。</p>
<h3 id="逻辑学从命题到希尔伯特算符"><a href="#逻辑学从命题到希尔伯特算符">逻辑学·从命题到希尔伯特算符</a></h3>
<p>柏拉图的学生亚里士多德，今天在低年级的物理教科书里基本是个反面典型，但他对逻辑学进行了系统化和全面的研究，提出了许多逻辑学的基本概念和原理。这些成果后来成为了欧洲哲学和逻辑学的基石，对西方哲学和科学的发展产生了深远影响。</p>
<p>所谓逻辑，就是研究命题的对错，以及如何判断命题对错的学问。而命题，就是能被判断对错的句子。但是句子显然可以再分成不同成分，于是就发明/发现了主体、客体、谓词、谓词的量词……等等概念，以及用这些概念构造命题的方法。</p>
<p>但是要注意，虽然逻辑主要由语言来表达，但是逻辑还是和语言不同，主体、客体也不等于句子的主语、宾语。这两者的区别，基本可以类比于之前洞穴之壁寓言里的实体和影子。</p>
<p>这种努力到目前为止的巅峰，基本上要数希尔伯特形式化逻辑系统了。感兴趣的朋友可以自行查阅戈得门特《代数学教程》的第一章，这玩意相当于思想界的引体向上，反正我是一个也拉不上去……</p>
<h3 id="计算机从半导体到抽象语法树"><a href="#计算机从半导体到抽象语法树">计算机·从半导体到抽象语法树</a></h3>
<p>希尔伯特是德国的数学家，《代数学教程》也是数学而不是哲学教材。显而易见，逻辑虽然由哲学家奠基，但是主导权很快落到数学家，至少是哲学家兼数学家手里了。</p>
<p>命题的“真”与“非真”同构于 {1, 0}，各种逻辑运算都可以分解成“或”与“非”两种基本逻辑运算的组合，这就是以数学家乔治·布尔 (George Boole) 命名的布尔代数。因为 {1, 0} 又可以同构于半导体电路的高低电位，和各种类似继电器的门电路组合，所以很容易用计算机在物理世界表示出这些逻辑运算。</p>
<p>我们的电脑由上亿个这样的电位和逻辑门组成，一般的科普文章应该会去介绍芯片啊光刻机之类的东西，本文关注的是另一个方面：虽然生产电脑配件的厂商很多，不同的型号的元器件设计不同，组装出的成品应该千差万别，但是他们可以运行同样的程序，理想条件下（虽然实际工程中常常不理想）我们也可以期望他们跑出同样的结果。</p>
<p>这说明所谓计算机科学，并不等同于研究计算机元件的电子科学和工程，这里电科和电子工程相当于洞穴岩壁上的影子，而计算机科学就相当于火光前的物体。这种超越物理的计算本质，一般用一种叫做“抽象语法树”的数据结构来表示。</p>
<h3 id="生物学从神经元到神经网络"><a href="#生物学从神经元到神经网络">生物学·从神经元到神经网络</a></h3>
<p>人们发明计算机的时候，基本上还是把它当作工具，就没期望它有什么主体性和智慧。</p>
<p>而随着生物学逐渐发现了神经系统及其作用，也随着物理学在二十世纪初的大发展之后的相对平静，很多物理学家开始插手其他学科。既然生命和非生命体的背后都服从同一套物理规律，既然物理学的众多成功经验说明，搞清楚构成系统的所有微观组成就可以理解宏观的系统，那么搞清楚人类的智力器官的基本单元以及相互作用，按理说也就能够理解什么是智慧。</p>
<p><img src="/photos/2023-06-17-neuron.png" alt="a cartoon illustrating a neuron"/></p>
<p>上图是一个神经细胞的结构示意图。从其他神经细胞释放出来的名为神经递质的化学物质，到达神经元左侧短且密集的树突之后，激活细胞膜表面的离子泵，主动运输离子跨过细胞膜，从而产生电信号。电信号沿细胞膜传导到右侧的树突，刺激凸触释放神经递质给下一个细胞。</p>
<p><img src="/photos/2023-06-17-perceptron.png" alt="a handdrawing style illustration of perceptron"/></p>
<p>上图就是根据神经元的工作原理抽象出的数学模型，名为 perceptron。一个 perceptron 就是一个函数，接受多个输入的自变量，加权求和之后套一个非线性的激活函数，得到一个输出。很多个这样的 perceptron 并连和串联，就构成下图，计算机算法中的神经网络。</p>
<p><img src="/photos/2023-06-17-neural-network.png" alt="a handdrawing style illustration of a neural network"/></p>
<p>而从实验方向研究神经系统，我们隔壁系就有，经常来我们系招人。基本上就是在小鼠的天灵盖上锯开一个天窗，然后给它带上个头盔，头盔上有能从天窗伸进去的电极，采集脑神经的电信号。以前头盔有网线伸到实验室天花板，实时传到数据中心的超算。现在好像进步了，改用 Wi-Fi 了。</p>
<p>这实验怎么通过的伦理审查，咱也不知道，咱也不敢问……</p>
<h3 id="管理学dikw-数据-信息-知识-智慧模型"><a href="#管理学dikw-数据-信息-知识-智慧模型">管理学·DIKW “数据-信息-知识-智慧”模型</a></h3>
<p><img src="/photos/2023-06-17-DIKW.png" alt="a pyramid of DIKW model"/></p>
<p>DIKW 四个字母分别代表 data, information, knowledge, wisdom，即数据、信息、知识、智慧，是一种知识管理中的心智模型。</p>
<p>四个层次，前一层都是后一层的基础，后一层都是对前一层的理解。</p>
<p>如果是书面文字，数据就是笔画和字母；如果是语言，数据就是人声的响度、频率和音色。由笔画/字母/声音组成的有含义的字词就是信息。表示信息之间的关系的，可以判断对错的命题就是知识。包含和统摄各条知识的思想体系，就是智慧。</p>
<p>反过来说，虽然智慧高于思想，但它仍需要通过把各条知识的表达汇总起来，才能被人感知。对知识的命题的理解依赖于构成名字的各个概念的涵义，属于信息水平的内容。而每个字都有不考虑其涵义的笔画字母构成。</p>
<p>这层与层之间<strong>看似</strong>并没有插入额外的内容，智慧可以直接由笔画构成。但是我们一层层理解的深入，其实是不自觉地借用了我们当前社会约定俗成的解读方式。</p>
<p>比如下面这个图片里的符号，对于现代人就只是数据，无法解读成信息。但是对于苏美尔人，这是用楔形文字表示的数字，是等腰直角三角形的腰和直角边的比值，也就是 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msqrt><mn>2</mn></msqrt></mrow><annotation encoding="application/x-tex">\sqrt{2}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.04em;vertical-align:-0.1328em"></span><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9072em"><span class="svg-align" style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord" style="padding-left:0.833em"><span class="mord">2</span></span></span><span style="top:-2.8672em"><span class="pstrut" style="height:3em"></span><span class="hide-tail" style="min-width:0.853em;height:1.08em"><svg xmlns="http://www.w3.org/2000/svg" width="400em" height="1.08em" viewBox="0 0 400000 1080" preserveAspectRatio="xMinYMin slice"><path d="M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z"></path></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1328em"><span></span></span></span></span></span></span></span></span> 的近似值。</p>
<p><img src="/photos/2023-06-17-ancient-root-2.png" alt="sumerian numerical approximation to square root of two"/></p>
<p>约定俗成的数据解读方式，也就是关于<strong>数据的数据</strong>，根据西方的构词法，可以叫做“<strong>元</strong>数据”(meta-data)。</p>
<p>数据和元数据一起构成信息，信息和元信息一起构成知识，知识和元知识一起构成智慧。俺坚持写博客的动机，就是用费曼学习法，把无意间使用的元知识显式地表达出来，而且记录下来，争取学而不退转。</p>
<h2 id="2"><a href="#2">2</a></h2>
<p>回顾了这些，再来看大语言模型，就会发现它落在了各方努力的延长线的交点。</p>
<p>大语言模型里有一个重要概念叫做“嵌入”(embedding)，就是把语言的基本字元 (token) 可逆地映射到一个超多维度的向量空间里。本来“国王”和“儿子”之间没办法加减乘除，但是嵌入后的向量空间里有加法和数乘，如果嵌入函数选得好，“国王”的向量 + “儿子”的向量，结果向量就约等于“王子”的向量。</p>
<p><img src="/photos/2023-06-17-vector-addition.png" alt="illustration of vector addition from wikipedia"/></p>
<p>生成式语言模型的核心就是一个超多元函数，接受前一个字嵌入后的向量作为输入，给出另一个向量作为输出，用嵌入函数的逆映射翻译成字元；再把旧的输出作为新的输入，直到输出结果是“语段结束”这样一个特殊字元为止。模型训练的过程，主要就是通过现成的语料，拟合这个超多元函数的参数。</p>
<p>从 DIKW 模型来看，语言模型操作的是最基本的数据，它的输出究竟是什么信息，是不是正确的知识，体现了多少智慧，是人根据当下的社会文化来解读的。</p>
<p>而实现 AI 的电子计算机，或是复杂生命的大脑，他们和智能之间的关系，应该就类似于具体的计算机电路和抽象语法树之间的关系。以此类比，未来的智能科学应该会成为一门独立的专业，它和计算机科学和神经生物学的区别，就像今天的电子科学与工程，和计算机科学之间的区别一样。当下神经生物学的热度，将来恐怕多半会被分流。</p>
<p>这种对字符的计算不同于逻辑运算，语言模型不判断输出结果在逻辑上的正确与错误，这既给了他啥都能说几句的 feature，又给了它经常编假消息的 bug。</p>
<p>想要改掉这种错误，引入对 AI 的纠错机制，治本之道恐怕还是诉诸于对世界的正确描述，与理论相关的还是要靠逻辑，与现实相关的还是要靠科学。</p>
<p>只不过，大语言模型提供了一种数据结构，有希望把人类已知的真理储存在一起。对这种数据结构本身的研究，有可能反过来启发科学的发展。柏拉图的洞穴之壁可能不再是一个比喻，未来更大的语言模型的，亿万维度的参数空间有希望成为洞穴门口的那团火。</p>
<p>只不过这一切都是“可能”，现在还只是 AI 的萌芽阶段，还没有足够的证据来证实或者证伪这种畅想。而且 AI 的参数量再大也是有限的，它所能表达的信息也就有限，而真理应当是无限的，就像科学一样，总要训练更新更大的模型，总要发现已知的未知，然后欣然接受更多未知的未知之存在。</p>
<p>如果电子计算机实现的 AI 独立于人类产生了意识和超出人类的智慧，很难想象他们会继续用人类语言这种对他们来说很不方便的方式来交流。</p>
<p>所以，哪怕是做个 AI 生成内容的质检员，科学家依然有事可做。这算是科学的堕落吗？当然不算，如果算的话，那从计算物理也被当作理论物理的那天起，人类就已经投降了（逃）</p>
<h2 id="3"><a href="#3">3</a></h2>
<p>现在正面来回答问题：AI透過程式組合出回答你問題的文字組合，有「涵義」嗎？</p>
<p>答：有。</p>
<p>因为语言的「涵義」来自于语言的内容，和整个社会的文化，并不来自于这句话的作者的身份。即便是人与人之间的交流，诉诸身份也是一种非形式逻辑谬误，是理性不足的表现。只有在信息不足仍不得不下结论的时候才该使用，比如法律判决时的自由心证主义和/或法定证据主义。</p>
<p>而鹿妈眼里真人鹿酱与 AI 鹿酱的区别，如果有的话，好像主要体现在动机的区别。动机这种东西，很多智慧不高的生物，比如小猫小狗都会有；而现在的 AI，似乎还没有展现出超出编程者设计的动机。编程写入的信息有限，现有 AI 的动机也就有限，鹿酱的赢面还是很大的。</p>
<p>而动机是生物与非生物的区别吗？而什么是生物 ≠ 生物是什么，那就是另一个含混而复杂的问题了。</p>
<h2 id="4"><a href="#4">4</a></h2>
<p>这篇博文发布的时候，高考应该已经结束了，马上该填报志愿了。</p>
<p>那么，西元 2023 年，AI 来袭的当下，该选个啥专业在 AI 浪潮中幸存，或者选个啥专业给 AI 老爷带路呢？</p>
<p><img src="/photos/2023-06-17-three-body-quotation.png" alt="a screenshot of a quotation from Three Body about attitudes towards aliens"/></p>
<p>我的建议是，不要听别人的建议，按自己的兴趣来就好了。</p>
<p>刚刚改开的时候，有一个超级热门的专业，叫科技英语。科技落下了好多年，对外开放需要语言交流，两者一结合应该是热门又稀缺了。结果呢，你现在还听说过这个专业吗？</p>
<p>科技很重要是不错，语言很重要也不错，但是搞科技的人自己可以学英语，学英语的有几个搞得了科技？社会的进步主要靠创新，而创新的方向难以预测，不论这种预测分析听起来多有道理。</p>
<p>如果真的找不到兴趣，那就在能力范围之内，找个难度最高的。如果想从事智力劳动，那数学含量是个不错的衡量标准；如果不排斥体力劳动，那训练时间越长越值得考虑。</p>
<p>但这只是填志愿来不及时的权宜之计，发掘兴趣是人一生的课题。</p>
<p>兴趣不是为了让你成功的时候更得意，毕竟成功的话不论做什么都很得意；</p>
<p>兴趣是为了你不成功时也可以不失意，毕竟平凡才是人生的真谛。</p></div><div class="absolute right-0 lg:-right-20 top-0 h-full"><div class="sticky top-[calc(100vh-60px)] lg:top-[calc(100vh-250px)] flex flex-row-reverse lg:flex-col"><a class="inline-flex items-center justify-center whitespace-nowrap text-sm font-medium ring-offset-background transition-colors focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:pointer-events-none disabled:opacity-50 dark:text-slate-400 text-primary-foreground hover:bg-primary/90 bg-[#3c70c6] h-fit w-fit px-2 py-2 m-2 rounded-full opacity-75 lg:opacity-100 hover:opacity-100" title="顶部" href="/articles/what-is-intelligence-not-same-as-intelligence-is-what#top"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-arrow-big-up stroke-white dark:stroke-slate-400"><path d="M9 18v-6H5l7-7 7 7h-4v6H9z"></path></svg></a><a class="inline-flex items-center justify-center whitespace-nowrap text-sm font-medium ring-offset-background transition-colors focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:pointer-events-none disabled:opacity-50 dark:text-slate-400 text-primary-foreground hover:bg-primary/90 bg-[#3c70c6] h-fit w-fit px-2 py-2 m-2 rounded-full opacity-75 lg:opacity-100 hover:opacity-100" title="评论" href="/articles/what-is-intelligence-not-same-as-intelligence-is-what#comments"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-message-circle-more stroke-white dark:stroke-slate-400"><path d="M7.9 20A9 9 0 1 0 4 16.1L2 22Z"></path><path d="M8 12h.01"></path><path d="M12 12h.01"></path><path d="M16 12h.01"></path></svg></a><button class="inline-flex items-center justify-center whitespace-nowrap text-sm font-medium ring-offset-background transition-colors focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:pointer-events-none disabled:opacity-50 dark:text-slate-400 text-primary-foreground hover:bg-primary/90 bg-[#3c70c6] h-fit w-fit px-2 py-2 m-2 rounded-full opacity-75 lg:opacity-100 hover:opacity-100" title="分享"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-share2 stroke-white dark:stroke-slate-400"><circle cx="18" cy="5" r="3"></circle><circle cx="6" cy="12" r="3"></circle><circle cx="18" cy="19" r="3"></circle><line x1="8.59" x2="15.42" y1="13.51" y2="17.49"></line><line x1="15.41" x2="8.59" y1="6.51" y2="10.49"></line></svg></button><button class="inline-flex items-center justify-center whitespace-nowrap text-sm font-medium ring-offset-background transition-colors focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:pointer-events-none disabled:opacity-50 dark:text-slate-400 text-primary-foreground hover:bg-primary/90 bg-[#3c70c6] h-fit w-fit px-2 py-2 m-2 rounded-full opacity-75 lg:opacity-100 hover:opacity-100" title="恰饭"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-hand-heart stroke-white dark:stroke-slate-400"><path d="M11 14h2a2 2 0 1 0 0-4h-3c-.6 0-1.1.2-1.4.6L3 16"></path><path d="m7 20 1.6-1.4c.3-.4.8-.6 1.4-.6h4c1.1 0 2.1-.4 2.8-1.2l4.6-4.4a2 2 0 0 0-2.75-2.91l-4.2 3.9"></path><path d="m2 15 6 6"></path><path d="M19.5 8.5c.7-.7 1.5-1.6 1.5-2.7A2.73 2.73 0 0 0 16 4a2.78 2.78 0 0 0-5 1.8c0 1.2.8 2 1.5 2.8L16 12Z"></path></svg></button></div></div></div><div class="rounded-lg bg-card text-card-foreground w-full md:m-auto lg:w-[800px] relative top-14 shadow-2xl border-0 dark:shadow-gray-700" id="collections"><div class="flex flex-col space-y-1.5 p-6">本文收录于以下合集：</div><div class="p-6 pt-0"><div data-orientation="vertical"><div data-state="closed" data-orientation="vertical" class="border-b border-none"><h3 data-orientation="vertical" data-state="closed" class="flex"><button type="button" aria-controls="radix-:Rjbkm:" aria-expanded="false" data-state="closed" data-orientation="vertical" id="collection-doc" class="flex flex-1 items-center justify-between py-4 font-medium transition-all hover:underline [&amp;[data-state=open]&gt;svg]:rotate-180" data-radix-collection-item=""><p><span class="scroll-m-20 text-2xl font-semibold tracking-tight">.<!-- -->doc</span><span class="ml-4">随笔散文</span></p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-chevron-down h-4 w-4 shrink-0 transition-transform duration-200"><path d="m6 9 6 6 6-6"></path></svg></button></h3><div data-state="closed" id="radix-:Rjbkm:" hidden="" role="region" aria-labelledby="radix-:R3bkm:" data-orientation="vertical" class="overflow-hidden text-sm transition-all data-[state=closed]:animate-accordion-up data-[state=open]:animate-accordion-down" style="--radix-accordion-content-height:var(--radix-collapsible-content-height);--radix-accordion-content-width:var(--radix-collapsible-content-width)"></div></div><div data-state="closed" data-orientation="vertical" class="border-b border-none"><h3 data-orientation="vertical" data-state="closed" class="flex"><button type="button" aria-controls="radix-:Rlbkm:" aria-expanded="false" data-state="closed" data-orientation="vertical" id="collection-md" class="flex flex-1 items-center justify-between py-4 font-medium transition-all hover:underline [&amp;[data-state=open]&gt;svg]:rotate-180" data-radix-collection-item=""><p><span class="scroll-m-20 text-2xl font-semibold tracking-tight">.<!-- -->md</span><span class="ml-4">IT类业余笔记</span></p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-chevron-down h-4 w-4 shrink-0 transition-transform duration-200"><path d="m6 9 6 6 6-6"></path></svg></button></h3><div data-state="closed" id="radix-:Rlbkm:" hidden="" role="region" aria-labelledby="radix-:R5bkm:" data-orientation="vertical" class="overflow-hidden text-sm transition-all data-[state=closed]:animate-accordion-up data-[state=open]:animate-accordion-down" style="--radix-accordion-content-height:var(--radix-collapsible-content-height);--radix-accordion-content-width:var(--radix-collapsible-content-width)"></div></div><div data-state="closed" data-orientation="vertical" class="border-b border-none"><h3 data-orientation="vertical" data-state="closed" class="flex"><button type="button" aria-controls="radix-:Rnbkm:" aria-expanded="false" data-state="closed" data-orientation="vertical" id="collection-tex" class="flex flex-1 items-center justify-between py-4 font-medium transition-all hover:underline [&amp;[data-state=open]&gt;svg]:rotate-180" data-radix-collection-item=""><p><span class="scroll-m-20 text-2xl font-semibold tracking-tight">.<!-- -->tex</span><span class="ml-4">科学类说明文</span></p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-chevron-down h-4 w-4 shrink-0 transition-transform duration-200"><path d="m6 9 6 6 6-6"></path></svg></button></h3><div data-state="closed" id="radix-:Rnbkm:" hidden="" role="region" aria-labelledby="radix-:R7bkm:" data-orientation="vertical" class="overflow-hidden text-sm transition-all data-[state=closed]:animate-accordion-up data-[state=open]:animate-accordion-down" style="--radix-accordion-content-height:var(--radix-collapsible-content-height);--radix-accordion-content-width:var(--radix-collapsible-content-width)"></div></div><div data-state="closed" data-orientation="vertical" class="border-b border-none"><h3 data-orientation="vertical" data-state="closed" class="flex"><button type="button" aria-controls="radix-:Rpbkm:" aria-expanded="false" data-state="closed" data-orientation="vertical" id="collection-ai" class="flex flex-1 items-center justify-between py-4 font-medium transition-all hover:underline [&amp;[data-state=open]&gt;svg]:rotate-180" data-radix-collection-item=""><p><span class="scroll-m-20 text-2xl font-semibold tracking-tight">.<!-- -->ai</span><span class="ml-4">机器学习和人工智能</span></p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-chevron-down h-4 w-4 shrink-0 transition-transform duration-200"><path d="m6 9 6 6 6-6"></path></svg></button></h3><div data-state="closed" id="radix-:Rpbkm:" hidden="" role="region" aria-labelledby="radix-:R9bkm:" data-orientation="vertical" class="overflow-hidden text-sm transition-all data-[state=closed]:animate-accordion-up data-[state=open]:animate-accordion-down" style="--radix-accordion-content-height:var(--radix-collapsible-content-height);--radix-accordion-content-width:var(--radix-collapsible-content-width)"></div></div></div></div></div><div class="rounded-lg bg-card text-card-foreground w-full md:m-auto lg:w-[800px] relative top-20 shadow-2xl border-0 dark:shadow-gray-700" id="comments"><div class="p-6 pt-0"><div id="giscus" class="pt-16 utils_giscus__lppxx"></div></div></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"metadata":{"slug":"what-is-intelligence-not-same-as-intelligence-is-what","filename":"2023-06-17-what-is-intelligence-not-same-as-intelligence-is-what.md","date":"2023-06-17","title":".tex | 什么是智能≠智能是什么","layout":"post","keywords":["tex","doc","md","ai"],"hasMath":true,"excerpt":"“什么是智能”的问题每每得不到回答，是因为它的逆问题“智能是什么”没有答案。","content":"\n## 0\n\n这是一篇酬和之作。\n\n徵文标题说的是：\n\n\u003e **機器會製造「內涵」嗎？**\n\u003e \n\n但是正文提出的问题是：\n\n\u003e AI透過程式組合出回答你問題的文字組合，有「涵義」嗎？\n\u003e \n\n可是，「內涵」和「涵義」两个词的——内涵/涵义——就不完全一样啊……\n\n“内涵”(connotation) 通常指词语或表达方式所隐含的情感、态度、暗示或附加的意义。它涉及到词语或表达方式所引起的情感、联想或隐含的观点。也就是弦外之音。\n\n而“涵义”(meaning) 一般指词语、表达方式或行为所传达的字面意义或字面上的定义。它强调的是直接的、明确的意义。\n\n看热闹不嫌事大，那我们再把问题搞复杂一点——在逻辑学里也有一个“内涵”(intension)，和“外延”(extension) 相对应。用**面向对象编程**的说法来理解，一个类里面定义的所有状态量和内部方法的集合，就构成这个类的“**内涵**”；所有（已经和将来能够）从这个类实例化出来的对象的集合，就构成这个类的“**外延**”。\n\n所以看起来，征文者想问的是日常“内涵”也就是言外之意，但是怕杠精（比如我）用有严格定义的逻辑“内涵”解构掉，所以换了“涵义”一词。\n\n这个问题很显然是因应最近大语言模型掀起的这一波 AI 浪潮。这个问题往前再问一句，就是“大语言模型是智能体/有智能吗？”\n\n- 前两年 DeepMind 的 AlphaGo/AlphaZero 系列 AI 在围棋中击败人类棋手时，人们也在问这个问题。\n- 上世纪四五十年代专家系统 (expert system) 刚刚开发的时候，人们也在问这个问题。\n- 从电子计算机往前追溯到机械计算机，甚至是巴比奇的差分机的时候，人们就已经开始问这样的问题了。\n\n这些问题求并集，然后在问题数量趋近于无穷下的极限，就是“什么是智能”。\n\n这样的问题每每得不到回答，是因为它的逆问题“智能是什么”没有答案。我们并没有智能的准确定义，只能一事一论。而之前的智能和非智能体的区别太明显，以至于作出判断也不能对智能的定义有所启发。\n\n## 1\n\n而对“智能是什么”的探究，哲学、逻辑学、计算机科学、生物学、管理学，不同领域的研究者有着不同的思路。\n\n### 古哲学·洞穴之壁与理念世界\n\n古希腊哲学家柏拉图在《理想国》里提到了“洞穴之壁”的寓言故事。\n\n有一群被囚禁在一个深洞的囚徒，从出生开始就被束缚在这个洞穴里，脖子和腿都被铁链锁住，没办法转身或离开。囚徒身后的洞穴入口处有一道火焰，火焰后有人持物体走过，物体的投射在洞穴内的墙壁上形成了影子。囚徒们就以为这些影子就是唯一的存在。\n\n这里的囚徒代表着人类，洞穴代表着世界，影子则代表着我们对于现象世界的感知和观念。人们的知识和信念往往受限于自己的经验和感知，就像囚徒们只看到了洞穴墙壁上的影子，而在影子之外还存在一个理想的理性世界。柏拉图用这个寓言故事表达了他对于人类认识和智慧的理解。所谓智慧，就是从洞穴的影子反过去推测火把前物体的能力。\n\n当然，这种思想被 Marx 主义定性为一种客观唯心主义、唯理论，是受其批判的。\n\n### 逻辑学·从命题到希尔伯特算符\n\n柏拉图的学生亚里士多德，今天在低年级的物理教科书里基本是个反面典型，但他对逻辑学进行了系统化和全面的研究，提出了许多逻辑学的基本概念和原理。这些成果后来成为了欧洲哲学和逻辑学的基石，对西方哲学和科学的发展产生了深远影响。\n\n所谓逻辑，就是研究命题的对错，以及如何判断命题对错的学问。而命题，就是能被判断对错的句子。但是句子显然可以再分成不同成分，于是就发明/发现了主体、客体、谓词、谓词的量词……等等概念，以及用这些概念构造命题的方法。\n\n但是要注意，虽然逻辑主要由语言来表达，但是逻辑还是和语言不同，主体、客体也不等于句子的主语、宾语。这两者的区别，基本可以类比于之前洞穴之壁寓言里的实体和影子。\n\n这种努力到目前为止的巅峰，基本上要数希尔伯特形式化逻辑系统了。感兴趣的朋友可以自行查阅戈得门特《代数学教程》的第一章，这玩意相当于思想界的引体向上，反正我是一个也拉不上去……\n\n### 计算机·从半导体到抽象语法树\n\n希尔伯特是德国的数学家，《代数学教程》也是数学而不是哲学教材。显而易见，逻辑虽然由哲学家奠基，但是主导权很快落到数学家，至少是哲学家兼数学家手里了。\n\n命题的“真”与“非真”同构于 {1, 0}，各种逻辑运算都可以分解成“或”与“非”两种基本逻辑运算的组合，这就是以数学家乔治·布尔 (George Boole) 命名的布尔代数。因为 {1, 0} 又可以同构于半导体电路的高低电位，和各种类似继电器的门电路组合，所以很容易用计算机在物理世界表示出这些逻辑运算。\n\n我们的电脑由上亿个这样的电位和逻辑门组成，一般的科普文章应该会去介绍芯片啊光刻机之类的东西，本文关注的是另一个方面：虽然生产电脑配件的厂商很多，不同的型号的元器件设计不同，组装出的成品应该千差万别，但是他们可以运行同样的程序，理想条件下（虽然实际工程中常常不理想）我们也可以期望他们跑出同样的结果。\n\n这说明所谓计算机科学，并不等同于研究计算机元件的电子科学和工程，这里电科和电子工程相当于洞穴岩壁上的影子，而计算机科学就相当于火光前的物体。这种超越物理的计算本质，一般用一种叫做“抽象语法树”的数据结构来表示。\n\n### 生物学·从神经元到神经网络\n\n人们发明计算机的时候，基本上还是把它当作工具，就没期望它有什么主体性和智慧。\n\n而随着生物学逐渐发现了神经系统及其作用，也随着物理学在二十世纪初的大发展之后的相对平静，很多物理学家开始插手其他学科。既然生命和非生命体的背后都服从同一套物理规律，既然物理学的众多成功经验说明，搞清楚构成系统的所有微观组成就可以理解宏观的系统，那么搞清楚人类的智力器官的基本单元以及相互作用，按理说也就能够理解什么是智慧。\n\n![a cartoon illustrating a neuron](/photos/2023-06-17-neuron.png)\n\n上图是一个神经细胞的结构示意图。从其他神经细胞释放出来的名为神经递质的化学物质，到达神经元左侧短且密集的树突之后，激活细胞膜表面的离子泵，主动运输离子跨过细胞膜，从而产生电信号。电信号沿细胞膜传导到右侧的树突，刺激凸触释放神经递质给下一个细胞。\n\n![a handdrawing style illustration of perceptron](/photos/2023-06-17-perceptron.png)\n\n上图就是根据神经元的工作原理抽象出的数学模型，名为 perceptron。一个 perceptron 就是一个函数，接受多个输入的自变量，加权求和之后套一个非线性的激活函数，得到一个输出。很多个这样的 perceptron 并连和串联，就构成下图，计算机算法中的神经网络。\n\n![a handdrawing style illustration of a neural network](/photos/2023-06-17-neural-network.png)\n\n而从实验方向研究神经系统，我们隔壁系就有，经常来我们系招人。基本上就是在小鼠的天灵盖上锯开一个天窗，然后给它带上个头盔，头盔上有能从天窗伸进去的电极，采集脑神经的电信号。以前头盔有网线伸到实验室天花板，实时传到数据中心的超算。现在好像进步了，改用 Wi-Fi 了。\n\n这实验怎么通过的伦理审查，咱也不知道，咱也不敢问……\n\n### 管理学·DIKW “数据-信息-知识-智慧”模型\n\n![a pyramid of DIKW model](/photos/2023-06-17-DIKW.png)\n\nDIKW 四个字母分别代表 data, information, knowledge, wisdom，即数据、信息、知识、智慧，是一种知识管理中的心智模型。\n\n四个层次，前一层都是后一层的基础，后一层都是对前一层的理解。\n\n如果是书面文字，数据就是笔画和字母；如果是语言，数据就是人声的响度、频率和音色。由笔画/字母/声音组成的有含义的字词就是信息。表示信息之间的关系的，可以判断对错的命题就是知识。包含和统摄各条知识的思想体系，就是智慧。\n\n反过来说，虽然智慧高于思想，但它仍需要通过把各条知识的表达汇总起来，才能被人感知。对知识的命题的理解依赖于构成名字的各个概念的涵义，属于信息水平的内容。而每个字都有不考虑其涵义的笔画字母构成。\n\n这层与层之间**看似**并没有插入额外的内容，智慧可以直接由笔画构成。但是我们一层层理解的深入，其实是不自觉地借用了我们当前社会约定俗成的解读方式。\n\n比如下面这个图片里的符号，对于现代人就只是数据，无法解读成信息。但是对于苏美尔人，这是用楔形文字表示的数字，是等腰直角三角形的腰和直角边的比值，也就是 $$\\sqrt{2}$$ 的近似值。\n\n![sumerian numerical approximation to square root of two](/photos/2023-06-17-ancient-root-2.png)\n\n约定俗成的数据解读方式，也就是关于**数据的数据**，根据西方的构词法，可以叫做“**元**数据”(meta-data)。\n\n数据和元数据一起构成信息，信息和元信息一起构成知识，知识和元知识一起构成智慧。俺坚持写博客的动机，就是用费曼学习法，把无意间使用的元知识显式地表达出来，而且记录下来，争取学而不退转。\n\n## 2\n\n回顾了这些，再来看大语言模型，就会发现它落在了各方努力的延长线的交点。\n\n大语言模型里有一个重要概念叫做“嵌入”(embedding)，就是把语言的基本字元 (token) 可逆地映射到一个超多维度的向量空间里。本来“国王”和“儿子”之间没办法加减乘除，但是嵌入后的向量空间里有加法和数乘，如果嵌入函数选得好，“国王”的向量 + “儿子”的向量，结果向量就约等于“王子”的向量。\n\n![illustration of vector addition from wikipedia](/photos/2023-06-17-vector-addition.png)\n\n生成式语言模型的核心就是一个超多元函数，接受前一个字嵌入后的向量作为输入，给出另一个向量作为输出，用嵌入函数的逆映射翻译成字元；再把旧的输出作为新的输入，直到输出结果是“语段结束”这样一个特殊字元为止。模型训练的过程，主要就是通过现成的语料，拟合这个超多元函数的参数。\n\n从 DIKW 模型来看，语言模型操作的是最基本的数据，它的输出究竟是什么信息，是不是正确的知识，体现了多少智慧，是人根据当下的社会文化来解读的。\n\n而实现 AI 的电子计算机，或是复杂生命的大脑，他们和智能之间的关系，应该就类似于具体的计算机电路和抽象语法树之间的关系。以此类比，未来的智能科学应该会成为一门独立的专业，它和计算机科学和神经生物学的区别，就像今天的电子科学与工程，和计算机科学之间的区别一样。当下神经生物学的热度，将来恐怕多半会被分流。\n\n这种对字符的计算不同于逻辑运算，语言模型不判断输出结果在逻辑上的正确与错误，这既给了他啥都能说几句的 feature，又给了它经常编假消息的 bug。\n\n想要改掉这种错误，引入对 AI 的纠错机制，治本之道恐怕还是诉诸于对世界的正确描述，与理论相关的还是要靠逻辑，与现实相关的还是要靠科学。\n\n只不过，大语言模型提供了一种数据结构，有希望把人类已知的真理储存在一起。对这种数据结构本身的研究，有可能反过来启发科学的发展。柏拉图的洞穴之壁可能不再是一个比喻，未来更大的语言模型的，亿万维度的参数空间有希望成为洞穴门口的那团火。\n\n只不过这一切都是“可能”，现在还只是 AI 的萌芽阶段，还没有足够的证据来证实或者证伪这种畅想。而且 AI 的参数量再大也是有限的，它所能表达的信息也就有限，而真理应当是无限的，就像科学一样，总要训练更新更大的模型，总要发现已知的未知，然后欣然接受更多未知的未知之存在。\n\n如果电子计算机实现的 AI 独立于人类产生了意识和超出人类的智慧，很难想象他们会继续用人类语言这种对他们来说很不方便的方式来交流。\n\n所以，哪怕是做个 AI 生成内容的质检员，科学家依然有事可做。这算是科学的堕落吗？当然不算，如果算的话，那从计算物理也被当作理论物理的那天起，人类就已经投降了（逃）\n\n## 3\n\n现在正面来回答问题：AI透過程式組合出回答你問題的文字組合，有「涵義」嗎？\n\n答：有。\n\n因为语言的「涵義」来自于语言的内容，和整个社会的文化，并不来自于这句话的作者的身份。即便是人与人之间的交流，诉诸身份也是一种非形式逻辑谬误，是理性不足的表现。只有在信息不足仍不得不下结论的时候才该使用，比如法律判决时的自由心证主义和/或法定证据主义。\n\n而鹿妈眼里真人鹿酱与 AI 鹿酱的区别，如果有的话，好像主要体现在动机的区别。动机这种东西，很多智慧不高的生物，比如小猫小狗都会有；而现在的 AI，似乎还没有展现出超出编程者设计的动机。编程写入的信息有限，现有 AI 的动机也就有限，鹿酱的赢面还是很大的。\n\n而动机是生物与非生物的区别吗？而什么是生物 ≠ 生物是什么，那就是另一个含混而复杂的问题了。\n\n## 4\n\n这篇博文发布的时候，高考应该已经结束了，马上该填报志愿了。\n\n那么，西元 2023 年，AI 来袭的当下，该选个啥专业在 AI 浪潮中幸存，或者选个啥专业给 AI 老爷带路呢？\n\n![a screenshot of a quotation from Three Body about attitudes towards aliens](/photos/2023-06-17-three-body-quotation.png)\n\n我的建议是，不要听别人的建议，按自己的兴趣来就好了。\n\n刚刚改开的时候，有一个超级热门的专业，叫科技英语。科技落下了好多年，对外开放需要语言交流，两者一结合应该是热门又稀缺了。结果呢，你现在还听说过这个专业吗？\n\n科技很重要是不错，语言很重要也不错，但是搞科技的人自己可以学英语，学英语的有几个搞得了科技？社会的进步主要靠创新，而创新的方向难以预测，不论这种预测分析听起来多有道理。\n\n如果真的找不到兴趣，那就在能力范围之内，找个难度最高的。如果想从事智力劳动，那数学含量是个不错的衡量标准；如果不排斥体力劳动，那训练时间越长越值得考虑。\n\n但这只是填志愿来不及时的权宜之计，发掘兴趣是人一生的课题。\n\n兴趣不是为了让你成功的时候更得意，毕竟成功的话不论做什么都很得意；\n\n兴趣是为了你不成功时也可以不失意，毕竟平凡才是人生的真谛。\n"},"tocStr":"{\"index\":null,\"endIndex\":null,\"map\":{\"type\":\"list\",\"ordered\":false,\"spread\":true,\"children\":[{\"type\":\"listItem\",\"spread\":false,\"children\":[{\"type\":\"paragraph\",\"children\":[{\"type\":\"link\",\"title\":null,\"url\":\"#0\",\"children\":[{\"type\":\"text\",\"value\":\"0\"}]}]}]},{\"type\":\"listItem\",\"spread\":true,\"children\":[{\"type\":\"paragraph\",\"children\":[{\"type\":\"link\",\"title\":null,\"url\":\"#1\",\"children\":[{\"type\":\"text\",\"value\":\"1\"}]}]},{\"type\":\"list\",\"ordered\":false,\"spread\":false,\"children\":[{\"type\":\"listItem\",\"spread\":false,\"children\":[{\"type\":\"paragraph\",\"children\":[{\"type\":\"link\",\"title\":null,\"url\":\"#古哲学洞穴之壁与理念世界\",\"children\":[{\"type\":\"text\",\"value\":\"古哲学·洞穴之壁与理念世界\"}]}]}]},{\"type\":\"listItem\",\"spread\":false,\"children\":[{\"type\":\"paragraph\",\"children\":[{\"type\":\"link\",\"title\":null,\"url\":\"#逻辑学从命题到希尔伯特算符\",\"children\":[{\"type\":\"text\",\"value\":\"逻辑学·从命题到希尔伯特算符\"}]}]}]},{\"type\":\"listItem\",\"spread\":false,\"children\":[{\"type\":\"paragraph\",\"children\":[{\"type\":\"link\",\"title\":null,\"url\":\"#计算机从半导体到抽象语法树\",\"children\":[{\"type\":\"text\",\"value\":\"计算机·从半导体到抽象语法树\"}]}]}]},{\"type\":\"listItem\",\"spread\":false,\"children\":[{\"type\":\"paragraph\",\"children\":[{\"type\":\"link\",\"title\":null,\"url\":\"#生物学从神经元到神经网络\",\"children\":[{\"type\":\"text\",\"value\":\"生物学·从神经元到神经网络\"}]}]}]},{\"type\":\"listItem\",\"spread\":false,\"children\":[{\"type\":\"paragraph\",\"children\":[{\"type\":\"link\",\"title\":null,\"url\":\"#管理学dikw-数据-信息-知识-智慧模型\",\"children\":[{\"type\":\"text\",\"value\":\"管理学·DIKW “数据-信息-知识-智慧”模型\"}]}]}]}]}]},{\"type\":\"listItem\",\"spread\":false,\"children\":[{\"type\":\"paragraph\",\"children\":[{\"type\":\"link\",\"title\":null,\"url\":\"#2\",\"children\":[{\"type\":\"text\",\"value\":\"2\"}]}]}]},{\"type\":\"listItem\",\"spread\":false,\"children\":[{\"type\":\"paragraph\",\"children\":[{\"type\":\"link\",\"title\":null,\"url\":\"#3\",\"children\":[{\"type\":\"text\",\"value\":\"3\"}]}]}]},{\"type\":\"listItem\",\"spread\":false,\"children\":[{\"type\":\"paragraph\",\"children\":[{\"type\":\"link\",\"title\":null,\"url\":\"#4\",\"children\":[{\"type\":\"text\",\"value\":\"4\"}]}]}]}]}}","htmlAst":{"type":"root","children":[{"type":"element","tagName":"h2","properties":{"id":"0"},"children":[{"type":"element","tagName":"a","properties":{"href":"#0"},"children":[{"type":"text","value":"0","position":{"start":{"line":2,"column":4,"offset":4},"end":{"line":2,"column":5,"offset":5}}}]}],"position":{"start":{"line":2,"column":1,"offset":1},"end":{"line":2,"column":5,"offset":5}}},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"这是一篇酬和之作。","position":{"start":{"line":4,"column":1,"offset":7},"end":{"line":4,"column":10,"offset":16}}}],"position":{"start":{"line":4,"column":1,"offset":7},"end":{"line":4,"column":10,"offset":16}}},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"徵文标题说的是：","position":{"start":{"line":6,"column":1,"offset":18},"end":{"line":6,"column":9,"offset":26}}}],"position":{"start":{"line":6,"column":1,"offset":18},"end":{"line":6,"column":9,"offset":26}}},{"type":"text","value":"\n"},{"type":"element","tagName":"blockquote","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"element","tagName":"strong","properties":{},"children":[{"type":"text","value":"機器會製造「內涵」嗎？","position":{"start":{"line":8,"column":5,"offset":32},"end":{"line":8,"column":16,"offset":43}}}],"position":{"start":{"line":8,"column":3,"offset":30},"end":{"line":8,"column":18,"offset":45}}}],"position":{"start":{"line":8,"column":3,"offset":30},"end":{"line":8,"column":18,"offset":45}}},{"type":"text","value":"\n"}],"position":{"start":{"line":8,"column":1,"offset":28},"end":{"line":9,"column":3,"offset":48}}},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"但是正文提出的问题是：","position":{"start":{"line":11,"column":1,"offset":50},"end":{"line":11,"column":12,"offset":61}}}],"position":{"start":{"line":11,"column":1,"offset":50},"end":{"line":11,"column":12,"offset":61}}},{"type":"text","value":"\n"},{"type":"element","tagName":"blockquote","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"AI透過程式組合出回答你問題的文字組合，有「涵義」嗎？","position":{"start":{"line":13,"column":3,"offset":65},"end":{"line":13,"column":30,"offset":92}}}],"position":{"start":{"line":13,"column":3,"offset":65},"end":{"line":13,"column":30,"offset":92}}},{"type":"text","value":"\n"}],"position":{"start":{"line":13,"column":1,"offset":63},"end":{"line":14,"column":3,"offset":95}}},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"可是，「內涵」和「涵義」两个词的——内涵/涵义——就不完全一样啊……","position":{"start":{"line":16,"column":1,"offset":97},"end":{"line":16,"column":35,"offset":131}}}],"position":{"start":{"line":16,"column":1,"offset":97},"end":{"line":16,"column":35,"offset":131}}},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"“内涵”(connotation) 通常指词语或表达方式所隐含的情感、态度、暗示或附加的意义。它涉及到词语或表达方式所引起的情感、联想或隐含的观点。也就是弦外之音。","position":{"start":{"line":18,"column":1,"offset":133},"end":{"line":18,"column":83,"offset":215}}}],"position":{"start":{"line":18,"column":1,"offset":133},"end":{"line":18,"column":83,"offset":215}}},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"而“涵义”(meaning) 一般指词语、表达方式或行为所传达的字面意义或字面上的定义。它强调的是直接的、明确的意义。","position":{"start":{"line":20,"column":1,"offset":217},"end":{"line":20,"column":60,"offset":276}}}],"position":{"start":{"line":20,"column":1,"offset":217},"end":{"line":20,"column":60,"offset":276}}},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"看热闹不嫌事大，那我们再把问题搞复杂一点——在逻辑学里也有一个“内涵”(intension)，和“外延”(extension) 相对应。用","position":{"start":{"line":22,"column":1,"offset":278},"end":{"line":22,"column":70,"offset":347}}},{"type":"element","tagName":"strong","properties":{},"children":[{"type":"text","value":"面向对象编程","position":{"start":{"line":22,"column":72,"offset":349},"end":{"line":22,"column":78,"offset":355}}}],"position":{"start":{"line":22,"column":70,"offset":347},"end":{"line":22,"column":80,"offset":357}}},{"type":"text","value":"的说法来理解，一个类里面定义的所有状态量和内部方法的集合，就构成这个类的“","position":{"start":{"line":22,"column":80,"offset":357},"end":{"line":22,"column":117,"offset":394}}},{"type":"element","tagName":"strong","properties":{},"children":[{"type":"text","value":"内涵","position":{"start":{"line":22,"column":119,"offset":396},"end":{"line":22,"column":121,"offset":398}}}],"position":{"start":{"line":22,"column":117,"offset":394},"end":{"line":22,"column":123,"offset":400}}},{"type":"text","value":"”；所有（已经和将来能够）从这个类实例化出来的对象的集合，就构成这个类的“","position":{"start":{"line":22,"column":123,"offset":400},"end":{"line":22,"column":160,"offset":437}}},{"type":"element","tagName":"strong","properties":{},"children":[{"type":"text","value":"外延","position":{"start":{"line":22,"column":162,"offset":439},"end":{"line":22,"column":164,"offset":441}}}],"position":{"start":{"line":22,"column":160,"offset":437},"end":{"line":22,"column":166,"offset":443}}},{"type":"text","value":"”。","position":{"start":{"line":22,"column":166,"offset":443},"end":{"line":22,"column":168,"offset":445}}}],"position":{"start":{"line":22,"column":1,"offset":278},"end":{"line":22,"column":168,"offset":445}}},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"所以看起来，征文者想问的是日常“内涵”也就是言外之意，但是怕杠精（比如我）用有严格定义的逻辑“内涵”解构掉，所以换了“涵义”一词。","position":{"start":{"line":24,"column":1,"offset":447},"end":{"line":24,"column":66,"offset":512}}}],"position":{"start":{"line":24,"column":1,"offset":447},"end":{"line":24,"column":66,"offset":512}}},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"这个问题很显然是因应最近大语言模型掀起的这一波 AI 浪潮。这个问题往前再问一句，就是“大语言模型是智能体/有智能吗？”","position":{"start":{"line":26,"column":1,"offset":514},"end":{"line":26,"column":61,"offset":574}}}],"position":{"start":{"line":26,"column":1,"offset":514},"end":{"line":26,"column":61,"offset":574}}},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"前两年 DeepMind 的 AlphaGo/AlphaZero 系列 AI 在围棋中击败人类棋手时，人们也在问这个问题。","position":{"start":{"line":28,"column":3,"offset":578},"end":{"line":28,"column":64,"offset":639}}}],"position":{"start":{"line":28,"column":1,"offset":576},"end":{"line":28,"column":64,"offset":639}}},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"上世纪四五十年代专家系统 (expert system) 刚刚开发的时候，人们也在问这个问题。","position":{"start":{"line":29,"column":3,"offset":642},"end":{"line":29,"column":50,"offset":689}}}],"position":{"start":{"line":29,"column":1,"offset":640},"end":{"line":29,"column":50,"offset":689}}},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"从电子计算机往前追溯到机械计算机，甚至是巴比奇的差分机的时候，人们就已经开始问这样的问题了。","position":{"start":{"line":30,"column":3,"offset":692},"end":{"line":30,"column":49,"offset":738}}}],"position":{"start":{"line":30,"column":1,"offset":690},"end":{"line":30,"column":49,"offset":738}}},{"type":"text","value":"\n"}],"position":{"start":{"line":28,"column":1,"offset":576},"end":{"line":30,"column":49,"offset":738}}},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"这些问题求并集，然后在问题数量趋近于无穷下的极限，就是“什么是智能”。","position":{"start":{"line":32,"column":1,"offset":740},"end":{"line":32,"column":36,"offset":775}}}],"position":{"start":{"line":32,"column":1,"offset":740},"end":{"line":32,"column":36,"offset":775}}},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"这样的问题每每得不到回答，是因为它的逆问题“智能是什么”没有答案。我们并没有智能的准确定义，只能一事一论。而之前的智能和非智能体的区别太明显，以至于作出判断也不能对智能的定义有所启发。","position":{"start":{"line":34,"column":1,"offset":777},"end":{"line":34,"column":93,"offset":869}}}],"position":{"start":{"line":34,"column":1,"offset":777},"end":{"line":34,"column":93,"offset":869}}},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{"id":"1"},"children":[{"type":"element","tagName":"a","properties":{"href":"#1"},"children":[{"type":"text","value":"1","position":{"start":{"line":36,"column":4,"offset":874},"end":{"line":36,"column":5,"offset":875}}}]}],"position":{"start":{"line":36,"column":1,"offset":871},"end":{"line":36,"column":5,"offset":875}}},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"而对“智能是什么”的探究，哲学、逻辑学、计算机科学、生物学、管理学，不同领域的研究者有着不同的思路。","position":{"start":{"line":38,"column":1,"offset":877},"end":{"line":38,"column":51,"offset":927}}}],"position":{"start":{"line":38,"column":1,"offset":877},"end":{"line":38,"column":51,"offset":927}}},{"type":"text","value":"\n"},{"type":"element","tagName":"h3","properties":{"id":"古哲学洞穴之壁与理念世界"},"children":[{"type":"element","tagName":"a","properties":{"href":"#古哲学洞穴之壁与理念世界"},"children":[{"type":"text","value":"古哲学·洞穴之壁与理念世界","position":{"start":{"line":40,"column":5,"offset":933},"end":{"line":40,"column":18,"offset":946}}}]}],"position":{"start":{"line":40,"column":1,"offset":929},"end":{"line":40,"column":18,"offset":946}}},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"古希腊哲学家柏拉图在《理想国》里提到了“洞穴之壁”的寓言故事。","position":{"start":{"line":42,"column":1,"offset":948},"end":{"line":42,"column":32,"offset":979}}}],"position":{"start":{"line":42,"column":1,"offset":948},"end":{"line":42,"column":32,"offset":979}}},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"有一群被囚禁在一个深洞的囚徒，从出生开始就被束缚在这个洞穴里，脖子和腿都被铁链锁住，没办法转身或离开。囚徒身后的洞穴入口处有一道火焰，火焰后有人持物体走过，物体的投射在洞穴内的墙壁上形成了影子。囚徒们就以为这些影子就是唯一的存在。","position":{"start":{"line":44,"column":1,"offset":981},"end":{"line":44,"column":116,"offset":1096}}}],"position":{"start":{"line":44,"column":1,"offset":981},"end":{"line":44,"column":116,"offset":1096}}},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"这里的囚徒代表着人类，洞穴代表着世界，影子则代表着我们对于现象世界的感知和观念。人们的知识和信念往往受限于自己的经验和感知，就像囚徒们只看到了洞穴墙壁上的影子，而在影子之外还存在一个理想的理性世界。柏拉图用这个寓言故事表达了他对于人类认识和智慧的理解。所谓智慧，就是从洞穴的影子反过去推测火把前物体的能力。","position":{"start":{"line":46,"column":1,"offset":1098},"end":{"line":46,"column":154,"offset":1251}}}],"position":{"start":{"line":46,"column":1,"offset":1098},"end":{"line":46,"column":154,"offset":1251}}},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"当然，这种思想被 Marx 主义定性为一种客观唯心主义、唯理论，是受其批判的。","position":{"start":{"line":48,"column":1,"offset":1253},"end":{"line":48,"column":40,"offset":1292}}}],"position":{"start":{"line":48,"column":1,"offset":1253},"end":{"line":48,"column":40,"offset":1292}}},{"type":"text","value":"\n"},{"type":"element","tagName":"h3","properties":{"id":"逻辑学从命题到希尔伯特算符"},"children":[{"type":"element","tagName":"a","properties":{"href":"#逻辑学从命题到希尔伯特算符"},"children":[{"type":"text","value":"逻辑学·从命题到希尔伯特算符","position":{"start":{"line":50,"column":5,"offset":1298},"end":{"line":50,"column":19,"offset":1312}}}]}],"position":{"start":{"line":50,"column":1,"offset":1294},"end":{"line":50,"column":19,"offset":1312}}},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"柏拉图的学生亚里士多德，今天在低年级的物理教科书里基本是个反面典型，但他对逻辑学进行了系统化和全面的研究，提出了许多逻辑学的基本概念和原理。这些成果后来成为了欧洲哲学和逻辑学的基石，对西方哲学和科学的发展产生了深远影响。","position":{"start":{"line":52,"column":1,"offset":1314},"end":{"line":52,"column":111,"offset":1424}}}],"position":{"start":{"line":52,"column":1,"offset":1314},"end":{"line":52,"column":111,"offset":1424}}},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"所谓逻辑，就是研究命题的对错，以及如何判断命题对错的学问。而命题，就是能被判断对错的句子。但是句子显然可以再分成不同成分，于是就发明/发现了主体、客体、谓词、谓词的量词……等等概念，以及用这些概念构造命题的方法。","position":{"start":{"line":54,"column":1,"offset":1426},"end":{"line":54,"column":107,"offset":1532}}}],"position":{"start":{"line":54,"column":1,"offset":1426},"end":{"line":54,"column":107,"offset":1532}}},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"但是要注意，虽然逻辑主要由语言来表达，但是逻辑还是和语言不同，主体、客体也不等于句子的主语、宾语。这两者的区别，基本可以类比于之前洞穴之壁寓言里的实体和影子。","position":{"start":{"line":56,"column":1,"offset":1534},"end":{"line":56,"column":80,"offset":1613}}}],"position":{"start":{"line":56,"column":1,"offset":1534},"end":{"line":56,"column":80,"offset":1613}}},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"这种努力到目前为止的巅峰，基本上要数希尔伯特形式化逻辑系统了。感兴趣的朋友可以自行查阅戈得门特《代数学教程》的第一章，这玩意相当于思想界的引体向上，反正我是一个也拉不上去……","position":{"start":{"line":58,"column":1,"offset":1615},"end":{"line":58,"column":88,"offset":1702}}}],"position":{"start":{"line":58,"column":1,"offset":1615},"end":{"line":58,"column":88,"offset":1702}}},{"type":"text","value":"\n"},{"type":"element","tagName":"h3","properties":{"id":"计算机从半导体到抽象语法树"},"children":[{"type":"element","tagName":"a","properties":{"href":"#计算机从半导体到抽象语法树"},"children":[{"type":"text","value":"计算机·从半导体到抽象语法树","position":{"start":{"line":60,"column":5,"offset":1708},"end":{"line":60,"column":19,"offset":1722}}}]}],"position":{"start":{"line":60,"column":1,"offset":1704},"end":{"line":60,"column":19,"offset":1722}}},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"希尔伯特是德国的数学家，《代数学教程》也是数学而不是哲学教材。显而易见，逻辑虽然由哲学家奠基，但是主导权很快落到数学家，至少是哲学家兼数学家手里了。","position":{"start":{"line":62,"column":1,"offset":1724},"end":{"line":62,"column":75,"offset":1798}}}],"position":{"start":{"line":62,"column":1,"offset":1724},"end":{"line":62,"column":75,"offset":1798}}},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"命题的“真”与“非真”同构于 {1, 0}，各种逻辑运算都可以分解成“或”与“非”两种基本逻辑运算的组合，这就是以数学家乔治·布尔 (George Boole) 命名的布尔代数。因为 {1, 0} 又可以同构于半导体电路的高低电位，和各种类似继电器的门电路组合，所以很容易用计算机在物理世界表示出这些逻辑运算。","position":{"start":{"line":64,"column":1,"offset":1800},"end":{"line":64,"column":156,"offset":1955}}}],"position":{"start":{"line":64,"column":1,"offset":1800},"end":{"line":64,"column":156,"offset":1955}}},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"我们的电脑由上亿个这样的电位和逻辑门组成，一般的科普文章应该会去介绍芯片啊光刻机之类的东西，本文关注的是另一个方面：虽然生产电脑配件的厂商很多，不同的型号的元器件设计不同，组装出的成品应该千差万别，但是他们可以运行同样的程序，理想条件下（虽然实际工程中常常不理想）我们也可以期望他们跑出同样的结果。","position":{"start":{"line":66,"column":1,"offset":1957},"end":{"line":66,"column":150,"offset":2106}}}],"position":{"start":{"line":66,"column":1,"offset":1957},"end":{"line":66,"column":150,"offset":2106}}},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"这说明所谓计算机科学，并不等同于研究计算机元件的电子科学和工程，这里电科和电子工程相当于洞穴岩壁上的影子，而计算机科学就相当于火光前的物体。这种超越物理的计算本质，一般用一种叫做“抽象语法树”的数据结构来表示。","position":{"start":{"line":68,"column":1,"offset":2108},"end":{"line":68,"column":106,"offset":2213}}}],"position":{"start":{"line":68,"column":1,"offset":2108},"end":{"line":68,"column":106,"offset":2213}}},{"type":"text","value":"\n"},{"type":"element","tagName":"h3","properties":{"id":"生物学从神经元到神经网络"},"children":[{"type":"element","tagName":"a","properties":{"href":"#生物学从神经元到神经网络"},"children":[{"type":"text","value":"生物学·从神经元到神经网络","position":{"start":{"line":70,"column":5,"offset":2219},"end":{"line":70,"column":18,"offset":2232}}}]}],"position":{"start":{"line":70,"column":1,"offset":2215},"end":{"line":70,"column":18,"offset":2232}}},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"人们发明计算机的时候，基本上还是把它当作工具，就没期望它有什么主体性和智慧。","position":{"start":{"line":72,"column":1,"offset":2234},"end":{"line":72,"column":39,"offset":2272}}}],"position":{"start":{"line":72,"column":1,"offset":2234},"end":{"line":72,"column":39,"offset":2272}}},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"而随着生物学逐渐发现了神经系统及其作用，也随着物理学在二十世纪初的大发展之后的相对平静，很多物理学家开始插手其他学科。既然生命和非生命体的背后都服从同一套物理规律，既然物理学的众多成功经验说明，搞清楚构成系统的所有微观组成就可以理解宏观的系统，那么搞清楚人类的智力器官的基本单元以及相互作用，按理说也就能够理解什么是智慧。","position":{"start":{"line":74,"column":1,"offset":2274},"end":{"line":74,"column":162,"offset":2435}}}],"position":{"start":{"line":74,"column":1,"offset":2274},"end":{"line":74,"column":162,"offset":2435}}},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"element","tagName":"img","properties":{"src":"/photos/2023-06-17-neuron.png","alt":"a cartoon illustrating a neuron"},"children":[],"position":{"start":{"line":76,"column":1,"offset":2437},"end":{"line":76,"column":66,"offset":2502}}}],"position":{"start":{"line":76,"column":1,"offset":2437},"end":{"line":76,"column":66,"offset":2502}}},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"上图是一个神经细胞的结构示意图。从其他神经细胞释放出来的名为神经递质的化学物质，到达神经元左侧短且密集的树突之后，激活细胞膜表面的离子泵，主动运输离子跨过细胞膜，从而产生电信号。电信号沿细胞膜传导到右侧的树突，刺激凸触释放神经递质给下一个细胞。","position":{"start":{"line":78,"column":1,"offset":2504},"end":{"line":78,"column":123,"offset":2626}}}],"position":{"start":{"line":78,"column":1,"offset":2504},"end":{"line":78,"column":123,"offset":2626}}},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"element","tagName":"img","properties":{"src":"/photos/2023-06-17-perceptron.png","alt":"a handdrawing style illustration of perceptron"},"children":[],"position":{"start":{"line":80,"column":1,"offset":2628},"end":{"line":80,"column":85,"offset":2712}}}],"position":{"start":{"line":80,"column":1,"offset":2628},"end":{"line":80,"column":85,"offset":2712}}},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"上图就是根据神经元的工作原理抽象出的数学模型，名为 perceptron。一个 perceptron 就是一个函数，接受多个输入的自变量，加权求和之后套一个非线性的激活函数，得到一个输出。很多个这样的 perceptron 并连和串联，就构成下图，计算机算法中的神经网络。","position":{"start":{"line":82,"column":1,"offset":2714},"end":{"line":82,"column":137,"offset":2850}}}],"position":{"start":{"line":82,"column":1,"offset":2714},"end":{"line":82,"column":137,"offset":2850}}},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"element","tagName":"img","properties":{"src":"/photos/2023-06-17-neural-network.png","alt":"a handdrawing style illustration of a neural network"},"children":[],"position":{"start":{"line":84,"column":1,"offset":2852},"end":{"line":84,"column":95,"offset":2946}}}],"position":{"start":{"line":84,"column":1,"offset":2852},"end":{"line":84,"column":95,"offset":2946}}},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"而从实验方向研究神经系统，我们隔壁系就有，经常来我们系招人。基本上就是在小鼠的天灵盖上锯开一个天窗，然后给它带上个头盔，头盔上有能从天窗伸进去的电极，采集脑神经的电信号。以前头盔有网线伸到实验室天花板，实时传到数据中心的超算。现在好像进步了，改用 Wi-Fi 了。","position":{"start":{"line":86,"column":1,"offset":2948},"end":{"line":86,"column":133,"offset":3080}}}],"position":{"start":{"line":86,"column":1,"offset":2948},"end":{"line":86,"column":133,"offset":3080}}},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"这实验怎么通过的伦理审查，咱也不知道，咱也不敢问……","position":{"start":{"line":88,"column":1,"offset":3082},"end":{"line":88,"column":27,"offset":3108}}}],"position":{"start":{"line":88,"column":1,"offset":3082},"end":{"line":88,"column":27,"offset":3108}}},{"type":"text","value":"\n"},{"type":"element","tagName":"h3","properties":{"id":"管理学dikw-数据-信息-知识-智慧模型"},"children":[{"type":"element","tagName":"a","properties":{"href":"#管理学dikw-数据-信息-知识-智慧模型"},"children":[{"type":"text","value":"管理学·DIKW “数据-信息-知识-智慧”模型","position":{"start":{"line":90,"column":5,"offset":3114},"end":{"line":90,"column":29,"offset":3138}}}]}],"position":{"start":{"line":90,"column":1,"offset":3110},"end":{"line":90,"column":29,"offset":3138}}},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"element","tagName":"img","properties":{"src":"/photos/2023-06-17-DIKW.png","alt":"a pyramid of DIKW model"},"children":[],"position":{"start":{"line":92,"column":1,"offset":3140},"end":{"line":92,"column":56,"offset":3195}}}],"position":{"start":{"line":92,"column":1,"offset":3140},"end":{"line":92,"column":56,"offset":3195}}},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"DIKW 四个字母分别代表 data, information, knowledge, wisdom，即数据、信息、知识、智慧，是一种知识管理中的心智模型。","position":{"start":{"line":94,"column":1,"offset":3197},"end":{"line":94,"column":79,"offset":3275}}}],"position":{"start":{"line":94,"column":1,"offset":3197},"end":{"line":94,"column":79,"offset":3275}}},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"四个层次，前一层都是后一层的基础，后一层都是对前一层的理解。","position":{"start":{"line":96,"column":1,"offset":3277},"end":{"line":96,"column":31,"offset":3307}}}],"position":{"start":{"line":96,"column":1,"offset":3277},"end":{"line":96,"column":31,"offset":3307}}},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"如果是书面文字，数据就是笔画和字母；如果是语言，数据就是人声的响度、频率和音色。由笔画/字母/声音组成的有含义的字词就是信息。表示信息之间的关系的，可以判断对错的命题就是知识。包含和统摄各条知识的思想体系，就是智慧。","position":{"start":{"line":98,"column":1,"offset":3309},"end":{"line":98,"column":109,"offset":3417}}}],"position":{"start":{"line":98,"column":1,"offset":3309},"end":{"line":98,"column":109,"offset":3417}}},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"反过来说，虽然智慧高于思想，但它仍需要通过把各条知识的表达汇总起来，才能被人感知。对知识的命题的理解依赖于构成名字的各个概念的涵义，属于信息水平的内容。而每个字都有不考虑其涵义的笔画字母构成。","position":{"start":{"line":100,"column":1,"offset":3419},"end":{"line":100,"column":97,"offset":3515}}}],"position":{"start":{"line":100,"column":1,"offset":3419},"end":{"line":100,"column":97,"offset":3515}}},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"这层与层之间","position":{"start":{"line":102,"column":1,"offset":3517},"end":{"line":102,"column":7,"offset":3523}}},{"type":"element","tagName":"strong","properties":{},"children":[{"type":"text","value":"看似","position":{"start":{"line":102,"column":9,"offset":3525},"end":{"line":102,"column":11,"offset":3527}}}],"position":{"start":{"line":102,"column":7,"offset":3523},"end":{"line":102,"column":13,"offset":3529}}},{"type":"text","value":"并没有插入额外的内容，智慧可以直接由笔画构成。但是我们一层层理解的深入，其实是不自觉地借用了我们当前社会约定俗成的解读方式。","position":{"start":{"line":102,"column":13,"offset":3529},"end":{"line":102,"column":75,"offset":3591}}}],"position":{"start":{"line":102,"column":1,"offset":3517},"end":{"line":102,"column":75,"offset":3591}}},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"比如下面这个图片里的符号，对于现代人就只是数据，无法解读成信息。但是对于苏美尔人，这是用楔形文字表示的数字，是等腰直角三角形的腰和直角边的比值，也就是 ","position":{"start":{"line":104,"column":1,"offset":3593},"end":{"line":104,"column":77,"offset":3669}}},{"type":"element","tagName":"span","properties":{"className":["katex"]},"children":[{"type":"element","tagName":"span","properties":{"className":["katex-mathml"]},"children":[{"type":"element","tagName":"math","properties":{"xmlns":"http://www.w3.org/1998/Math/MathML"},"children":[{"type":"element","tagName":"semantics","properties":{},"children":[{"type":"element","tagName":"mrow","properties":{},"children":[{"type":"element","tagName":"msqrt","properties":{},"children":[{"type":"element","tagName":"mn","properties":{},"children":[{"type":"text","value":"2"}]}]}]},{"type":"element","tagName":"annotation","properties":{"encoding":"application/x-tex"},"children":[{"type":"text","value":"\\sqrt{2}"}]}]}]}]},{"type":"element","tagName":"span","properties":{"className":["katex-html"],"ariaHidden":"true"},"children":[{"type":"element","tagName":"span","properties":{"className":["base"]},"children":[{"type":"element","tagName":"span","properties":{"className":["strut"],"style":"height:1.04em;vertical-align:-0.1328em;"},"children":[]},{"type":"element","tagName":"span","properties":{"className":["mord","sqrt"]},"children":[{"type":"element","tagName":"span","properties":{"className":["vlist-t","vlist-t2"]},"children":[{"type":"element","tagName":"span","properties":{"className":["vlist-r"]},"children":[{"type":"element","tagName":"span","properties":{"className":["vlist"],"style":"height:0.9072em;"},"children":[{"type":"element","tagName":"span","properties":{"className":["svg-align"],"style":"top:-3em;"},"children":[{"type":"element","tagName":"span","properties":{"className":["pstrut"],"style":"height:3em;"},"children":[]},{"type":"element","tagName":"span","properties":{"className":["mord"],"style":"padding-left:0.833em;"},"children":[{"type":"element","tagName":"span","properties":{"className":["mord"]},"children":[{"type":"text","value":"2"}]}]}]},{"type":"element","tagName":"span","properties":{"style":"top:-2.8672em;"},"children":[{"type":"element","tagName":"span","properties":{"className":["pstrut"],"style":"height:3em;"},"children":[]},{"type":"element","tagName":"span","properties":{"className":["hide-tail"],"style":"min-width:0.853em;height:1.08em;"},"children":[{"type":"element","tagName":"svg","properties":{"xmlns":"http://www.w3.org/2000/svg","width":"400em","height":"1.08em","viewBox":"0 0 400000 1080","preserveAspectRatio":"xMinYMin slice"},"children":[{"type":"element","tagName":"path","properties":{"d":"M95,702\nc-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14\nc0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54\nc44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10\ns173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429\nc69,-144,104.5,-217.7,106.5,-221\nl0 -0\nc5.3,-9.3,12,-14,20,-14\nH400000v40H845.2724\ns-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7\nc-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z\nM834 80h400000v40h-400000z"},"children":[]}]}]}]}]},{"type":"element","tagName":"span","properties":{"className":["vlist-s"]},"children":[{"type":"text","value":"​"}]}]},{"type":"element","tagName":"span","properties":{"className":["vlist-r"]},"children":[{"type":"element","tagName":"span","properties":{"className":["vlist"],"style":"height:0.1328em;"},"children":[{"type":"element","tagName":"span","properties":{},"children":[]}]}]}]}]}]}]}]},{"type":"text","value":" 的近似值。","position":{"start":{"line":104,"column":89,"offset":3681},"end":{"line":104,"column":95,"offset":3687}}}],"position":{"start":{"line":104,"column":1,"offset":3593},"end":{"line":104,"column":95,"offset":3687}}},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"element","tagName":"img","properties":{"src":"/photos/2023-06-17-ancient-root-2.png","alt":"sumerian numerical approximation to square root of two"},"children":[],"position":{"start":{"line":106,"column":1,"offset":3689},"end":{"line":106,"column":97,"offset":3785}}}],"position":{"start":{"line":106,"column":1,"offset":3689},"end":{"line":106,"column":97,"offset":3785}}},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"约定俗成的数据解读方式，也就是关于","position":{"start":{"line":108,"column":1,"offset":3787},"end":{"line":108,"column":18,"offset":3804}}},{"type":"element","tagName":"strong","properties":{},"children":[{"type":"text","value":"数据的数据","position":{"start":{"line":108,"column":20,"offset":3806},"end":{"line":108,"column":25,"offset":3811}}}],"position":{"start":{"line":108,"column":18,"offset":3804},"end":{"line":108,"column":27,"offset":3813}}},{"type":"text","value":"，根据西方的构词法，可以叫做“","position":{"start":{"line":108,"column":27,"offset":3813},"end":{"line":108,"column":42,"offset":3828}}},{"type":"element","tagName":"strong","properties":{},"children":[{"type":"text","value":"元","position":{"start":{"line":108,"column":44,"offset":3830},"end":{"line":108,"column":45,"offset":3831}}}],"position":{"start":{"line":108,"column":42,"offset":3828},"end":{"line":108,"column":47,"offset":3833}}},{"type":"text","value":"数据”(meta-data)。","position":{"start":{"line":108,"column":47,"offset":3833},"end":{"line":108,"column":62,"offset":3848}}}],"position":{"start":{"line":108,"column":1,"offset":3787},"end":{"line":108,"column":62,"offset":3848}}},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"数据和元数据一起构成信息，信息和元信息一起构成知识，知识和元知识一起构成智慧。俺坚持写博客的动机，就是用费曼学习法，把无意间使用的元知识显式地表达出来，而且记录下来，争取学而不退转。","position":{"start":{"line":110,"column":1,"offset":3850},"end":{"line":110,"column":92,"offset":3941}}}],"position":{"start":{"line":110,"column":1,"offset":3850},"end":{"line":110,"column":92,"offset":3941}}},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{"id":"2"},"children":[{"type":"element","tagName":"a","properties":{"href":"#2"},"children":[{"type":"text","value":"2","position":{"start":{"line":112,"column":4,"offset":3946},"end":{"line":112,"column":5,"offset":3947}}}]}],"position":{"start":{"line":112,"column":1,"offset":3943},"end":{"line":112,"column":5,"offset":3947}}},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"回顾了这些，再来看大语言模型，就会发现它落在了各方努力的延长线的交点。","position":{"start":{"line":114,"column":1,"offset":3949},"end":{"line":114,"column":36,"offset":3984}}}],"position":{"start":{"line":114,"column":1,"offset":3949},"end":{"line":114,"column":36,"offset":3984}}},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"大语言模型里有一个重要概念叫做“嵌入”(embedding)，就是把语言的基本字元 (token) 可逆地映射到一个超多维度的向量空间里。本来“国王”和“儿子”之间没办法加减乘除，但是嵌入后的向量空间里有加法和数乘，如果嵌入函数选得好，“国王”的向量 + “儿子”的向量，结果向量就约等于“王子”的向量。","position":{"start":{"line":116,"column":1,"offset":3986},"end":{"line":116,"column":153,"offset":4138}}}],"position":{"start":{"line":116,"column":1,"offset":3986},"end":{"line":116,"column":153,"offset":4138}}},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"element","tagName":"img","properties":{"src":"/photos/2023-06-17-vector-addition.png","alt":"illustration of vector addition from wikipedia"},"children":[],"position":{"start":{"line":118,"column":1,"offset":4140},"end":{"line":118,"column":90,"offset":4229}}}],"position":{"start":{"line":118,"column":1,"offset":4140},"end":{"line":118,"column":90,"offset":4229}}},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"生成式语言模型的核心就是一个超多元函数，接受前一个字嵌入后的向量作为输入，给出另一个向量作为输出，用嵌入函数的逆映射翻译成字元；再把旧的输出作为新的输入，直到输出结果是“语段结束”这样一个特殊字元为止。模型训练的过程，主要就是通过现成的语料，拟合这个超多元函数的参数。","position":{"start":{"line":120,"column":1,"offset":4231},"end":{"line":120,"column":135,"offset":4365}}}],"position":{"start":{"line":120,"column":1,"offset":4231},"end":{"line":120,"column":135,"offset":4365}}},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"从 DIKW 模型来看，语言模型操作的是最基本的数据，它的输出究竟是什么信息，是不是正确的知识，体现了多少智慧，是人根据当下的社会文化来解读的。","position":{"start":{"line":122,"column":1,"offset":4367},"end":{"line":122,"column":73,"offset":4439}}}],"position":{"start":{"line":122,"column":1,"offset":4367},"end":{"line":122,"column":73,"offset":4439}}},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"而实现 AI 的电子计算机，或是复杂生命的大脑，他们和智能之间的关系，应该就类似于具体的计算机电路和抽象语法树之间的关系。以此类比，未来的智能科学应该会成为一门独立的专业，它和计算机科学和神经生物学的区别，就像今天的电子科学与工程，和计算机科学之间的区别一样。当下神经生物学的热度，将来恐怕多半会被分流。","position":{"start":{"line":124,"column":1,"offset":4441},"end":{"line":124,"column":153,"offset":4593}}}],"position":{"start":{"line":124,"column":1,"offset":4441},"end":{"line":124,"column":153,"offset":4593}}},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"这种对字符的计算不同于逻辑运算，语言模型不判断输出结果在逻辑上的正确与错误，这既给了他啥都能说几句的 feature，又给了它经常编假消息的 bug。","position":{"start":{"line":126,"column":1,"offset":4595},"end":{"line":126,"column":76,"offset":4670}}}],"position":{"start":{"line":126,"column":1,"offset":4595},"end":{"line":126,"column":76,"offset":4670}}},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"想要改掉这种错误，引入对 AI 的纠错机制，治本之道恐怕还是诉诸于对世界的正确描述，与理论相关的还是要靠逻辑，与现实相关的还是要靠科学。","position":{"start":{"line":128,"column":1,"offset":4672},"end":{"line":128,"column":69,"offset":4740}}}],"position":{"start":{"line":128,"column":1,"offset":4672},"end":{"line":128,"column":69,"offset":4740}}},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"只不过，大语言模型提供了一种数据结构，有希望把人类已知的真理储存在一起。对这种数据结构本身的研究，有可能反过来启发科学的发展。柏拉图的洞穴之壁可能不再是一个比喻，未来更大的语言模型的，亿万维度的参数空间有希望成为洞穴门口的那团火。","position":{"start":{"line":130,"column":1,"offset":4742},"end":{"line":130,"column":116,"offset":4857}}}],"position":{"start":{"line":130,"column":1,"offset":4742},"end":{"line":130,"column":116,"offset":4857}}},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"只不过这一切都是“可能”，现在还只是 AI 的萌芽阶段，还没有足够的证据来证实或者证伪这种畅想。而且 AI 的参数量再大也是有限的，它所能表达的信息也就有限，而真理应当是无限的，就像科学一样，总要训练更新更大的模型，总要发现已知的未知，然后欣然接受更多未知的未知之存在。","position":{"start":{"line":132,"column":1,"offset":4859},"end":{"line":132,"column":136,"offset":4994}}}],"position":{"start":{"line":132,"column":1,"offset":4859},"end":{"line":132,"column":136,"offset":4994}}},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"如果电子计算机实现的 AI 独立于人类产生了意识和超出人类的智慧，很难想象他们会继续用人类语言这种对他们来说很不方便的方式来交流。","position":{"start":{"line":134,"column":1,"offset":4996},"end":{"line":134,"column":66,"offset":5061}}}],"position":{"start":{"line":134,"column":1,"offset":4996},"end":{"line":134,"column":66,"offset":5061}}},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"所以，哪怕是做个 AI 生成内容的质检员，科学家依然有事可做。这算是科学的堕落吗？当然不算，如果算的话，那从计算物理也被当作理论物理的那天起，人类就已经投降了（逃）","position":{"start":{"line":136,"column":1,"offset":5063},"end":{"line":136,"column":83,"offset":5145}}}],"position":{"start":{"line":136,"column":1,"offset":5063},"end":{"line":136,"column":83,"offset":5145}}},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{"id":"3"},"children":[{"type":"element","tagName":"a","properties":{"href":"#3"},"children":[{"type":"text","value":"3","position":{"start":{"line":138,"column":4,"offset":5150},"end":{"line":138,"column":5,"offset":5151}}}]}],"position":{"start":{"line":138,"column":1,"offset":5147},"end":{"line":138,"column":5,"offset":5151}}},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"现在正面来回答问题：AI透過程式組合出回答你問題的文字組合，有「涵義」嗎？","position":{"start":{"line":140,"column":1,"offset":5153},"end":{"line":140,"column":38,"offset":5190}}}],"position":{"start":{"line":140,"column":1,"offset":5153},"end":{"line":140,"column":38,"offset":5190}}},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"答：有。","position":{"start":{"line":142,"column":1,"offset":5192},"end":{"line":142,"column":5,"offset":5196}}}],"position":{"start":{"line":142,"column":1,"offset":5192},"end":{"line":142,"column":5,"offset":5196}}},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"因为语言的「涵義」来自于语言的内容，和整个社会的文化，并不来自于这句话的作者的身份。即便是人与人之间的交流，诉诸身份也是一种非形式逻辑谬误，是理性不足的表现。只有在信息不足仍不得不下结论的时候才该使用，比如法律判决时的自由心证主义和/或法定证据主义。","position":{"start":{"line":144,"column":1,"offset":5198},"end":{"line":144,"column":126,"offset":5323}}}],"position":{"start":{"line":144,"column":1,"offset":5198},"end":{"line":144,"column":126,"offset":5323}}},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"而鹿妈眼里真人鹿酱与 AI 鹿酱的区别，如果有的话，好像主要体现在动机的区别。动机这种东西，很多智慧不高的生物，比如小猫小狗都会有；而现在的 AI，似乎还没有展现出超出编程者设计的动机。编程写入的信息有限，现有 AI 的动机也就有限，鹿酱的赢面还是很大的。","position":{"start":{"line":146,"column":1,"offset":5325},"end":{"line":146,"column":129,"offset":5453}}}],"position":{"start":{"line":146,"column":1,"offset":5325},"end":{"line":146,"column":129,"offset":5453}}},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"而动机是生物与非生物的区别吗？而什么是生物 ≠ 生物是什么，那就是另一个含混而复杂的问题了。","position":{"start":{"line":148,"column":1,"offset":5455},"end":{"line":148,"column":47,"offset":5501}}}],"position":{"start":{"line":148,"column":1,"offset":5455},"end":{"line":148,"column":47,"offset":5501}}},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{"id":"4"},"children":[{"type":"element","tagName":"a","properties":{"href":"#4"},"children":[{"type":"text","value":"4","position":{"start":{"line":150,"column":4,"offset":5506},"end":{"line":150,"column":5,"offset":5507}}}]}],"position":{"start":{"line":150,"column":1,"offset":5503},"end":{"line":150,"column":5,"offset":5507}}},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"这篇博文发布的时候，高考应该已经结束了，马上该填报志愿了。","position":{"start":{"line":152,"column":1,"offset":5509},"end":{"line":152,"column":30,"offset":5538}}}],"position":{"start":{"line":152,"column":1,"offset":5509},"end":{"line":152,"column":30,"offset":5538}}},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"那么，西元 2023 年，AI 来袭的当下，该选个啥专业在 AI 浪潮中幸存，或者选个啥专业给 AI 老爷带路呢？","position":{"start":{"line":154,"column":1,"offset":5540},"end":{"line":154,"column":58,"offset":5597}}}],"position":{"start":{"line":154,"column":1,"offset":5540},"end":{"line":154,"column":58,"offset":5597}}},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"element","tagName":"img","properties":{"src":"/photos/2023-06-17-three-body-quotation.png","alt":"a screenshot of a quotation from Three Body about attitudes towards aliens"},"children":[],"position":{"start":{"line":156,"column":1,"offset":5599},"end":{"line":156,"column":123,"offset":5721}}}],"position":{"start":{"line":156,"column":1,"offset":5599},"end":{"line":156,"column":123,"offset":5721}}},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"我的建议是，不要听别人的建议，按自己的兴趣来就好了。","position":{"start":{"line":158,"column":1,"offset":5723},"end":{"line":158,"column":27,"offset":5749}}}],"position":{"start":{"line":158,"column":1,"offset":5723},"end":{"line":158,"column":27,"offset":5749}}},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"刚刚改开的时候，有一个超级热门的专业，叫科技英语。科技落下了好多年，对外开放需要语言交流，两者一结合应该是热门又稀缺了。结果呢，你现在还听说过这个专业吗？","position":{"start":{"line":160,"column":1,"offset":5751},"end":{"line":160,"column":78,"offset":5828}}}],"position":{"start":{"line":160,"column":1,"offset":5751},"end":{"line":160,"column":78,"offset":5828}}},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"科技很重要是不错，语言很重要也不错，但是搞科技的人自己可以学英语，学英语的有几个搞得了科技？社会的进步主要靠创新，而创新的方向难以预测，不论这种预测分析听起来多有道理。","position":{"start":{"line":162,"column":1,"offset":5830},"end":{"line":162,"column":85,"offset":5914}}}],"position":{"start":{"line":162,"column":1,"offset":5830},"end":{"line":162,"column":85,"offset":5914}}},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"如果真的找不到兴趣，那就在能力范围之内，找个难度最高的。如果想从事智力劳动，那数学含量是个不错的衡量标准；如果不排斥体力劳动，那训练时间越长越值得考虑。","position":{"start":{"line":164,"column":1,"offset":5916},"end":{"line":164,"column":77,"offset":5992}}}],"position":{"start":{"line":164,"column":1,"offset":5916},"end":{"line":164,"column":77,"offset":5992}}},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"但这只是填志愿来不及时的权宜之计，发掘兴趣是人一生的课题。","position":{"start":{"line":166,"column":1,"offset":5994},"end":{"line":166,"column":30,"offset":6023}}}],"position":{"start":{"line":166,"column":1,"offset":5994},"end":{"line":166,"column":30,"offset":6023}}},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"兴趣不是为了让你成功的时候更得意，毕竟成功的话不论做什么都很得意；","position":{"start":{"line":168,"column":1,"offset":6025},"end":{"line":168,"column":34,"offset":6058}}}],"position":{"start":{"line":168,"column":1,"offset":6025},"end":{"line":168,"column":34,"offset":6058}}},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"兴趣是为了你不成功时也可以不失意，毕竟平凡才是人生的真谛。","position":{"start":{"line":170,"column":1,"offset":6060},"end":{"line":170,"column":30,"offset":6089}}}],"position":{"start":{"line":170,"column":1,"offset":6060},"end":{"line":170,"column":30,"offset":6089}}}],"position":{"start":{"line":1,"column":1,"offset":0},"end":{"line":171,"column":1,"offset":6090}}},"collectedBy":[["doc",[{"slug":"autumn-tour-grand-teton-and-yellowstone","filename":"2024-10-13-autumn-tour-grand-teton-and-yellowstone.md","date":"2024-10-13","title":".xls | 秋游大提顿和黄石公园","layout":"post","keywords":["doc","xls","jpg"],"excerpt":"今年秋假去了 Grand Teton 大提顿和 Yellowstone 黄石国家公园。","cover":"2024-10-13-Yellowstone.png","content":"\n今年秋假去了 Grand Teton 大提顿和 Yellowstone 黄石国家公园。两个公园南北相邻，向南最近的城镇是 Jackson Hole，也就是美联储开会的地方。\n\nGrand Teton 依托于提顿山脉，山的海拔不算特别高，但是胜在山脉东侧的湖泊和盆地，衬托得山势雄伟。园内落叶阔叶林和常绿针叶林混搭，秋季色彩明丽。\n\nYellowstone 整个公园相当于一个活火山的火山口，园内有大量温泉、间歇泉、地热，还有黄石河切削而成的峡谷和瀑布，一年四季的景色都很优美。但是园内植被主要是常绿针叶林，所以秋天并不额外出彩。\n\n另外秋季两个公园内的野生动物都不活跃，不过对于像我这样没有 100 - 400 mm 长焦镜头的普通游客影响不大～\n\n两个公园的门票都是一周内无限次进入，好处是景区内商家不存在行政垄断，吃饭加油的价格并不比外面贵；坏消息是外面和公园里一样贵，提顿村汽油每加仑超过 $5，杰克逊稍微便宜一些但也很贵。\n\n![大提顿公园地图](/photos/2024-10-13-GrandTeton.png)\n\n![黄石公园地图](/photos/2024-10-13-Yellowstone.png)\n\n### 流水账\n\n- 第一天\n    - 在盐湖城中转，期间下载了两个公园的 Google 离线地图\n    - 中午到 Jackson Hole 机场，在租车行取车\n    - 在 Dornans 下车拍照\n    - 下午开车沿 Hwy 191 向北，在沿途的停车区短停拍照，看到 Pack Trial Fire 山火的烟\n    - 经过大提顿和黄石公园南入口，分别买票\n    - 到达黄石公园 Moose 瀑布后返回\n    - 前往提顿村，路上在 Jenny Lake outlook 拍照\n    - 入住酒店\n- 第二天\n    - 从提顿村前往黄石，经过 Teton Park Road，沿途在停车区拍照\n    - 在黄石公园依次游览 West Thumb 间歇泉、Mud Volcano 火山湖、Upper \u0026 Lower 瀑布\n    - 试图前往 Dunraven ，因为不太会用离线地图，一直开到了 Roosevelt Lodge 才发现走错了，而且那里秋季不开门\n    - 返回，抵达 Dunraven Pass 时日落，没找到上山途径，在路对面的山坡上看晚霞\n    - 在公园内吃晚饭，不比宾馆贵\n    - 夜行回宾馆\n- 第三天\n    - 试图在宾馆附近拍日出，东方有山，看见太阳时高度角已经很高，未见红日\n    - 在黄石公园依次游览 Old Faithful 间歇泉、Steamboat 间歇泉、Congress pool\n    - 试图返回 Jenny lake 拍日落，没太赶上，发现另一个拍日出的机位 Cathedral Group Turnout，事后证明不是秋季的最佳机位\n- 第四天\n    - 早起前往 Cathedral Group Turnout，\n        - 看提顿峰顶的第一缕日光\n        - 在长焦镜头取景框里看到大角公鹿，没能拍下清晰照片\n        - 回程发现了更好的机位，向东可以看到地平线上的日出，向西有若干金黄色落叶阔叶林作为提顿峰的前景，非常后悔\n    - 在 Jenny lake 拍照\n    - 退房后，大提顿以东的整个平原/盆地隐没于远方山火扩散来的烟霾中\n    - 前往 Mormon Row，是一处早期定居点，遇花栗鼠一只，蓝鸟一对\n    - 穿过 Jackson 镇，进入 Elk Refuge，此处的烟霾比提顿附近略轻。\n    - 返回 Jackson 镇吃午饭，加油，回机场"},{"slug":"doc-graduation-ceremony","filename":"2024-05-26-doc-graduation-ceremony.md","date":"2024-05-26","title":".doc | 参加毕业典礼","layout":"post","keywords":["doc"],"excerpt":"写点尬的，先来首高雅音乐～","content":"\n\u003e 写点尬的，先来首高雅音乐～\n\u003e \n\n【爱江山更爱美人.mp3】\n\n美国的冬春学期五月份就结束了，毕业典礼中的 hooding ceremony 一般在此学期的最后一个周五，commencement 在接下来的周一。\n\n我早就毕业了，但是答辩的时候刚刚过了毕业典礼的日期没多久。毕业之后回了一趟家，然后返回原实验室做博士后。\n\n---\n\n今年春天，我们研究方向的师生每周茶聚的时候，我们老板盛赞我选择下一年的春天参加毕业典礼是个明智的选择，毕竟冬学期的活动太冷清了，都没有人重视。\n\n当时我的面部肌肉稍微拉起来一个微笑，没有接话。\n\n这是在提醒我，他可没有忘，所以请我也不要忘了报名今年的毕业典礼哟～\n\n刚入学的时候，我觉得我们老板怎么这么水，数学水编程也水，就这还是两校出来的学生？现在的我真心地除了佩服还是佩服，我们老板不论安排什么任务，还是讲什么知识，说起来都笑吟吟的，我认识他六年有余了，从来没见他红过脸——这个世界如此地混沌，他却能微笑着一句话，剪断一个将来可能有无限枝叶的世界线分叉——那还有多少事，多少人的“自由意志”，都包含在他的自由意志之中呢？\n\n---\n\n说第二年参加毕业仪式是一种“选择”，是因为至少还有两种做法：1. 答辩之前就参加当年的活动，我们组比我高一级的学姐就是这么干的；2. 毕业之后的冬季学期结束的时候还有一次 hooding ceremony, HK 兄弟就是这么干的。\n\n两种做法我都没有选，因为我本来完全不打算参加任何毕业活动的。\n\n毕竟对于迂腐的小知识分子，一个行为能带来的世俗荣誉感越大，做了就越庸俗，拒绝就越优雅～\n\n我们本科的毕业纪念册上，居然有一个文本框要填什么“所获荣誉”，简直是人心不古世风日下，于是咱仗义执言：“物理学院院内足球联赛亚军；BiliBili 放映室写评论赢电影票活动获奖观众”——优雅，实在是太优雅了.gif\n\n![https://syimg.3dmgame.com/uploadimg/xiaz/2022/0607/1654566377264.gif](https://syimg.3dmgame.com/uploadimg/xiaz/2022/0607/1654566377264.gif)\n\n纪念册打印出来之后我一看，律神的第一行写着“珠峰物理实验班第一学年平均分数第一名”。律神是名副其实的大学霸，大一的时候专业课都接近满分。我只是个 90 分飘过的混子，而体育课思政课之类的我又碰巧分数比他高，算 GPA 的话居然比他还高了一点点。大一下学期的电磁学期末考试实在是太简单，直接把他气到在群里@老师，大二转学到了数学班。\n\n~~对了，数学班的班花毕业纪念册好像写得也很优雅。我自己其实真无所谓，但我有个朋友真的很想知道一下人家的联系方式。~~\n\n---\n\n出来混总是要还的嘛。\n\n仗着成绩不错，在外校选修了拓扑学，尼玛人话都听不懂多少，就要去理解紧致啊同伦啊这些概念，这回 GPA 真的下来了。而且在外校的一个学期 ~~让喜欢的女孩子终于好意思拒绝了我的纠缠~~ 打断了之前在本校的研究进度，回来之后又去了一个完全不同的课题组混经验，到毕业也没混出什么名堂。\n\n寄出去 19 封申请信，只回来 2.5 份录取通知书，那半份硕士录取的老师还发邮件催了好几次，意思是硕士录取就已经很给我面子了，不要不识好歹哟～\n\n来到博士学校，才知道这份 offer 是因为和我同年招聘来了一个新老师，系里估计他需要学生，就把申请信里说对这个方向有兴趣的我给捞进来了。\n\n来了之后我依然笑嘻嘻地在混，但是研究方向的不确定性，技术路线能不能走通都不好说，更别说知道能做出什么结论，结论的科学意义有多大了。\n\n最焦虑的时候，我背着书包回家，包还在背后往床上一躺就睡着了，然后一个实验失败的噩梦又把我给惊醒，起身时屋内漆黑，窗外华灯初上。\n\n确定答辩日期之后，其实是可以提前报名当年的毕业典礼的，但是我实在是怕给自己立 flag，觉得干脆就不参加典礼了更好，跟家里也是这么说的。\n\n结果下半年， 比我晚入学两年的老乡 HK 兄弟毕业了，朋友圈发了很帅气的照片。这回破防的是我了，“富贵不发朋友圈，如锦衣夜行，谁知之者！”老板提了一嘴我就报名了。\n\n---\n\n大三的时候，物理学院出钱，让我们班三个同学去美国一所学校做一个学期的访问学生。外校的新生报到时，我们刚进大厅，身前不远有一家三口，小姑娘转身面向爸爸妈妈，笑着说她要去排队咯，然后又转身，金发单马尾一蹦一跳地往前跑去了。确定她走远后，妈妈倒在了爸爸的怀中，眼中泪水不住地流。\n\n然后我们几个相视一笑，就这？我们坐了快二十个小时的飞机，天没亮在麦当劳眯了好几个小时再过来，你们开了几个小时的车就感动成这样？\n\n后来我觉得当年的自己真幼稚。人生道阻且长，怎能不抓紧一切可庆祝的机会去庆祝，给自己加油打气，说服自己正走在正确的方向呢？\n\n现在我觉得当年觉得自己真幼稚的自己真幼稚。稍微没品且无自觉了一点，但当时的我们，其实也是在给自己打气啊。\n\n【Fear brought me this far.mp3】\n"},{"slug":"txt-graduation-ceremony","filename":"2024-05-26-txt-graduation-ceremony.md","date":"2024-05-26","title":".txt | 参加毕业典礼","layout":"post","keywords":["doc"],"excerpt":"今日发布的另一文，是写同一件事的姊妹篇。","content":"\n\u003e 今日发布的另一文，是写同一件事的姊妹篇。\n\u003e \n\n【Fear brought me this far.mp3】\n\n美国的冬春学期五月份就结束了，毕业典礼中的 hooding ceremony 一般在此学期的最后一个周五，commencement 在接下来的周一。\n\n我早就毕业了，但是答辩的时候刚刚过了毕业典礼的日期没多久，所以今年才参加毕业典礼。毕业之后回了一趟家，然后返回原实验室做博士后。\n\n今年春天，我们研究方向的师生每周茶聚的时候，我们老板盛赞我选择下一年的春天参加毕业仪式，毕竟冬学期的活动太冷清了，都没有人重视——真是个机智的选择。\n\n当时我的面部肌肉稍微拉起来一个微笑，没有接话。\n\n---\n\n典礼之前气氛凝重。\n\n去年10月份，驻在且主宰巴勒斯町 (Palestine) 之加沙地带的哈马斯组织突击以色列，随后以色列动员正规军反击。（我在博客上做了[新闻剪报。](war-in-2023-between-hamas-and-israel)）局势发展到典礼前两周的周五，包含本校学生在内的人群，要声援“巴勒斯坦 (Palestinian) 人民”，反对购买了美国军火的以色列进行“种族灭绝 (genocide)”，在学校扎帐篷、静坐、抗议。\n\n典礼前两周的周六深夜，校内所有建筑上锁，警察以侵犯私有地产 (trespassing) 为名警告，后扫荡逮捕了空地上拒绝离去的人群，有人受伤，有人被控拒捕和袭警，有人指控暴力执法。\n\n典礼前一周的周四深夜到周五凌晨，一夜之间，校方围绕校园一圈，以及在校内部分建筑之间，搭起了钢管作边框的预制栅栏，栅栏的底座钉在水泥地上，之间用铁皮绑住，尽头钉在建筑的墙上，只在个别位置留下专人把守的路口，下班时间门将走后上锁。\n\n不过物理系正好位于这个铁网迷宫的一个拓扑范畴边界上，即使下班时间也可以从不同的门去往不同的路径同伦 (path homotopy) 等价类，算是影响不大。\n\n---\n\n典礼前的周四上午，物理系年例集体合照，拍完照片去取预租的学位袍。一套衣服租需要 $90，嫌贵？买或者租而不还的话 $900，不穿参加不了典礼哟～\n\n取衣服的位置在学校书店地下一层，看到队伍已经拍到一楼门口了，心里咯噔一下子。好在美国人排队的平均自由程比国内大不少，每个人在自己位置上跍踊的热运动温度也比较低，所以队伍排起来也不算压抑。问清楚了还衣服的时间和地点。\n\n美国的学位袍是各校自己定的样式，连颜色都没有本黑硕蓝博红的规定。来这里的第一个寒假前，看到冬天毕业的学生都穿着土褐色的袍子，霍格沃茨里面最没有特点的赫奇帕奇的专属色，真想退学重新申请，唉，沉没成本啊沉没成本……\n\n装学位服的袋子里有一帽子，一袍子，还有一扁三角形布料，应该是就是 hood。这个词的本意是兜帽，此处展开后前端是一个类似于 ~~JK制服~~ 水手服的三角形领子，后半截确实可以被理解成兜帽，但是长度够垂到大髀，估计没人想套头上。\n\nHooding 就是由导师把这个大领子套到学生身上，类似于国内的拨穗。所以邮件里才要求博士生集合的时候不要自己把领子穿上了。\n\n---\n\n周四晚上睡不着，于是去校园里散步，顺便听秦晖先生的油管视频。自从养成散步的习惯以来，已经在校园里摸索出一条路线，可以全程户外的同时校园 WiFi 也不掉线，省流量。视频在全屏模式下，按电源键关掉屏幕，然后立即抬起屏幕点继续播放，可以熄屏且免广告。\n\n很多学生呼朋引伴在校园里，大家都穿便服，毕业周也是期末考试周，可能既有毕业生也有觉得考试是件大事的屁孩。大家几人十几人一群庆祝和发泄，做一些平时不敢做的事，美其名曰创造回忆。\n\n比如有一男生在中央草坪边一座亮黄色消防栓旁边酝酿半天，半蹲半跪，我经过之后，他身旁的朋友发出一阵欢呼。我什么也没有闻到。\n\n比如有一女生坐在法学院门廊旁边的亮黄色消防栓上，我经过之后，她身旁的朋友发出一阵欢呼。除此以外我什么也没有看到。\n\n比如有一群学生，在中央草坪一角的大树下，和我打照面经过，举起手里的易拉罐畅饮。我没看清易拉罐是何种饮料之容器。\n\n---\n\n周五是 hooding ceremony 的正日子。早上穿便服去学校假装工作一上午，中午系里有烤肉自助，吃完回家换衬衫正装裤子和鞋，毕竟穿一天实在是太热了。\n\n本来答辩的时候也想这么穿的，结果写论文期间吃饭不太克制，答辩那天早晨发现胖得裤子穿不上了。现在减肥初见成效，很高兴。\n\n换衬衫回来之后赶紧去卫生间看打领带的教学视频，勉强赶在集合时间之前打好了。\n\n学姐也赶回来参加庆祝活动，在我办公桌上放了一束花。\n\n然后我们系的老师和同学一起往准备厅走。我老板也穿戴好了。 陈郝问他的学位袍是 H 家的还是 M 家的，我老板答曰 Amazon 家的——我们之前闲聊的时候说到学位袍很贵的事，陈郝一本正经地给我科普，如果留在学界的话，一般都是买本校的袍子将来正式场合一直穿的——这都让我知道你不懂装懂了～\n\n在准备厅门口，让我们扫二维码填表，好让学校请的摄影师根据信息把照片发给我们。准备厅内签到，拿到自己预先填好的名片，学院的头儿问清楚每个人名字的读音，用笔在名字卡的读音位置补充标记。然后按导师的名字排队。\n\n离开准备厅前往礼堂。整队很快，在路上等了一会。没话找话，我问我们老板最喜欢的球队是哪只，他说巴黎圣日尔曼，卧槽我不该问的，他问我，我只能照实答多特蒙德，然后他开始说奇怪的话，什么门柱啊运气啊，啊对对对，祝你们下赛季走得更远好吧……\n\n进入堂内，先进去的家长们举着手机录视频拍照；落座后放音乐，原来管风琴是摆设；讲台后面的彩色玻璃很好看；老板问我们之前来没来过，我说维尔切克的讲座好像在这里，也可能是我记错了。\n\n各种讲话，每人的讲稿倒都不长，校友代表是文学男博士，成就是创业写了个给图书馆使用的 APP；毕业生代表是生化女博士，丈夫是神父，说在这里讲话让她感受到了他工作的不易和家庭的重要。\n\n然后依次上台，院长念名字，导师站上祭台，学生站在台前，挂上领子，和校教务长握手，合影，下台，拿一个毕业证书摆拍大头照，回座位。\n\n仪式结束后，外面的院子里准备了酒水和零食。我和老板匆匆吃了两大盘，然后赶回系里，师妹上午通过了博士资格考试，我们回去之后开了一瓶香槟给她庆祝，然后闲聊胡扯。我们老板非常得意地发表了他对于为什么要按老师而不是学生名字排队的归因。\n\n晚上，庆祝毕业的同学们一起去了一家中餐馆吃饭。老外们一个两个都在点地道中餐，于是我点了一个蜜汁核桃虾仁。\n\n---\n\n周六穿着袍子和一起毕业的硕士学弟还有他妈妈一起，在校园里拍照。啊，妈妈不在身边的孩子像根草～\n\n---\n\n周日搬家，新家远，需骑自行车上学，且是丘陵坡路。\n\n---\n\n周一早晨 commencement，来学校的路上经过操场边，有抗议人群站在栅栏外举巴勒斯坦旗和标语牌示威。本来 commencement 还打算参加一下的，但是骑车过来实在是太累了，于是没去，中午时间到了就把袍子还回去了。\n\nCommencement 结束后 ~~大宴群臣~~，非毕业生对毕业最有实感的活动才开始：吃。校园里各处已经搭好了棚子，每个棚子有一家本地的餐厅在不限量供应食品，有韩式墨西哥菜、珍珠奶茶、烤肉三明治、冰淇淋、可乐……\n\n不想喝可乐的话，也有白水。每个亮黄色的消防栓接出来一根银白色的金属装置，然后连到一个蓝色大水桶，桶底有水龙头可以接水喝。\n\n你们谁爱喝谁喝，我反正不喝。\n\n![](/photos/2024-05-26-legal-high-SP.png)"},{"slug":"farewell-grandpa","filename":"2024-04-08-farewell-grandpa.md","date":"2024-04-08","title":".doc | 爷爷没等到我回来","layout":"post","keywords":["doc"],"excerpt":"消息的时候，脑子觉得我应该哭，心里却只是有点堵，哭不出来。","content":"\n去年春末夏初的时候，每周和爸妈视频电话的时间没人接听，一小时后他们发消息说爷爷住院了，接着视频电话就打过来了。爷爷听力早就不好了，画面里看他还戴着呼吸机，我没有看到他的反应，不知道听见我没有。\n\n这就是我最后一次见到爷爷。本来接下来的周末听说情况稳定了，结果迅速恶化，总共住院不到两周。\n\n看到消息的时候，脑子觉得我应该哭，心里却只是有点堵，哭不出来。当时这边是晚上，我在办公室，我们老板从来不黑脸，从他很有耐心的话中，我觉得他对我的论文初稿很不满意。于是浅呼一口气，继续作图继续写。\n\n---\n\n我上小学前一直和爷爷奶奶住在一起，周末被爸爸妈妈接“回”家里去。那时候我爸白天工作，晚上复习考研；我妈在一个国企内部的小医院，经常上夜班。\n\n那段日子的绝大多数时间并不开心。小孩子哪懂什么考研和夜班，只怀疑爸爸妈妈是不是不要我了。和爷爷奶奶在一起按理说是一种安慰，但是要上幼儿园。\n\n奶奶退休前是小学老师。搞教育的人，经常对自己家孩子的教育有点不切实际的想法，入园就给我插到了中班去。发现跟大孩子一起根本融不进去之后，又给我调到小班，完美错过小孩子刚认识的时候交朋友的阶段。\n\n再加上幼儿园学的东西真的好难，老师教折纸，先这样折，那样折，翻过来，在这个地方剪一刀——三步以后我必然听不懂。只能在其他小朋友举起劳动成果的时候偷瞟左右，趁大家不注意把自己的纸团藏到桌洞里去。开始是伸手放进去，后来熟练了，手腕一抖就投进去了。半个学期就能攒满一桌洞，需要老师举起桌子倒出一摊，拿笤帚扫成一堆。\n\n于是毫无意外成为大家的笑柄，我们班里有个傻子，嘿嘿。\n\n所以不想上幼儿园。早上和爷爷一起走着去幼儿园，走两步我就战战兢兢嘱咐一句“可千万别忘了来接我啊”，很短的一段路，我能一字不差地问三百遍。爷爷攥着我的手，严肃认真地回“嗯，一定不会忘”，一字不差地回答三百遍。\n\n中午回家吃饭睡午觉，爷爷说，要是睡过头了的话，下午就不去幼儿园了吧。于是直到上小学的前一年，我每天中午都会睡过头，“睡醒了”就在家里跑来跑去，等到太阳西斜了，跟着爷爷去门球场，他跟老伙计们打门球，我在细红线勒出的边线外面玩沙子和计分板。\n\n就这样一直玩到天色幽蓝，那样的幽蓝仿佛日日如此，总有爷爷带我回家。\n\n---\n\n这样的日子回不去是理所当然，但是没想到连这样的回忆也再难听到了。这种糗事本来是家里聚餐时饭桌上最常听到的谈资，毕业回国和伯伯、姑姑、叔叔家相聚，席间还是他们在说，话题却变成他们记得的爷爷奶奶的故事，我成了远方来客，成了不便开玩笑的别人家的孩子。\n\n我才意识到，和我记忆中的退休老人形象不同，父辈们眼中的爷爷，还是他年轻时候的样子。\n\n我又意识到，不是爷爷年轻时候的样子，而是和我一样，是记忆中我们小时候他的样子。\n\n那时候的爷爷在我市周围某县的一家半导体工厂，所谓半导体其实指的是收音机，所谓工厂是一间福利厂，就是给县里机关和事业单位的家属安排工作用的，不以赚钱为首要目的。比如厂里的会计是副县长的老婆，进厂的时候是个文盲，算账是和认字一起学的。\n\n姑姑说爷爷是厂长；我爸说不对，爷爷是厂里唯一的技术员。\n\n这些故事总有许多个版本，就比如厂房施工的时候，有辆满载一板车砖头的驴车上坡，结果有个熊孩子喊了一声“吁~”，驴子停在半坡不走了，把赶车的工人吓了一跳又气得不轻，但是不敢发作，因为不知道是谁家的孩子。\n\n那熊孩子是谁来着，是不是我爸？我爸说不是，是会计的儿子。真实情节说不清楚，只有“吁”是全世界驴子停车的通用口令，是无数大语言模型在甚高维嵌入空间的交点。\n\n后来改革开放，厂子散了，爷爷联系了隔壁县的一所师范类大专，成了一名物理老师。后来全家跟着学校搬迁到城市，我出生长大的地方。\n\n很奇怪，这些回忆日常得异常，那些惊天动地的大事，模糊得像烛光投在墙上的影子。\n\n日寇侵华的时候，往我们老家所在的海边渔村仍了一颗未爆的哑弹就再没来过；爷爷的叔叔带船队北上做生意，结果连人带货和船一起被扣了，爷爷的爸爸卖了好多田地去赎人，于是土改只被定了个中农；工农兵推荐取代高考前的一两届考上了大学；政治运动里整人没资格，被整也没把柄……\n\n可能也不奇怪，能活到今天的大家，要么是惊涛骇浪的幸存者，要么是幸存者的后代，除了史书上主角中胜利的那一小撮，谁还不是有点运气在身上呢。除了胜利的一小撮，比起一句顶一万句，一天顶一万年的狂飙突进，谁不更希望打门球，看孩子玩沙子，玩到天色深蓝呢？\n\n---\n\n于是继续玩沙子，玩到天色幽蓝，爷爷带我回家。\n\n有一天在幼儿园里，画册上有一道益智题，一个四乘四的方阵，横竖斜线四个方向上四种水果排列起来。我觉得图案很漂亮很有趣，想回家给爷爷看，于是中午一到家，就要找纸笔要画下来。结果画到一半，剩下的图案就不记得了，急得我要哭。\n\n爷爷在旁边微笑着看，见我画不出来了，把笔要了过去，把剩下的水果补全。爷爷画不好水果，就把水果的名字写在空白的地方。这么神奇的吗？爷爷不是没看过那本画册吗？这两字是什么，真的是菠萝没骗我吗？\n\n这叫规律，世界上的所有事情都是有规律的。爷爷拉开木柜子上一个很沉的抽屉，金属和润滑油的气味飘散出来，他拿出好多小灯泡、电闸和电池盒，组了一个电路，拉下不同的电闸，不同地方的小灯泡次第亮起来。\n\n我还是不信，把那几个水果的名字认全了，下午要去幼儿园验证一下爷爷猜的对不对。晚上回家吃完晚饭看新闻，播音员说的话和电视上写的字是对应的吗？只有爷爷很高兴，出门溜弯的时候，逢人就说他孙子脑子开窍了。\n\n还是继续玩沙子，慢慢有其他小孩加入了进来，社恐的我也有了一两个朋友。在幼儿园还是不敢出去做广播体操，于是在教室里上凳子，算是帮老师打扫卫生。爷爷出门溜弯的时候，逢人就对他说，听说你孙子脑子开窍了……\n\n---\n\n然后回到爸爸妈妈那里上小学，从朝夕相见，到周末不见，到只有周末相见，再到每个寒暑假相见，再到没能再见。\n\n这次回家，爸妈开车去接我，回城那日天气极好，车在河边新城一侧的快速路行驶，对岸的天际线满满当当是已经炒到上万一平米的住宅楼。师专的图书馆曾经是河边唯一的高楼，如今隐没难寻，仔细找到后却发现像人群中站着个矮胖墩，尴尬，不如不复相见。\n\n回到家里，旧桌旧椅旧沙发，其中一个沙发上坐着一只洗得干干净净的黄色玩具熊。小熊也已经等了我五年了啊，于是趁着爸妈还在收拾其他行李的机会，把行李箱推进自己房间，轻轻关门，无声地任由泪水流下。\n\n---\n\n有一年我爸从外面回来，兴高采烈要找爷爷，进门就喊“老头儿呢？老头儿呢？”奶奶在包饺子，双手沾了面粉和馅料，只瞪了他一眼。当时的我坐在地上玩，抬头看我爸，感到很不高兴，这个大人怎么这么没大没小的。\n\n如今我也快到了当年我爸的年龄，被他抓到他笔记本电脑面前，问他的网页浏览器图标上怎么有一个头像。原来是不知道什么时候无意中打开了个没用的功能，这点小事都整不明白，想叫他“老头儿”的冲动愈发强烈。"},{"slug":"state-of-blog-2023","filename":"2023-08-15-state-of-blog-2023.md","date":"2023-08-15","title":".doc | 博客咨文 2023","layout":"post","keywords":["doc"],"excerpt":"以往过年的时候，都会在博客写一篇年终总结。今年没写，主要是当时觉得自己春季学期结束就要毕业了，需要努力工作，至少要摆出一副努力工作的样子。","content":"\n以往过年的时候，都会在博客写一篇年终总结。今年没写，主要是当时觉得自己春季学期结束就要毕业了，需要努力工作，至少要摆出一副努力工作的样子。\n\n之前的文章里大概也提到了，论文投稿之前各种小修小补、准备参加三月会、投简历、面试且自以为成功找到了工作，毕业已经是再努力也没办法在春季学期结束之前完成的事情，于是博客恢复了更新。\n\n跟老板说了一下，毕业推后到了暑假，~~上周~~ 上上周完成。45 分钟的演讲，理论上无限实际上 20 来分钟的问答，难度最高的环节是演讲之前打领带，温莎结。\n\n越到这种时候越觉得，如果把人生比作道路，更准确的喻体应该是高速公路，显然不是因为速度高，而在于其中一个一个的进出口，真正留给人选择的机会有限，剩下的时间和路程里，能改变行程的基本都是事故。所谓答辩，不过是出口收费站过 ETC，其重要性还不如和老板半闲聊时，假装不经意地说自己想毕业。\n\n---\n\n### 数据\n\n毕竟是博客咨文，说回博客和各个内容平台的事。\n\n博客的流量比较稳定，每天都能有一点，一月无事也能有 300-400 访问，有更新的时候能到 600-700，碰巧击中流量密码的话能上千。最近新加了 Google Ads，迄今累计收益 50 美分，成了名副其实的洋五毛。(~_~;)\n\n微信公众号一如既往地店大欺客，一篇文章的展示周期在三天以内，不加星标的话几乎看不到推送，打开率基本是关注人数的 1/10。两次例外，可能是击中了流量密码，分别是 《python decorator 装饰器笔记》和《PINN 基于物理的神经网络笔记》，完全不知道为什么，后一篇文章甚至因为格式问题，根本没放读书笔记的主体部分，就因为提了一句《三体》？总之关注人数比上次年终总结多了两倍，接近 200 人了…… (−_−＃)\n\nMatters 吧，不提了，简体中文作者还剩几个？基本上成了微信被删文的备份站。(╯°□°）╯︵ ┻━┻\n\n作为对比，我的前女友·隔壁课题组的学妹·托洛茨基主义者·未明子粉丝，在 B 站发了一个未明子式的视频（一个白板写写画画，旁边一张帅脸，讲授哲学，时长半小时甚至一小时以上），内容是从哲学角度批判高等教育的布尔乔亚劣根性，一把子攒了几千粉丝，几十万观看量。我只看了文字稿和评论区——我看不懂，但我大受震撼.jpg\n\n两相比较，一言以蔽之，曰：彻底失败。\n\n---\n\n### 反省\n\n既然已经做了流量生意，说自己从来就不在乎做没做好是在是有点虚伪了。\n\n和前女友的交往，确实让我学到了很多东西。很多“常识”，要么不是那么寻“常”，要么只是观点而不是知“识”。世界的全貌，观众的参差，是从小走学习考试独木桥的我，需要时刻提醒自己才能意识到的现实。\n\n另一方面，自己作为一个内容消费者，看自己喜欢和订阅的自媒体的时候，也试图摸索出来一些成功密码。一言以蔽之曰：同义反复，频率恰当（前提是以视频为媒介，微博既出，阅读已死）\n\n看自媒体生产的内容这件事，毕竟是一个娱乐行为，只能从各种正事之外挤出来。正事不论多少，每天每周基本是固定的，自媒体要想干得长久，就必须把自己嵌入观众的生活习惯中，尽可能降低观众点进你作品的注意力成本。\n\n频率要求必然导致同义反复，批量生产已被证明是算法青睐的模式。在前期已经做出规模的自媒体大号已经拉起团队，进行专业化分工的当下，个人还想作出点东西，这些模式大概率是自己之前的专业技能，比如做饭之于王刚师傅，下象棋之于徐银川特大，玩红警之于月亮3。暂时能想到的例外，就是时评键政，在此不表。\n\n---\n\n### 嘴硬\n\n作为一个物理博士·生物狗·小镇做题家·眼红美本富二代的苦逼研究生，我写博客的本意是践行费曼学习法，我的本职工作是学习，不是练习。\n\n练习可以同义反复，甚至以耐得住枯燥为法门；学习不行，学习需要耐得住枯燥，但不能以枯燥为荣。\n\n在公理化体系被发明之后，学习需要尽快摆脱同义反复的阶段，从排比句式的知识中识别逻辑关系，添加进自己已有的知识体系，用『逻辑推演』替代『倒背如流』，整个过程就叫领悟，如果这个相变过程比较快速，就叫顿悟。\n\n很多学习类网红搞不清这个道理，也做不到，所以学习笔记一篇一篇地晒，一考试就是大专。\n\n熟练之后，领悟也许可以重复，但无法保证频率的稳定，因为你不能保证所学的每件东西难度都相同。既然不能同义反复，这注定是个于流量无缘的分类，我们也看到，有些学人涉足流量之后，确实变了。\n\n---\n\n### 求职\n\n说到本职工作，之前谈好的博士后岗位无了。\n\n交上两封推荐信之后，那老哥来和我视频联系。他所在大学的研究生罢工活动前不久取得了胜利，博士和博士后的工资待遇提高了不少。老哥的入职 offer 是在工运之前谈妥的，所以启动资金不足以支付新的博士后工资。老哥推荐了一个基金会的资助机会，让我去申请一下。\n\n毕业前 Paul 办了个在家聚会，同学听了我的事之后骂声一片，说这一看就说明之前只打算给最低工资。此事我办得也不太体面，只在申请截止日期之前，用 ChatGPT 随手糊了一套申请材料。差老哥自己的推荐信，他看了一眼说算了。然后说还是瞄准其他资助机会，我也没说啥，基本默认结束了。\n\n老板写推荐信那周的全系茶歇，为了在同仁面前装叉，跟我说他认识那老哥的博导，我不如直接去找她，毕竟博后的门派声望重要得很——我心说，难道门派对博士就不重要了吗。\n\n发邮件过去完全没有回复，老板又给我列了个接近20人的名单。用 ChatGPT 写邮件挨个轰炸了一遍。收到两份回信。\n\n我比较感兴趣的是我们老板做博士时的隔壁。收到两封推荐信之后，约在答辩的第二天面试。面试的结果我拿不准，可能老大爷比较有经验吧，各方面都没有说死，比之前的老哥显得“渣男”一些。这样也好，博后本来就不是个稳定关系，像之前的老哥，话说得太好听反而更加让人失望。\n\n有两条困难，一是经费获批的时间不确定，二是明年他会 sabbatical，不见得有很多时间指导我。最后说 8 月底会有一场相对公开的组会，让我在那之前联系他拿 zoom 链接，了解一下组里的工作方向。\n\n即便顺利的话，入职时间也是明年，在此之前我会在现在的课题组里继续之前没完成的一些项目。\n\n---\n\n### 攀比\n\n说到工作，之前的同学基本上今年或去年也都毕业了。\n\n本科同学里，风神在某公司深圳分部做量子计算，跟我说的年薪是 50 多万，因为入职的时机不好；木木在同一家公司的上海分部，做光刻胶材料，据风神说年薪 100。有望留在学界者，聊哥在锦屏山实验，毕业之后好像要 gap 一年；班长欧陆硕士英国两校读博，没听说毕业的消息，看 arxiv 好像在做凝聚态；潇洒哥继续在国内做天文。其他人都没有消息。\n\n博士同学里，同实验室的学姐在一家生物公司做临时工，年薪 6 万有望转正；小马去了一家量化交易商实习；低我一级的阿卷年底毕业，先去大摩实习。我们年级的天文美国三人组应该都会留在本专业，国际学生则不一定。凝聚态热门组的两人都在本组或合作组继续做博士后。方做粒子物理，回国做博士后。美国博后的普遍工资也就是 5 万上下。比较在意者，和我关系不好的一个女生去某芯片硬件公司做软件工程师，年薪 16 万，上网查了一下，这个工资是“资深软件工程师”，博士刚毕业就能资深吗？狠狠地嫉妒了。\n\n本来觉得毕业去搞钱挺划不来的，毕竟内卷一辈子，可能只是将将摸到别人的起跑线。而且学界身份比之企业界还是更唬人一点，作为一个自由派、民族主义者和中国人，看到前女友那种宗教狂热，又总觉得有些事需要有人做，而我依然有希望够身份去做。“够身份”是一种非形式逻辑谬误，但就像清教徒“上帝的选民”的信念一样，逻辑上不必然成立的东西，实践中却可能有效。\n\n但是看到自己的本职工作投出去连同行评议环节都到不了，直接被编辑就给毙了；业余玩票的天文反而水了两篇文章了，又觉得这评价体系，这逼科研，谁爱做谁做吧。\n\n现在的想法是，暂且做完下面一期的博士后，看看这个学界对我到底如何定价，稍不顺意，就去搞钱。趁休假改掉之前超时工作的毛病，业余时间继续搞点 IT 副业。以我的执行力，希望明年博客总结的时候能做出一些东西。\n"},{"slug":"what-is-intelligence-not-same-as-intelligence-is-what","filename":"2023-06-17-what-is-intelligence-not-same-as-intelligence-is-what.md","date":"2023-06-17","title":".tex | 什么是智能≠智能是什么","layout":"post","keywords":["tex","doc","md","ai"],"hasMath":true,"excerpt":"“什么是智能”的问题每每得不到回答，是因为它的逆问题“智能是什么”没有答案。","content":"\n## 0\n\n这是一篇酬和之作。\n\n徵文标题说的是：\n\n\u003e **機器會製造「內涵」嗎？**\n\u003e \n\n但是正文提出的问题是：\n\n\u003e AI透過程式組合出回答你問題的文字組合，有「涵義」嗎？\n\u003e \n\n可是，「內涵」和「涵義」两个词的——内涵/涵义——就不完全一样啊……\n\n“内涵”(connotation) 通常指词语或表达方式所隐含的情感、态度、暗示或附加的意义。它涉及到词语或表达方式所引起的情感、联想或隐含的观点。也就是弦外之音。\n\n而“涵义”(meaning) 一般指词语、表达方式或行为所传达的字面意义或字面上的定义。它强调的是直接的、明确的意义。\n\n看热闹不嫌事大，那我们再把问题搞复杂一点——在逻辑学里也有一个“内涵”(intension)，和“外延”(extension) 相对应。用**面向对象编程**的说法来理解，一个类里面定义的所有状态量和内部方法的集合，就构成这个类的“**内涵**”；所有（已经和将来能够）从这个类实例化出来的对象的集合，就构成这个类的“**外延**”。\n\n所以看起来，征文者想问的是日常“内涵”也就是言外之意，但是怕杠精（比如我）用有严格定义的逻辑“内涵”解构掉，所以换了“涵义”一词。\n\n这个问题很显然是因应最近大语言模型掀起的这一波 AI 浪潮。这个问题往前再问一句，就是“大语言模型是智能体/有智能吗？”\n\n- 前两年 DeepMind 的 AlphaGo/AlphaZero 系列 AI 在围棋中击败人类棋手时，人们也在问这个问题。\n- 上世纪四五十年代专家系统 (expert system) 刚刚开发的时候，人们也在问这个问题。\n- 从电子计算机往前追溯到机械计算机，甚至是巴比奇的差分机的时候，人们就已经开始问这样的问题了。\n\n这些问题求并集，然后在问题数量趋近于无穷下的极限，就是“什么是智能”。\n\n这样的问题每每得不到回答，是因为它的逆问题“智能是什么”没有答案。我们并没有智能的准确定义，只能一事一论。而之前的智能和非智能体的区别太明显，以至于作出判断也不能对智能的定义有所启发。\n\n## 1\n\n而对“智能是什么”的探究，哲学、逻辑学、计算机科学、生物学、管理学，不同领域的研究者有着不同的思路。\n\n### 古哲学·洞穴之壁与理念世界\n\n古希腊哲学家柏拉图在《理想国》里提到了“洞穴之壁”的寓言故事。\n\n有一群被囚禁在一个深洞的囚徒，从出生开始就被束缚在这个洞穴里，脖子和腿都被铁链锁住，没办法转身或离开。囚徒身后的洞穴入口处有一道火焰，火焰后有人持物体走过，物体的投射在洞穴内的墙壁上形成了影子。囚徒们就以为这些影子就是唯一的存在。\n\n这里的囚徒代表着人类，洞穴代表着世界，影子则代表着我们对于现象世界的感知和观念。人们的知识和信念往往受限于自己的经验和感知，就像囚徒们只看到了洞穴墙壁上的影子，而在影子之外还存在一个理想的理性世界。柏拉图用这个寓言故事表达了他对于人类认识和智慧的理解。所谓智慧，就是从洞穴的影子反过去推测火把前物体的能力。\n\n当然，这种思想被 Marx 主义定性为一种客观唯心主义、唯理论，是受其批判的。\n\n### 逻辑学·从命题到希尔伯特算符\n\n柏拉图的学生亚里士多德，今天在低年级的物理教科书里基本是个反面典型，但他对逻辑学进行了系统化和全面的研究，提出了许多逻辑学的基本概念和原理。这些成果后来成为了欧洲哲学和逻辑学的基石，对西方哲学和科学的发展产生了深远影响。\n\n所谓逻辑，就是研究命题的对错，以及如何判断命题对错的学问。而命题，就是能被判断对错的句子。但是句子显然可以再分成不同成分，于是就发明/发现了主体、客体、谓词、谓词的量词……等等概念，以及用这些概念构造命题的方法。\n\n但是要注意，虽然逻辑主要由语言来表达，但是逻辑还是和语言不同，主体、客体也不等于句子的主语、宾语。这两者的区别，基本可以类比于之前洞穴之壁寓言里的实体和影子。\n\n这种努力到目前为止的巅峰，基本上要数希尔伯特形式化逻辑系统了。感兴趣的朋友可以自行查阅戈得门特《代数学教程》的第一章，这玩意相当于思想界的引体向上，反正我是一个也拉不上去……\n\n### 计算机·从半导体到抽象语法树\n\n希尔伯特是德国的数学家，《代数学教程》也是数学而不是哲学教材。显而易见，逻辑虽然由哲学家奠基，但是主导权很快落到数学家，至少是哲学家兼数学家手里了。\n\n命题的“真”与“非真”同构于 {1, 0}，各种逻辑运算都可以分解成“或”与“非”两种基本逻辑运算的组合，这就是以数学家乔治·布尔 (George Boole) 命名的布尔代数。因为 {1, 0} 又可以同构于半导体电路的高低电位，和各种类似继电器的门电路组合，所以很容易用计算机在物理世界表示出这些逻辑运算。\n\n我们的电脑由上亿个这样的电位和逻辑门组成，一般的科普文章应该会去介绍芯片啊光刻机之类的东西，本文关注的是另一个方面：虽然生产电脑配件的厂商很多，不同的型号的元器件设计不同，组装出的成品应该千差万别，但是他们可以运行同样的程序，理想条件下（虽然实际工程中常常不理想）我们也可以期望他们跑出同样的结果。\n\n这说明所谓计算机科学，并不等同于研究计算机元件的电子科学和工程，这里电科和电子工程相当于洞穴岩壁上的影子，而计算机科学就相当于火光前的物体。这种超越物理的计算本质，一般用一种叫做“抽象语法树”的数据结构来表示。\n\n### 生物学·从神经元到神经网络\n\n人们发明计算机的时候，基本上还是把它当作工具，就没期望它有什么主体性和智慧。\n\n而随着生物学逐渐发现了神经系统及其作用，也随着物理学在二十世纪初的大发展之后的相对平静，很多物理学家开始插手其他学科。既然生命和非生命体的背后都服从同一套物理规律，既然物理学的众多成功经验说明，搞清楚构成系统的所有微观组成就可以理解宏观的系统，那么搞清楚人类的智力器官的基本单元以及相互作用，按理说也就能够理解什么是智慧。\n\n![a cartoon illustrating a neuron](/photos/2023-06-17-neuron.png)\n\n上图是一个神经细胞的结构示意图。从其他神经细胞释放出来的名为神经递质的化学物质，到达神经元左侧短且密集的树突之后，激活细胞膜表面的离子泵，主动运输离子跨过细胞膜，从而产生电信号。电信号沿细胞膜传导到右侧的树突，刺激凸触释放神经递质给下一个细胞。\n\n![a handdrawing style illustration of perceptron](/photos/2023-06-17-perceptron.png)\n\n上图就是根据神经元的工作原理抽象出的数学模型，名为 perceptron。一个 perceptron 就是一个函数，接受多个输入的自变量，加权求和之后套一个非线性的激活函数，得到一个输出。很多个这样的 perceptron 并连和串联，就构成下图，计算机算法中的神经网络。\n\n![a handdrawing style illustration of a neural network](/photos/2023-06-17-neural-network.png)\n\n而从实验方向研究神经系统，我们隔壁系就有，经常来我们系招人。基本上就是在小鼠的天灵盖上锯开一个天窗，然后给它带上个头盔，头盔上有能从天窗伸进去的电极，采集脑神经的电信号。以前头盔有网线伸到实验室天花板，实时传到数据中心的超算。现在好像进步了，改用 Wi-Fi 了。\n\n这实验怎么通过的伦理审查，咱也不知道，咱也不敢问……\n\n### 管理学·DIKW “数据-信息-知识-智慧”模型\n\n![a pyramid of DIKW model](/photos/2023-06-17-DIKW.png)\n\nDIKW 四个字母分别代表 data, information, knowledge, wisdom，即数据、信息、知识、智慧，是一种知识管理中的心智模型。\n\n四个层次，前一层都是后一层的基础，后一层都是对前一层的理解。\n\n如果是书面文字，数据就是笔画和字母；如果是语言，数据就是人声的响度、频率和音色。由笔画/字母/声音组成的有含义的字词就是信息。表示信息之间的关系的，可以判断对错的命题就是知识。包含和统摄各条知识的思想体系，就是智慧。\n\n反过来说，虽然智慧高于思想，但它仍需要通过把各条知识的表达汇总起来，才能被人感知。对知识的命题的理解依赖于构成名字的各个概念的涵义，属于信息水平的内容。而每个字都有不考虑其涵义的笔画字母构成。\n\n这层与层之间**看似**并没有插入额外的内容，智慧可以直接由笔画构成。但是我们一层层理解的深入，其实是不自觉地借用了我们当前社会约定俗成的解读方式。\n\n比如下面这个图片里的符号，对于现代人就只是数据，无法解读成信息。但是对于苏美尔人，这是用楔形文字表示的数字，是等腰直角三角形的腰和直角边的比值，也就是 $$\\sqrt{2}$$ 的近似值。\n\n![sumerian numerical approximation to square root of two](/photos/2023-06-17-ancient-root-2.png)\n\n约定俗成的数据解读方式，也就是关于**数据的数据**，根据西方的构词法，可以叫做“**元**数据”(meta-data)。\n\n数据和元数据一起构成信息，信息和元信息一起构成知识，知识和元知识一起构成智慧。俺坚持写博客的动机，就是用费曼学习法，把无意间使用的元知识显式地表达出来，而且记录下来，争取学而不退转。\n\n## 2\n\n回顾了这些，再来看大语言模型，就会发现它落在了各方努力的延长线的交点。\n\n大语言模型里有一个重要概念叫做“嵌入”(embedding)，就是把语言的基本字元 (token) 可逆地映射到一个超多维度的向量空间里。本来“国王”和“儿子”之间没办法加减乘除，但是嵌入后的向量空间里有加法和数乘，如果嵌入函数选得好，“国王”的向量 + “儿子”的向量，结果向量就约等于“王子”的向量。\n\n![illustration of vector addition from wikipedia](/photos/2023-06-17-vector-addition.png)\n\n生成式语言模型的核心就是一个超多元函数，接受前一个字嵌入后的向量作为输入，给出另一个向量作为输出，用嵌入函数的逆映射翻译成字元；再把旧的输出作为新的输入，直到输出结果是“语段结束”这样一个特殊字元为止。模型训练的过程，主要就是通过现成的语料，拟合这个超多元函数的参数。\n\n从 DIKW 模型来看，语言模型操作的是最基本的数据，它的输出究竟是什么信息，是不是正确的知识，体现了多少智慧，是人根据当下的社会文化来解读的。\n\n而实现 AI 的电子计算机，或是复杂生命的大脑，他们和智能之间的关系，应该就类似于具体的计算机电路和抽象语法树之间的关系。以此类比，未来的智能科学应该会成为一门独立的专业，它和计算机科学和神经生物学的区别，就像今天的电子科学与工程，和计算机科学之间的区别一样。当下神经生物学的热度，将来恐怕多半会被分流。\n\n这种对字符的计算不同于逻辑运算，语言模型不判断输出结果在逻辑上的正确与错误，这既给了他啥都能说几句的 feature，又给了它经常编假消息的 bug。\n\n想要改掉这种错误，引入对 AI 的纠错机制，治本之道恐怕还是诉诸于对世界的正确描述，与理论相关的还是要靠逻辑，与现实相关的还是要靠科学。\n\n只不过，大语言模型提供了一种数据结构，有希望把人类已知的真理储存在一起。对这种数据结构本身的研究，有可能反过来启发科学的发展。柏拉图的洞穴之壁可能不再是一个比喻，未来更大的语言模型的，亿万维度的参数空间有希望成为洞穴门口的那团火。\n\n只不过这一切都是“可能”，现在还只是 AI 的萌芽阶段，还没有足够的证据来证实或者证伪这种畅想。而且 AI 的参数量再大也是有限的，它所能表达的信息也就有限，而真理应当是无限的，就像科学一样，总要训练更新更大的模型，总要发现已知的未知，然后欣然接受更多未知的未知之存在。\n\n如果电子计算机实现的 AI 独立于人类产生了意识和超出人类的智慧，很难想象他们会继续用人类语言这种对他们来说很不方便的方式来交流。\n\n所以，哪怕是做个 AI 生成内容的质检员，科学家依然有事可做。这算是科学的堕落吗？当然不算，如果算的话，那从计算物理也被当作理论物理的那天起，人类就已经投降了（逃）\n\n## 3\n\n现在正面来回答问题：AI透過程式組合出回答你問題的文字組合，有「涵義」嗎？\n\n答：有。\n\n因为语言的「涵義」来自于语言的内容，和整个社会的文化，并不来自于这句话的作者的身份。即便是人与人之间的交流，诉诸身份也是一种非形式逻辑谬误，是理性不足的表现。只有在信息不足仍不得不下结论的时候才该使用，比如法律判决时的自由心证主义和/或法定证据主义。\n\n而鹿妈眼里真人鹿酱与 AI 鹿酱的区别，如果有的话，好像主要体现在动机的区别。动机这种东西，很多智慧不高的生物，比如小猫小狗都会有；而现在的 AI，似乎还没有展现出超出编程者设计的动机。编程写入的信息有限，现有 AI 的动机也就有限，鹿酱的赢面还是很大的。\n\n而动机是生物与非生物的区别吗？而什么是生物 ≠ 生物是什么，那就是另一个含混而复杂的问题了。\n\n## 4\n\n这篇博文发布的时候，高考应该已经结束了，马上该填报志愿了。\n\n那么，西元 2023 年，AI 来袭的当下，该选个啥专业在 AI 浪潮中幸存，或者选个啥专业给 AI 老爷带路呢？\n\n![a screenshot of a quotation from Three Body about attitudes towards aliens](/photos/2023-06-17-three-body-quotation.png)\n\n我的建议是，不要听别人的建议，按自己的兴趣来就好了。\n\n刚刚改开的时候，有一个超级热门的专业，叫科技英语。科技落下了好多年，对外开放需要语言交流，两者一结合应该是热门又稀缺了。结果呢，你现在还听说过这个专业吗？\n\n科技很重要是不错，语言很重要也不错，但是搞科技的人自己可以学英语，学英语的有几个搞得了科技？社会的进步主要靠创新，而创新的方向难以预测，不论这种预测分析听起来多有道理。\n\n如果真的找不到兴趣，那就在能力范围之内，找个难度最高的。如果想从事智力劳动，那数学含量是个不错的衡量标准；如果不排斥体力劳动，那训练时间越长越值得考虑。\n\n但这只是填志愿来不及时的权宜之计，发掘兴趣是人一生的课题。\n\n兴趣不是为了让你成功的时候更得意，毕竟成功的话不论做什么都很得意；\n\n兴趣是为了你不成功时也可以不失意，毕竟平凡才是人生的真谛。\n"},{"slug":"OPT-workshop-notes","filename":"2023-05-19-OPT-workshop-notes.md","date":"2023-05-19","title":".doc | OPT 申请手续笔记","layout":"post","keywords":["doc","xls"],"excerpt":"快毕业了，学校组织了一个毕业实习 OPT 相关的签证手续的讲座，以下为讲座的笔记。仅供参考，可能有理解错误，不构成法律建议，不构成移民广告。","content":"\n\nOPT 是 Optional Practical Training 的缩写，指的是在美国完成学业期间或者之后，允许有一段实习时间，可以工作。但是此时这个人的身份依然是学生，不是工作签证，更不是移民签证。\n\n## 谁有资格申请\n\n已进入美国，并且维持了至少2个学期 F1 学生身份者。\n\n### 学业完成前 pre-completion\n\n在毕业前，学习过程中间可以申请，但是需要与导师商量，很少见。\n\n### 完成后 post-completion\n\n毕业后找工作，应付非移民工作签证还没通过的那段时间，这是最常见的情况。\n\n或者除了没提交论文，其他毕业要求都已满足的准毕业生 all but dissertation, 简称 ABD。申请这种情况的学生需要在EAD日期结束之前完成学位，否则需要回国完成学位（远程答辩）。\n\n## OPT 基础知识\n\n每个有学位的学习阶段都有一次 OPT 机会，但是必须在毕业前申请，不申请就毕业视为放弃，不能攒到下一段学习中多用一次。\n\n仍然是F1学生，不是工作签证。\n\n申请时不需要已经找到工作。也允许换工作和同时干多个工作。可以做有偿/无偿工作，本大学不提供无偿工作的职位。\n\n不需要雇主做任何事情，申请人自己赞助。\n\n## 在 OPT 期间维持学生身份\n\n\u003e 也就是说，下面各条没做到的话会被取消合法居留身份。\n\n![how to maintain the OPT status](/photos/2023-05-19-main-status.png)\n\nOPT 期间从事的工作必须与你的专业领域相关。\n\nOPT 申请通过后会得到一张 EAD 卡作为资格证明。在获得 EAD 卡之前不具备工作资格，没有例外。\n\n只能在 EAD 卡上指明的起止日期内工作。\n\n### 信息申报\n\n在OPT获批并开始日期后，需要创建SEVP门户账户。\n\n以下信息变更之后的 10 天之内，需要通过 SEVP 通知当局：美国家庭地址、雇主及其地址、就业起止日期。\n\n### 不超过90天的失业时间\n\n每周工作少于20小时被视为失业。\n\n在EAD卡的授权期内累计失业时间不能超过90天。\n\n### 上课\n\n只能兼修与当前或未来领域无关的课程，具体情况可以联系 OISS 咨询。\n\n### 离开美国旅行\n\n可以，但是需要携带相关文件\n\n1. 护照\n2. 有效F1签证\n3. 带签名的I20\n4. EAD\n5. 就业证明（可选，如果已经找到工作了的话）\n\n**在OPT申请挂起期间出美国也是可能的，但存在风险**，因为EAD将会寄到申请人的美国地址。\n\n再次进入美国后要在 I-94 记录 ([i94.cbp.dhs.gov](http://i94.cbp.dhs.gov/)) 中确认自己的身份仍然是 F1 学生和 “D/S” 状态。\n\n## 谁可以申请延长 OPT\n\n### 24个月STEM OPT延期\n\n科学、技术、工程、数学专业的毕业生可以申请将 OPT 再延长24个月。\n\n在OPT结束的最后90天内申请。USCIS必须在 OPT 结束前收到申请。\n\n申请时必须有一份工作，雇主必须注册E-verify，需要填 I-983 表。\n\n### H1B 申请获批之前，可以申请“配额间隙” Cap Gap\n\n配额间隙雇主在每年 4 月 1 日提交 H1B 申请。如果获批，H1B从10月1日开始。如果OPT在10月1日之前到期，“配额间隙”将在SEVIS中延长，直到H1B的决定出来为止。\n\n## 申请流程\n\n- 必须在美国申请OPT，应做好准备直到收到 EAD 都一直呆在美国。\n1. 参加研讨会，获取一份申请资料包\n2. 从 OISS 顾问处申请 OPT I-20 表格\n3. 准备并提交 I-765 表格给USCIS，申请OPT\n\n### 需要上传的 I-765 文件与OPT I-20的证明文件\n\n- 移民文件：护照、I-94\n- 之前的 OPT 和CPT，如果没有可以跳过\n- 2英寸 × 2英寸的白底照片；拍摄日期在30天内；公共图书馆、Walgreens和CVS提供此服务\n\n### 时间窗口\n\n**估计申请审批过程需要4周时间。**（往年是3个月）\n\n申请窗口：学业结束日期前的90天 ↔ I-20 结束日期 ↔ 节目结束日期后60天\n\n选择 OPT 的开始日期：I-20 结束日期后的第1天 ↔ 60天宽限期结束\n\n![time window of OPT application](/photos/2023-05-19-time-window.png)\n"},{"slug":"job-search-efforts-during-march-meeting","filename":"2023-04-05-job-search-efforts-during-march-meeting.md","date":"2023-04-05","title":".doc | 三月会·两面试·一座谈","layout":"post","keywords":["doc"],"excerpt":"记录今年三月会上找工作的相关努力和随想。","content":"\n### 会前\n\n今年的三月会在拉斯维加斯。\n\n距离会议大约两周，下巴有根胡子根部起了毛囊炎，很快恶化成带状孢疹，俗称上火。美国学年的下学期在5月份就结束了，毕业论文还没怎么写。找不到工作，博士后项目的申请季正忙着论文投稿，眼睁睁地看着滑过了两个项目的申请截至期；隔壁组老师转发的博后项目，在我发了申请材料之后不久，就在网上又发了一遍招聘广告；申请了两个德国研究所，一个已经发了拒信，另一个没回应……还要准备会议上的演讲，不上火才怪。\n\n周五组里演讲排练，本打算重用去年开会的幻灯片，发现论文稿中的主要内容居然都没有，才意识到过去的一年着实也还是做了不少事情。收到德国研究所的拒信消沉了一天，但是很快 ChatGPT 热潮兴起，于是把读博期间所有听说过的教授拉了一个名单，把他们的课题组主页介绍，和 AI 生成的我论文稿的摘要一起喂给 ChatGPT，让它写套词邮件。效果还行，收到一条回复，约我会议期间见面。回信的老哥用跨尺度方法研究动物行为，同时在欧洲和东亚经营两个课题组，去年会上很出风头。会后不久来我校作讲座，我们一起吃过一顿午饭。会前不久 APS 的邮件列表里有人群发邮件招人，是今年即将在西部某校开始教职的现任博后，目前正工作于给我拒信的德国研究所，尽管嫉妒，但也让他吃了一发 ChatGPT 邮件。\n\n### 开会\n\n步行去会场签到，领到了名牌，居然就是一张厚纸上挂了根绳，连个塑封都没有。\n\n签到后勘查了一下会场的布置。生物物理有关的会场都在主走廊第一条岔路的两侧，每个门口有展板，上书每个时段的演讲主题。不像去年只有我一个人参会，今年组里三个人都有演讲，再加上之前在我们组的硕士凯蒂和她现在的同学，把日程一标记，发现感兴趣的小节基本上都冲突了。我虽然社恐，但还是觉得人际关系比较重要，或者说因为社恐所以觉得人际关系比较重要？\n\n回到宾馆，收到了群发邮件哥的回复，要我确定三月会后一两周的时间，安排一次远程面试，介绍一下我现在的工作，以及说明我对他的实验室可以有何贡献。有点意外，因为他群发的邮件说了自己会来参加会议，我的邮件也说了自己愿意在会议期间见面，这个回信是兴趣缺缺的意思？略感失落。\n\n### 面试 1\n\n在不同的分会场之间乱窜的时候，去得晚会场坐满的话需要在后排站着。在其中一个场子站听的时候，一个穿绿色冲锋衣，背运动背包的老哥走了进来，听了一会就出去了。感觉有点面熟，和群发哥个人网站上的照片有点像。\n\n于是悄悄离场，看到他正在走廊里和朋友谈话，身前的名牌有名字那面翻过去了。暂时离开闲逛，发现主走廊里供应咖啡，排队接了一杯，回来再刺探，确定是他。\n\n打招呼，原来是 ChatGPT 太啰嗦了，邮件最后才说我也在三月会，他没看到。问了我的演讲在什么时候，他会来听，就不需要 zoom 会议了。\n\n我的演讲所在时段的题目十分古怪，感觉跟我做的东西关系不大。进去候场的时候发现，整个小节除了我们实验室的俩人，其余的演讲都是同一个方向，而且研究的都是同一模式生物的同一结构。我们组俩人的演讲在整个小节的最后，自我感觉表现不错，破题的时候拿小节的题目开了个玩笑，结束的时候时间正好用完。群发哥在我演讲前半小时左右到会场，我们结束之后就离开了，没有上来和我说话。\n\n结束后回到宾馆，收到了他的邮件，约我第二天下午见面。在会议网站搜索了一下他的日程，发现正是在他的演讲结束之后。看他的演讲题目题目，似乎是关于自己的最近一篇论文的，把文章喂给 AI 工具生成了一份摘要，没来得及看就被同学拉出去玩了……\n\n他的演讲在第二天下午，时间上处于整个时段的中间。我在时段开始的时候就坐进会场了，他比我晚来了一会，坐了一会又出去了，。会场比我演讲的时候大一倍，但是也没坐满。他在上面讲的时候，我在下面看昨天 AI 生成的文章总结，感觉信息量比截至当时听过的绝大多数演讲的内容丰富，演讲的水平也远在平均水平之上。研究对象比我现在的研究对象在生物系统的层次上高一个层级，但是感觉研究的方式和流程类似。研究的完成度高得多，理论和实验都有。虽然理论似乎是所在领域挺传统的一个模型，但是毕竟比我们只是摆实验数据好太多了。\n\n会后见面，我们在走廊旁边找了俩座位，我拿出了自己的幻灯片，他从头到尾把细节问了一遍，确认我会做我们组模式生物的基因编辑，又问了我在整个项目中的工作占比。\n\n然后就问了一个堪比谈恋爱时被表白一方“你为什么喜欢我”的天问：“你贯穿整个研究生涯的问题是什么？”啊这，大脑直接宕机。看我实在憋得不行了，老哥让我可以说中文的，原来他能听懂，但是不会说。最后实在逼急眼了，说了实话——我就是想在学界混吃等死，ChatGPT 都来了，强人工智能还会远嘛，物理学不存在了啦。老哥噗哧一声乐了。\n\n然后让我提问，我问假期有多少，答曰按学校政策办，养过细胞的应该知道周末可能是要去学校的，所以只要工作量够，周中不作出勤要求。\n\n感觉聊得比较顺利，最后说回头会发邮件给我，让我给他两三个联系方式，要推荐信。\n\n### 面试 2\n\n本来感觉反馈不错的大佬，感觉开会第一天可能大家都还没安顿下来，于是周二才再发邮件确认见面时间，这样一来就约到了周五中午，在 DBIO 的服务台旁边见面。\n\n本来打算事前准备的，他的课题组有三四个演讲，但是基本上都和我的演讲或者前一个面试冲突，最后只听了一个。\n\n结果周五中午就是整个会议的最后一节了，DBIO 的桌子已经撤走了，大家心猿意马归心似箭。12点过了十分钟左右见到了人，五分钟之后和他谈话的人离开。到处都在收拾关门，我们找了一个桌椅已经搬空的房间，屋里只有一个学生躺在地上睡觉，我们两人相对席地而坐。\n\n刚要掏电脑出来给他复述一下我的演讲，他摆了摆手，让我尽可能简短地口头概括自己的工作。后面的一系列问答都是这个风格，两个人在地上打坐，快问快答，跟禅宗的机锋一样。上来是为什么要做现在的工作，我自认为说的还行，但是说着说着发现这™不是上一场面试的答案嘛，早怎么没想到呢？然后得意忘形，忘了这位大佬不是做组织结构而是研究行为的，本来很容易圆过去的，没发挥好。然后是我本科时最喜欢的一门课是什么，这个问题上次吃午饭的时候他就说自己喜欢问，于是让我狠狠地装了一逼。再然后就是“臭做实验的怎么来我们理论组要饭来了”和“臭做结构的怎么来我们研究行为这儿要饭来了”两大问题，回答得不好。之前得意忘形，把自己手里另有 offer 的事说出去了，导致后半截我们双方又有点敷衍。\n\n最后和我交底，东亚的研究所有足够的经费，虽然他一年还是花不少时间去当地，但是并不总是能见面指导。欧洲方面如果决意入职的话，他需要专门申请经费。这个意思已经比较明确了，所以我们很客气地互相说可以在花时间考虑一下，日后可以邮件联系。\n\n### 座谈\n\n座谈会其实发生在我演讲所在当天中午，两次面试之前。\n\n所谓座谈，是德国几个研究机构联合组织的，在德国从事研究的相关信息的宣讲会。 ~~~~位置离生物物理的几个会场比较远，凯蒂朋友的演讲结束之后从会场溜出来，赶到的时候已经开始了。进门两侧有贝果面包、果酱和乳酪，这么多天第一次吃午饭。\n\n会场有六七个圆桌，每个桌上放了一个写有研究机构名字的小标签，桌边坐一两个对应研究所的工作人员。主持人依次介绍了每个桌的研究机构，然后给机构的负责人三五分钟的时间介绍自己。之后就是自由时间，我看马普所的桌子就在面包附近，其他机构也不怎么做基础科学的研究，就找了个椅子坐下了。马普所的负责人是一个满头白发的长者，着装在与会者中偏正式，气场很足，神似觉爷。\n\n第一个问题关于移民德国的难度，以及德国是不是有点排外。长者答曰德国现在是移民政策最为开放的国家之一，考虑到“之一”，再考虑到灯塔国的外国人毕业在非学术岗位连工作签证都要抽签才能拿到，不算边境“走线”的话，这个回答确也不能算错。德国之声之前也出过一个视频，也有同样的论断，但是那个视频后面还有半截 “aber…”。\n\n随后有人问马普所的项目和岗位结构。回答的结果和之前了解的差不多。每个研究所有自己的招生权，一期博士后的时间一般是两年，只在必要时延长至三年或更长。新知识是马普所的研究职位除所长之外几乎没有终身职务，研究所就是研究者找终身教职之前的副本村。但是不知是政府还是啥，有条规则说拿到博士学位的九年后雇佣单位必须提供终身职，结果就是九年之期以前就会被清退。此条是否与前一个回答相矛盾，没问。\n\n被问到马普所更青睐于何种研究时，长者明确说是更困难更有野心的题目，而非为了拿到教职而做的安全题目，甚至很明确地说，就是要发掘那些将来有可能拿诺奖的题目和学者。这一点自然是不同于美国式的口头上的自由放任和实际操作中的专打安全球，有趣是也和国内的看法不同。当年 CLS 面试的时候，我也不知天高地厚地说自己有鸿鹄之志，台下老师们答曰“不要那么功利嘛”，没想到两个视计划与顶层设计为制度优势的文化，对于功利的理解竟有如此大的不同。\n\n还有人问马普所选择申请人的标准。回答是每个研究所有自己的标准。但是他也谈到了自己的标准：学习成绩为主，*做研究的时候应该很清楚自己在做的东西底层的物理是什么。*那么如何判断学习成绩呢？主要是来自名校，然后是成绩排名，如果是没太听说过名字的学校，成绩排名的要求也就更高。回答很真诚，让人对如此明显的第一学历歧视也反感不起来，甚至没有反驳的意愿。\n\n这个回答显然是针对美国模式，选拔以科研经历为主，在论文作者栏占位更重要。研究组的骨干是不需要花时间上课的博士生，申请博士的本科生，能对整个项目有多少贡献呢？而且这些贡献基本上都是献祭对课业努力得来的，上方标注的句子就是对此的吐槽。更何况考虑到国内学术环境的昭著臭名，基本上都需要海外的研究经历，除了珠峰计划这类特殊项目提供公派名额，基本都是自费联系国外导师，不仅把学术选拔变成了与学术无关的信息差，和学生经济条件的选拔，还豢养了一个倒卖此类信息的中介产业，此种政策的学术正义性何在？\n\n但是再回来看呢，成绩单能反映出多少成绩？想出国的学生，有多少明目张胆地跟上课老师要成绩？某校直到几年前还有“及格重修”这种合法的更改成绩单的操作，配合上国内各科考试历年题目的高度相似性，不利于安定团结的话就不说了。既然要求名校，那么高中出国又比本科毕业出国优势不止一点半点，又是阶级筛选，殊途同归。\n\n两者一邱之貉，都是在不同的大义名分之下，落实为可执行的操作时，原有的大义不断被解构和歪曲，过程中不断把自己的判断力外包。外包无所谓，毕竟分工既是社会发展的表现，又是重要的动力。然而，拿飞机举例，把乘客安全送到目的地需要机组和地勤配合和信任，但是安全送达毕竟是有客观标准的。而教育和研究系统，其培养成果本身就由自己消化，选拔过程天然构成运动员兼任裁判员的现象，我选出来的人有不行的，没被选中的也有行的，那好办，把资源全圈给胜选者，再把落选者的路掐断就好了嘛……\n\n两种模式菜鸡互啄，谁也取代不了谁，归根到底，还是今天的物理学自己乏善可陈。量子力学和相对论肇起之时的摧枯拉朽不提，就连标准模型构建过程中的萧规曹随，今天也难望其项背。扎实理解知识，你的实际研究中能用到多少？早早上手研究，这么多年有多少有价值的工作自己心里没数？年轻人觉得攸关前途命运的大事，不过是老板平凡一天中收件箱里的一段字节罢了，录不录你，录你还是录他，区别不大。\n\n一言以蔽之，时无英雄，遂使竖子成名。\n\n下午，前一天和群发哥聊天的一个女士在他之后不久演讲，看幻灯片首页的工作单位，似乎是他博士实验室的后辈，今天即将入职他现在的研究所，~~也就是说，这位正是抢走了我 offer 的仇雠~~。于是在朋友圈发了一条吐槽。她的演讲超时被主持人叫停了，刚想追加一条评论“超时了，真菜”，想到最近几天朋友圈发的有点多，忍住了。\n\n大学四年我唯一佩服过的人峰神在下面回复，说他本来也想申请同一个研究所。我问他现在去哪，过了几分钟看到回复“上班了”，赶紧揶揄说“富哥V我50”，但是座谈会上的话在此刻反刍，叹息收敛成深呼吸，默默收起了手机。\n"},{"slug":"say-her-name-mahsa-amini","filename":"2022-10-06-say-her-name-mahsa-amini.md","date":"2022-10-06","title":".doc | 说出她名字：Mahsa Amini","layout":"post","keywords":["doc"],"excerpt":"A nation only deserves as much as its people have courage for.","content":"\n\u003e A nation only deserves as much as its people have courage for.\n\u003e \n\n物理系每周三有一个例行的讲座，讲座之前会找一些学生和演讲者一起吃午饭，公款报销。本周轮到我们的研究方向，于是我们几个报名吃饭。中午隔壁组的同学来和我们碰头，学姐摘下耳机来，说抱歉自己走神了没听见敲门，在自己国家正在革命关头，实在是很难静下心来。\n\n几天前，一位伊朗的库尔德族年轻女子 Mahsa Amini 走在街头，因为头巾佩戴“不规范”，被道德警察带走，随后死于警察局。以此为导火索，抗争运动骤起。相关消息雪片般传到 Twitter，当时和她同行的两位女士用身体堵在将 Animi 带走的警车前阻止警车离开，被警车推行数米仍不放弃；有位男士帮其他示威者挡子弹，衬衫脱下，背上一片霰弹枪打出的血红的浅浅弹坑；有一名警察独自持械在街上巡逻，被周围涌上来的路人围攻；夜里，地上一垛篝火，用女士点燃的头巾组成，一名年轻女孩解下头巾手握住一端，头巾在转圈的舞步中飘扬在空中，落入火中溅起一片掌声；成群的示威者攀爬一座建筑，撕下了挂在建筑外墙上的政治人物画像……\n\n周四，我在办公室摸鱼刷推特，学姐进来看到了我的屏幕，让我赶紧往上翻，对对，就是这个，快转发，谢谢了。于是我就转发了，但那条推特是 BBC 中文网对这件事的一个追踪报道，配的文案是“伊朗总统同意调查阿米尼之死”，转发了之后显得我很像是“你看青天大老爷这不是开恩显灵了嘛”的小粉红，不过解释起来实在是过于复杂，于是就没说啥。\n\n问学姐伊朗国内的情况，她说之前已经有过多次抗争了，希望这次不一样。现在伊朗国内已经断网，很多消息传不出来，听说政府已经开始使用致命武器，至少有几十个人牺牲。其实三十多年前，学姐的母亲还年轻的时候，就因为类似的原因被道德警察拘捕，受了鞭刑，和她一起被捕的人里面甚至还有孕妇；她的一个叔叔因为曾经在君主制时期伊朗的军队中服役，被新政府处决，前两年伊斯兰共和国政府甚至捣毁了被处决者墓园的墓碑，就为了阻止人们纪念他们……海外的伊朗人决定在周末集会示威，希望我有时间的话也能参加，还短信发给了我活动的海报。\n\n我给海报配文“学姐的社会实践活动”，发到微信朋友圈，没有被微信删掉。今年新到美国的一个学弟看到了，来问我情况，还问我能不能来我办公室问学姐一些问题。学弟的朋友圈里经常转一些妇女劳工等等弱势群体权益方面的公众号文章，偶尔原创一些痛斥新自由主义的全球化架空了工会剥夺了工人机会的进步主义小作文。虽然我不是左派，但是当下的中国，能站在 political apathy 的对立面，至少能算是个短途的同路人吧。\n\n周五中午物理系自助烧烤的时候，我们年级好多人都收到并且接受了集会的邀请。学姐还说找了当地的 FOX 记者报道此事，于是大家一起开共和党及其中下层选民的玩笑，像什么“我看了一眼 Tucker Carlson 的节目，就一眼哈，说的也不是完全没道理嘛”、“我现在不那么怀疑地平说了，你看我们脚底下这块地不就挺平的嘛”……学弟也来吃烧烤了，本来想把他拉过来直接介绍给学姐，但毕竟我很社恐，看他在和自己的同级同学社交，一直没有机会。\n\n下午学弟发微信来，我去他办公室把他领过来，介绍给学姐，学姐很高兴有更多人关注这件事。学弟的问题主要是这个活动有什么女性主义的组织在策划，有什么女性主义的诉求。学姐说没有，大家主要是靠想法而不是靠组织在一起的，都觉得这个政权该完蛋了。学姐问他中国的情况，学弟说他只是反对清零政策。学弟问我能不能让他搭车，我说我也不打算开车，毕竟虽然是和平示威，但仍有出人意料发展的可能性。然后学弟离开我们办公室又折回来问，现场会不会有伊朗政府的特工。学姐说每个人的想法都不一样，可能也有支持伊朗政府的伊朗人，但他们应该不会来这种活动。\n\n集会在市区一个公园的东南角，是围绕公园四条路中最繁华的路口。因为之前提到的顾虑，我把 UBER 终点定到了公园中心，结果到了发现公园比想象中大很多，走到集会地还要接近 20 分钟。下车的地方正在赶集，具象化的“人类的悲欢并不相通”。走着走者隐约听到汽车的鸣笛声和人声，才确定自己没走错方向。\n\n![]({{ site.baseurl }}/assets/photos/2022-10-06-say-her-name-wide.jpg)\n\n不断说着“借过”在人群中穿行，找到了学姐，身边是好几位同级的同学。安鲁和女朋友也在，手里举着一个大标语板，背后写的是“拜登，别为了核协议出卖伊朗的百姓”。\n\n迟到有一个好处，就是人群已经成型，你需要做的就是融入人群，滴水入海，心理上的门槛和负担要小很多。饶是如此，刚开始的时候也不敢大声喊口号，之前从学姐那里拿的海报也不好意思举高。后来发现正常说话的声音会完全淹没在人群中，再发现全力喊的声音依然会淹没在人群中，于是渐渐放松下来。\n\n![]({{ site.baseurl }}/assets/photos/2022-10-06-say-her-name-narrow.jpg)\n\n“Women, life, freedom!”\n\n“Say her name! Mahsa Amini!”\n\n“Your silence, their violence!”\n\n“Down with the dictators!”\n\n“No to Islamic Republic!”\n\n“Iranians, make your choice! USA, be their voice!”\n\n人群里有几个喇叭和音箱，有人领头，其他人附和。短的口号领头者念一遍大家重复一遍，长的口号领头人说上句大家说下句。喊过几句英语口号，就用波斯语重复一遍。我一边跟着喊口号，一边替最后一句话捏一把汗，复习在中国，社会活动者需要付出怎样的努力与境外势力切割，即便如此仍不能幸免于诉诸动机谬误。\n\n人群原本占据公园一角的部分草坪和人行道，后来几个活跃分子提了一个音箱到右转和直行车道之间的安全岛上，面对大家带头喊口号。经过的汽车很多鸣笛响应，打开车窗对我们竖大拇指或者比 V 字。活动的最高潮，有几位伊朗女孩在安全岛上，用剪刀剪掉了自己的长发。街对面商铺的玻璃墙上，人群的映像蔚然。\n\n午后，活动在大家齐唱一首伊朗歌曲后解散。临走之时，一对老夫妇看到我胸前捧着的黑白宣传画，很喜欢，于是送给了他们。学姐和她丈夫开车来，把我送了回去，我们在我住处附近的奶茶店喝了奶茶。\n\n在游行现场我没有看到学弟，但他晚上在朋友圈发了三个单词，是波斯语中 women, life, freedom 的拉丁字母转写。\n"},{"slug":"how-to-narrate","filename":"2022-10-03-how-to-narrate.md","date":"2022-10-03","title":"转载 | 《叙述》","layout":"post","keywords":["doc","html"],"excerpt":"近日在喷嚏网发现了一个讲如何写记叙文的系列","content":"\n\u003e 近日在喷嚏网发现了一个讲如何写记叙文的系列，但是给出的链接已经过期，通过搜索引擎找到了豆瓣用户[username太长了](https://www.douban.com/people/usernametoolong/)的转载。([https://www.douban.com/note/509996701](https://www.douban.com/note/509996701))\n\u003e\n\u003e 转载日期在 2015 年，豆瓣的发帖者称作者是 xilei, 给出的原文链接也来自喷嚏网，也已过期。\n\n\u003chr class=\"slender\"\u003e\n\n## 【叙述-1】厕所的故事\n\n\u003e xilei 发布于 2009-3-1 14:36:00\n\n### （一）\n\n小时候，看电影的时候，我一直有一个疑问：为什么电影里的人从来不上厕所？\n\n那时，我还是个8、9岁的小男孩。学校里还经常包场看电影。小孩子好动，看电影前总是乱跑。电影放到一半了，才想起好像应该去上厕所。\n\n那时，好看的电影很多：《红孩子》、《地道战》、《平原游击队》、《闪闪的红星》还有《黑三角》。尿憋久了，忍不住就往电影院后面的厕所里跑。每次都是摸黑横穿过排椅，猫腰跑到最后，掀起左边的布帘，杀将进去。\n\n有时，晚上也做梦，有人追。自己在前面跑啊，跑啊。怎么也找不到厕所。\n\n白天的时候，回想起来。才觉得自己发现了一个天大的秘密：\n\n自己从来没有在电影里看见有人上厕所。就是写有男、女的那种。\n\n这念头只是一闪而过，觉得自己思想觉悟不高，有点低俗了。电影里都是革命英雄人物。他们每天都在忙正事。哪好意思象小屁孩那样，一天到晚没事，就想上厕所。\n\n我想，电影里不谈厕所的事情，才是正常的事情。厕所都是无关紧要的事情，不好意思谈。\n\n后来，看电影就习惯了。再也没想过那个问题：\n\n为什么电影里的人从来不上厕所？\n\n### （二）\n\n长大后，看《真实的谎言》。突然发现，电影里的人居然还是可以上厕所的。\n\n电影里有这样一个情节：\n\n有人要杀施瓦辛格，老施从容地走进厕所里，对着洗手间的镜子，从容地洗手。他预先把一个伪装成肥皂盒一样的袖珍摄影机对着门口。门外和他背后的人都干些什么，在他的特工眼镜里，一清二楚。\n\n那个追踪过来的杀手，走到老施的身后，掏出枪来，指着老施的后脑勺，正好扣动扳机。说时迟，那时快，老施抢在子弹出来的前面，头一低。然后，反手给了杀手一家伙，把他就给解决了。\n\n后来，又有人追杀过来。老施在厕所的隔间里，躲来躲去。杀手的子弹象水枪一样，一排排一阵阵扫过去。厕所里精彩的枪战定格在这样一个画面中：一位正在上厕所的老者，战战兢兢地打开隔间门。裤子还在脚下，他吓坏了。不知道发生了什么。\n\n我这个乐啊：这老施太帅了。导演也太可爱了。可算看见电影里的人上厕所了。\n\n这是我印象比较深的一次电影记忆。好比如说之前，你发现一些东西怎么从来没人碰过，你会以为：肯定那里面有什么禁忌和秘密。\n\n结果，有一天，你发现：怎么就不能碰呢。看吧，就算是厕所，人家也可以用的那么好。\n\n再一想，也很简单：既然在生活中就存在，为何就不能在电影中存在呢？\n\n### （三）\n\n后来，电影看多了。才发现电影中的厕所，其实，跟生活中的厕所的用途不大一样。\n\n生活中的厕所，就是生理意义上的用途。而电影，则是一种转折的秘密结构，或者是一种意外迭出的神秘角落。\n\n在《疯狂的石头》中，那个假装断腿打上石膏骗自己父亲的儿子，在厕所里得意洋洋地打电话。他不知道外面正站着七窍生烟的老爸。这时的厕所，是一个很容易泄密的地方；\n\n在《三峡好人》中，赵涛就是在洗手间墙上的海报上，了解到了情敌的秘密；\n\n在《非诚勿扰》中，葛优用手抹去脸上的水，对着镜子，心里说：豁出去了。他走出洗手间，跟舒淇讲他过去的故事。\n\n在导演们看来，厕所是这样的一种地方：\n\n一个人面对自己，短暂休息的机会；\n\n一个意外事件发生的秘密场所；\n\n一个用来泄密，发生转折的时间和空间。\n\n所以，重要的，是不是电影中的人要不要上厕所。而是，电影的故事需不需要他们上厕所。\n\n如果软弱的人，需要面对自己，给自己打气的话，他们就上厕所；\n\n如果有些事情不好交代，或者需要一个承上启下的引子的时候，他们就上厕所；\n\n如果需要枪战的时候，有两个比较好的选择，一个是楼顶，另一个就是厕所。\n\n### （四）\n\n有一次，儿子给我说：老师说，有些东西好写，比如说，一件最难忘的事情，或记一件好人好事，或者是某年的春天什么的，就好写。再比如说，让你写厕所，你怎么写呢？\n\n估计大多数人都会写成：在厕所里发生的一件好人好事。某个同学生病了。其它的同学学习雷锋，自告奋勇，发扬互助精神，帮他完成任务。最后，得到了老师的表扬。\n\n我说：“其实，写什么都可以。没有什么禁忌。关键在于你怎么写。你要明白你为何要写它。”\n\n我就跟他讲了关于厕所的一些想法。“有时，事件发生的地点，是有特别的意义的。或者因为真实，或者因为叙述的逻辑需要。当你觉得一个东西，不便于直接书写的时候，你可以把它放在一连串的事件中来看。或许它什么都不是，只是一个架子，只是一个纽带或者转折。讲什么很重要，但如何讲也是非常重要的。你得抓住一些特别的东西，把它直接写下来就成。”\n\n\u003chr class=\"slender\"\u003e\n\n## 【叙述-2】假如苏东坡和陶渊明来到现代\n\n\u003e xilei 发布于 2009-3-8 15:03:00\n\n\u003e \n\n### （一）\n\n本周末，儿子的家庭作业要写一篇作文。题目是：《假如苏东坡和陶渊明来到现代》。\n\n我喜欢这样的题目：有挑战，也很有想象的空间。\n\n早上起来的时候，我看见儿子已经在开始动笔了。我很想知道，他会写些什么呢。但是，我拿定主意，在他写完成之前，不要按奈不住自己，提前给他什么想法。\n\n反过来，我给自己出了一个题：如果这是我的作业，我会怎么写呢？\n\n### （二）\n\n首先，做一些随意的，但是适当的假设和推理：\n\n苏东坡和陶渊明都是古代的人了。如果他们来到现代，肯定有多东西都没见过。\n\n苏东坡会问陶渊明：家乐福是干什么的啊？什么东西叫法国啊？\n\n陶渊明说：咱没见识过。走，去看看先。\n\n.....\n\n他们也没看过电影。放电影前，看见周围的人手里都拿个小盒子，凑在耳边说话。\n\n电影放一半了，进入到卧室部分，苏轼说：世风日下啊，世风日下啊.....\n\n.....\n\n换了一部《后天》，陶渊明说：现代的人越来越不象话了，你看他们把气候搞成啥样了。能跟我悠然见南山的时候比吗？\n\n.....\n\n信马由缰后，罗列出一些素材，最简单的办法就是：想想这两个人会是什么性格，他们在现代的生活中，会有哪些不适应，会有哪些冲突，或者可以接受的地方。\n\n接下来，就是把这些松散的东西，组合在一起。\n\n换句话说，想归想。最终，你得把这些东西用一种形式串起来。给大家讲一个有点意思的故事。\n\n讲故事，就是寻找一种叙述的结构：\n\n你得有一种符合逻辑的讲述方式，把你认为重要的那点东西讲出来，让人们感觉是真的一样。\n\n接下来，怎么做呢？\n\n\n### （三）\n\n我打算借鉴或者说是山寨一下《非诚勿扰》的结构。\n\n让我们换一种角度来看这部电影。如果我们现在是故事的讲述者，我们很容易发现故事的叙述逻辑。\n\n简单的说，《非诚勿扰》分为三个部分：\n\n第一部分【和谐终端机】是个引子。这部分主要的作用是解释秦奋相亲的缘由。就跟后面故事要安排笑笑为第三者一样，都是为了一个目的：叙述的合理性。\n\n故事的讲述，首先要设置一些合理的背景，让人容易接受这样的假设，从而对故事的延续造成驱动的悬念。\n\n故事的前提，如果设置的很好。读者的好奇心，就会自然而然地引发。而对于故事的叙述者来说，故事的主角在合理的背景中可以自己活动、讲话，展开故事。\n\n第二部分【相亲】是一种相亲大集合。各种相亲的场合堆叠在一起，有一种现实的对应感，不乏幽默的细节。\n\n在这样的部分中，细节和重复，是叙述的基本要素。\n\n细节可以跟人留下真实、幽默等等愉悦的情感。而重复可以象多棱镜一样，折射出同一件事情的不同的侧面。\n\n有时，重复也是一种衬托。为主线的叙述铺路。\n\n冯小刚曾经说：《非诚勿扰》的第三部分【北海道】才是电影中最重要的部分。这部分流露出某种公路电影的气质，自然主义的写实的叙述，贯穿其中。\n\n所以说，讲好一个故事，是不简单的事情。\n\n你得有足够的细节，有简练而充分的交待和暗示。在叙述的过程中，有各种各样合理的花絮铺垫、转承。\n\n不见得都是刻意为之，但是，为了避免叙述的平淡，你都找到很多方法，让叙述的吸引力一直牵着人走。\n\n### （四）\n\n我们暂时不用考虑过于复杂的处理。不过，我们仍然可以借鉴到一些可用的东西。\n\n让我这样来考虑《假如苏东坡和陶渊明来到现代》如何使用类似的结构：\n\n我先要安排一个理由，说明苏东坡和陶渊明来到现代：他们怎么来的，来干什么。然后，自然牵连出后面的故事。\n\n然后，就用重复的方法，写他们在不同的地方的不同经历：他们在家乐福怎么样；他们看电影怎么样，看见手机会怎么反应.....\n\n最后，该如何收场呢：给他们一个选择，他们是会选择留在现代，还是古代呢？\n\n我把整个这个构思的过程这样梳理一次：\n\n1. 我们都某处获取了一个想法，或者一些素材，我们感觉有一个可写的主题，或者我们认为讲述本身能带来很多东西，我们有叙述的冲动，但是，可能不知道叙述会带我们到什么地方；\n2. 寻找一种合理的结构，把之前的素材，串联起来。\n\n你要判断的是：是否来龙去脉讲清楚了。人家是否能接受这样叙述的逻辑，你所想要表达的情感强度，是否被合理地从线性的叙述中被释放出来。\n\n什么地方不够充分，什么细节又该舍弃。\n\n有时，故事中的人会带你到你想去的地方，甚至会超过你最初的想象。\n\n\n### （五）\n\n到此为止，我并没有真正开始写这篇文章，我只是在说一种思路：怎么用一种简单的方法，把一些松散的素材连接在一起。\n\n到中午的时候，儿子的作文写完了。我问他是怎么写的。\n\n儿子说：“我是分开写的。先写的是苏东坡。”\n\n“苏东坡去参加高考，一看试卷，他就乐坏了。难题都是文言文，这不都是送分嘛。还有几道题，考的就是他自己的文章。”\n\n我说：“这很有趣。后来呢？”\n\n儿子说：“不过，苏东坡很惨。他没考上大学。他语文考满分，但是数理化，都不会。只是偏才。他跟教育部写信申述，没用。我想，苏东坡在现代社会肯定呆不下去。大学生毕业都找不到工作，还去卖猪肉。苏东坡肯定不愿意卖猪肉。”\n\n我都听乐了：“好玩。说说陶渊明。”\n\n儿子说：“陶渊明简单。他去超市买牛奶，去哪儿都抱怨：你们这是怎么搞的？又是三聚氰胺，又是特仑苏的......”\n\n我说：“想法都挺好的。把你的作文拿来，我看看。”\n\n看了一下，我跟儿子说：“你的想法，真的很棒。”\n\n“写作文，能这样，已经不错了。不过，如果有时间的话，还可以写的更充分一点。最好的叙述是，把一些个想法，还原成一个故事。你去寻找一些简单可用的结构，把这些素材重新组织一下。”\n\n儿子说：“什么是结构？”\n\n我说：“你可以把它简单理解为一种容器。一种有逻辑、有形状的东西。很多人只所以有很多想法写不出来，是因为没有注意或者说是不知道一些基本的容器结构。这些结构有时会帮你把一些零碎的东西安排好，帮你梳理出其中的联系。”\n\n早在09年到来之前，我就答应儿子，给他讲一些简单的写文字的方法。你不需要把自己变成作家，但是，你也不要认为写字有多么高深莫测。\n\n我想把这些文字都放在一个栏目下，名字就叫做《叙述》。\n\n我希望能讲出一些真正简单、实用的东西。我会讲：为什么大多数人都喜欢听故事，故事是如何写出来的；为何《读者文摘》总是从讲故事开始；discovery的叙述有什么特点；什么是马尔克斯的叙述；以及如何从看一部电影，或是听一部交响乐，一个奇怪的建筑中，获取结构的启示。\n\n当然，我还是想告诉他：最重要的是忠实于自己的直觉，不要被流传已久的谬误所禁锢。很多东西，都是时间积累下来的。素材，或者情感，甚至所谓的技巧。保持对外部世界的敏感，保持自己独立的思考，比是否会写字更为重要。\n\n我想告诉他：只有一个人有需要表达的动力，加之适当的合理的技巧，他就能感觉到叙述的快乐。各种各样的叙述方式，也会被合理的创造并使用起来。\n\n\n\u003chr class=\"slender\"\u003e\n\n## 【叙述-3】为何我们需要从故事开始 \n\n\u003e xilei 发布于 2009-3-22 20:57:00 \n\n### （一） \n\n我从不给儿子买任何版本的作文精选，尤其是那种高考或者中考的满分作文选。我今天还在网上看见这么一篇博文，里面贴了几篇近几年的全国高考的满分作文。\n\n我摘一些文字如下：\n\n【摘选1】\n\n\u003e 他想起了周瑜。“小乔初嫁了，雄姿英发，羽扇纶巾，谈笑间樯橹灰飞烟灭。”他问自己难道自己不正是那东吴的督都吗？自己满腹经纶，胸中有的是治国平天下的笔墨，而此时？面对这一片漫漫江水，他陷入沉思。 \n\u003e \n\u003e 他的思绪像长了翅膀似的，继续飞扬，斟一杯酒，临江而酾，是祭奠那死去的英雄，也是祭奠自己的往昔。是啊！他清醒了：哀吾生之须臾倒不如托遗响于悲风，取山间之色，听江上清风之歌唱。他不再悲观，不再耿耿于怀。 \n\u003e \n\u003e 后来，他用自己的行动证实自己的顿悟。他在黄州兴修水利，奖励耕织，清廉从政。黄州的百姓感念这一位父母官。后来修了一祠庙来缅怀这一伟大的文人，知心的父母官。文学的殿堂里永远可以听见那《赤壁赋》华美的乐章。\n\n【摘选2】\n\n\u003e 古人说：“上有天堂，下有苏杭。”美丽的西湖承载了多少中国文人的梦，苏轼虽被贬至此，然而他没有悲怆，没有哭天地，没有愤愤不平，风雨任平生。他懂得了“为官一任，造福一方”的简单道理。于是，一道苏堤便横卧西湖。他要让西湖储藏的心灵，淹没他所有的痛苦，所有的忧伤。 \n\u003e \n\u003e 《明月几时有》一词道尽了诗人在外想家想人的心境。“不知天上宫阙，今夕是何年……”然而他也没有因此悲伤叹息，对生活失去勇气，失去信心。他让天上的明月传达自己对远在千里之外的故人、家人的思念。一句“但愿人长久，千里共婵娟”不仅让他的风雨一生得到升华，而且抚慰了多少中国人思乡思人的心灵。这个时候，他把其豁达的心交给了饱怀思念之情的人们。他要告诉他们――生活的风雨摧不倒我们，我们同在。 \n\n\n【摘选3】\n\n\u003e 生命如风。  \n\u003e \n\u003e 好一个亘古的比喻。你也许感慨于它的来也匆匆，去也匆匆，不着一丝痕迹。我却跋山涉水，在时空里淘尽沙砾，找到了这个比喻的真谛：  \n\u003e \n\u003e 唯有风，可以穿越荆棘。  \n\u003e \n\u003e 狄金森把人生描绘成篱笆墙的内外，我们一层又一层地爬过，事实上，这层层篱笆缀满荆棘，我们通过时，往往遍体鳞伤，身心俱毁。这时，你看到，风在墙外千萦百折，不屈地呼啸而过，空气中凝结下壮观的痕迹。  \n\u003e \n\u003e 我们趋行在人生这个亘古的旅途，在坎坷中奔跑，在挫折里涅盘，忧愁缠满全身，痛苦飘洒一地。我们累，却无从止歇；我们苦，却无法回避。烈日暴雨来过，飞沙走石来过，我们布满伤痕，还要面对一片片荆棘的丛林。  \n\n\u003chr class=\"slender\"\u003e\n\n我不知道是不是真的。如果这样的作文，能代表中学作文的高水平的话, 我只能说：这种小老头似的文风要不得。\n\n貌似文采飞扬，不过是拾人牙慧，东拼西凑。所有的词语都是奇形怪状地堆砌在一起。内心空洞，毫无生气。\n\n看着累不说，而且看过后给你留下了什么呢？空空荡荡，什么也没有留下。\n\n不过，我敢说，很多语文老师，都喜欢这种。而且也希望学生们写成这种东西。\n\n你打眼一看，觉得好有才气，其实不然。\n\n文字的东西，才气不是这样表现的。而且，才气在文字里面根本就什么也不是。\n\n如果你的文字根本就没有什么东西，什么样的才气都是空气。\n\n### （二） \n\n我觉得现在的学校里面根本教不好孩子写字。\n\n作文是从小学就开始了。小学生写记叙文，中学生写散文，或议论文。\n\n这样的安排还给人一种错觉：散文、议论文比记叙文更高级。你看每年的高考，好像都是写议论文的。没有说写记叙文的。\n\n但是，事实上：很多学生都不能完整地表达一件事情，完整地记录下一个属于自己的想法。\n\n写记叙文，记一次春游，记一件难忘的事情，还是记一次雷锋活动。能不能记点别的？\n\n写散文，老师就说：“散文就是形散而神不散”。\n\n什么都是散的，最后，就只有散架。\n\n老师说：你们要多背好字好句。写作文都会用上了。\n\n所以，我们都看见了满分作文都是这么干的：词藻华丽无比，思想都是来自于课本。所有的口水都是用经典名言粘上的。最后，都成了应试的八股文。\n\n这不是写字，这是报纸剪贴。\n\n\n### （三）\n\n其实，对于学生来说，他们写作文真正困惑的是这样两件事情：\n\n1. 不知道什么东西是应该写的；\n2. 想写东西，但是不知道如何写。\n\n老师们给出的建议是：你要多读多看多背多写啊。这些话说了等于没说一样！\n\n干什么事情，不需要勤奋啊。\n\n其实，我们生活的世界如此丰富，很多时候，我们并没有真正用心去看，去发现。\n\n有一次，儿子问我：“你怎么知道什么该写，什么不该写呢？” \n\n我说：“你得明白人们为何要写。绝大多数人写东西的动机，其实，很简单。”\n\n“比如说，你要矫正牙齿，我就得给老师写假条。我就得写明合理的原因。因为这时你是在解释，说明理由。你就必须简单明了。”\n\n“我们在公司里面，也经常写东西。比如说，你去给客户讲解一个产品。你不能总是说：我们的东西好啊，赶快来买啊。这样做没人会理你。你得跟人家讲一个关于产品的故事。你们为什么要做这样一件产品，人们为何需要它，你们是如何做到的。人们为什么要相信你们。这种叙述的逻辑，是理所应当的，就是要和人家建立信任。”\n\n“当然，很多人写文字，是为了自己。为自己留下一些记忆。因为每个人的一生中，都有很多这样的时刻。要么让人触动，要么让人反省。很多人想写下来，就是为了留住并传递那种难忘的感情。”\n\n“你知道为什么人们喜欢唱歌吗？唱歌是一种宣泄，歌词和音乐，能把人内心的记忆、想象和憧憬，激发出来。它能帮助人释放能量，发现自我。文字也能做到这一点。”\n\n“从来没有人规定，什么东西不可以写。但是，对于写的人一定要知道自己想写什么，这很重要。如果认为华丽的文字就是好的文字，这从根本上就错了。文字展示的不是关于词语的记忆，而是展示人的行为、情感，乃至心灵。”\n\n“对学生来说，文字首先是一种沟通工具。写得结构简单，逻辑清晰，简单明了。是最起码的要求。”\n\n### （四）\n\n我一直认为，学校里的作文课，应该教学生写故事。\n\n先不要把故事想的很复杂，可以是很简单的那种，不是复杂到小说家写的那种。\n\n我说应该这样做的五个理由：\n\n1. 故事可以培养人的观察能力；\n2. 故事可以训练人的逻辑结构的能力；\n3. 故事可以让人习惯把握自己的思维和情感；\n4. 故事不仅仅是关于文字的，也可以是图像的，音乐的。我们生活的世界中，有很多现成的例子，可以让我们自我习得。\n5. 故事不光是属于文学家，不光是属于语文课。而是属于我们所有人，所有的事业和工作。\n\n### （五）\n\n要了解故事，首先要学习感受：什么样的故事是好故事。\n\n这不需要任何人教。好的故事是有影响力的。它能让你笑，让你痛哭，让你沉思。不需要任何专家告诉你里面含有何种高深的理论。你的直觉和情感能告诉你一切。\n\n让我们来看看好故事的能量。\n\n春晚的小品《不差钱》是好故事。好故事能让人发笑，让人快乐；\n\n加菲猫说：球状就是身材。这是语录，也是故事，也是广告。能对自我宽恕和幽默的故事，是好故事。哪怕这故事，只有一句。但是，它能引发一大堆人会心的微笑；\n\n关于人性，巴菲特就爱跟投资的人经常讲一个故事：\n\n有一个搞投资的人死了，去见上帝，要求上帝，安排他到天堂里去过。\n\n上帝说：“天堂里都是你们这些搞投资的，都满了，只有地狱里还有空。”\n\n那个人说：“那麻烦你跟他们说一句：地狱里有石油。”\n\n过了一阵，天堂里的人都开始往地狱里跑。\n\n上帝对那个人说：“你可以搬到天堂了。他们都下地狱了。”\n\n那个人说：“不行，我也得去地狱看看，万一是真的呢。”\n\n好的故事好像什么也没说，但是，该说的它都说了。\n\n昨天晚上，我和儿子一起看了一个视频短片《世界构建》。这个短片讲述了这样的一个故事：\n\n只剩下1个小时了。\n\n一个男人，一跃而起。你看他在用手比划着什么。\n\n你看见他3D建模一样复制楼房。雕刻窗户，建造银行、路灯和一切。他在建造梦想中的美丽的世界。那个世界里有古老的街道，美丽的花朵，还有温暖的阳光。\n\n当他快建立好的时候，他把时间调到阳光明媚的时候。他跑进街道旁的一扇木门里躲了起来。\n\n这时，风吹着旗子开始动起来。远处传来钟声。\n\n一位美丽的女人打着赤脚，从街道的另一扇门出来，她惊喜地看着这一切。她突然转身向男人的方向走来。\n\n男人看见屋顶有一块还没有上色，脸上掠过一丝焦虑。\n\n女人走到盛开的花朵的面前，开心地笑了。在门后的男人，也释然了。\n\n女人回望着街道上的一切，念念不舍的转过街道。离去。\n\n男人从门后走出来。他听见风的声音，他听见破裂的声音，他的世界，被风吹走了。\n\n男人伸手从空中攫取一张照片，照片上是那个美丽的女人，男人送上一个吻。他的脸上还有泪水。\n\n画面变黑了。\n\n男人推开病房的门。\n\n他俯身亲吻没有知觉的妻子。\n\n桌上放着一个水杯，杯里插着那朵梦境中的花朵。\n\n我跟儿子都被这个故事所深深感动、震撼。美妙的感受，无法言语。\n\n我跟儿子说：“这就是我心目中的好故事。有些故事，可以带给你生理上的愉悦。但是，有些故事能真正激发你内心崇高的情感。”\n\n儿子说：“故事好像很简单，但是，好像又很多。”\n\n我说：“这就是故事的魅力。”\n\n儿子说：“为什么会这样呢？”\n\n我说：“这本来是一个很老套的故事。但是这样的讲述，出乎人的意料。故事的结尾，才揭示出男人建造世界的秘密。让你看了，才觉得：这是个多么美好、温暖的世界。”\n\n### （六）\n\n儿子说：“我可写不出这种故事。”\n\n我说：“这里面有很高的技巧了。我们一时都达不到。但是不妨碍我们从这些故事里面学到一些有用的东西。”\n\n“叙述是一种认知的流动。人的大脑很奇怪。你看，故事都是线性讲述的。但是，故事一旦讲完后，人们会在大脑里重新组合，并梳理出他们理解的逻辑。”\n\n“好的故事是这样的：人们不需要被教育，需要的是被触动。”\n\n“所以，写故事，不需要排比句。不需要花俏的东西。因为华丽的东西，没有情感的流动，那就是一张纸，单薄，没有体积，也没有温度。写字，不是炫耀，而是真诚地叙述。你得首先说服自己相信，你才能写好它。故事里面的场景，人物，始终有一种东西在里面流动。它会把写字的人带到该去的地方，也会把读者带到该去的地方。”\n\n“所以，写好作文最简单的做法就是：先学习叙述一件事情。让叙述的过程来说明一切。”\n\n“在写的过程中，你会看见自己曾经看见过什么，你记住了什么，什么东西对你很大的影响，什么东西是你一直不能面对的。什么东西是你不能忘记的。当然，最重要的是，你要写下来，当时的过程，或者一种情绪。”\n\n儿子说：“我什么时候才能做到这点呢？”\n\n我说：“现在就开始，你可以做到。”\n\n我说：“你们老师让你们看《读者》文摘，你们总是记得抄句子。其实，《读者》的文章有个特点，你们却没有学。”\n\n儿子说：“什么特点。”\n\n我说：“就是讲故事。你看《读者》是不是都是这样的：讲一个故事，然后，说明一个道理。”\n\n儿子说：“好像差不多都是这样。”\n\n我说：“如果你仔细看，你会发现每个故事里面还是有很多细节上的不同。故事的讲述，有很多不同的技巧，你需要自己自己反复体会，你有故事，故事就象水一样，你让它从山上流下来，就是瀑布，让它绕着村庄走，它就是河流。有时，你的任务，就是得寻找适合的容器，把它盛起来。容器就是结构，下次我们就会讲到，到底有哪些常用的结构，可以让我们把水盛起来。”\n\n“《读者》这类的文章，基本上是属于启发、教益类的故事。你会发现很多讲述是如此简单，只是需要足够的细节，足够的真实而已。不过，你想过没有：我们生活不就是这样吗？每天有太多这样那样的故事，我们从来没有时间写下来。我们觉得那都是小事，不值得写，或者说，谁会在乎呢。其实，只要你在乎，你就可以写下来。”\n\n儿子说：“这跟我们的作文有啥不同呢？”\n\n我说：“故事是一种平实的叙述，不需要言必名言名句。不需要排比句，不需要形容词。只需要老老实实地讲述。你要用真实的情感去带动你的文字，不要去在意它如何艰涩，不好看，而要在乎它是否在人的内心中有情绪的积淀，它是否是有重量，是否有温度。它是否忠实于你的个性，是否忠实于你的情感。是否有一种消融内心坚冰的力量。”\n\n“故事就是写自己想说的话，那是一种精神的印记。如果你想放弃自己的想法，只是想什么是符合标准的做法，那么，可以肯定的是，你永远也不会知道如何开始。”\n\n\u003chr class=\"slender\"\u003e\n\n## 【叙述-4】叙述的基本原理 \n\n\u003e xilei 发布于 2009-3-29 17:55:00 \n\n### （一）\n\n每年的三月，儿子都会参加市上的电脑比赛。作品通过后，有一个技术的答辩会。答辩会的主要内容有两部分。第一部分是由作者自己讲解作品，第二部分是由老师提问。\n\n我一直觉得讲解作品是参加竞赛中，最有价值的部分。孩子们在学习里，总是学太多的书面知识，有太多的书面作文、太多的书面考试。而面对很多人做口头讲演的机会却少之有少。\n\n一个人，在大众面前演讲，向大家介绍自己和自己的作品，展示自己的思维，是一种非常重要的能力。这需要适当的思维的训练，需要养成构思、叙述、传达的循环反复的习惯过程。\n\n我一直希望学校的语文课有口头语言表达的训练内容：它是一些可以参照的简单可用的基本构思框架，任何孩子一个都可以模仿和练习，任何人都可以用自己的话，运用素材，讲在众人面前述一个跟自己有关的故事。\n\n这听起来有点难，或许就算是大人都不容易做到。因为在我们从小到大的课程中，没有人告诉过我们该如何做。\n\n### （二）\n\n我至今还记得儿子小学六年级的时候，我第一次跟他讲这样的一课：如何介绍自己的作品。\n\n我当初的想法是这样的：不仅要教会儿子如何完成这件事情，而且我要把思考的全部逻辑过程展示出来。让他以后遇到类似的事情，就知道该如何去做。\n\n我跟儿子说：不论是文字，还是图像，还是口头表达，有一条重要的法则，要牢记：所有的叙述，都是向受众传递信息。\n\n儿子说：“什么是受众？”\n\n我说：“你去讲你的作品，你是讲给评委老师听，还是讲给下面的同学听呢？”\n\n儿子说：“当然是老师听了。”\n\n我说：“老师就是受众。就是那些想听你话的人，或者说是你想讲给他听的人。”\n\n儿子说：“明白了。”\n\n我说：“受众确定了。那么，接下来，你就要想啊：老师想听我讲什么呢？”\n\n儿子说：“我怎么知道老师想听什么？”\n\n我说：“如果是平时的话，你的确不知道。但是，现在不同。你要想你是干什么去了？”\n\n儿子说：“我是去答辩。”\n\n我说：“老师呢？”\n\n儿子：“老师是评委。”\n\n我说：“对了。你做演讲有一个环境的因素在里面：就是要围绕你的作品来讲。老师来干什么来了？老师做两件事：第一件事是先听你说；第二件事是根据竞赛的规则提问、考核。第一件事就是我们现在正准备的，第二件事才真正是老师要做的。”\n\n“好，我们现在换一个角度来看。我告诉你，如果我是你们的老师，我会怎么评审呢？”\n\n儿子说：“你怎么知道我们老师想什么呢？”\n\n我说：“我仔细读过你们的电脑竞赛的要求，参赛标准。你来看看，老师想问什么，其实，这上面都有！”\n\n我把网站上的参赛和答辩要求，都指给儿子看。\n\n“这是主题要求。这就告诉我们，你给老师介绍的时候，要说明你的主题是符合要求的。”\n\n“这是技术要求。这就告诉我们，你要给老师讲你的作品里面使用了哪些特别的电脑技术。”\n\n“这是制作的过程介绍。这就告诉我们：你要给老师讲你的制作过程。”\n\n儿子说：“我明白，是不是相当于要把这些都放在演讲的过程中？”\n\n我说：“是的。”\n\n儿子说：“可是这么多东西，我怎么才能把它串在一起，一口气讲下来呢？”\n\n我说：“儿子，不急。刚才我们讲的是一个内容的分析过程。现在，我们知道了，老师可能想听哪些部分的内容，我们做的是一个内容的逻辑分析。但是，要口头演讲的话，要换一种方式，才能把所有的方式串起来。”\n\n“首先，我们把整个内容的逻辑简单梳理一下，让它听起来是流畅、合理的。”\n\n“当有很多主题，看起来比较散的时候，你又想把很多东西串起来。一个简单的办法，就是不要一个一个讲这些主题，而是讲述做事的过程。然后，把你想讲的东西再挂上去。”\n\n“如果我是你，我就会这样给老师讲这样几个方面：\n\n首先，我会讲我为何会做这个动画，我是如何想到的，我为何可以做这个动画；为什么要先讲这个呢？因为你和老师是初次见面。你是谁，老师也不知道。你怎么构思，怎么想，老师也不知道。\n\n讲事情的起源，看起来是多余的，其实不是。很多叙述是一种认知的需要。你要打动人，说服人，最重要的是要让人知道你是什么样的人。\n\n可是，老师不知道你是什么样的人。老师是从你说的话，做的事情中了解。\n\n所以说，你给老师讲你做动画构思的过程，就是在传递这样的信息：我是怎样的一个牛人。\n\n其次，我要讲在制作的过程中，我使用了何种技术，我是如何理解这种技术，如何使用它的，不去谈技术的细节，因为后面老师会考你的，你说的东西要简单明了，不要太复杂，复杂容易让自己偏离重点。\n\n所以，你还是这个办法，讲你用什么技术解决了什么问题，讲你使用技术的体会。你做的东西，那种独特的感受，一定有。你就讲最关键的一、两个体会就行。\n\n老师一听就明白了：你到了哪个层次。他心里基本上就有数了。\n\n再三，我会重点讲一些制作过程中的细节。我如何收集材料，如何准备，如何付出努力去完成项目等等。细节会为作品强说服力，也会感染他人。”\n\n我给儿子说：“演讲有两个屡试不爽的基本技巧：一是讲述一个关于自己的故事；二是讲述自己在做事的过程中，如何思考和行动。”\n\n儿子根据我的建议，做了一份讲演稿：\n\n\u003e 《我们的神舟》——演讲稿\n\u003e \n\u003e 开始提示：声音大点、语速不宜太快，讲话时，给老师交流眼神\n\u003e \n\u003e 各位老师们好！\n\u003e \n\u003e 我是某某小学六年级学生某某，我的参赛作品是一个Flash动画，名字叫做《我们的神舟》。\n\u003e \n\u003e 我为什么要做这个动画呢？\n\u003e \n\u003e 因为在2005年，我在学校观看了神舟六号发射的现场直播。之后我就开始对神舟火箭的整个发射过程有了兴趣，所以我回到家马上就在网上查找神舟六号的相关资料，知道了什么是逃逸塔，什么是返回舱，而且对神州发射的整个过程都有了比较详细的了解。因为在当时，我刚好又在自学Flash，就趁着这个对神舟感兴趣的时期，我就做了第一个版本的神舟动画，大体上的过程都有了，但比较粗糙。尽管这样，但还是得到了家长与老师的鼓励与支持。\n\u003e \n\u003e 1年多过去了，遇到这项比赛。第一，我还是挺喜欢神舟的；第二，我觉得自己的Flash水平也有所提高，所以我就又做了第二个版本的神舟动画，作为参赛作品。下面就请老师们观看我的第二个版本的神舟动画吧！\n\u003e \n\u003e ……（开始播放动画）\n\u003e \n\u003e 因为神舟构造本身就很复杂，所以我把火箭的许多部件做成一个一个的元件，便好组装、拆散；做元件的另一个优势是，神舟火箭不管怎么放大、缩小、旋转，都不会失真。\n\u003e \n\u003e 我在场景上又加入了淡入淡出，这样，每个场景的背景都可以循环多用，并且看起来也非常紧凑、连贯。\n\u003e \n\u003e 整个作品制作分成了三个阶段，第一个阶段是收集材料，花费了很多的业余时间：我自己上网站，查相关资料，反复看神州发射的录像。整个构思是在这一年中积累完成的，第二个阶段是元件制作，整个flash包含了上百个元件。最后第三个阶段是合成。第二个阶段和第三个阶段大约画了整整5天的时间。\n\u003e \n\u003e 今年，我国将发射嫦娥号一号。我现在又在自学3DsMax，我希望自己能在电脑上展现这一过程。\n\u003e \n\u003e 谢谢大家！\n\u003e \n\u003e ……（技术操作+答辩）\n\n这个演讲稿，儿子反复修改过好几次，又在口头上反复练习演讲了好多次。\n\n这是一个经过精心设计的看起来很普通的演讲，但仔细读来，里面包含了丰富的信息和情感。\n\n### （三）\n\n上面的演讲的设计过程，就是我教给儿子的关于叙述的基本原理的一次应用。\n\n所谓叙述的基本原理，是我自己给出的一个思维基本框架的定义。\n\n这个原理的核心内容如下：\n\n任何目标导向的叙述过程的设计，都可以用一种如下的方式或者步骤来进行思考：\n\n1. 内容分析：确定受众；\n2. 逻辑设计：确定内容要素、梳理内容的逻辑结构；\n3. 结构设计：在生活中寻找叙述的框架，最简单有效的方法就是讲述一个关于自己的故事，完成逻辑的基本诉求：把所有的要点元素或素材组织在一起，合理地放置在故事的背景中；\n4. 情感设计：在讲述事件本身的同时，讲述态度、情感和方法；\n\n文字只是叙述的一种表象。本质上说，叙述是一种思维的过程，只要通过合理地训练、反复地练习，久而久之，每个人都能获得这种人性的技巧。\n\n\n\u003chr class=\"slender\"\u003e\n\n## 【叙述-5】海明威的叙述\n\n\u003e xilei 发布于 2009-5-1 15:37:00 \n\n### （一）\n\n在很多年前，我读过一篇海明威的短篇故事，名字叫《三声枪响》。讲的是一个叫尼克的小孩跟随父亲在湖边度假时发生的故事。\n\n故事一开始，就写尼克在帐篷里脱衣服，看见帐篷外父亲和叔父的影子在灯光的照耀下晃动。尼克为自己感到羞愧。\n\n是什么事情让尼克感到羞愧呢？\n\n接下来的文字开始转入正题。\n\n原来，很多天前的一天傍晚，父亲和叔父要离开宿营地去远处的地方打猎。临走的时候，父亲跟尼克说：如果有什么事情，需要他们返回的话，就朝天放上三枪。\n\n尼克看见：父亲把船往湖里推，叔父坐在船头。过了一阵，尼克听见叔父开始唱歌，也听见父亲划桨的声音。\n\n尼克开始往回走，周围的黑暗迫使他尽快返回。\n\n尼克返回营地，在帐篷里躺下，怎么也睡不着。他不时听见很多声音，但是，他不能确定那些声音到底是不是猫头鹰的声音。\n\n尼克干脆坐起来看书，他强迫自己不去想其他的事情。尼克看书，直到天亮。\n\n直到后来的某天晚上，尼克听到从森林的远处传来了一种怪异的声音，他马上拿起了猎枪，对着天放了三枪。\n\n等尼克的父亲和叔父回来的时候，他们看见尼克已经睡着了。\n\n叔父说：看吧，我说没事的。这小家伙就是胆小罢了。\n\n尼克的父亲说：他还小。毕竟是孩子呢。\n\n第二天早上尼克起来后，父亲带他到外面。父亲让他意识到，晚上听见的声音，可能是风刮到树枝发出的。\n\n所以，这天晚上，尼克躲在帐篷里，感到羞愧。\n\n然后，故事回到开始的部分。\n\n故事结尾的时候，是这样的：父亲叫尼克赶快穿上衣服，对他说：我们一起出去。\n\n\n### （二）\n\n昨天晚上，我跟儿子讲《三声枪响》，我还找出原文让他自己看了一遍。儿子看完的时候，还又往后翻了一页。\n\n我说：没有了。他的结尾有点突然。戛然而止。\n\n儿子点了点头。\n\n我说：你有什么感觉没有？\n\n儿子说：他写了尼克的害怕。但是，居然在故事里面几乎没有提到过“害怕”这个词。\n\n我说：对的。他没有说，但是你看过后，就是感觉到眼前自然就有那种画面，那种感觉可以直达你这里。\n\n儿子使劲点了点头。\n\n我说：换了是你们，你们会写成什么呢？你们肯定会写：我害怕，我很害怕，我害怕得毛骨悚然，我非常非常害怕。然后，举例说哪个古人或者名人曾经在哪个角落里说过孤独、害怕之类的话。你说是不是？\n\n第一段A名人的名言名句，第二段B古人的经典诗句。第三段本大人的总结发言，得出关于害怕的意义。\n\n儿子几乎笑倒。\n\n不过，我不是信口雌黄。现在的中学生作文里面充斥着这样的作文思路。\n\n比如很多的中学生最佳作文，一般都是这样的：满篇都是格言名句，充满了华丽而空洞的形容词。一段段的排比句，貌似看起来很有气势和文采。\n\n我跟儿子说：这种写法，只能败坏一个人的感觉。一个人老是想着别人说过的话，那他不可能相信自己的眼睛，他也不敢写出自己的真实情感。如果是这样的话，写文字成了什么？炫耀记忆和词藻的游戏？\n\n儿子问：按你的说法，记忆好词好句的做法，是不对的？\n\n我说：做摘要，没有错。但是，不能那么用。你要写东西的时候，最好先把别人的东西忘掉。你要首先想的是：我要表达一个什么，我要传递一个什么感觉，或者想法。\n\n我跟儿子说，我可以证明：不用名言名句，不用排比句，一样可以写出非常棒的文字。\n\n那个时候，我想到了海明威的《三声枪响》。\n\n### （三）\n\n你可以把《三声枪响》，看成是海明威写的一篇假期的游记作文。\n\n海明威可能就是那个尼克，一个跟父亲和叔父出去外面打猎度假的小孩。\n\n海明威没有写度假的那个地方风景如何的优美，因为优美的风景太多，以至于小孩总是忘乎所以，根本没有时间来欣赏美景。美景太多了，小孩只是高兴，不需要象大人那样热爱抒情。\n\n小孩有小孩担心的事情，比如说害怕，或者死亡。\n\n海明威想写一个独特一点的东西：他在自己的头脑里，寻找自己作为孩子时的独特感受。或许，有一天，他发现了一个小秘密：每个孩子都有害怕的经历。那种感觉，常被大人忽略，其实，却常常隐藏在孩子心里。\n\n海明威决定写一个关于尼克的故事，故事的主题是：害怕。那个时候，他还没有想好作文题目，该起一个什么样的名字。\n\n海明威想：我要怎样才能写出一个关于“害怕”的故事呢？\n\n海明威没有去用google搜索，那个时候，还没有互联网。海明威也没去图书馆找。他不喜欢去翻别人说过的话，他也不喜欢用很多的形容词。他喜欢站着写作，因为这样，他能写得简短，快些写完，才好休息。\n\n在海明威的头脑里，“害怕”不是那样一个被压扁的形容词，而是由一个又一个有形状能刺激人想象的场景：\n\n他不觉得非要说出那个词，才能表现出那样的效果。他认为环境、人的动作和对话，会泄露出很多的秘密。\n\n比如说：你可以想象一下，在伸手不见五指的夜晚，你一个人在回家的路上。突然身后，传来了细碎的脚步声。那声音跟你的脚步的节奏一样，人走的时候，那声音在，人不走的时候，那声音也停止了。\n\n环境会改变一个人的心理感受，人的内心的想象也会助长那种你想要的暗示。\n\n你可以想象：一个小孩独自穿过漆黑的森林返回帐篷时的心情。他的脚踩在路上干燥的树叶上发出的声音，周围猫头鹰的叫声，有一声没一声。\n\n如果是你，你会怎样？\n\n为了抵御某种内心的情绪，一个人会采取很多行为来抵制，或者反抗。\n\n尼克躺在帐篷里，怎么也睡不着。他只能爬起来看书，让自己什么也不去想。\n\n海明威并不把所有的东西都记录下来，他只是在陈述，一个动作接着一个动作。动作之间的逻辑关联，他认为读者其实完全可以想到的。无须详细一一描述。\n\n他相信：文字的张力是靠读者的想象去完成的。\n\n海明威写了一个孩子在黑夜中的挣扎：先是努力去分辨各种声音，然后，爬起来读书。最后，实在忍无可忍，尼克还是爬起来，朝天上放了三枪。\n\n看，到这里，你终于明白了，海明威为何把作文题目叫《三声枪响》了。\n\n尼克的父亲和叔父的对话，简单明了。寥寥数语，人物的性格和态度栩栩如生。\n\n但这一切都不是写作者说出来的，而是从故事中人物的言语中自然渗透出来的。\n\n\n### （四）\n\n我跟儿子说：一般来说，写文字和其他的任何创作和设计，没有任何不同。你不要认为，写作和设计都是一拍脑袋，就什么都有了。其实不是，基本的过程是有章法可循的。\n\n比如说，你要写一个东西。可能是因为某个触动和感觉在你心里蕴藏很久了，或者你有一种希望表述的愿望，让你有了一种想要捕捉的愿望。\n\n一般来说，很多创作的起点，都是来源于灵感或想法的。\n\n接下来的是，如何用一系列的手术刀似的方法，来捕捉这种创意。很多时候，你会发现，你开始的想法或许是不完整的，甚至会导出相反的感觉。\n\n接下来的工作是搜集材料，收集很多素材。围绕主题的，或者跟主题相反的材料。然后，做归纳、过滤、剔除。\n\n你要确定的是这些材料对于叙述的主题是否是有益的和互补的。\n\n接下来，要寻找一种把主要材料组织在一起的结构。\n\n创造任何一个东西，结构都是非常重要的。因为文字如同建筑，你没有构造的话，再好的素材，都是一遍散沙。\n\n我们经常会看见很多简单的故事，因为结构的妙用，变得很容易让人理解。\n\n比如说：《平民窟的百万富翁》，用《开心辞典》的方式，把主角成长的片段很方便地串联起来。这种结构，重点突出，详略得当。根本不需要中间的过渡。因为这种娱乐问答的方式，自己本身就是允许跳跃的。\n\n再比如说：《海角七号》也是同样的道理。那七封情书，是一个多么温暖的线索，把所有的元素，串在一起。如果没有这根线。你得花多大的力气，把那么多的场景和素材，放在一起，而不感觉凌乱？\n\n接下来是安排场景，安排段落之间的逻辑。\n\n海明威写尼克害怕，段落之间是有非常明显的逻辑推演的。\n\n尼克看见父亲推船下湖，他看见叔父坐在船头。他听见歌声和船桨划动的声音。他的周围弥散着夜晚的气息。\n\n他回家的路上，一种情绪开始弥漫。\n\n他想听清，到底是什么动物在叫。\n\n他睡不着，从床上爬起来看书。\n\n后来的一天晚上，他实在是无法肯定，拿起枪对天空放了三枪。\n\n海明威知道：这世界上有一种很厉害的结构，就叫因果关系。你只需要把它一层一层地撕开，无须多言，每个人都会有真切的体验。\n\n### （五）\n\n我建议儿子把《三声枪响》多读几次，甚至可以学着模仿这种写法。我告诉他，以后写作文的时候，不要觉得自己什么都没有，也不需要靠什么好词好句。\n\n我说：我不相信，你没有自己熟悉的东西。你的生活中，有很多细节的东西存在。你只是没有仔细想过。比如说，我经常听说你们中午打饭很热闹。那么，你为何不回忆一下有那些细节，让你有如此体会。你为何不把这些画面，用最简单的方式写下来呢？\n\n儿子想了想，马上很激动的说：我们中午送饭过来时，盆子下面有油。大家抢饭的时候，不小心，总会把油蹭在衣服上......\n\n 我说：对了。这就是细节，这就是画面啊。你只需要简单的回忆，用简单明了的话，把你看到的东西描写下来。就已经是很棒的素材了。为什么不这样写呢。为什么要想象看不见的古人，而不是先从自己的身边下手呢？\n\n我把《三声枪响》讲完后，帮儿子做了一个总结：\n\n1. 睁开眼睛，相信自己内心是有东西可写的\n    首先，写文章的时候，要相信自己的内心的感觉，要确定自己到底要表达一个什么感受或者观点；\n    一次只表达一个感受，或一个观点；\n2. 观点明了、逻辑清晰、结构易懂\n    写的时候，要挑选素材，然后寻找一个简单的结构，确定好段落的逻辑。\n    用自己的话写，只要逻辑上没有问题。\n    任何文字是为内容服务的。所以，对一个初学者来说，重要的不是文笔好不好，而是首先要写清楚。只有写清楚，读者才知道你在说什么。\n3. 海明威的叙述\n    如果你肯定的话，别人早就知道的事情，你可以不写。\n    你只写事情浮现的部分。就如同那些气势磅礴的冰山，我们虽然看见的只是浮出海面的八分之一，而剩下的部分，通过想象就可以顺利完成。\n    忘掉形容词，忘掉格言名句。当你根本没有什么可依靠的时候，你的眼睛就会睁开。你的能力就会自然而然地生长出来。\n    做任何事情，最重要的是要树立信心。写字也是一样。\n    一个对自己有信心的人，简洁不是一种叙述的形式，它就是叙述本身，它就是叙述的动力和方向。\n\n\n\u003chr class=\"slender\"\u003e\n\n## 【叙述-6】结构:因果和重复 \n\n\u003e xilei 发布于 2009-5-21 17:11:00 \n\n### （一）\n\n现在，每到周末，我都要求儿子写一篇钢笔字。\n\n他握笔的姿势不好，整个笔是竖起来的，写字象鸡啄米。遇到作业多的时候，力气用到最后，字都是东倒西歪的。\n\n我说：背打直了。手放在桌子上，吊在外面着怎么能写呢。\n\n儿子听着，有时会很不耐烦地挪挪屁股，象征性地动一下。\n\n有一天，作业的书写太乱了，简直都看不下去了。我大光其火。\n\n我说：这样可不行。你写的字，别人都看不清楚。你得花时间练习练习写字。\n\n我让他先注意写字的姿势，然后，给他说了一个练习写字的办法。\n\n我的要求很简单：我不要求你写成一个书法家，但是，你起码要把字形写清楚。\n\n我找了一本字帖，翻到其中的一页，拿了一张纸，让儿子先临摹了一排。\n\n儿子写了一排。比划着写，怎么也不赖。\n\n写好后，我把字帖拿开，让他自己写一排。字象弹簧一样，立马回到从前。 \n\n我说：你知道你的字，为何跟刚才的字不一样吗？\n\n儿子在发愣，不知道该怎么办。\n\n我说：我跟你说一个写字的技巧。你马上就会改进很多。我们先不谈什么笔锋，你看看你的字形结构！\n\n我指着纸上的一个“仁”字，我在字上面各自画了一个长方形，把字分割看。\n\n“你先不把字看成是字，而是一幅画。那么‘仁’这幅画，包含了左右两个部分。我们用方形把它们圈起来。你注意感受人家写字时，这两个部分的相对大小。还有块与块之间的位置。”\n\n“写字首先，是要横平竖直。你看看你的单“仁”旁，你左边的这个竖，都是歪的。站直了，别趴下！”\n\n“你看看右边的方形的位置，相对于左边，大概是在什么位置。这是一种的平衡的感觉。你多看看别人起笔的位置，多体会，自己写一写。”\n\n“同样，象这个‘音’字，是上下结构，上面的立，是一个扁平的长方形，下面的日，可以写成竖立着的长方形结构。如果你写成同样的形状，感觉就象压扁了一样。竖立的形状，才能撑得住。”\n\n挨着把一排字的结构，和儿子的字做了对比和分析。每个字，写了几十个。\n\n儿子把字重新写了一遍，象换了个人写的一样。\n\n我说：一个字的字形有很多变化。但是，刚开始，学一种就行。重要的是，你要先学会看懂人家是怎么结字的，然后，才好模仿。模仿好了，就可以加入自己的特色。那个字，也变成你的风格了。\n\n我认为对于初学者来说，写字的关键，在于识别字形结构。\n\n写字是这样，其他任何领域里的学习都是一样。\n\n记住：不论你干什么，你首先要了解其中的结构。\n\n### （二）\n\n举一个例子来说明。\n\n在《读库》0804里面有这样一篇文字：《电影编剧的秘密》。里面讲了这么一个故事：说的是老外怎么写西部片。\n\n在老外眼里，西部片是一种类型电影。\n\n什么是类型电影？就是电影的故事结构，已经模式化了。凡是这种电影，结构都这么写。\n\n美国西部片的结构通常是这样的：\n\n【故事开始】\n\n1.英雄来到了小镇；\n2.小镇被恶势力控制，小镇人民无力反抗；\n3.英雄初露本色，有超强本领，打破小镇平静；\n\n【故事中间】\n\n4.英雄与恶势力发生冲突；\n5.小镇人民不理解英雄；\n6.英雄与恶势力冲突升级，因为得不到支持，英雄受到磨难；\n7.英雄在频临绝境时，凭个人智勇杀入重围，消灭恶势力；\n\n【故事结尾】\n\n8.小镇人民挽留英雄，为英雄拒绝；\n9.英雄离开小镇。\n\n仔细想想，好像是这样。\n\n我们明明知道美国电影很有模式化，也明明知道很多是假的，但是，我们还是乐意去看，我们乐意上当！\n\n为什么我们总是愿意相信这样的一种“偏见”：美国电影的故事比中国电影的好看？\n\n答案很简单：\n\n美国的各种类型电影积累了很多千锤百炼的叙述方式，它的故事的结构满足了人性的基本需求，也符合人的认知心理。\n\n因为经典结构能很好地沿着受众的认知的路径，一步一步释放信息，跟受众达成了一种沟通的默契。\n\n在这种结构的约束下叙述，整体的基础是稳定的。\n\n### （三）\n\n上周周末，儿子他们布置的作文内容是：给某名人写一个短小的墓志铭和写一篇关于李白的小传，老师从网上下载了李白的文字资料，让他们参考。\n\n墓志铭上周曾经写过一次，老师还把班上同学写的好的文字打印出来给大家分享。\n\n有些写的真不错，但是，有一些文字，我一看就笑了，感觉不妥。\n\n我马上在网上挑选了一些墓志铭，让儿子看。\n\n我说：“文体的真实性，很重要。墓志铭是写给死去的人，重点是对一个人的总结或评价。不能成了作者的抒情描写，这是喧宾夺主啊。这样写，对死者缺乏尊重。”\n\n儿子说：“反正也是练笔，又不是真的墓志铭。”\n\n我说：“不行。你得当真的来写。你明明是写的就是墓志铭，你却写成了抒情散文诗。方向都偏了。”\n\n儿子还在等我后面的话。\n\n我说：“你这样理解吧：你打算写一个请假条给老师，本来写清楚就行了。结果，却写成了抒情诗。老师看了后，猜了半天，也没有搞清楚，你到底要干什么。”\n\n“所以，不能乱来。文体的真实性，很重要。你的文字用途不同，你叙述的结构就不同。”\n\n我一下意识到，学生的作文训练缺少必要的结构训练：\n\n把什么东西都写成了散文，是要不得的。\n\n### （四）\n\n因为前面的事情，晚些时候，我还专门看了看儿子写的小传，眉头又皱紧了。\n\n“怎么都写成了排比句。你不觉得这样很单薄吗？你这样写东西，结构都有问题，文章都不成形。”\n\n儿子说：“我们班上都是这样写的。”\n\n我不知道从什么时候开始，中学作文的文字都变成了抒情散文。不论是男生和女生，都在写这种软绵绵的东西。\n\n我在网上，找了一篇别人写的“鲁迅小传”。我给儿子分析，别人是怎么写的。\n\n我说：我觉得它这个结构，你是可以借鉴的。他的写法是这样的：分成是三个大的部分。第一大部分，是对鲁迅的一个总的概述，第二段按照他的生平时间来写，只写重点的事件。最后的部分是对鲁迅的评价。\n\n我说：你写李白的小传，也可以拿来主义，分成三部分。第一部分，对李白做一个总结性描述。第二部分，也是按照生平时间来写，写他的生平经历。最后部分，因为他是伟大的诗人，当然要对他的诗歌做一些评价。\n\n儿子按我的话去做了，很快写好了。\n\n我看了看，“整体的结构是没什么问题了。第一部分和第三部分，都没有什么问题。在第二部分，有点乱。”\n\n“第二段，我们的目标是写李白的生平。好的，我们把我们认为的事件，都写出来。但是，这不能搞成排比的方式。在这种事件的挑选中，我们要做的是：你选出的事情，要让它呈现出一定的逻辑关系。它不应该是一堆无关紧要的东西，随便堆放在一起。”\n\n我说：“我为什么反对用过多的排比句？因为排比句，过于重视词语的优美，而忽略了叙述的逻辑。如果你前面有很多的叙述铺垫，你可以用。但是，如此简单的重复，留给读者常常是空洞的感觉。因为华丽的词语本身需要让读者去想，而不是自然感受到。这会破坏语言的感觉，也会影响到阅读的流畅。”\n\n“所以，好的做法是沿着设计好的结构的轨道跑。好的故事形状，会带来很美妙的感受。”\n\n“好了，我们回过头来说，怎么写第二部分。我们怎么去寻找第二部分的叙述逻辑。一个很简单的办法，就是依据事实，自己提问。”\n\n“李白是个很自负的人，16岁前就云游四方。那我们就要问了，他为何要去云游？他为何要自负？他想干什么？答案是：李白对自己有一个很高的期待，希望自己在仕途上，能一展抱负。”\n\n“接下来才有李白进京。他自己希望得到唐太宗的赏识，不过，皇上看中的只是他的诗才。在皇帝眼里，他不过是一个用来娱乐的工具而已。”\n\n“如果你是李白会怎么样？你肯定会很失落。”\n\n“所以，李白才写下了‘抽刀断水水更流，借酒浇愁愁更愁’的诗句。他在表达自己内心的郁闷。”\n\n“李白在宫廷里不受重用，刚直不阿的性格，就会带来麻烦，最后，他肯定是得罪了不少权贵。离开了皇宫。”\n\n“你看我说的这么干巴巴的。但我把整个叙述的线索拉出来了。我这么做是为了在素材中寻找叙述的逻辑。你在阅读李白的材料的时候，也要做同样的事情，在脑子里面一定要去寻找可以把素材联系起来的因果关系。这样，你写下的东西，才会非常的结实。”\n\n“因果关系是一种重要和常见的结构。也是一种没有任何认知障碍的逻辑结构。一旦你在素材中梳理或找到了这种关系，基本上，剩下的问题是写的问题了。你在下笔之前，你的眼睛已经看见了远方。虽然你还不完全知道过程的细节，但是，你知道自己的文字会带你到哪儿。你的文字是有方向感的，它自己不会乱跑，不会散架，更不会崩溃。”\n\n儿子花了些时间，重写了第二部分。\n\n文字上还有问题。但我能确定的是：总体的结构对了。\n\n我不能确定的是：用完全不使用排比句的方式写作文，儿子在考试的时候，会被判一个什么样的分数呢？\n\n文字的逻辑好解决，而生活的逻辑，却很难很难。\n\n\n### （五）\n\n再补充一点：\n\n排比句的用途在于：不断重复，增加层次感。但是，如果通篇都只是在词语上做排比功夫，则会呈现出一种华丽而虚伪的文字。\n\n其实，本文的结构，也是：重复。我依赖故事的主题重合来表达。没有用排比句来重复，而是依次讲了几个看起来不一样的故事。\n\n有时，简单的故事比豪华的句子更有质感。它的感受性更自然一些。因为故事的叙述往往是从容的，符合人的心理感知。\n\n我们以后会提到叙述的多声部手法。那是一种象音乐“排比句”的叙述结构。\n\n我们可以看到很多好的作品，如何把简单的重复，运用得如多棱镜一样，绚丽夺目。\n\n\n\u003chr class=\"slender\"\u003e\n\n## 【叙述-7】一味地抒情没有力量 \n\n\u003e xilei 发布于 2009-6-14 20:32:00 \n\n### （一）\n\n周末的时候，我和儿子去小区周围的书店，买《读者文摘》。学校的老师要求他们经常看这个杂志。我经常看儿子他们做一些摘抄的笔记。\n\n我们买了杂志出来，儿子跟我说：爸爸，我发现了一个秘密。\n\n我说是什么。\n\n儿子说：我刚才付款的地方，放了一排的影碟。都是《变形金刚》、《终结者》之类的。\n\n我说：哦，这说明了什么呢？\n\n儿子说：我猜是因为新的《星际迷航》要上映了。商家就把这同类的电影，摆放出来，让大家看。\n\n我说：有道理。商家是在做营销。\n\n儿子：什么是营销？\n\n我说：就是商家想一些办法来帮助推销商品给顾客。比如说《星际迷航》的新电影要上映了，商家就找一些同类的书、影碟和杂志，摆放在一起让大家选。他们就是想办法，让人能顺便多买一点。\n\n儿子说：原来他们是这样想的！\n\n我说：那当然，他们是动了脑子的。\n\n儿子说：哪还有没有其他促销的办法呢？\n\n我说：有啊。刚才这种，叫同类促销。还有一种，貌似没有关系，但实质上，有关系。\n\n我说：曾经有超市在卖小孩纸尿裤的地方，也摆放啤酒。结果，这两种东西都同时卖得很好。\n\n儿子说：我不明白。\n\n我说：看起来两件东西没有关系是不是？其实，如果这样想，就很容易理解：妈妈在家里带小孩，爸爸到超市买纸尿裤，顺便买点啤酒犒劳自己。\n\n儿子说：原来是这样。但是，他们是怎么知道的呢？\n\n我说：是的，这只是一种猜测。比较合理的猜测。其实，没有人一开始就知道会出现这个结果。\n\n儿子说：那谁会知道？\n\n我说：电脑。我们去家乐福购物，你到出口的地方付款。收银员扫描货物，那些销售商品的数据就汇集在一起。数据会发现很多人在做同样的事情：买了纸尿裤的人，同时也买了啤酒。\n\n我说：是电脑数据在提示人该怎么做，可以更方便卖出更多的东西。他们也会根据自己的需要，经常调整货物摆放的位置。\n\n儿子说：难怪上次，我发现卖文具的地方挪了地方，我在里面转了一大圈才找到。\n\n我说：你进去后，会重新定位。有很新鲜的感觉，也会发现旁边有不同的东西，可以选。\n\n晚上的时候，儿子要写作文了，还没有题目。我就说白天，我们聊的事情，可以写一写。\n\n儿子说：就是一些想法。也可以写吗？\n\n我说：那当然。这是一个真实的事情，为什么不能写？\n\n### （二）\n\n我对现在学校里的语文课，基本上不抱过高的希望。尤其是作文课。\n\n初中生，应该写一些说理性的文字了，很多时候，老师还在强调学生写抒情文字。\n\n我倒不觉得抒情的东西不能写，关键看你如何抒情。没事乱哼哼，写很多肉麻的排比句。太恐怖。\n\n我不知道现在是不是一个抒情时代，是不是现在学校的老师都是阴盛阳衰，不过，我觉得男孩子如果在课外推荐阅读的只是三毛作品，写一些软绵绵的抒情文字，真的是太恐怖。\n\n有时，我看儿子摘抄《读者文摘》的时候，就忍不住说：乱弹琴！\n\n我跟儿子说：其实，《读者文摘》最大的好处，不是里面的单个句子。语言，或者词语是有环境的。你这个词放在这里很好，放在其他地方就未必合适。词语可以在不同的语境下，呈现出不同的含义。\n\n儿子茫然。\n\n我说：不知你发现没有，《读者文摘》里面的文章，一般来说，最大的特点是什么？\n\n儿子摇头。\n\n我说：最大的特点就是，很多文字，都是这样的模式：讲一个故事，得出一个结论。\n\n儿子说：这样写，难道不是很简单？\n\n我说：简单又没有错。复杂往往才容易出错。很多人都用这样的简单技巧：讲一个故事，作一个启发，引导出一个结论。\n\n我说：华尔街日报的文章里面，也有一个很常见的叙述技巧：每次文字开头的时候，写一个具体的人物，或者场景。有点象电影里的处理：用一个近景聚焦，话题引出后，然后，再拉开来。最后，在结尾的地方，有可能的话，又再回到具体的场景中。\n\n他们为什么会一而再，再而三地这样做呢？经验证明，这样很有效。符合人的认知习惯。\n\n人的大脑从来都是很容易理解故事的。故事本身具有一种可感知的形状，比任何华丽的词语都更有形状。你想，故事是一个自然展开的链条：能帮助读者逐渐融入到叙述中，让他们理解信息、激发他们判断和发现的本能。\n\n因为人的认知有一个启动的过程。过于突兀的东西，会造成认知上的困惑。人对自己无法理解的东西，不能产生应有的情感。\n\n如果是这样的话，那么，那种一开头就用排比句来轰炸读者的做法，其实，相当愚蠢：给人对词语的炫耀以外，没有任何意义。\n\n也就是说，人的情绪没有任何准备，你一下就把情绪掀起得老高的，这种做法很突兀。如果你一直这么做，也不解释，人家很难理解。\n\n你自己也可以感受一下，就会明白：排比句中填充了很多抽象、压缩的词语，这些词破坏了语言本身的流畅感觉，而且要让人使劲去想它到底想表达什么。这样会造成认知的迟疑。很多时候，给人的感觉是，在阅读的过程中，依稀感觉到一些听令框框的干硬的东西，相互碰撞，但是，完全不能理解到底说了些什么。\n\n文字就是这样的：如果你组织的东西，人不能把握，你就等于什么也没说。一味地抒情没有力量。\n\n### （三）\n\n后来，我跟儿子讲德波顿的《旅行的艺术》。因为我想给他做一个示范，让他理解一本书可能是怎样构思出来的。\n\n我假设我自己就是作者，我被老师布置了一堂作文课，写一些关于以旅行为主题的文字。\n\n我不想写这样的文字：我去了某个地方旅行，然后，看见了一些风景，我有一些感想。\n\n我是一个哲学系的学生，这样写旅行为主题的文字，感觉上的确是还不够独特。所以，我想加入一点思考，写一些更有普遍意义一点的东西。\n\n我问自己的第一个问题：什么是旅行？\n\n我去 google 查了一下，上面有一个很好的答案：旅行，就是为了某个目的，从一个地方到另外一个地方。\n\n我马上明白：旅行的移动，需要一个工具。\n\n我知道：乘坐汽车、火车、飞机，有一种不同的感觉。我想起汽车的加油站、火车前进时，树木缓缓加速离开的情景，我在黑夜中，看见窗外玻璃中的自己。我看见飞机机翼下面的白云。那种不同工具下的体验，是一种旅行的独特体验。\n\n我想，每个人旅行的原因都不同：公干、游乐。人们总是向往一个远离自己地方，仿佛奔向梦想之地。\n\n人们为何总是喜欢那种异国情调的感觉？\n\n旅行，又会不会是一种逃离现实的办法？\n\n我发现，在一些文字中，也能发现旅行的痕迹：故国神游--思维从一个地方，到另外一个地方。\n\n好了，我想了很多，我从中发现了关于旅行的普遍含义，可以有这样的一些关键要素：\n\n1. 对旅行工具的不同感受；\n2. 人们在种异国情调的感觉面前，会有怎样的感受；\n3. 思想的旅行也是一种旅行。\n\n......\n\n当然，德波顿可能不是这样想的。我只是用了一个反向思维试图来理解书的结构。\n\n作为一个阅读者，如果发现了这种结构，那么，书就算看懂了。\n\n作为一个写作者，如果完成了这种结构，那么，书的骨架也基本完成了。\n\n这种逻辑结构，使得每个段落中的情节，前后关联，互有意义，浑然天成。\n\n任何叙述的难点，不在于叙述的词语，而在于叙述的角度、逻辑。是不是值得，是不是有它本身的意义。它有没有自己的思索，有没有自己的声音。\n\n其实，这是一个基本的方法。\n\n我跟儿子说：观察，然后提问，多问几个为什么，试着想办法回答它。把思考的过程和分析的过程、结果写下来。\n\n我说:写文字简单地说，就是确定一个主题，寻找一个结构，用好的细节或故事形式来表达。如此反复。\n\n如果你这样想过，这样做过。很多的东西，就会源源不断地冒出来。你根本不用担心，要用什么词语写。你能控制好内容，词语自然就如泉水一样涌出来。\n\n我们的身边有很多这样的素材，如开头的部分，我们提到的关于营销的对话。\n\n你无须写出象德波顿那样复杂的文字，但是，思考的原理，本质上是完全相同的。\n\n其实，这就是思考的力量，逻辑的力量。\n\n文字的叙述，无论如何表现，其背后都有一种思考的过程。只有那种内在的逻辑的表达，才能让读者知道你想说什么。\n\n你看见一些现象，你问到底是为什么。你去寻找那个驱动向前的逻辑力量。你就可以找到文字，找到答案。\n\n\n\u003chr class=\"slender\"\u003e\n\n## 【叙述-8】叙述的态度 \n\n\u003e xilei 发布于 2009-10-27 13:42:00 \n\n### （一）\n\n10月1日上午，在家里看了今年的阅兵式。\n\n以前的两次大阅兵也看过，“小平您好”的那次最有记忆和感触。\n\n今年的阅兵怎么说呢？一种说不出的味道：看起来很热闹，感觉却很拘谨，不舒服。\n\n过了两天，就在网上看见了一个卫报记者做的几分钟的照片和影片的剪辑合成。这个精彩的片段，给我们提供了另外的一种视角。\n\n我看见了一种cctv从来没有展示的一些细节：阳光透过云层在天安门洒下游动的阴影；在微风中舒展的红旗；战士们整齐的步伐在舒缓的音乐中形成一种反差；游行队伍里人们舞动的手臂，他们的面容；还有孩子骑在父亲的头上在队伍中走过....\n\n作为一个旁观者的角度，卫报记者配了一个冷峻的音乐。于是，网上有很多人在骂，说这个片子不怀好意。\n\n一个热闹的欢腾的场面，为什么不是锣鼓喧天呢？\n\n又过了几天，我在网上下了一个凤凰卫视版的阅兵视频下来。我看了一段，马上就明白那种不舒服的感觉来自哪儿了。\n\n这种感觉就如同当初看NBC制作的北京奥运会转播一样：任何东西，只有通过对比，才能发现，同样的事情，用不同的方式讲述，其品质的差异会很大。\n\n所以，有网友批评cctv是：一流的设备，二流的解说，三流的摄影，不入流的导播。\n\n其实，cctv的问题不在于其专业水平，而在于其叙事美学的选择不同。\n\ncctv已经习惯于一种宣讲、致敬的美学。其美学特征为：字正腔圆、句式标准规范、场面宏大、激情澎湃。\n\n凤凰卫视则提供了另外的一种平民美学的范本，其美学特征为：基于沟通和理解的讲述方式，关注细节，语气是舒缓、亲切的。\n\n凤凰卫视的国庆阅兵有很多值得回味的细节：\n\n主持人会事先告诉观众，整个庆典仪式是怎么一个流程。每个流程中有什么看点。国外的媒体关注什么重点。\n\n整个庆典有现场报道，主持人会讲到一些有趣的佚闻：胡总从天安门层楼的某号电梯下去阅兵的；游行的人群中有人的鞋掉了，光着脚走过了检阅区；每个彩车里面还藏有很多人，万一车子出故障了，就用人推....\n\n丰富的细节，并不会给人揭丑的感觉，反而感觉到温暖。一种体量的叙述，会让人有一种可以亲近的参与感。\n\n当士兵列队走过的时候，主持人会请主持嘉宾介绍他们手中的各种武器。\n\n阅兵有阅兵的主题。\n\n听了凤凰卫视的国庆阅兵解说，我终于有了一回人民当家作主的感觉。\n\n### （二）\n\n有一天，儿子从学校拿回了一摞同学的作文本。\n\n儿子的作文，总是只得了40多分（总分：60）。老师不喜欢他实话实说的写字方式，说他的文字没有感染力。\n\n老师直接推荐了班上写得很好的同学的作文，让儿子学习。\n\n儿子给我推荐了一篇，叫《风声》。老师说这篇特别具有“哲学”气质。\n\n这篇作文得了56分。\n\n文章的开篇是这样的一段话：\n\n\u003e 我如泥一般烂在沙发上，四周都是疯狂涌入的笛鸣。静止的空气，囚禁的心灵。\n\n老师在旁边打了一个很大的五角星。五角星表示：good,very good!\n\n文中还有很多这样类似的五角星句子：\n\n\u003e 哪个男儿，在红日烈风下，不心生豪情，哪个男儿，在啸风中，不欲定天下。\n\u003e \n\u003e 即使此刻立即化为灰烬，也要对酒当歌，与风合吟。\n\u003e \n\u003e 好个狂气的风声，撩我发髻，夺我衣衫。\n\u003e \n\u003e 好个放肆的风声，湮灭我的声音，同化我的灵魂。好个自由的风声，吹断了我的枷锁，笑塌了我与尘世的隔阂。\n\u003e \n\u003e 我是风的信徒，我是自由的奴隶。我不求长生不死。我只愿跪拜于这风声中直至消亡。\n\n儿子问我：“什么感觉？”\n\n我说：“我很高兴，你没有写成这样。看了半天，我还不知道，他到底在说什么。”\n\n我说：“看了第一段，就看不下去了。我对这种文风，只有六个字的评价。”\n\n儿子停下手中的作业，望着我。\n\n我说：“这就叫：抒情败坏叙述。我并不是说，任何抒情都不可以，而是，你得讲求基本的叙述逻辑。”\n\n“叙述，是一种沟通的过程。你要传递信息，你要选择什么信息，如何组织和结构。你不能一站上来，就扯开嗓子喊，一个人在哪里莫名其妙地抒情。这一点没有任何哲学气质，完全是发神经的做法。”\n\n“你看这些作文都写成什么样子了：一上来就用排比句，而且是大段大段的排比。然后呢，把所谓的名人名言切割、包装，整理成段，也不管他逻辑上跟文字有没有关系，就往上搭理。简直就是收破烂的。这些写，迟早要把脑子给写废了。”\n\n儿子说：“老师说，如果这种文章能写的话，以后什么文章都可以写了。”\n\n我说：“这简直就是在误人子弟！你们都在看余华的《活着》，你们看作品中，也没有，动不动就乱排比的，乱抒情的。”\n\n儿子说：“就是。老师让我们找好字好句，结果，我们找了好久都没有找到。”\n\n我说：“其实，你们有一个同学写的挺好的。他写他在乡下和奶奶在一起的时候的一些小事。我觉得很真实，至少我知道他在告诉我值得记忆的是什么东西。可惜，老师让他重写。我觉得越写越差了。为什么不相信哪些真实的细节，而要去追求哪些虚无缥缈的形容词呢？”\n\n这位同学在重写之前，在本子上写了自己的“罪状”：语言太没有感染力了。\n\n这位同学重写了一篇。在文章的开头，他写了一段这样的话：（此处为排比句）。老师给了个大大的问号。\n\n我想：这也是同学无声的抗议吧。\n\n我把其他同学的作文也很快翻了一下。没有办法看下去，都是写成了一个模子，都跟打了鸡血一样。\n\n我跟儿子说：“有空，我推荐你看看卫报记者和凤凰卫视的国庆阅兵。”\n\n“cctv版的解说，就跟你们的作文一样，打了鸡血一样。你回过头来，看看不同的叙述方式。你会发现，叙述其实，就是一个沟通和理解的过程。”\n\n“如果你不想让人家理解，你只管自己在哪里嚎就是了。没人理你，因为，你也不想让人家理解你！”\n\n“可是，如果你想和人家沟通。那你就要想好了：我想表达什么，我该如何表达得真实、贴切和有亲和力。”\n\n“你们同学的《风声》的这种写法是根本要不得的：基本上属于逻辑混乱，词不达意。这种矫揉造作的文风在现在的学校里备受推崇，而且很容易在考试中得高分。但是，这些真的是垃圾啊！”\n\n“我宁愿你仍然写成现在的样子：关注自己的观点和感受，然后，思考如何把它表达清楚。文字上的技巧，可以慢慢来。”\n\n“我们宁愿真实一点，笨拙一点，也不要胡言乱语。如果一个人脑子坏了，就没法弄了。如果脑子还好，傻一点也没关系。”\n\n\n### （三）\n\n有什么样的社会，就会有什么样的文本。\n\n我的移动硬盘上有一个10多个G的NBC剪辑的北京奥运会开幕式。我最喜欢的是那个好莱坞似的片头。\n\n每次我看到那些画面的时候，我真的想到了四个字：山河壮丽。\n\n可是，我也看过cctv的版本：插播了很多无关紧要的领导的画面。很多美轮美奂的细节都牺牲掉了。\n\n最让人无法忍受的是耳朵里充满了打鸡血一样的解说的声音。这些声音老是让我想起了朝鲜人民的新闻联播。\n\n从今年的阅兵式后，我得出了一个结论：如果以后有类似的转播，最好不要听cctv的解说。\n\n有什么样的态度，就有什么样的叙述。\n\n\n\u003chr class=\"slender\"\u003e\n\n## 【叙述-9】好莱坞电影的秘密\n\n\u003e xilei 发布于 2010-1-18 14:48:00\n\n### （一）\n\n要到期末考试了。儿子他们开始准备“万能作文”。\n\n第一次听说“万能作文”的时候，我是大吃一惊。\n\n还在想：怎么写，作文才能写成万能的啊？\n\n儿子把作文题目给我看，我才明白，原来“万能”是另外一种意思。\n\n比如说，作文题目是这样的：带着....出发。\n\n我想：这种题目当然是万能。\n\n我问儿子：可不可以写“带着怀疑出发...”\n\n儿子说：不能。老师说了，一般都写这种，“带着感动出发”、“带着回忆出发”.....\n\n卖糕得，原来万能也不是真的万能的。\n\n我说：可不可以写“带着锤子出发.....”\n\n儿子说：为什么？\n\n我说：我们这里公交经常自燃....\n\n儿子在一边笑弯了腰。\n\n“那肯定是一篇很牛逼的零分作文。”\n\n### （二）\n\n儿子上中学后，语文就改写散文了。不写记叙文了。\n\n我问他：为什么呢？\n\n儿子说：老师说的，记叙文，只能用第一人称来写。\n\n我说：这很容易。下次，你开篇第一句就写：我是某某某。接下来，就用第一人称写。\n\n儿子跟我说：这个寒假，他们老师要他们多读满分作文。\n\n那些满分作文，上百度就可以搜到。\n\n儿子和我，都认为，这些作文，大部分都惨不忍睹。（其实，我想说:大部分都是垃圾！）\n\n我说：等你考试完后，马上去看《阿凡达》。另外，我还推荐你看两部电影：《心灵捕手》和《大鱼》。\n\n好多次，我跟儿子说过：好莱坞的电影，会教给你很多东西。\n\n### （三）\n\n在我看来，很多好莱坞的电影，其实，讲的是同一个故事。\n\n经典的好莱坞的故事一般都是这样的：\n\n一个有理想但有缺陷的主人公，遇到一个突然降临的事件或动机，他面临着克服自己的弱点，展现出面对困难所需要的勇气和信心。经过一系列的痛苦、坚持和奋斗，他终于做回了自己--一个有点害羞的大英雄。同时，伴随着正义战胜了邪恶的大结局。\n\n我们拿两部电影来看。\n\n在《功夫熊猫》里，阿宝是个胖子，他的梦想是当功夫高手。阴差阳错的，他被选为神龙大侠。但是，他的师傅不待见他。他自己也曾经心灰意冷。但是，后来太郎出来了，这就是一个转折点。所有正义的一方没有选择的机会了。他的师傅接受了乌龟大师的批评教育，改变了想法，教给大宝功夫。阿宝也在关键时刻领悟到自我的力量。阿宝，最终打败了太郎。\n\n同样的，在《2012》用另外的元素，把故事重新讲了一次。\n\n主人公是一个在现实中不太走运小说家。他面临工作和家庭的双重困难：含辛茹苦写的科幻小说只卖了500本，而自己跟妻子离异，自己的儿子跟前妻的男友打得火热。他不知到底该如何面对现实和梦想的距离。\n\n这时，2012到了，地球开始毁灭了。小说家迎来了转机。\n\n他隐藏在平常现实里的聪明才智和勇气，有了向亲人展示的机会。\n\n爱、自由和责任，把这个渺小的凡人，变成了一个伟大的拯救行动的化身。\n\n最后，连导演都帮了他的忙，帮他在逃亡的途中，让事故消灭了他的情敌。他重新赢回了家庭和尊严。\n\n好莱坞的经典故事莫不如此：《变形金刚》如此，《阿凡达》也是如此。\n\n### （四）\n\n那为什么同样的故事模式，无论重复多少次，人们都爱看？\n\n大概可以有如下的几个理由：\n\n1. 人类天生喜欢故事；\n    一切叙述的本质就是沟通。而好的故事模式具有好的沟通模式。\n    好莱坞电影跟观众的沟通堪称典范：\n    它由巧妙的结构、鲜明的动机，以及一系列观众容易理解的事件前后因果联系所构成。\n2. 人类的好奇心；\n    对真实性、对细节的渴望，源于人的本能。\n    人类喜欢惊悚、恐怖，或酷爱遥远的宇宙，都是出于人的本能。\n    即便在虚拟的空间中，人也追求感觉上的真实一体性。\n    人可以理解在一种假想环境中的真实，但是，他们还是需要逻辑的连贯性。否则，他们会产生质疑。\n3. 人类存在对高尚情感的追求\n    人在现实中，不能获得的正义感，需要一个感情的出口。\n    人类的普世价值，在电影中，可以用一种有距离的现实来观察、呈现，并引起共鸣。\n    谁要是不承认普世价值，带他去看《阿凡达》就好了......\n\n### （五）\n\n好莱坞电影，其实，可以告诉我们如何叙述。\n\n首先，你要想清楚，你到底要表达什么。\n\n其次，叙述必须要有一个用来沟通的结构，或者逻辑。\n\n接下来准备或挑选素材中，你想呈现的因果联系。你的细节最好能感觉故事比较真实、准确、足够丰富。\n\n当然，现实中的叙述的冲动，可能是这样产生的：\n\n你看见了一个素材，触发了你的一个想法。然后，你才想，我到底要表达什么，然后，再去想如何弄。\n\n因此，不管你做任何叙述，还是创造其他的什么玩意。\n\n重要的是思维的过程要是正常的。如果思维的过程都不正确，那怎么弄都是瞎弄。\n\n当然，还有更重要的一点：\n\n你必须忠实于自己的内心，你不可能用一种黑灯瞎火的方式去写出一个伟大的故事。\n\n### （六）\n\n现在学校里的教作文的方式，是让我真正开了眼的。\n\n有老师这么教孩子写作文的：\n\n作文分三段，第一段写100字，第二段写200字，第三段写100字。\n\n这就是我说的：黑灯瞎火的方式。不知道在干嘛。\n\n你教孩子给段落写个小标题，不很好吗？字数能表明段落的逻辑吗？\n\n当然了，还有比这个更让人头大的。\n\n据说，有人送小孩回国读小学。小孩写了春天，说春天是潮湿的。\n\n老师就把作文给毙了，让小孩重写。春天都是温暖的，怎么能是潮湿的呢....\n\n家长吓得内牛满面。赶紧给小孩退学了。\n\n### （七）\n\n有一天，我在家收拾抽屉，发现儿子小学军训时写的日记。\n\n其中有一段，写他早上起来喝牛奶。美滋滋的感觉，跃然纸上。\n\n儿子上中学后，就没有写过这样的文字了。\n\n他们成天听写汉字、背古文，看满分作文。写字的时候，感觉和信心全无。\n\n学校是非常重视数理化，也貌似非常重视逻辑，但是，写出来的作文大多是没有逻辑的。\n\n这是为什么呢？\n\n我想了好久，终于明白了：你这也不能弄，那也不能弄。怎么可能有逻辑呢？\n\n如果有，那逻辑也多半是精神分裂、言不由衷，自断经脉的。\n\n没有人性，就没有情趣，更谈不上什么逻辑。没有人性，也不会有好的故事或叙述。\n\n就是这样。W\n\n那好吧！\n\n把作文书扔一边去，让孩子去看好莱坞的电影吧！\n\n[http://www.dapenti.com/blog/more.asp?name=xilei\u0026id=25936](http://www.dapenti.com/blog/more.asp?name=xilei\u0026id=25936)"},{"slug":"logical-science-from-west","filename":"2022-08-22-logical-science-from-west.md","date":"2022-08-22","title":".doc | 也谈近代科学从西方起步","layout":"post","keywords":["doc","tex","phy","m","phi"],"excerpt":"为什么近代科学偏偏是在丢过一次古典传统的西方起步的呢？为什么那些成功继承了古典时代智慧的中古文明，比如伊斯兰文明或古中国文明，反而没有成功萌发近代科学思想呢？","content":"\n前不久在公众号转载过“海边的西塞罗”写的《**嗯！您关注的是一个早晚要“凉凉”的公众号**》，标题起得让人不知所云，但是文章内容讨论的是“近代科学为什么从西方起步”的问题，原文说：\n\n\u003e 既然你所讲述的，欧洲从古典时代到文艺复兴、科学曾经出现过一次“断层”，欧洲人是通过翻译阿拉伯人转译的古典时代文献才继承了希腊罗马先贤们的思想的。\n\u003e \n\u003e \n\u003e 那么**，为什么近代科学偏偏是在丢过一次古典传统的西方起步的呢？为什么那些成功继承了古典时代智慧的中古文明，比如伊斯兰文明或古中国文明，反而没有成功萌发近代科学思想呢？**\n\u003e \n\n作者立了一个靶子——\n\n\u003e 我之前听到的比较靠谱的解答，**是古希腊罗马有较好的数学思想，当定量的数学思想与定性的“自然哲学”发生结合，近代科学就诞生了。**\n但这种解释，其实也回答不了一个问题——你可以说古代东方离着希腊远，没有受到希腊某些思想的“药引”的启发。但特别奇怪的是，中世纪的中东却不是这样。\n伊斯兰文明的伍麦叶王朝在公元九世纪曾经掀起过一场声势浩大的“百年翻译运动”，……近代启发西方的那些古典思想典籍，阿拉伯人全有，且早获得了好几百年。\n\u003e \n\n给出的回答是所谓**“托勒密困境”**，即诸文明中的科学技术研究者因为要满足当权者/赞助者的功利性需要，将时间与精力耗费于附会科学（比如天文学）的非科学甚至伪科学（比如占星术）之上，而——\n\n\u003e 这种错误的职业拼接，锁死了天文学的进一步发展的通路，导致其无法实现向近代科学的飞跃——即便托勒密会数学、引入定量计算，也依然没用。\n\u003e \n\u003e **而这种“托勒密困境”，其实也是所有古典时代学者的困境——他们在研究学问时，必须回答“求用”的问题。**\n\u003e \n\u003e ……\n\u003e \n\u003e **于是从托勒密到哥白尼，我们会发现西方在这一轮对天文学的失而复得中，其实并没有增添什么，而是丢掉了一种东西——那就是“求用”的思维。**\n\u003e 欧洲知识分子们研究科学的正义性，来自于他们认定：自然作为一种上帝的造物，其本身就是美的。因此研究它、探索它本身，就是在赞美上帝，所以科学研究不必“求用”也有天然的正义性。\n\u003e \n\n作者写近代西方科学的不求用，是为了托物言志，检讨自己为了读者的关注不得不在历史写作之外“写时评、表达观点、带情绪”，预告自己将来可能会去写作崇高的钻研历史的题目。\n\n给蹭热点找理由这件事，我也做过嘛，感觉写的比这篇文章还简约隽永且立意高远呢～（文人相轻.jpg）\n\n但是科学革命发源于西方这个问题，我也很感兴趣，而且有自己的思考，而且思考的结果和上文不同。\n\n\u003chr class=\"slender\"\u003e\n\n学物理的孩子应该都听说过《费曼物理学讲义》的大名，没听说过的话建议听说一下，自主招生考试面试装逼的时候用的上。费曼先生在引言中也立了个靶子说——\n\n\u003e 你们可能会问，在讲述欧几里德几何时，先是陈述公理，然后作出各种各样的推论，那为什么在讲授物理学的时候不能先直截了当地列出基本规律，然后再就一切可能的情况说明定律的应用呢？\n\u003e \n\n然后上来就讲原子论，开篇问：\n\n\u003e 假如由于某种大灾难，所有的科学和知识都丢失了，只有一句话可传给下一代，那么怎样才能用最少的词汇来传达最多的信息呢？\n\u003e \n\n可惜这个问题仅仅是为了引出原子论，实在是大材小用。这说明费老先生浸淫于西方科学中，“不识庐山真面目，只缘身在此山中”。而我对近代科学起自西方的解释，正好就是这两句话串起来。下面就要兜一个大圈子，把两句话圆起来。\n\n\u003chr class=\"slender\"\u003e\n\n科学者，对世界之正确认知也。\n\n根据这个定义，把人们已知的，关于这个世界的所有知识罗列到一个集合里，这个集合就是科学。我们只谈到了一个集合，不涉及逻辑推演，也不涉及数学带来的定量优势，更不判断从事科学研究的人是否功利。\n\n但是，集合这种知识结构过于简单——\n\n- 集合里的各个元素都是平等的，要想表示出整个集合，除了全默写出来没别的办法；\n- 集合里的元素之间没有顺序，想取得其中的某一条科学命题，只能像抓阄一样，凭运气抽到为止。\n- 一旦由于天灾人祸，集合中的部分内容丢失，除了重新把当初发现它们时经历的艰难困苦重复一遍，也没有别的办法。（哦，也可以去隔壁文明的图书馆翻译。）\n\n所以，必须找到一种更复杂的结构，来组织这些信息，解决上述问题。\n\n\u003chr class=\"slender\"\u003e\n\n计算机专业有门基础课《数据结构与算法》，谈数据结构，最基础的两种就是数组和链表；谈算法，最基础的概念就是函数。注意，这里说的是数据结构，刚才说的是知识结构，两者可以类比，但并非同一概念。\n\n数组，和集合几乎一样，只不过给每个元素标记了一个序号。在计算机里，由于规定数组连续存放，每个元素占用内存长度相等，所以可以通过序号，从数组开头偏置指针，以 O(1) 的时间复杂度取得任意元素，快。\n\n类比到知识结构，语数外理化政史地生，一年级二年级三年级，第一章第二章第三章，第一第二第三个知识点，背吧。列表与列表之间井水不犯河水，你数学老师说你体育老师拉稀了不能上课，你体育老师说你数学老师放屁，两者完全可以在你的知识体系里共存。\n\n链表，和数组一样有顺序，但是并不给每个元素标号，而是在前一个元素的末尾，写上下一个元素的位置指针。找到一个元素需要从链表的开头一个一个往后捋，慢。好处是修改方便，在链表中间塞进去一个新元素，只需要把前面一个的指针指向新元素，新元素的指针指向后一个元素，删除一个旧元素也类似，只对增删点附近一个很小的区域进行改动，整个链表不会伤筋动骨。\n\n但是不论数组还是链表，都需要把所有的知识全写出来，随着时间的积累，科学的总量早晚要超越人脑的记忆力，超越笔记的厚度，对于个人，要皓首穷经，要韦编三绝，才有希望提出一点新内容；对于全人类，图书馆越造越大，一轮战乱，从头再来。\n\n于是函数登场。给定一个/一组输入，根据函数体描述的算法，返回确定的输出。那我们找到一种方法，写一个函数，接收链表的前一个元素作为输入，找到后一个元素输出。这样我们只需要存储第一个元素和这个函数，就可以恢复出整个链表，用计算换空间。\n\n\u003chr class=\"slender\"\u003e\n\n类比到知识结构，这个函数就是逻辑推演。\n\n科学内容中的每一条知识都是一个**命题**。\n\n从少数几条知识出发，这几条在逻辑上就称为**公理**，自然科学里也称之为**定律**。\n\n命题之间可以做**逻辑运算**，**或**、**且**、**非**、**蕴含**等等，运算的结果也是一条新的命题。命题的正确与否，取决于逻辑运算的规定。\n\n通过对公理和已经算出的真命题反复进行逻辑运算，产生的新的真命题，叫做**定理**。\n\n\u003chr class=\"slender\"\u003e\n\n欧几里德几何式的，也就是从有限多个命题出发，承认逻辑推演进行生成的新命题的正确性，这样的一种组织方式——\n\n- 对于学习，科学不再是一家之言，门户之见。一句话的正确性不再由说话者的身份决定，诉诸人身、诉诸权威成了谬误，“我爱吾师，但我更爱真理”一句话有了切实的落脚点。\n- 对于研究，降低了难度，后来者不必从头再来，而是站在前人的终点起跑。发现的新科学有办法整合进现有的科学，证伪的旧科学有办法剔除，而不会让科学整体伤筋动骨。愚弄黔首的矛盾和谬误，真理有办法与之势不两立。\n- 自带有容灾能力，科学得以在摧毁科学记录和科学家人身的重大灾难之后，在几百年的人才断档之后，依然有办法恢复。\n\n\u003chr class=\"slender\"\u003e\n\n刚才说数据结构和知识结构不同，知识管理界有个 DIKW 模型，也就是数据 (Data)、信息 (Information)、知识 (Knowledge)、智慧 (Wisdom)。\n\n纸张上的墨迹组成的字符只是数据，当这些单词按照语法理解为句段篇章之后才构成信息，这些篇章内容指代的概念、关系等等含义构成知识。如何理解知识与知识之间的关系需要智慧。\n\n“继承了古典时代智慧的中古文明，比如伊斯兰文明或古中国文明”——从各个文明没能演化出科学革命来看，**这些文明最多是有一部分学者继承了古典时代的知识，而没能认识到 *用逻辑组织知识* 这一智慧的价值**，而西方发掘出了这种智慧。至于这种发掘发生在西方，是偶然还是必然，由哪些条件促成，那是另一个很有趣的问题了。\n\n“我们会发现西方在这一轮对天文学的失而复得中，其实并没有增添什么，而是丢掉了一种东西——那就是‘求用’的思维。”——西方对天文学的失去，对应的是罗马统治下的和平结束时的战乱与社会崩溃，不论之后的文艺复兴如何光辉灿烂，**这种失落都是对科学乃至整个文明的威胁**，如果没有这种失落，科学革命想必会更早更容易发生。况且这种失落到复兴的整个过程中，对科学有影响的因素实在是太多了，既有正面又有负面，实在是难以分析归因。\n\n至于不求用的思维，有了逻辑推演，科学工作者的产出提高，高到了让社会愿意供养其全职研究的地步，那么不求用的思维，自然会建立起来；不求用对科研效率的提升，良性反哺科学的发展，自然会蔚然成风。反过来，**只有不求用的态度，研究者没有逻辑推演发展科学的能力，资助者没有逻辑推演评价成果的本事，不求用的态度只会鼓励灌水，产出真没用的水货。**\n\n\u003chr class=\"slender\"\u003e\n\n数学对自然科学的作用，定量化只是一个副产品。更重要的是作为逻辑科学的集大成者，发明/发现逻辑推演的规则，探索逻辑推演作为方法论的能力边界。一言以蔽之，欧几里德之后，数学已不只是“数字的学问”。\n\n至于费曼先生，他怎么可能不知道四大力学确实就是按照欧几里德式的，从基本定律出发的方式讲授的呢？面对一伙学普通物理的本科新生，说这种话实在有点骗小孩儿的嫌疑，怪不得那门课上到后来，本科生全都跑了。物理和数学的区别，在于理论和实验两条腿走路，但是理论的这条腿，实实在在地来自于超越了“数字的学问”的数学。\n"},{"slug":"my-teacher-mr-liu","filename":"2022-07-30-my-teacher-mr-liu.md","date":"2022-07-30","title":".doc | 柳师傅当上年级主任了","layout":"post","keywords":["doc"],"excerpt":"本文纯属虚构，请勿对号入座。","content":"\n\u003e 本文纯属虚构。\n\n\n\u003e 我怕你会读书吗？名校研究生都别想那么容易进我单位。这个人当年仗着自己会读书，看不起我们这种靠父母的，社会会教他做人。\n\u003e\n\u003e ——~~《史记·绛侯周勃世家》~~ \n\n刷朋友圈的时候，看到了园老师转载的一中公众号的宣传文章，随便点进去看了看，居然发现了柳师傅的名字，再定眼儿一看，已经当上年级主任了。回过神来，才发现自己高中毕业都快十年了。\n\n柳师傅是我高中的第一位语文老师，教到我们高二文理分班。他是13班的班主任，兼代我们14班的语文课。经过了新生报道的慌乱哦不忙乱之后，第二天早上第一节课就是语文。上课铃响老柳踩点跨进教室，于是来到了我最不擅长的人物外貌描写环节：老柳胖胖的，圆头圆脸，超短发平头，常年轮换几套显瘦的黑色运动服，上衣外套也多是圆领。黑框眼镜，浓眉厚嘴唇，望之忠厚长者，不似滔滔不绝……下笔轻浮了，抱歉抱歉。\n\n人不可貌相，柳师傅极其能侃，上课的主要内容就是侃。高中文言文多，老柳上课侃历史，堪比百家讲坛。只不过虽然没有实锤证据，字里行间总让人感觉他把梁启超和康有为俩人的名字和经历搞反了。年轻人找茬的第一集百家讲坛，又何必是百家讲坛……\n\n他不仅自己侃，还让我们侃。每堂语文课的前十分钟，让我们轮流上台讲一个成语，从字音字形到典故出处，顺便锻炼公共讲演。有人讲“刮目相看”，他说好，同学们，我们把月考的试卷拿出来，看一看最后这道阅读题，最后的剃头匠哈，哎，你们知道剃头师傅可以刮脸吧，知道哈，那你们知道还可以刮目吗？不知道了吧，以前东关城隍庙那条街，赶集的时候就有个老师傅，他的绝活就是刮目，就用刮脸的剃刀，在砂布上噌噌磨两下，然后扽起你眼皮儿，刷，刷，还没来得及打激灵呢，眼皮儿内侧的眼睑就刮完了，一个学期耳聪目明，舒服啊……\n\n讲《滕王阁序》，说到都督阎公想把自家女婿扶上马，送一程，没想到小王同志很不讲究，驳了主人的面子，还驳得这么漂亮。于是教我们打业务球——小王同志，假如你是今年新入职的老师，教职工篮球赛，对面李校长下场了，你要不要去防他，防他要不要放水？要不怎么说“世事洞明皆学问，人情练达亦文章”呢，有你们琢磨的。\n\n说到了打篮球，还是人不可貌相，老柳喜欢打球。入学一个月左右，两家班主任一拍即合，13、14两班班会课的时候来了一次篮球赛，柳师傅做裁判。我们班的体育特长生多，喜欢打篮球的也多，小马哥、大琛哥，各种哥可谓兵强马壮。开局两边各得了十来分了，我们这边的主力就不装了，一波得分潮接管比赛。柳师傅照样往返球场，吹罚也依然公平。下半场小马哥找了对方一个三加一，老柳果断吹哨，加罚投中，第二天上课的时候还在夸我们打得好，兴奋的劲头就像他自己打成的一样，说我们篮球联赛有夺冠的实力。\n\n篮球联赛在高一下学期，整个年级每班出一队。一中自诩“素质教育”，但啥是“素质教育”，咱也不知道，咱也不敢问，可能在要过高考独木桥的学校里，干任何与学习不直接相关的事情，就叫素质教育了吧。哎呦喂，哦不，anyway，我们一路过关斩将，来到了决赛，对阵10班。我们班主任小佟老师虽为巾帼英雄，终究不善骑射，所以老柳带着队员们赛前动员。最终我们以两分之差惜败对手，但是大家满脸都写着高兴。比赛结束之后回到教室，小佟老师很激动，说比赛结束之后全年级老师开会，她在会上说她为我们感到骄傲；还买了五大桶可乐和雪碧，给每个人倒了一纸杯，我们像独立团的战士喝酒一样闷下去了（没摔杯子）。小马哥说他哭过了，但是不遗憾，毕竟人少打人多。下周一的升旗仪式上，大琛哥拿到了最佳球员奖。放学回家的路上遇到了现在15班的初中同学一平，他问我你们班的那个最佳球员学习不错吧，我说你怎么知道，他没有接话。\n\n我高中入学的时候成绩不是特别好，语文尤其是。但是在老柳的感染下，发愤图强，对语文发生了很大的兴趣。期中考试的成绩还是不理想，于是立志每天晚上晚自习回家，做一道语文阅读理解，夜夜如此，直到高考。开始的时候完全不得要领，甚至急得要哭，到后来像中文房间里的猴子，勉强能找到点感觉了。半个学期之后，我的成绩有了很大进步，从年级三百多名进步到第十名；我们班整体也是，有的同学从一千名开外进了前一百名。高二的时候有一次考试，老柳讲评试卷时说，你们下课可以看看小山的卷子，虽然作文分数低了点，但是人家前面整个卷子只扣了一分，这叫什么，这就叫双保险啊，哪怕主观题发挥失常了，但是客观的语文能力是不会骗人的。\n\n唉，作文成绩是我学生时代一直的痛。也不是不想写，也不是没写好过，但就是分数低。正经人不写日记，柳师傅就要求我们每周写一篇周记，练作文。\n\n一中北校区的班号跟在南校区后面顺延，从9开始。按小道消息，一中的文理比例大约是1:2，所以不出意外的话，文理分科应该是我们两个班和12班一起。可惜，很意外地，文理分科的时候我们班并没有和老柳的班级一起整编，而是攀上了隔壁15、16班的高枝，班主任老熊带出了上一届一本率最高的班级。14班变成文科班，我来到16班。之后见到老柳就几乎只是下课时的走廊里了。之后的时间仿佛变成了可压缩流体，再浮出水面就是高考之后了。14班的同学们又聚到一起，请了老师们一起吃饭。老柳挨个桌子地转，来和我们说话。走到我们桌的时候问我，语文上130了没有，我说差两分，刚要说对不起老师，他抢先一步摆手，“挺好了！”\n\n\u003chr class=\"slender\"\u003e\n\n好像也就没什么了，确实没什么了，毕竟有的没的，都是不能再说的了。\n\n一中开学的时候分班名单贴在刚进门左手边的墙上，我看了一圈又一圈，几乎每个班都有初中同学的名字，除了13和14班。对，13和14班没有初中班里任何同学的名字，包括我的。看了一圈又一圈，每个班都没有，我被落下了。录取通知书都有，学费也交了，发票攥在手里，人吓傻了。回过神来赶紧钻出人堆，还好爸妈都还没回去，于是一起到年级主任办公室去，发现已经有好几组学生和家长在等着了。当时应对我们的是年级副主任青老师，用像亲戚唠家长里短时的埋怨一样，说我们太着急了，何必呢？我看我爸拳头已经硬了，赶紧自己上去介绍情况。青老师继续和蔼可亲地把我安排到了14班，因为班主任小佟老师上一届和青老师一起搭档，是一位年轻有冲劲的老师，教历史；和她搭档的是柳师傅，经验丰富又稳重，挺好的。\n\n我们14班是个什么水平呢？说放牛班有点不正确，因为按官方口径，一中没有放牛班。每个班都一样好，不放弃每一个孩子也一直是一中的宣传重点。但是呢，我的初中是分快慢班的，除了凭学习成绩考进去的，非富即官，没有一个在这个班；闲聊八卦的时候，得知班里分数不够要交择校费的大约占三分之二；入学后第一次考试的时候，我的数学和英语都有大面积的正确题目被误判为错，可能老师改卷也很辛苦，只能先做有错推定了；第一次期中考试，政治老师拿着我的卷子看了半天说不错，努努力可以考个二本。\n\n我的中考成绩不好，考前一个星期的时候，被班主任拉出教室去罚了一天的站，回来桌子被拖到教室的最后一排。原因是考前最后一次家长会上，班主任在上面讲话的时候，我妈和其他家长在下面“交头接耳”。回家之后和家里一说，听到了事情的另一个原因。说出来要负法律责任，要讲证据，那我就不说了。总之中考考得很差，刚刚够一中的分数线。其实熬过中考就很不容易了，考完之后整个人就像失了魂一样，每天坐在小区花坛边上唉声叹气。知道14班的情况后反而松了一口气，好学生我已经当够了，平平淡淡才是真。\n\n所以老柳很对我的脾气，知道我们的水平，不做过高的要求，那些文学典故，文士风流，完全是以一种“我觉得这些很有意思，应该让你们也知道”的心态在分享，能“指点江山”，“粪土当年万户侯”的文字，恐怕多发自“渔樵于江渚之上”，大概率不怎么激扬。但是小佟老师不一样，激扬得很，那时候《士兵突击》已经开始火了，但是很多同学还没看过，于是我们每天晚自习之前两集连播，绝赞连载中。但是这剧实在是太长了，新学校的新鲜感过去得差不多了，年级主任就以不务正业为由突袭取缔了该活动。时机倒也恰当，不久许三多就要回老家，和我一样消沉了。\n\n这样的我，期中考试不可能考得好了，但是成绩之差还是有一点戏剧性在里面：历史和化学试卷是两张八面大纸，结果我把第二张对折得太完美了，以至于做的时候以为是双面小纸，各有两面题目没做……结果消息传到初中同学那里，上学路上一辆捷安特三档变速直梁自行车呼啸而过，车上传来以前比我低一两个名次的“兄弟”的口哨声。原来我的平平淡淡才是真，是一件能给他人带来快乐的事情啊，我，操，你，妈。后面的事情前文已经写过了，阅读理解，牛奶加糖，睡觉，起床，冷水洗脸，妈妈做的早饭，黎明将至的青空。\n\n寒假里柳师傅给我爸妈打来电话告知成绩，因为一中讲素质教育，给学生分组指定了导师，我分到了柳师傅。为什么素质教育包括这个，咱也不知道，咱也不敢问。总之柳师傅很高兴，像是好多年没见到这么好的学生了。过完年返校发奖状，三好学生、学习标兵、学习进步奖。全班第一拿了学习进步奖，小佟老师说完，还有些暖烘烘懒洋洋的教室一瞬间安静下来，14班变了。二年级，我们攀上了隔壁15、16班的高枝，走班一段之间之后，再见到老柳就只能是下课时的走廊里了。\n\n但是有些事情还是没有变的。分班去到16班是第一天的晚自习，课间休息的时候，后来知道是叫浩哥的一位活泼开朗的同学来跟我们说话，让我们好好学习，有不会的问题可以问小惠，就你同桌的同学。我赶紧点头称是，旁边的琛哥和韦哥没有说话。篮球联赛的决赛，我们差两分追平的防守回合，球贴着小马哥出界，裁判判定球碰到了小马哥的球衣。第二天柳师傅讲“冯唐易老，李广难封”，说的是有些人年轻的时候要论资排辈，领导让你给长者让让路；等到自己磨砺出一身本事了，领导又喜欢活力有为的年轻人了……直到高三，学校开的自主招生辅导班的入班资格还要和中考成绩挂钩。\n\n本地的微信朋友圈，每隔一段时间就会出一篇文章，批评一中成绩不行，收着全市的好学生，结果高考清北几年出一个，然后就转到抓纪律加作业上面去了。一中的老师们就在朋友圈里反驳说一中搞素质教育问心无愧，一中的同学们就说母校就是你每天骂八百遍，但不许别人骂一句的地方。我也不知道该说什么，但是柳师傅升职总是个好事吧。\n\n于是给园老师的朋友圈点了个赞。\n"},{"slug":"taile-celebration-of-life","filename":"2021-09-23-taile-celebration-of-life.md","date":"2021-09-23","title":".doc | 泰勒的追思会","layout":"post","keywords":["doc"],"excerpt":"距离开学还有两周的星期日深夜，临睡之前瞥了一眼手机的邮箱，看到一封来自系主任的邮件。标题只有两个词 “sad news”","content":"\n距离开学还有两周的星期日深夜，临睡之前瞥了一眼手机的邮箱，看到一封来自系主任的邮件。标题只有两个词 “sad news”，下面的一行摘要里有维教授的名字，心下一惊。点开邮件全文，教授的儿子泰勒因为抑郁去世。\n\n维教授是我们系生物方向的四人组之一，主攻神经科学。我们入学那年，他的课题组正从实验向理论方向转型，需要新人手。当时我已经有中意的课题组了，但是抱着多听听也没坏处的想法，也参加维教授的组会，一起听的还有陈豪和一个伊朗女生，以及本科生松岩。感恩节的时候，维教授邀请生物方向的所有老师同学和家人到他家聚会，席间老师的孩子们在一起玩，泰勒就在其中吗？\n\n转过天来，系主任又转发了来自维教授的消息，通知大家泰勒的追思会在周四下午举行。转发的时候，系主任还请大家到系办公室，留言向维教授一家致哀。地点细节若干，唯一印象深刻的是对追思会的用词—— \"celebration of his life\"。\n\n当时已经决定要参加活动，但是放眼衣柜，没有合适的衣服。于是周一下午去村中心的商场购物。上衣很快选定了一件纯黑色长袖衬衫，因为很少逛商场于是顺便买了一件深蓝色同款。裤子在同一个店试了几条，总觉得不如衬衫的颜色正。于是去了隔壁的一家西装店买了一条西裤，各方面都好，除了价格。还想买一件白衬衫，试了几次尺寸，发现店家已经准备关门了，出门见天光依然大亮，才知道疫情给生活带来的变化远不止于口罩。\n\n第二天和导师线上见面讨论了一下研究进度，顺便问了下葬礼的着装要求。导师回答说尽可能正式，于是上网搜了一下，说是黑衬衫不太合适，于是再次去购物。白衬衫黑领带口袋巾不说，来回试了几件西服外套，猜了猜尺码的意思，在肩宽和袖长之间找了个平衡，然后去买鞋。在我连续问了几款鞋子有没有黑色之后，鞋店大妈问我，你是想买自己喜欢的鞋子还是只要黑鞋子？我一听这是打算教我做人的道理了，于是诚实地说黑色就行。大妈中断片刻后继续施法，还是教育我说应该买我喜欢的鞋子，然后指给了我隔壁的百货商场。\n\n为了进一步防止着装出问题，于是想到了陈豪，问他要不要一起去，然后商量统一着装。于是周四下午，对着网上的教程打了个半温莎结，然后开车接上陈豪，前往教堂。\n\n我们到的时候已经有不少人在了，教堂大约由三个正方形空间并排而成，入口在中间一个大厅，进入后左侧是礼拜堂，右侧布置成了一个餐厅。维教授和夫人站在中间大厅靠近礼拜堂的一角，身旁地上撑着有一幅等身高的画像，画中一个少年挎着一个篮球，身着比赛服。大厅内的访客排成一个回环的长队，在访客名册上签字，再依次问候教授一家。系秘书萨拉和图书管理员艾莉森和我们几乎同时到，于是我们一起排队。环顾四周，发现我和陈豪两人穿得已经算是很正式了。现场的年轻人很多，白衬衫黑领带者多，穿外套者少；年长者则更不正式，衬衫不必纯白，鞋不必正黑，甚至有老者穿浅色套装。维教授一身黑衣，领口一个白领节。\n\n队伍前进很慢，我和陈豪就跟萨拉和艾莉森闲聊。我不太懂这种场合该聊什么，聊着聊着就聊到了我导师最近申请下来的一个大额研究资助，聊到了我们两个人打算什么时候毕业，倒也不是十分严肃。见到维教授的时候，也就是很普通地寒暄了两句，维教授的夫人还记得四五年前感恩节聚会的情形，感谢我们来参加这个仪式。我又代已经回国的松岩向教授致意。\n\n已经慰问过教授的访客分散在中央大厅里交谈，我们认识的人不多，回头看见唐学长和维教授之前的学生詹森一起排队，我们就返回队伍和他们聊了一会，依然是主要关于各自未来的打算。詹森有了孩子，现在为了奶粉钱又兼职了一家科技公司的远程工作；唐学长准备毕业，打算去纽约的医院找一些医学研究类的工作。齐学姐已经去了纽约的实验室，托唐学长带了一个花篮，和其他花篮一起摆在大厅中间的一个小桌上。今年新进入我们系的两位中国老师也来了，毕竟学生和老师之间还是有隔阂，我们没有去和他们说话。系主任和夫人来的较晚，带了一个小姑娘，不知道是女儿还是孙女。\n\n系主任到时，很多访客都已在礼拜堂落座，我们想找地方时礼拜堂已满，教堂在中间大厅靠礼拜堂左半边摆了七八排椅子，椅子上摆了一个小册子。我和陈豪相邻坐下，身旁是一对夫妻，男士是维教授曾经的学生，现在在医学院工作，和陈豪握了握手。女士坐在我身旁，仪式还没开始，就可以感觉到她在啜泣。大约维教授的亲学生，每年都会去教授家做客，应该是看着泰勒长大吧，就像我看导师家的孩子一样。\n\n新牧师上台，声音从礼拜堂门上方的一个小音箱传来。带领大家冥想之前说，他知道今天到场的人并不都信仰同一个上帝，大家可以把他说的神替代成自己信仰的至高。于是开始冥想，牧师引用了圣经、伊索和多萝西·代伊，声音流利温和。祈祷之后是合唱圣歌，歌词在小册子上，我不知道曲调，只能默默地听。我们右前方靠中间走廊的座位有一位女士，声音洪亮，不知道是平时的积极分子还是教堂安排的领唱。\n\n然后是亲友上台追忆泰勒的生平。最先上台的是泰勒的舅舅，两三句话之后泣不成声，一个劲地说他爱泰勒，爱所有人。之后是泰勒从幼儿园到高中的好朋友，回忆了和他一起玩的种种，一起在高中篮球队时对泰勒的受欢迎的羡慕。还有篮球队的教练，高中新认识的亚裔朋友，说他是怎样默默地关注着身边的每一个人，用最自然的方式把他们拉入谈话，让他们能够在一个集体中感到被接受。一个女孩子登台，中学的周末，两个人对家里说谎后去约会吃饭，然后在餐厅遇见双方的家人……她知道今天到场的这么多女孩子，几乎所有人都和她一样喜欢过泰勒，他值得这样的喜欢。\n\n又唱了另一首圣歌，牧师再次登台，说自杀让人困惑，安静的会场沉入了更深的安静，我们不理解，神还有很多安排让我们不理解。小册子的最后有关于自杀救助的热线，建议大家都记到自己的手机里。\n\n仪式结束后，想留下来陪伴家人的访客可以去旁边的大厅用餐。我们不知道礼数，于是问詹森和唐学长，他们都不打算留下，于是我们也一起离开。开车一路东行，后视镜一片眩目，夕阳薄暮，晚霞如血。\n\n开学前的一个星期是迎新周，作为研究生教学主管，维教授依惯例需要在迎新会上讲话，向研究生新生介绍项目教学计划。迎新活动的主办方是高年级学生组成的 mentor committee，committee 的头儿安鲁写信给卡教授，问他愿不愿意代替维教授出席。卡教授回复说他已经问过了，维教授觉得自己还能正常履职。作为 committee 的一员，维教授讲话的时候我也在场，介绍十分顺利，只是在教授对着幻灯片的一处幽默的时候，我不知道该做什么表情。\n"},{"slug":"summary-of-this-blog","filename":"2021-02-10-summary-of-this-blog.md","date":"2021-02-10","title":".doc | 口嫌体正直的年末总结","layout":"post","keywords":["doc"],"excerpt":"元旦的年度问卷也就图一乐，真总结还得看春节","content":"\n“一颗小小的蓝色行星，又绕着它的太阳转了一圈。”这是科普作家 Ent 的公众号今年元旦更新的标题。我还看过这个句子更富丽堂皇的版本，还多了什么总星系什么银河系的啥啥悬臂之类的科学术语，虽词藻相殊，其志一也，都是说我们觉得值得暂停回顾的一段时空，不过是更广阔宇宙里的沧海一粟。\n\n确实，年不是一个纪念的好理由。从周期性的角度来说，虽然这是地球公转一周的周期节点，但太阳也在绕着银心旋转，今年的地球和一个恒星年以前的地球并不处于相同的位置；另一方面，我们生活的节奏也不再以年为单位了，今天绝大多数人已经离开了春种秋收的农耕生活，投入了基本在工业时代定型的生活模式，学生的生活阶段基本上是由升学划分的，小学两个三年，初中三年，高中三年，大学虽然有四年，但是最后一年吊儿郎当，懂的都懂，最关键的是，这种划分是别人替你决定的，你并不需要做什么，也做不了什么。\n\n如今不同了，虽然依然是学生，但是距离毕业，然后彻底结束学生身份的日子越来越近了。其实博士前两年上完必修课，时间上的自由度就很大了，如果完全放任追求即时的舒适，时间的流失真的超乎想象，过去的一年大家应该都收获了很多例子。因为有太多的时间可以利用，我又的确不是一个喜欢和善于把精力集中到一个方向的人，于是开了公众号，还学了一点 HTML+CSS+JS、了解了一点 jekyll， 在 Github Pages 上搭建了一个博客。\n\n对，公众号和博客，终于让我给圆回来了，我们回到正题……之前再稍微跑题一下？早在上高中的时候，晚饭到晚自习之间的空闲里，我基本上也都在教室拉着朋友胡侃，指点江山，于是经常被问道：“你聊这些有什么用，有这时间多刷几道题不好吗？”现在写公众号和博客，都用不着别人问，我自己都很有负罪感，有这时间，多读几篇论文不好吗？\n\n所幸咱的脸皮够厚，能够脸不红心不跳地指出来，这是一种非形式逻辑谬误，名叫“假两难推理 (false dilemma)”，不干这些不务正业的事情省下来的时间，往往也没办法全都用在所举例的正事上，而绝大多数都用在了比之前不务正业更没有质量的娱乐当中——人的精力是有限的，沾床就睡只是精力耗尽一种表现，还有一种就是眼睛睁着，却像行尸走肉一样什么都做不下去。稍微分散一点精力，把精力放在有兴趣的地方，可以延缓精力的耗尽。我这样的人，在高考这样需要血拼到最后一丝精力的游戏规则里不会有什么太大的出息，我知道、理解、接受这种规则的结果，但是没办法认同规则本身。\n\n另外，所谓的没用，往往只是不知道有什么用。觉得没用就不学，等到发现要用的时候，也就是发现自己来不及学会的时候。在目标明确的功利主义者的常年内卷之下，兴趣者依然没有被淘汰，除了进化的速度不足以在短期内产生足够效果之外，更是因为兴趣带来的技能积累，以及觉得好玩就可以学的开放心态，还有在这过程中锻炼出的学习能力和执行力，使得兴趣者并不比功利者有太大的劣势，如果你能在内卷中幸存下来的话。\n\n## 弃号重练的旧事\n\n从时间线上来说，\n\n- 2017年中，公众号发出了第一次非原创的测试性推送，是《诗经》的《鹤鸣》篇，之后长期沉寂。\n- 2018年中，接近一年之后，第一次发表原创内容，胡侃为主，有一点点收割韭菜的动机，贩卖一点带有小确丧的焦虑，目标对象是比我爸妈稍年轻一点的，孩子还在上高中的父母。;-)\n- 2018年末到2019年初，初步搭建好了个人博客。\n- 2019年中，翻译《为什么说物理不是一门学科》，事后看来这是一个很大的转变，在这之前，是“哎，我有一个博客，我得写点什么放到上面”，之后则是“我写了写这个，哦，这个东西可以放到博客上去”。\n- 2019年8月24日，翻译了保罗·格雷厄姆的《How to disagree》，这是这个博客阅读量最高的一篇文章，也是为我在微信平台上收获最多关注者的一篇文章。\n- 2019年末，听说了 Matters 博客平台，开始在博客、微信、Matters 同步上传自己的文章。中间短暂地在豆瓣和 Bilibili 上同步过，但是数据太差，自觉不值，放弃了。\n- 进入2020年，空闲时间多了起来，开始下决心一月一更。\n- 进入2021年，没有下任何决心，但是似乎有能力一月两更了。\n\n但其实，~~高墙之内埃尔迪亚人的记忆是被第145代王篡改过的~~，目前这个公众号是我注册的第二个号了，在这前面还有一个，是本科期间注册的。当时找了一群高中时候玩的不错的同学，准备像地下刊物一样一起写点东西。事实证明，玩的不错和志同道合之间还是有点距离，想法和付出努力之间也有点距离，努力和收获之间更有很大一段不在自己掌握之中的距离。\n\n结果那个号的整个生命周期里，写字最多的是被我剥削无偿劳动的我爸，为了维持周更，我开始每周写一篇德甲的战况综述，不仅是没有专业背景的过家家，而且改变了整个公众号的内容定位。于是大约一年的时间，数据太差，自觉不值，放弃了。（好眼熟）\n\n## 总结和展望\n\n今天回头总结这段失败历史，我觉得以下几个选择很重要：\n\n### 实名 vs. 匿名\n\n这里说的实名和匿名，指的是在前台，读者和观众是否知道作者是谁，头衔身份几何。后台匿名是不存在的，所有在大陆司法管辖范围内的内容平台，注册账户的时候，个人信息就全交出去了，有点什么事，用伏拉夫的话说，“马上就到你家门口”。\n\n实名的话，一个典型的好处就和微商一样，可以利用你已经有的交际圈，在刚刚起步的阶段就收获一群关注者，拿一个对新人来说很不错的阅读量。坏处也和微商一样，这种红利会随着时间的推移迅速衰减，熟人的转发很快就会变成人情上的亏欠。而且既然熟人在，聊些有他们出现的故事自然就会考虑他们的感受，写出来的东西要么是朋友之间互相吹捧的肉麻之作，要么是炸翻友谊小船的低情商雷管，如果都不是，那就恐怕逃不出“人在蒙古，刚下航母”的自吹自擂。\n\n实名的另一个好处是各种头衔带来的光环效应。《How to disagree》中已经提到了，依据说话者的身份推断可信度和含金量，这其实算是一种诉诸人身谬误 (ad hominem)，但这招确实是管用，毕竟判断对错是需要背景知识的，很多事情读者掌握的唯一相关知识就是作者身份了，那还怎么批判接受理性思考呢？不谈说服力，就只考虑班门弄斧贻笑大方的威慑，也能让很多质疑和辩论的语气缓和很多。\n\n现在这个博客和公众号，除了父母，就只有极少数趣味相投的朋友知道，而且基本上都是自己看出这些文章是我写的，没有强买强卖的现象。之前偶尔转发到朋友圈，不可能没有其他人看过，但是既然看了又没有表示，那也就算了，毕竟年纪大了，知道世界广阔，朋友不可能在每一个方面都玩到一起去。不谈头衔，能检验我言论对错的就只有逻辑和作为论证起点的三观和客观事实，这也能督促我检视自己文章的严谨性，排除很多杂音和不理性的粉丝情结（哎哎说粉丝也太不自量力了吧）。\n\n### 平台 vs. 自立 \n\n前面已经说了我在自己的博客之外，还曾经在B站和豆瓣同步，现在依然在微信公众号和 Matters 同步。虽然严格来讲，我的博客也不算自立，是依附于 Github 的服务，但其毕竟不是一个专门的博客平台，需要作者上传构成一个网站所需的各种源文件，姑且算是在大平台提供的方便性和个人的独立性之间取得了一定的平衡。\n\n据说微信一篇文章的绝大多数流量来自推送后的48小时之内，但是后台看数据的时候让我很吃惊的是，不少微信公众号上关注我的朋友，绝大多数都是来自“搜一搜”，绝大多数来自《保罗·格雷厄姆谈如何反对》一文，都是文章推送之后很久关注的。像微信这个体量的产品，可能推荐算法稍微一个概率的调整，就超过我这种内容小贩苦心孤诣引流的努力。\n\nMatters 和 B 站的推荐算法客观来讲和微信有差距，而且 Matters 的用户数量比微信少太多了，Bilibili 的主业毕竟是视频而不是文章。但是两个平台所形成的社区的氛围都很不错，在 Matters 上活跃的大多数自己也是写作者，所以我的关注数和被关注数差不多，评论互动也最活跃；Bilibili 的年轻人比例很高，不谈意识形态的话，对于新事物的兴趣和接受程度远高于社畜。\n\n至于豆瓣？呵呵。\n\n### 单干 vs. 合伙\n\n上一个公众号的最大失败就是一厢情愿地把朋友们拉了进来。毕竟本科的时候足球赛、CUPT、建模美赛，各种活动班里的同学说组队就组队了，说通宵就通宵了，完全不食人间烟火，没有意识到这种默契是建立在那些共同目标可以为所有人带来利益的基础上的，虽然这个利益更多的是无形的荣誉。和我一起写文章，能给我的朋友带来什么？用爱发电，这种事情最多是用来追求自己的兴趣，况且即便是自己的兴趣也不必跟钱过不去。所以这次，既然我没有回报朋友的能力和精力，博客和公众号的一切都只与我自己有关。\n\n### \"干货\" vs. “私货”\n\n简体中文里，“干货满满”已经成了绝对的褒义词，“夹带私货”是攻击对方的重要借口。去掉感情色彩，干货基本上约等于对事实的陈述，私货约等于对观点的表达，陈述和表达真的这么明显的高下之别吗？更何况在这个时代，我们不是缺少信息，而是被过量的信息淹没过载，缺乏处理和利用信息的能力。在一个人的观点中，我们可以看到他处理信息的方式，看到不同信息在他眼中的逻辑关系，最终管窥他的知识结构和三观，这是远比头疼医头的“干货”更有意义的信息。\n\n授人以鱼不如授人以渔，我没有能力写出一本钓鱼书，但是记录自己挂饵挥竿的过程，也比单纯的画一条鱼更有意义。\n\n### 我 vs. 我\n\n以上总结纯属放屁，完全是站在今天回望过去的马后炮，有着归因谬误的巨大嫌疑。这个号相对于上一个号最大的区别，就在于多吃了几年饭，多过了几座桥——这不是倚老卖老，几年摸鱼姑且折腾出来一些技能，两年研究生课程学到的东西确实不是当年那个幼稚的本科生能比。我能输出比当年更高质量的内容，是因为我变成了一个比当年更高质量的人。\n\n更重要的是，我的价值观更加坚定了，我知道自己写的东西水平如何，不需要阅读量、Likecoin 数来替我估值，豆瓣上发的东西没人看，那就去他妈的。我上不了更好的大学，申请 PhD 没拿到更好的 offer，这不是我单方面的损失，只要我继续努力，就有希望变成“这不是我的损失”。\n\n我对现在的博客和公众号的内容不满意的地方也有很多。一是连载不够多，没有输出更成体系的知识；二是曲高和寡，最近的非平衡态物理笔记，很明显没有让人看懂的意思，将来应该补充更基础的入门知识。更重要的是，现在文章的互动和反馈几乎没有，我哪里写的不好，很少收到来自读者的直接批评。\n\n## 正题：发红包！\n\n所以，希望已经关注的朋友，在将来的推文里多留言，批评反驳都没问题，毕竟翻译过《how to disagree》的人，不应该见到批评就炸毛。也欢迎大家把觉得不错的文章转发给几个志同道合，觉得可能会喜欢的朋友，毕竟批评的人多了，问题才更容易暴露出来。每篇推文里都来这么一段广告实在是太煞风景，这里一次性地拜托了。\n\n还是上一个号的时候，自己哼叱哼叱地坚持了很久之后，终于有朋友看不下去了，像红三连五班可怜许三多一样告诉我：“你难道不知道公众号的流量是要花钱的吗？”我当时也很许三多地回答，我希望大家是因为内容而不是薅羊毛而来看我的文章。现在看来的确幼稚得很，不是因为这句话本身有什么问题，而是当你没有拿出真金白银的时候，是没办法证明自己不是穷酸还嘴硬的。\n\n所以趁着过年，准备了总计 200 元的红包（小程序上限就是 200），分成 6 个，放在了这篇文章微信版的末尾，大年初四迎灶神的时候开奖，祝大家好运。\n\n本来想根据互动点赞之类的数据来分红包的，但是一方面微信小程序好像没这个功能，另一方面各个指标都没有没有竞争的价值。每一项都是 hk 兄弟遥遥领先，等疫情过去了，一定请你吃饭。\n\n在 B 站上，曾有一个朋友每一篇文章都读过并且点赞，希望你能看到这篇文章，也祝你的运气比别人更好一点点。\n"},{"slug":"matters-yearly","filename":"2021-01-09-matters-yearly.md","date":"2021-01-09","title":"【2020Matters年度问卷】Normality Matters","layout":"post","keywords":["doc"],"excerpt":"原文于2020年12月28日首发于 Matters，当时在赶平台奖励的截止日期，所以很多东西没有展开写，大家凑合看吧。","content":"\n\u003e Matters 是一个博客平台，在前几年区块链和去中心化思想还很热的时候上线，底层所有的博文会上传到 IPFS 区中心化平台，账号可以绑定一种叫做 LikeCoin 的数字货币，用来赞赏作者。2019年11月28日，我在 Matters 上传了自己翻译的 Paul Graham 的《How to Disagree》，算是正式加入了 Matters 社区。在那之前，我虽然建立了自己的博客(https:mountaye.github.io/blog)和公众号，但是一直没有规律性的更新。可以说，过去一年所有的不务正业，都可以以 Matters 为线索串联起来。我一向不擅长赶 deadline，所以压根没做周年纪念的打算。结果发现 Matters 官方有红包，没办法，只能为五斗米折腰了。\n\n## 2020年只剩下最後十天，分享一件在年初想不到今年會發生的一件事？這件事對你的生活帶來什麼樣的改變？\n\n美国股市对新冠大流行的 V 形走势。标普500指数从2月中下旬开始跳水，用了一个月的时间跌去三分之一，然后开始了大牛市一样的攀升，八月底重回疫情前的高点。\n\n2018年1月份，我的开户申请被 Robinhood 批准，从此成为了美国股市里茁壮成长的一棵小韭菜。结果就在开户的那周，美股就开始下跌。\n\n那时候人们开始说，十年一个小周期，到了崩盘的时候了，结果涨涨跌跌地拉锯了几个月，创出新高；十月再次开始大跌，也就是段子里讲的10天4次熔断那次，那时候人们又说，十年一个周期，出来混，到了还的时候了，结果这回更快，只用了四个月就收复之前的高点；如今，已经没有人再说十年啊周期啊什么的了，YouTube 上的雷公视频的标题，也从“市场进入高危期”变成了“急跌就是买点”。\n\n“热闹是他们的，我什么也没有。”不只是没有的问题，我还在这期间花了很多仓位做空，兜兜转转，不断跌破止损线，累积下来，损失了大约一个月的工资。\n\n要说这对我生活的改变，可能是……让我最近两个月没有文章更新？不断的亏损之后痛定思痛，觉得问题主要出现在记账方面。之前和股票相关的账目都是用一个 MS Excel 电子表格记录，虽然自认为还算准确完备，但是一方面只包含了自己的已经发生的交易信息，另一方面数字也不如图形直观，很难给将来的交易决策提供帮助。\n\n因而，我决定把记账这件事转移到 Jupyter Notebook 上，完成 Excel 表的功能之外，还可以把自己的买卖价位和股价走势画在同一张图上，把自己的仓位和市场的交易量画在同一张图上，辅助决策。同时还可以练习 python 数据可视化的能力、`ipywidgets` 组件写 UI 的能力，一举多得。所以最近一段时间的业余都在忙这件事，博客也就暂时停更了。\n\n## 2020年，什麼事情讓你獲得最深的意義感？\n\n定期用无人机空拍校园。这件事刚刚付诸实施，每月选一天，沿着相同的线路拍一段延时摄影。预计明年的这个时候才会有结果，所以严格来讲不算是2020年的事情。\n\n定期是一种获得意义感的很有效的方式。高中的时候，我们学校的几个老师和同学们办了一份报纸，每周三晚自习前免费分发几十份给每个班。高三一年的每一期报纸我都集齐了，每个周三，我都在学校食堂暴风吸入晚饭，然后跑回教室等发报纸的同学。一年下来，攒下来的报纸对折起来，几乎和课桌的抽屉一样厚。人非星辰大海，没办法占据无限的时间，但是周期性的事件序列，具有时间上的平移对称性，周期函数的自变量是可以趋向无穷的，给人一种对于永恒的近似和安慰。\n\n摄影也是。像我这样的人，可能没办法给学校捐个楼，把自己的名字刻在楼基的石碑上，但是在这里学习和工作的经历，还有校园的景色，“耳得之而为声，目遇之而成色，是造物者之无尽藏也，而吾与子之所共适。”\n\n## 全球疫情依然嚴峻，請記錄一件你認為值得銘記的疫情事件。\n\n11月初的年度进展报告。从去年夏天的资格考试通过之后，每年11月初，我们都要向导师委员会汇报自己这一年的工作进展。今年的报告结束之后，第二天和老板单独 Zoom 的时候他表扬了我，说今年的进展依然不错，委员会的成员都印象深刻。\n\n我现在的工作主要包括两部分，一部分是用我们老板做博士后的时候用的成像方法，去观察和他当时的工作稍有不同的细胞，属于“承上”；另一部分是“启下”，因为旧的方法在性能上不足以研究更复杂的系统，所以需要开发新的图像处理技术。\n\n美国的疫情从2月份开始，那时候我刚刚拍了一些用于开发新技术所需要的参考图像，所以封城令最严格的几周有可以在家工作的素材。夏天限制条件放松之后，我又回到学校用旧方法拍了很多照片，这些照片在进行数据分析之前，需要在计算机集群上进行做“去卷积”计算，比较耗时，但同样可以在家做。所以今年的工作节奏，实际上并没受到病毒的太多干扰。\n\n但是我也知道，这种印象深刻的另一面是，在今年以前的我的工作效率，也没有比有疫情的时候有多大的优势。之前工作也有很多时候摸鱼，我也不会加班，业余时间也在折腾自己的爱好，简直约等于不务正业的同时顺便读了个博。疫情把“我们所浪费的时间本质上都是自愿选择的”这一点放大了，\n\n## 2020出行受限，如何改變了你與他人/世界的關係？有沒有什麼人/事，是疫情過去你一定要去見/做的？\n\n恍然发现原来自由出行也可能变成一种奢侈。\n\n疫情之后，我想我会成为一名 B 站的 up 主，像 Youtube 上的 Thomas Heaton 一样，到处走走拍拍，用拍视频这件事来鞭策自己，注视自己可能来之不易的日常，并珍视对这些日常的记忆。\n\n## 說一件你在2020年遭遇的、難以解決的矛盾，這裡的矛盾是指：你感受到自己的信念與行為產生了衝突。\n\n自视清高和渴望吸引眼球的矛盾。\n\n明知道在任何平台写东西，用真名挂各种头衔会多很多流量，但还是用化名，努力避免 ad hominem 谬误；明知道在 Matters 写东西，用简体字没有流量，不谈政治没有流量，不做赞赏公民没有流量，也还是坚持下来了；明知道在微信写东西，膜蛤可能会死，借古讽今可能会死，点了关注的读者可能再也没看过我的更新（微信信息流的锅），但还是在危险的边缘疯狂试探。\n\n## 分享一個你「忽然理解了我所反對的立場」的時刻。\n\n没有，而且我觉得也没办法有，“忽然”、“理解”和“立场”本来就不应该出现在同一个句子里，你要是真的能理解，那早干什么去了？\n\n公共讨论中的绝大多数问题并不是因为拒绝对话，而是急于对话，把根本不成立的对话强行推进下去，结果就是鸡同鸭讲，各自在自己的信息茧房里赢得一片掌声，然后一个共识，各自表述，直到其中一方得意忘形进入了对方的同温层，再开启一轮循环。\n\n## 相比一年前，你與身體的關係發生了什麼變化？你有更喜歡現在自己的身體嗎？\n\n老了，虽然我才 20 多岁。\n\n天冷的时候跑步，膝盖就会疼。刚来学校的时候，参加系里的老教授们每周踢足球的活动，还笑他们冬天休赛期太长，现在只能摸着自己的膝盖感叹自己还是 too young too simple, sometimes naive. 小时候练羽毛球，从场中央来到网前需要两步，退到底线需要三步；现在一步半就到了，但是再也不敢跑折返了。\n\n当然，作为一个减肥成功的胖子，我经历过更难受的肥胖状态，我对今天的身体已经足够满意，不敢再奢求更多。\n\n## 經過2020年，你內心是否有找到一個關於自己，不可停止之事嗎？\n\n认识自己——不断修正对自己的认识，并且证明这种认识。（你现在还认识“认识”这两个字吗XD）\n\n## 請與我們分享你在 2020年最常聽的一首歌、最愛的一本書或者印象最深刻的一部電影\n\n戈德门特《代数学教程》。\n\n## 最後，能否請你用一張照片代表你的 2020 年。\n\n![](/photos/2021-01-09-matters-rainbow.png)\n\n## 填空：2020，\u003cu\u003enormality\u003c/u\u003e matters\n\n好像真的赶不上 deadline 了，后面几个问题越来越敷衍……应该不影响拿红包吧（小声）"},{"slug":"doc-share-experience","filename":"2020-10-31-doc-share-experience.md","date":"2020-10-31","title":".doc | 经验介绍会","layout":"post","keywords":["doc"],"excerpt":"大三的时候，我们高中的班主任找到我，问我想不想回学校，给学弟学妹们介绍介绍经验，加油鼓劲。","content":"\n\u003e 本文为虚构。\n\n大三的时候，我们高中的班主任熊师傅找到我，问我想不想回学校，给学弟学妹们介绍介绍经验，加加油，鼓鼓劲。我当时直接傻了，没开玩笑吧？\n\n高三那年的清明假期，我们年级别出心裁，趁着已经上了大学的学长们放假回家，而我们还在学校自习的机会，把他们请回来，开个主题班会，和大家近距离地见见面，给我们介绍经验，加油鼓劲。\n\n我们年级当然有这样的底气。我们这样的十八线小城，一中那时北大清华还不是每年都有的，三年前的这批老师，一年教出了两个北大，老熊当时的班级，是全年级一本率最高的。我们这一年又是第一次推行北大校长实名推荐制度，校长为了给北大一个好印象，选中了本来上北大就没什么问题的年级第二，所以一场高考大捷已经十拿九稳。只不过年级第一有点不高兴，为学校实现了数学奥赛零的突破之后，赌气一般地直接和上交签约了。显然，这只是学校从一个胜利走向另一个胜利过程中的一点小小杂音，不足为虑。在这次经验介绍会之前，年级主任即将在高考结束之后升任副校长的传言，就已经在晚饭时段的闲谈中流传开了。\n\n听说这个的时候，我的心里很不以为意。不以为意很正常，十八线小城里的优等生，高考之前瞧得起谁呢？但还不那么简单，主要是因为同样的戏码，初中的时候已经看过一遍了。\n\n初中不幸，误入一个作业极多的学校。多到什么程度呢？我们入学的时候接近70个人，毕业之前转学走了12个。除了1个去东北的高考移民，剩下11个人都是转到了本区的其他中学——为啥？累的啊！什么996，什么福报？Too young too simple! 互联网大佬们啊还是要再学习一个，学不下去怎么办，打鸡血啊！开学一个月左右的时候，班主任请来了刚刚以全市第一名考上一中的学姐，还有另外两个学长，回来给我们介绍经验，加油鼓劲。\n\n温柔美丽的学姐和学长们如约而至。有一说一，这场经验介绍本身还是很成功的。一上来，学姐抚今追昔，指着教室中间一位漂亮可爱，马尾长辫黑又亮……咳咳，不好意思油腻了，指着一位女生说，当年她就坐在同样的位置。轻轻一番话语，就拉近了我们和学姐之间的距离。然后学习经验整理得井井有条，用排比的方式，介绍了课前认真预习、课上认真听讲、课后认真复习三大法宝。学姐还有一个姐姐，也曾是全市第一上一中，今年刚刚被北大录取，只要你足够努力，北大也在等着你，耶！作业多怎么办？这就是请多位学长学姐来的好处了，曾任语文课代表的学长站出来非常有斗争经验地说，你们课代表可以给老师求求情啊，老师又不是那么不近人情的人啊（班主任是语文老师）……总之，学长学姐学习成绩优异，人物形象丰富，总有一款你的菜。放学回家的时候，我骑车到路口，学姐站在不远处等公交车，牛仔裤，白衬衫，晚夏的风向后吹起了她的长发，哇，美！\n\n然后呢，作业还是他妈的多，撑不下去的同学开始转学。爸妈实在看不下去了，开始帮我写作业，期末最累的时候给我请病假让我在家补觉。\n\n班会开始之前大家的笔都没有停。老熊走进教室，看了一会自习，看了下门外，对我们说：“来，差不多了吧，让我们有请——”平时吱吱呀呀的塑钢门被很有礼貌地推开了，一位穿衬衫的干净男生走了进来。老熊作完介绍，学长的 PPT 也准备好了。一上来，学长抚今追昔，指着教室中间一位玉树临风的男生说，当年他就坐在同样的位置。然后……然后不是学习经验三大法宝了，但是究竟说了什么我也不记得了，记忆就是这么奇怪，久远如初中的事情还历历在目，后来的事情却朦胧不清。只记得他问了几个问题，我们给出了让聊天无法进行下去的尴尬回答。然后学长开始介绍山大的校园，讲到了自己正在领导的学生会部门，幻灯片上是学生会的照片，学长的旁边站着一位美丽大方的学姐。“这个嘛，工作上的同事。”学长解释时露出了微笑，男生们在意味不明的“哦”之后在微笑和大笑之间猥琐地笑，此刻写下这些文字的我自嘲地笑。\n\n几个月后的高考成了整个年级的滑铁卢。我的全校排名仅次于某次超水平发挥的期中考试，但总分依然比平时低了二三十分；北大校荐的同学加了六十分后依然不够被录取；全市第一名是一位补习学校的复读生。当然了，喜报还是发在了市里的报纸上，毕竟一本过线人数再创新高。只不过新任副校长人选是兄弟年级的主任，上大学后看学弟们转发的 QQ 空间里的动态，英姿勃发的校长一夜之间头发全白。\n\n写到这里的我实在是写不下去了，写这些干什么呢？是想幸灾乐祸吗？不应该，这样一场溃败，每个人都是受害者，我也并不置身事外。是想咀嚼一番回忆的苦涩吗？不存在，高考的成绩不是十分理想，客观上减轻了我在大学时竞争的激烈程度，让我得以进入第一志愿专业里一个不错的班级，交换生出国，奖学金，我的四年大学生活没有什么遗憾。\n\n退言思之，可能我想说的是，滑铁卢并不是拿破仑一个人的滑铁卢，真正承担伤亡的，是一个个籍籍无名的普通士兵。拿破仑尚且还可以在偏远孤僻的小岛上了此残生，内伊元帅甚至可以慷概骄傲地命令行刑队向自己开枪，如果“无内鬼，来点苏联笑话”，现代传媒甚至可以让人不知道拿破仑的失败，可那些败兵呢？他们只能包扎好伤口，掩埋好战友，然后解甲归田，或者继续前行。是的，可以解甲归田了。上了大学，及格万岁，只要你不违纪，不跳楼，课随便翘，游戏随便打，有了外卖更是可以屁股长在椅子上，眼睛长在屏幕上。那谁还努力啊，又有什么事情值得努力呢？我也不知道，但是晚上自己在教室里刷题的时候，为了公派背单词的时候，听身旁的同学骄傲地谈论自己全国闻名的高中母校的时候，我的眼前总是浮现出校长满头白发的那张照片。\n\n只是这些话，无论如何不可能出现在经验介绍会上。那我能说什么呢？不如抚今追昔，指一位教室中间的男生或女生，说当年自己就坐在同样的位置吧。\n\n"},{"slug":"xls-two-years-after-buying-car","filename":"2020-10-18-xls-two-years-after-buying-car.md","date":"2020-10-18","title":".xls | 聊聊买车这两年多","layout":"post","keywords":["doc","xls"],"excerpt":"如题","content":"\n2018 年初，趁着寒假的功夫考了驾照，趁热打铁买了车，是一辆二手的丰田花冠。\n\n最近有个朋友要买车了，知道我有车，所以来打听一下；碰巧我也临时起意想要换车，之前对买车和养车的花费，虽然一直在记账，但是脑子里是一团糨糊，需要理一理。\n\n结果呢，在这篇文章动笔之前，朋友就已经秀出新车的照片了；算完买车的帐之后，我也暂时决定不换车了。所以这篇文章，权当是周更凑数吧。\n\n## 先来算帐\n\n因为隐私的关系，所有价格都是约数，这样可以减小一点点[信息量](https://program-think.blogspot.com/2017/12/howto-cover-your-tracks-10.html)（虽然没什么用就是了）.\n\n与车有关的花费可以按照频率来归类：\n\n- 一次性的：\n    + 汽车本身，给车行的：约 $13,000\n    + 消费税：约 $1,300\n- 每年一次：\n    + 财产税：从第二年开始，年末交，至今只有一次，$300\n- 半年一次：\n    + 保险：大约是个等差数列，$2100 开始，之后每次下降 $200\n    + 保养：没有特殊项目的话，$20 - $30\n- 每月一次：\n    + 加油：车用的少，平均每月一箱油。最贵的时候每次 $30，最便宜的时候（现在）每次 $20 \n- 不定期的：\n    + 为了考驾照租了一辆车：$10\n    + 换驾照、车牌：每次 $20\n    + 换过一次轮胎：$700\n    + 换过一次刹车片：$500\n    + 洗车工具、清洗剂：$50\n    + 停车：在芝加哥两天停掉了接近 $150 :-( 其余至今累计不到 $100，包括在学校停车\n    \n## 再谈感受\n\n### 价格比想象中贵不少\n\n说实话，重新看到帐单的时候我有点不敢相信，虽然来这里的第一天就知道美国的东西标价和税是分开算的，但是我印象里一万三是税后价格。当初买车的预算只有一万元，结果不够，幸好当时爸妈在国内已经起床了，在签合同的当时紧急给我汇了一笔钱。估计是价格太高，花了家里这么多，自己实在惭愧，于是记忆就悄悄地自我修改，帮自己开脱了。\n\n一般而言，美国汽车新车的价格是比国内便宜一点，但并没有江湖传言说的差那么多。国内价格按照官价汇率换成美元，和美国税后价格区别就在误差范围以内了。有次我和几个朋友开车路过一家卖野马的车行，大家讨论起来野马新车的价格，我猜的最准。 ~~当然了，官价汇率和实际购买力也有一点不同，微信公众号上不能多说，说多了号就没了。~~\n\n美国买车便宜，主要在于这里的很多人买的都是二手车，一辆车在新车变成二手车的那一刻起贬值是最快的，五年左右的车价就能跌去一半。同时由于市场成熟，因为信息不对称被车商套路的可能性比国内小很多，有更多的人愿意买二手车。\n\n### 车险，及所谓“华人互坑”\n\n虽然车险价格以一个等差数列稳步下降，但是其实中间换过一家保险公司。头一年半的三次付费是卖车的时候经手的业务员推荐的，看起来像是本地华人经营的一家保险中介，自己没有保险产品，给一些知名度不是那么高的保险公司拉客户。\n\n卖车的小哥是华人，我当时房东的晚辈亲戚。月均 300+，听起来就好像不怎么靠谱是吧，总感觉应验了豆瓣名言“在海外坑中国人最狠的都是自己人”。第一次交保险的时候简直心头在滴血，就连我这种拖延症晚期患者，在一年半以后也坚决换了某 G 字头知名保险公司。结果回过头来看，价格下降的幅度并没有明显改善，基本是汽车折旧和个人信用记录的正常积累。\n\n当初把保险办好，所有的手续就都齐了，第二天直接拿着车行给的文件就把牌照办好了。我去看车的时候是开着考驾照租的车去的，买完车我再开回来，他帮我把新车开到我的公寓，然后打电话让嫂子开车接自己回家。这样一想，更为自己之前以小人之心度君子之腹而羞愧。这次朋友买车本来是想推荐给他的，结果前不久看朋友圈才知道他已经换了工作，有点可惜。\n\n### 我想换车——还是算了\n\n觉得现在反正住得离学校很近，每天去学校也不需要开车，真正用车的机会也就是每周一次踢球、一次去超市，以及不定期的短途瞎逛。这样的用车场景，小车的油耗优势并不明显，出去玩的时候，有时需要走一些石子路，SUV 的通过性有明显的优势。所以有想要换车的打算。\n\n但是，上网看了一些车的报价，标价一万五的都是凤毛麟角，两万以上很普遍。这也正常，毕竟绝大多数的 SUV 和 B 级车同平台，本来就比现在的车大一圈，SUV 本身的功能又有溢价。以我现在的存款，肯定是要先卖旧车再买新车，各种标价之外的成本，再想到各种行政手续也要消耗不少的时间和精力，恐怕只能望而却步。\n"},{"slug":"how-to-spend-life","filename":"2020-07-05-how-to-spend-life.md","date":"2020-07-05","title":".doc | 人生是马尔科夫过程吗？","layout":"post","keywords":["doc"],"excerpt":"最近报名了一个写作班，老师留了作业，命题作文——“我想怎样地度过余生”。","content":"\n最近报名了一个写作班，老师留了作业，命题作文——“我想怎样地度过余生”。\n\n我在电脑上敲下了一句“我想随遇而安地度过余生”，然后抓耳挠腮地度过了三天当时的我的余生，什么也没写下来。\n\n然后，疫情嘛，研究上的事，能在家里做的已经做得差不多了。闲来无事翻翻邮件订阅的论文摘要合辑，看到了一篇讲量子态马尔科夫演化的论文，于是想到了一个很有趣的问题：人生是一个马尔科夫过程吗？\n\n先等一下，马尔科夫过程是什么？\n\n用一个生活中的例子来解释吧。假设我们有一个感兴趣的对象（比如说我现在使用的牙膏的牌子）；这个对象有几种不同的可能性（我去的超市里有两种牌子，我可以但也只能选择其中的一种）；每过一段时间，这个对象有可能变化（每管牙膏用完之后，去超市再买一管，要么换牌子，要么不换）。\n\n重点来了：如果当我在货架面前考虑要买哪个牌子牙膏的时候，只考虑我现在用的是什么牌子，而不考虑上次买牙膏之前那管是什么牌子，不考虑之前任何一段时间用的是什么牌子，那么我使用的牙膏的牌子随时间的变化，就是一个马尔科夫过程。\n\n也就是维基百科上所说的：“未来的状态与任何历史的状态无关，仅与当前状态相关”。\n\n这么一说的话，人生还真的挺像是马尔科夫过程的。\n\n今天认识我的朋友可能想象不出来，我小学的时候还是个很听话的乖孩子。坐我后桌的小豪也很听话，班主任很信任我们几个孩子，经常让我们帮忙干点活儿，平时考试之后改改卷子啊，上面下来检查之前收拾收拾同学们的作业之类的。六年级下学期的某一天，老师放学后又把我们几个人留了下来，给了我们一叠材料，仔细一看，有一摞表格，上面是全班同学姓名和家庭住址的表格，前几天发给每个人回家填了，第二天连同房产证一起交上，房产证很快发回来了，表格在这；还有一张本市本区的地图，道路标的还挺详细的，道路围成的方块上标着不同的数字。我们的任务就是根据家庭住址，把大家的名字和地图上的数字对应起来，填到一张新表上。我和小豪的家有点距离，在地图上的数字也不一样。等我们拿到了不同初中的录取通知书，才后知后觉，当初那些数字代表的就是所谓的学区。高中之后骑车上学，有天早晨，在到校之前的最后一个路口等红灯的时候，眼睛余光觉得旁边等灯的人老是看我，我也看过去，端详许久，是小豪！我们都长出了一口气，不知道是舒气还是叹气。\n\n因为一个学区号，原本要好的朋友再相见，就只能努力注视然后努力回忆，再报以礼貌的微笑了；因为一场中考，因为一场高考，因为考研或是考公务员或是招聘面试，三年、四年、几十年的时间里，我们能做的事情就大体定下来了。至于在这决定未来的考试之前的每一个平凡一天里，我们的生活充实吗，枯燥吗，快乐吗，累吗，见到许久未见的老朋友了吗，不好意思，不太重要。\n\n所以说，把人生当成一个马尔科夫过程的话，一个很自然的推论就是，我只要在这些时间节点上，让自己拥有这些节点所考察的那些成绩，就可以在人生这条长河里一路顺风顺水，幸福美满了。\n\n大一结束的暑假，我们班的导师老史带我们去北京参观几个科研院所，用他的原话说，“见见世面”。从正负电子对撞机里出来的时候是下午，还有点时间，我们就去了北大。北大接待我们的老师是我们热学课本的作者，那是我第一次亲眼见到写教材的人，一种“我也见到大人物了”的自豪感油然而生。这样的参观活动本来也有为将来的保研做广告的意思，作者老师把我们引入一间教室，拿出了准备好的幻灯片。北大嘛，客观地指点指点江山就足以作为广告了。江山指点得差不多了，还没有看到我们学校的名字，老师大约也注意到了：“你们学校啊，还是差了一点。”老史也和我们坐在一起，除了点头也说不出话来。又有同学问，既然已经有这么大的差距了，那该怎么弥补呢？老师面带微笑地说：“人生啊还是很长的，你们现在虽然落后很多，但是一直努力的话，比别人晚个几年十几年，也是有希望达到同样高度的嘛。”我当时自然心里一百个不服，拳头在桌子后面攥得紧紧的。临走的时候，作者老师还问了一句：“我那本课本是写给非专业的课程用的，你们专业用这个，是不是浅了一点啊？”我很幸运，没看到史老师脸上的表情。\n\n反过来说，只要在某一个时间节点上没有做好，之后的人生就可以洗洗睡了。\n\n但是，人生也并不总是能简化成马尔科夫过程的。\n\n乔布斯在斯坦福大学的毕业典礼上做过一个演讲，讲过他读书时候的故事。领养乔布斯的夫妻当初答应了他的生母，一定要让孩子上大学。乔布斯年轻时也不知道父母的艰难，选了里德学院这样一所学费高昂的私立学校，很快就把父母给的钱花光了，只能退学。退学之后没有了学分要求，他得以选一些轻松有趣的课程，比如字体设计。等到后来兜兜转转，他被苹果公司开除，成功建立了 Pixel 动画公司和 NeXT 电脑公司，之后回到了经营不善来到破产边缘的苹果。这时候，之前的字体课派上了用场，苹果的新电脑取得了巨大的成功，优雅的字体居功至伟。\n\n人生并不是一个预先设计好的程序，也不是一台严丝合缝的机器，我们经历过的故事，有的深藏在记忆里，在不经意的时候闪现，击中我们；有的成为心中高悬的苦胆，时时提醒激励着人，去做一些可能徒劳但又不甘心的事情。\n\n北大的故事其实没有结束。当初提问该如何奋起直追的同学转到了数学专业，而一直志在生物的我反而留在了物理圈里。大四的时候，听说北大一个物理和生物的交叉学科项目还有一次补充录取的机会，申请美国学校结果不太满意的我就报了名。面试的一个环节是读一段英文学术论文的节选，然后根据材料回答几个问题。我抽到的文章是关于 microbiome （微生物组）的，老师们知道我的专业背景，本来没打算我回答得出什么问题，没想到我平时读过一些科普文章，对这个话题有些了解，老师的问题都很顺利地回答了出来。他们很意外，问我是怎么积累这些知识的，我很想给他们讲讲三年前那个下午，隔壁系教室里的故事。\n\n又觉得什么也不必讲，于是微笑着谦虚了两句。\n"},{"slug":"parents-visiting","filename":"2020-06-05-parents-visiting.md","date":"2020-06-05","title":".doc | 接机","layout":"post","keywords":["doc"],"excerpt":"今年寒假，疫情引而未发之时，爸妈来美国看我。","content":"\n今年寒假，疫情引而未发之时，爸妈来美国看我。\n\n他们英语不好，担心转机的时候不顺利，所以定了直飞芝加哥的机票，傍晚到，我早上出发开车去接他们。因为是一个人开车，手机定了计时，每隔一两个小时就离开高速停车休息一会儿。\n\n一路基本顺利，除了进城之后。前面几个收费站把硬币花完了，接下来的收费站没带够硬币，又误入人工硬币收费关卡，被收费大妈批了一顿之后滚蛋了，回去之后在网上补缴了费用；到机场之后，想到之前收费站的事，在航站楼里的小卖部买了包巧克力糖，换了一堆零钱；发现停车的航站楼不是爸妈要停靠的那个，导航又不明确，只能开车离开停车场，一边慢行一边看路牌。到了正确的航站楼，距离爸妈的飞机到达还有一个多小时。芝加哥当天极冷，于是也不好在周围闲逛，就只在接机口站着等。\n\n倒是没有着急，盯着出站口被人推开又自己关上的门，呆若木鸡，一种在被人漠视和假装无感于漠视中锻炼出来的，故作少年老成的冷漠表情。一会儿就要见到爸爸妈妈了，我会拥抱他们吗，会一惊一乍地嘘寒问暖吗，会有一股暖意从心底涌上心头吗？\n\n想来本科的时候，爸妈也是这样在等我的。那时候家乡还没有通高铁，去学校的时候坐的是夕发朝至的绿皮车，回家的时候动车票买到邻近的城市，爸妈开车到高铁站接我。有时候在高速的服务区吃饭，有时候他们先吃过了，来的路上买必胜客或者肯德基的套餐，这样我在车上就把午饭解决了。\n\n事情也不是一开始就这么顺利的。第一年寒假的时候，当时那个高铁站也才刚通车不久，出站口的广场和地下通道还扎着脚手架，第一次来这个车站，我有点找不清方向。打电话问他们，我爸习惯说东西南北，我只认识前后左右，指挥了半天又绕回了原地，我爸开始急了，吼了起来。我也不甘示弱，挂了电话，直奔汽车售票处要自己买票回家。拖着行李箱到了站台上，一路上手机一直在响。接了电话，那边爸爸的声音还是很强硬，但明显有一种不敢再发怒的谨慎。这是最容易让人蹬鼻子上脸的，我直接吼了一句“老子就是要让你不爽！”，又把电话挂了。\n\n挂完电话之后就后悔了，真的要自己回家吗？架总要吵完的，回家之后，该怎么收场呢？于是就在周围人的目光中离开站台，脸红得很，也不知道是冻，是怒，还是羞愧。离开站台后给妈妈打电话，重新核对周围的建筑位置，终于在广场去地下停车场的楼梯旁边会合。见面的时候，妈妈刚想说话缓解气氛，我先给了爸爸一个拥抱。然后找了一家餐厅吃饭，回家，比无事发生还更加其乐融融。后来回学校之后，爸爸在网上跟我告状，说妈妈因为这个拥抱嫉妒他，找了他好几次茬。\n\n父母和孩子的关系，在绝大多数时间里都是极不对等的权力关系，而且是总有一天要从一种不对等逆转成另一种不对等的权力关系。我们家和舅舅家住的近，所以常走动。表弟出生的时候，舅舅年纪已经不小了，表弟刚会走路的时候，在我们家小区的草坪上，鬓角已有白发的舅舅在表弟面前S形跑，双臂向后伸，嘴里念着小鸟小鸟飞。高中的语文老师讲课爱跑题，讲李密《陈情表》的时候，他感慨说人的衰老就像是时间的逆转一样，老人越来越像一个孩子，越来越需要子女像当初的父母一样照顾他们。不久，老师请了短假，回来后左臂上多了一块黑色的孝牌。《请回答1988》里面，德善的爸爸很因为总是忽视德善而道歉，说爸爸也是第一次当爸爸。可是，哪有那么容易的道歉呢，孩子也是第一次做孩子，有些愧疚连愧疚的资格都没有，只能在比无事发生还更加其乐融融的气氛里，被时间抛在身后。\n\n广播报出了他们飞机的班次，出站的人流渐渐拥挤又渐渐稀疏，在他们看到我之前，我先看到了他们，对眼前的陌生有些茫然，面有疲倦之色。我接过他们的行李箱，“还顺利吗？”“挺顺利的。”天很冷，出航站楼之后我们都打了哆嗦，快步往停车场去。\n"},{"slug":"betrayal-and-report","filename":"2018-06-14-betrayal-and-report.md","date":"2018-06-14","title":".doc | 也谈告密和告发","layout":"post","keywords":["doc"],"excerpt":"这个问题似乎没什么可谈的，告密是中国价值观里最低等的劣行之一","content":"\n也想谈谈告密的问题。\n\n这个问题似乎没什么可谈的，告密是中国价值观里最低等的劣行之一，也是我爸妈对我从小的一个教育重点，在学校闯了祸要被老师剋，但是敢把同伙供出来，被爸妈知道了得往死里打。告密是一种行为，其背后的本质是背叛，背叛是对别人信任的践踏，只要信任建立得有道理，这种践踏必然是令人不适甚至不齿的，没什么好讨论的。\n\n那就先按下这个话题不表，说我见到的另一个有趣现象。人们在谈起美国和加拿大（尤其是加拿大，也许因为其他方面乏善可陈）的民风淳朴的时候，经常用来举例子的一点就是，人们对于维护社会秩序的积极性，遇见违法行为（比如闯红灯）积极检举。还有就是在体罚还刚要被社会舆论否定的时候流传的故事里，爸妈体罚孩子，结果孩子报警把自己爸妈抓起来了。这算是告密吗，可耻吗？\n\n这两个例子或有意或无意地，忽略了告密和告发的区别。告密的内容应该是一个秘密，一个权力通过正常手段无法取得的证据。这也就意味着，在背叛之前有个过程，就是信任的建立，而且这种信任建立得要有道理。要么是两人有足够的私交，要么是事先明确进行约定，要么社会公德里默认的应当互信的情况，比如说，即便是陌生人，也要相信走在路上擦肩而过之际，对方不会从怀里取出一把匕首来捅你一刀。上面的例子里，报警者跟闯红灯的傻逼不熟，爸妈也只对孩子的抗揍能力有“信任”，所以并不存在背叛，只能算是告发。\n\n那么问题转换一下，告发还是一种为人不齿的行径吗？\n\n之所以要谈这个，是因为最近某选秀节目里，有些选手的粉丝看不惯另一位选手，向某部门举报该选手不符合社会主义核心价值观。霍老爷的公众号发布了一篇《xxxxx：世道变坏是从年轻人用36计开始的》，其中有这么一段话：“无论是权力任性导致的运动式治理也好，虚伪假大空的宣传也好，都必须基于‘告密’的群众基础才能存在，若是没有告密，至少，那些政治运动，只是统治者的一厢情愿，而一旦有了告密者，势必像病毒一样蔓延。”\n\n我们对告密的鄙视，其正义性的一个重要来源是，告密是极权主义控制人的一个重要工具。电影《窃听风暴》里，东德的国家安全部门通过威逼利诱，迫使文化界人士、社会活动积极分子周边的亲人朋友出卖隐私谈话内容，以此作为罪证抓捕审判。具体情节虽经过艺术加工，但这段历史是真实地发生过的。但是就在同一部电影里，当局除了发展眼线，还通过化妆成工人，在维修房屋的时候布设窃听装置来搜集情报。缺少了告密这一个工具，权力就控制不了人了吗？这种对告密的鄙视，和在行贿受贿问题里只谈行贿者是社会的蛀虫，在性工作者和嫖客之间只羞辱前者一样，算是老太太挑柿子——专捡软的捏。\n\n和朋友讨论这个问题的时候，他觉得告发是否应该，主取决于是否符合道德规范，是否出于当事人自己的正义感。然而一则动机实在难以揣测，再者道德规范的流动性又太强，既可以束缚在法律的框架内，又可以附身在狂热的意识形态甚至是宗教之上。宗教改革者胡格诺被教廷宣判火刑的时候，行刑的修女慈眉善目，添柴的动作谦恭优雅，引得绑在柱子上的胡格诺本人都感叹那“神圣的单纯”。威权既然能把触手伸向的素有信任的朋友伙伴之间，想要修改甚至扭转道德，简直轻而易举。\n\n所以，当权力已经膨胀到能吸引人自发检举的时候，还是双手合十地祈祷吧，毕竟除此之外，我们已经没什么能做的事情了。\n"}]],["md",[{"slug":"switched-domain-name","filename":"2024-09-30-switched-domain-name.md","date":"2024-09-30","title":"通知：本站网址変更","layout":"post","keywords":["md","html"],"excerpt":"未来可能无法自动跳转，请浏览器收藏和 RSS 订阅的读者更新网址。","content":"\n2024 年 9 月 30 日起，本站网址\n- 从 `https://mountaye.github.io/blog/` \n- 变更为 `https://blog.mountaye.com`.\n\n因为网站仍然架设在 GitHub Pages 上，所以旧网址可以自动跳转到新网址。\n\n但是下一步计划把网站迁移到 Cloudflare Pages 上，因为 Next.js 项目不同构建之间文件差异过多，不适合作为 git 仓库的内容进行托管。\n\n所以无法保证将来旧网址依然可以自动跳转。请使用浏览器收藏功能的读者更新收藏夹，使用 RSS 订阅功能的读者更新 RSS 源。\n\n根据初步计划，过渡期将持续到 2024 年 12 月 31 日。\n"},{"slug":"static-blog-with-nextjs-tailwindcss-shadcn","filename":"2024-09-17-static-blog-with-nextjs-tailwindcss-shadcn.md","date":"2024-09-17","title":".js | 博客改用 Next.js + TailwindCSS + Shadcn.UI","layout":"post","keywords":["md","js"],"excerpt":"我的博客本来是用 Jekyll 生成的静态网站，如今改用 Next.js + TailwindCSS + Shadcn.UI","content":"\n## 技术选型\n\n### 不再选择静态网站生成工具\n\n我的博客本来是用 Jekyll 生成的静态网站，因为网站架在 GitHub Pages 上，而 Jekyll 是 GitHub Pages 默认的构建工具。但是 Jekyll 的核心开发者年事已高，有的甚至已经去世，所以我感觉这个项目未来的活力堪忧。\n\nJekyll 是用 Ruby on Rails 写成的，其他编程语言，和 Jekyll 功能类似的工具有：\n\n- Python 的 Pelican\n- JavaScript 的 Hexo\n\n但是我还想做些更复杂的事情，预计需要靠服务端来实现。所以与其花时间移植到一个和 Jekyll 功能类似的工具，不如直接一步到位，学习一个全栈框架。\n\n### Next.js\n\n虽然不同的编程语言也都有自己的全栈框架，比如 Python 有 Django，但是既然浏览器主要支持 JavaScript，所以索性前后端都用 JS 比较方便，而且这样想的人很多，社区规模使得遇到问题更容易找到答案（这个优势在生成式语言模型的时代似乎没那么重要了）。\n\nJavaScript 语言之下，也存在至少 React 和 Vue 两大阵营。之所以选择 Next.js 这样一个基于 React.js 的框架，主要是路径依赖，很久很久以前学过[赫尔辛基大学的全栈公开课](https://fullstackopen.com/zh/)，那里教的就是 React。\n\n对于 Next.js 本身，Youtube@Fireship 有一个很简洁的介绍：[https://www.youtube.com/watch?v=Sklc_fQBmcs](https://www.youtube.com/watch?v=Sklc_fQBmcs)\n\n官网提供的教程在这里：[https://nextjs.org/learn](https://nextjs.org/learn)，和我学习的时候已经不一样了，那时候只有 page router，没有 app router。开发这一框架的 Vercel 公司也提供 Next.js 的云服务。但是鉴于其有过把用户引诱到 app router 架构然后给自家服务提价的黑历史，所以短期内不打算学和用 app router.\n\n### TailwindCSS\n\n[https://tailwindcss.com/](https://tailwindcss.com/)\n\n早就学过了，但是一直没有机会用。这次本来也可以不用的，直到决定使用 Shadcn.ui 组件库，因为 TailwindCSS 是它的一个依赖项。既然已经安装了，那不用白不用。\n\n### Shadcn.UI 而非 HeadlessUI\n\nhttps://ui.shadcn.com/\n\n一套组件库，也就是网页中经常出现的功能单元。这个库相对于竞品的最大优势，是允许直接复制粘贴代码而不用安装，用多少抄多少～\n\nTailwindCSS 自家也有一个组件库 [HeadlessUI](https://headlessui.com/)，但是主要是 `\u003cform/\u003e` 表单及其成员，侧重于向后端传数据的 HTML 元素的封装。而 Shacn 有很多炫酷的交互方面的组件，突出一个现成且好看。\n\n### 没用 Figma\n\n有用的功能都收钱，免费的功能不如直接 `next dev` 实时预览。\n\n## ~~踩过的坑~~ 学到的经验\n\n### 组件化作为一种思路\n\n正常的网页，HTML 负责内容，CSS 负责装饰，JavaScript 负责交互。这种分工，专业上叫做解耦。\n\n工程实践表明，这种解耦方式非常反人类。用户看到和使用的网页，是以功能上的相似、空间上的相邻为组织的，而要对其修改时，则需要到不同源代码的不同位置去；反之，某处代码的改动，无意中可能对远处的另一部分视觉效果和功能造成破坏。\n\n于是较新的全栈框架，其解耦的方式都是以组件为单位的，内容、样式、交互逻辑都写在一起。\n\n### JavaScript/JSX 语法中的 `{}`\n\nJSX 是对 JavaScript 语法的扩展，添加了类似于 HTML 标记的写法 `\u003cMyComponent\u003e\u003c/MyComponent\u003e`，用来表示 react 组件。\n\n在 jsx 语言的内部写 Javascript 时，需要将 JS 外面包裹一层 `{}`\n\nJavaScript 里类似 Python f-string 的结构写作``${}``，名叫 template literal: ``this is var: ${var}``\n\n——以上规则结合起来，会产生让初学者迷惑的现象：\n\n- `\u003cMyComponent className=’dark’/\u003e`: 一个正常的类名，直接用引号\n- `\u003cMyComponent className={dark}/\u003e:` 类名是一个 JS 字符串变量\n- 类名的一部分根据一个变量取值:\n  ```javascript\n  \u003cMyComponent className={`bg-${dark}`}/\u003e\n  ```\n\n### JavaScript 箭头函数中的 `{}` 和 `()`\n\nJavaScript 的箭头函数类似于 python 的 lambda 纯函数，但是有不同。\n\n`(var) =\u003e (expression(var))` 相当于 Python 中的 `lambda x: expression(x)` \n\n但是箭头函数的右侧可以是 `{}` 包裹的若干表达式，此时需要显式 return：\n\n```jsx\n(var) =\u003e {\n    expression1(var);\n    expression2(var);\n    return expression3(var);\n}\n```\n\nPython 的 lambda 必须是单一表达式的纯函数，不允许上面第二种写法。\n\n### 对象解包中的 `{}`\n\n当有一个包含若干键值对的对象时\n\n```jsx\nobj = {\n\t\tk1: \"value1\",\n\t\tk2: \"value2\",\n\t\tk3: \"value3\",\n\t\t...\n}\n```\n\n可以用 `const { k2 } = obj;` 的方式拿到 `obj[’k2’]` 的值，赋给 `k2`。\n\n结合上一节，\n\n- `(k1,k2)=\u003e(k1+k2);` 是一个两个自变量的函数；\n- `({k1,k2})=\u003e(k1+k2);` 是以一个 Object 为自变量的函数，这个 Object 的名字无所谓，也不确定一共有多少个属性，但属性中至少包含 `k1` 和 `k2`.\n\n### TailwindCSS 的 arbitrary value、JavaScript 的模板字符串、Shadcn 中的 `cn()` 函数\n\nTailwindCSS 的很多属性都允许在方括号中使用任意值，比如背景色 `bg-[#a4b4c4]`\n\n本以为可以直接在 `className` 里面用 template literal `\u003cdiv className={`bg-[${myColor}]`}\u003e`，但是并不总是生效。\n\n这个“并不总是”是个大坑，一开始在开发模式用得好好的，结果某次刷新页面之后就挂了，简直莫名其妙。\n\n好在 Shadcn 提供了一个 [`cn()` 函数](https://github.com/shadcn-ui/ui/blob/main/apps/www/lib/utils.ts)，接受一个或多个 TailwindCSS 类名字符串作为输入，就可以正常使用 template literal 了，例如 `\u003cdiv className={cn(\"block\",\"border-0\",`bg-[${myColor}]`)}\u003e\u003c/div\u003e`\n\n### HTML + CSS 布局\n\n两类套路：\n\n1. 传统的 `display`, `position`, `float` 属性；\n2. Flex 和 Grid 布局，看阮一峰先生的博客里的教程就挺方便：[Flex](https://www.ruanyifeng.com/blog/2015/07/flex-grammar.html), [Grid](https://www.ruanyifeng.com/blog/2019/03/grid-layout-tutorial.html).\n\n理论上后者新一些，消耗的脑力也更少一些，应该是更好的选择。\n\n但是前者也有一些优势场景。比如现在整个博客页面的上边栏和剩下的部分就是一个上下结构的 flex 布局，这导致屏幕最右侧的滚动条其实是页面一部分的，而不是整个页面的的滚动条。在 iOS 的 Safari 浏览器下，会导致网址栏不能自动隐藏，浪费很大一片屏幕空间。之后可能会换回传统功夫。\n\n而像是目录，文章跳转开头和评论区的按钮等等，需要在页面滚动时相对屏幕静止的元素，就不得不用 position，而且为了锚定在正文上，还需要嵌套好几层。\n\n按照传统功夫——\n\n- `display`可选的取值有 4 个: block | inline | inline-block | none\n    - block: 竖排，哪怕同一行内仍有空间容纳 html 中的下一个元素。\n    - inline: 横排，像文字内容一样，没有盒模型\n    - inline-block: 横排，但是有盒模型\n    - none: 不显示，和 `visibility:hidden` 的区别是，后者依然占有显示时的空间。\n- `position`可选的取值有 5 个: static | relative | fixed | absolute | sticky\n    - static: 默认值，按照 html 文件的顺序排列。\n    - relative: 相对于 static 默认值进行偏移，偏移量由 `top`, `right`, `bottom`, `left` 四个性质决定。所谓 `left: 50px` 的意思是左侧 margin 外多出 50 像素的空间，实际是向右偏移的效果。\n    - fixed: 相对于**视窗**的位置固定，位置由 `top`, `right`, `bottom`, `left` 四个性质决定。`left: 50px` 的意思是该元素的左侧 border **外沿**距离窗口左边 50 像素。\n    - absolute: 相对于最近一层父元素的位置固定，位置由 `top`, `right`, `bottom`, `left` 四个性质决定。`left: 50px` 的意思是该元素的左侧 border **外沿**距离父元素左侧**内沿** 50 像素。设计的时候需要考虑 border 宽度。\n    - sticky: 网页加载时按照 html 文件的顺序排列，直到网页滑动到某一位置，之后该元素固定在视窗，就像 fixed 一样，行为改变的位置由 `top`, `right`, `bottom`, `left` 四个性质决定。\n- `float`:  none | left | right | inherit\n    - none: 默认值，按照 html 文件的顺序排列。\n    - left: 保持在父元素左侧，其他元素环绕之。\n    - right: 保持在父元素右侧，其他元素环绕之。\n    - inherit: 和父元素的 float 的取值一致。\n\n### Unified.js 将 Markdown 文档转换为基于 JSX 的 HTML\n\nJekyll 等静态网站生成器的核心功能，就是把 markdown 文档翻译成 html 网页文档。在 Next.js 框架下，这一工作由以 unified.js 为基础的一群第三方库来完成。\n\n最早看到这个框架是在 DIYGOD 的博文《[如何优雅编译一个 Markdown 文档](https://diygod.cc/unified-markdown)》里，但是直接抄他在 xlog 里面的代码的话，在 next.js 之下好像会报错。所以又去官网仔细读了一下文档，现在可以说是略懂。\n\n这个话题本身值得专门写一篇文章，所以不在这里展开了。\n\n### Giscus 评论区切换黑夜模式\n\n自己写的组件的亮暗切换，是通过在 `\u003chtml/\u003e` 元素添加和删除 `dark` 类，然后搭配 TailwindCSS 的 `dark:` 来实现的。\n\nGiscus 官方支持切换黑夜模式：[https://github.com/giscus/giscus/blob/main/ADVANCED-USAGE.md#parent-to-giscus-message-events](https://github.com/giscus/giscus/blob/main/ADVANCED-USAGE.md#parent-to-giscus-message-events)。这套方法的关键，在于服务端返回的 iframe 有一个名为 `giscus-frame` ****的类。\n\nGiscus 为 react 提供了一套组件可以直接使用，但是在这套组件里面并没有这个类。\n\n所以只能弃用官方的组件，自己用 `useEffect` 模拟官网的 `\u003cscript/\u003e`\n\n```jsx\nexport function MyGiscus() {\n  useEffect(\n    () =\u003e {\n      const onPageLoad = () =\u003e {\n        console.log(\"\u003cMyGiscus/\u003e: activated on page load.\")\n        // START real business\n        const script = document.createElement('script');\n        script.src = \"https://giscus.app/client.js\";\n        script.setAttribute('data-repo',              '');\n        script.setAttribute('data-repo-id',           '');\n        script.setAttribute('data-category',          '');\n        script.setAttribute('data-category-id',       '');\n        script.setAttribute('data-mapping',           '');\n        script.setAttribute('data-strict',            '');\n        script.setAttribute('data-reactions-enabled', '');\n        script.setAttribute('data-emit-metadata',     '');\n        script.setAttribute('data-input-position',    '');\n        script.setAttribute('data-theme',             '');\n        script.setAttribute('data-lang',              '');\n        script.crossOrigin = 'anonymous';\n        script.async = true;\n        document.getElementById(\"comments\").appendChild(script);\n        // END real business\n      };\n      // Check if the page is already loaded\n      if (document.readyState==='complete') {\n        onPageLoad();\n      } else {\n        // Add event listener for page load\n        window.addEventListener('load',onPageLoad);\n        // Cleanup the event listener on component unmount\n        return () =\u003e { window.removeEventListener('load',onPageLoad); }\n      };\n    },\n    []\n  );\n  return (\u003cdiv id='comments'\u003e\u003c/div\u003e);\n}\n```\n\n### Next.js 的构建参数部分\n\n要让 next.js 构建静态网站，需要在 next.config.js 中写：\n\n```jsx\nmodule.exports = {\n  basePath: '/blog',\n  output: 'export',\n  generateBuildId: async () =\u003e \"buildID\",\n  // i18n: {\n  //   locales: ['zh-CN', 'en'],\n  //   defaultLocale: 'zh-CN',\n  //   localeDetection: false,\n  // },\n}\n```\n\n`basePath` 是因为博客的 GitHub 仓库 blog 不是默认的个人网站仓库；需要注意的是，next.js 自己的 Link 组件的 `href` 参数不需要包含这个值，但是图片等等的 `src` 参数需要。\n\n`generateBuildId` 函数的返回值是随便写的，不设定的话会导致输出里的 `_next/` 文件夹里有很多哈希值为名的文件夹，在 git 下会被当成不同的 blob 一直留在项目里。\n\n`i18n` 参数被注释掉了，因为静态生成的 next.js 项目不支持自动 i18n.\n\n### RSS 源和 sitemap\n\nRSS 由 feed 这个 npm 包来构建；\n\nsitemap 则是在 ChatGPT 的帮助下手写字符串。\n\n写好的字符串，通过在主页或者历史归档页面的 `getStaticProps()` 函数，写入 next.js 项目的 `public/` 文件夹。\n\n## 还没解决的问题\n\n### 手机端搜索框的汉字输入问题\n\n页面顶端的搜索框在手机触摸屏上，用汉字输入法输入关键词之后，直接按回车键，会导致已经输入的汉字被当成拼音，传递给搜索引擎。\n\n暂时的办法是在输入汉字之后，按回车键之前按一下空格。\n\n### Shadcn.Drawer 组件的上游代码报错；Google Ads\n\n新博客把谷歌广告撤了。赚不到多少钱不说，它还会往网页里动态添加元素，破坏原来的排版。\n\n话虽如此，每篇文章的右下角还是有个[要饭的图标](https://lucide.dev/icons/hand-heart)，计划用 shadcn 的 Drawer，放赞赏二维码，或者交换来的友站链接。\n\n但是目前 Drawer 的上游代码会报错，看起来作者已经在修复了，等更新。\n\n### Google Analytics\n\nnext.js 提供了 Google analytics: [https://nextjs.org/docs/pages/building-your-application/optimizing/third-party-libraries](https://nextjs.org/docs/pages/building-your-application/optimizing/third-party-libraries)\n\n但是加入之后没有反应，google 后台看不到，数据一落千丈。\n\n据说把相应的代码放到 `pages/_app.js` 可以解决问题，还没试。"},{"slug":"Python-test-notes","filename":"2023-10-16-Python-test-notes.md","date":"2023-10-16","title":".py | Python 测试笔记","layout":"post","keywords":["md","py"],"excerpt":"如题","content":"\n## 一些名词\n\n### 调试和测试\n\n调试一般是由代码的作者进行，用于自行检查程序运行过程，是否存在思路和实现不匹配的错误，调试的代码一般和程序主体写在一起，主要包括错误处理和日志记录。简单的可以用 print 和 assert，复杂的程序可以用 logging。\n\n测试一般由第三方进行，测试代码和程序代码分离，写测试的人甚至不需要理解程序的具体工作原理，只关注给定的输入能否得到程序宣称的输出。\n\n### 单元测试\n\n对一个模块、一个函数或者一个类来进行正确性检验的测试工作。\n\n检验的方法是写一堆测试用例，把测试员拍脑袋想的输入交给相应的模块，看模块的输出是否正确；以及不合理的输入是否被程序识别，抛出异常。\n\n单元测试全通过了不代表程序整体一定就没错误，但是单元测试通不过的程序一定有问题。\n\n### 文档测试\n\n文档是对代码的功能介绍，其中不免要举例子，给出实例代码和相应的输出，这个过程很像是单元测试，只不过是纯嘴炮。\n\n文档测试就是自动寻找文档中的示例代码，运行之后，和文档中的结果进行比对。\n\n### 集成测试\n\n集成测试模拟用户的行为，测试各个模块之间的配合，测试结果应该保证程序可以在生产环境中工作。\n\n## Python 中的测试\n\n### 文件结构\n\n上一篇文章里面提到了 src-layout，在 setuptools 的官方文档里提供了[一篇博客文章](https://blog.ionelmc.ro/2014/05/25/python-packaging/#the-structure)，里面提到，这种文件结构的一个优点就是，测试代码的文件夹一般和 `src/` 而不是 package 平级，这就导致运行测试的时候只能先（在虚拟环境里）安装待测试的包，而不会无意中出现测试的代码和用户下载到的内容不同的问题。\n\n```\n\u003cproject_name\u003e\n├── LICENSE\n├── pyproject.toml\n├── README.md\n├── src/\n│   └── \u003cpackage_name\u003e/\n│       ├── __init__.py\n│       └── example.py\n└── tests/\n```\n\n### 单元测试·`unittest`\n\n`unittest` 是 Python 自带的单元测试库。\n\n测试脚本的内容基本如下：\n\n```\nimport unittest\n\nclass TestName1(unittest.TestCase):\n\n    def test_sum(self):\n        self.assertEqual(sum([1, 2, 3]), 6, \"Should be 6\")\n\n    def test_sum_tuple(self):\n        self.assertEqual(sum((1, 2, 2)), 6, \"Should be 6\")\n\nclass TestName2(unittest.TestCase):\n\n    def test_sum(self):\n        self.assertEqual(sum([1, 2, 3]), 6, \"Should be 6\")\n\n    def test_sum_tuple(self):\n        self.assertEqual(sum((1, 2, 2)), 6, \"Should be 6\")\n\nif __name__ == '__main__':\n    unittest.main()\n```\n\n- 测试用例包装在一个 class 里面，这个 class 继承自 `unittest.TestCase`\n- 所有测试方法名字以 “test” 开头，能测试的性质有限，都是类自带的方法，以 `self.` 开头。支持的方法见下表，感觉 `[assertTrue(x)](https://docs.python.org/3/library/unittest.html#unittest.TestCase.assertTrue)` 和 `assertRaises(Exception)` 包打一切\n- 作为 `'__main__'` 运行，运行的是自带的函数 `unittest.main()`。\n- 运行时需要 python 指明脚本的文件名。安装了 nose2 这个库的话，可以直接运行 `python -m nose2`, 它会自动寻找所有的测试依次运行。（后面发现 `unittest` 好像也有自动发现功能）\n\n| Method | Checks that | New in |\n| --- | --- | --- |\n|         [unittest.TestCase.assertEqual](https://docs.python.org/3/library/unittest.html#unittest.TestCase.assertEqual) | a == b |  |\n|      [unittest.TestCase.assertNotEqual](https://docs.python.org/3/library/unittest.html#unittest.TestCase.assertNotEqual) | a != b |  |\n|          [unittest.TestCase.assertTrue](https://docs.python.org/3/library/unittest.html#unittest.TestCase.assertTrue) | bool(x) is True |  |\n|         [unittest.TestCase.assertFalse](https://docs.python.org/3/library/unittest.html#unittest.TestCase.assertFalse) | bool(x) is False |  |\n|            [unittest.TestCase.assertIs](https://docs.python.org/3/library/unittest.html#unittest.TestCase.assertIs) | a is b | 3.1 |\n|         [unittest.TestCase.assertIsNot](https://docs.python.org/3/library/unittest.html#unittest.TestCase.assertIsNot) | a is not b | 3.1 |\n|        [unittest.TestCase.assertIsNone](https://docs.python.org/3/library/unittest.html#unittest.TestCase.assertIsNone) | x is None | 3.1 |\n|     [unittest.TestCase.assertIsNotNone](https://docs.python.org/3/library/unittest.html#unittest.TestCase.assertIsNotNone) | x is not None | 3.1 |\n|            [unittest.TestCase.assertIn](https://docs.python.org/3/library/unittest.html#unittest.TestCase.assertIn) | a in b | 3.1 |\n|         [unittest.TestCase.assertNotIn](https://docs.python.org/3/library/unittest.html#unittest.TestCase.assertNotIn) | a not in b | 3.1 |\n|    [unittest.TestCase.assertIsInstance](https://docs.python.org/3/library/unittest.html#unittest.TestCase.assertIsInstance) | isinstance(a, b) | 3.2 |\n| [unittest.TestCase.assertNotIsInstance](https://docs.python.org/3/library/unittest.html#unittest.TestCase.assertNotIsInstance) | not isinstance(a, b) | 3.2 |\n\n### 文档测试\n\n[https://www.liaoxuefeng.com/wiki/1016959663602400/1017605739507840](https://www.liaoxuefeng.com/wiki/1016959663602400/1017605739507840)\n\nPython 的文档测试用的是 `doctest` 库，写法如下：\n\n```python\nclass Dict(dict):\n    '''\n    Simple dict but also support access as x.y style.\n\n    \u003e\u003e\u003e d1 = Dict()\n    \u003e\u003e\u003e d1['x'] = 100\n    \u003e\u003e\u003e d1.x\n    100\n    \u003e\u003e\u003e d1.y = 200\n    \u003e\u003e\u003e d1['y']\n    200\n    \u003e\u003e\u003e d2 = Dict(a=1, b=2, c='3')\n    \u003e\u003e\u003e d2.c\n    '3'\n    \u003e\u003e\u003e d2['empty']\n    Traceback (most recent call last):\n        ...\n    KeyError: 'empty'\n    \u003e\u003e\u003e d2.empty\n    Traceback (most recent call last):\n        ...\n    AttributeError: 'Dict' object has no attribute 'empty'\n    '''\n    def __init__(self, **kw):\n        super(Dict, self).__init__(**kw)\n\n    def __getattr__(self, key):\n        try:\n            return self[key]\n        except KeyError:\n            raise AttributeError(r\"'Dict' object has no attribute '%s'\" % key)\n\n    def __setattr__(self, key, value):\n        self[key] = value\n\nif __name__=='__main__':\n    import doctest\n    doctest.testmod()\n```\n\n在保持 docstring 缩进的前提下，`\u003e\u003e\u003e`  开头的注释会被当作测试运行，紧随其后的行将作为对比基准。只有当预期报错的时候，可以用 `...` 省略中间的报错信息。\n\n### 集成测试\n\n也可以用 `unittest` 完成。\n\n和单元测试的区别在于，一般来说需要构建测试数据集等等。这需要重写 `unittest.TestCase.setup()`\n\n```python\nclass TestComplexData(unittest.TestCase):\n    def setUp(self):\n        # load test data\n        self.app = App(database='fixtures/test_complex.json')\n\n    def test_customer_count(self):\n        self.assertEqual(len(self.app.customers), 10000)\n\n    def test_existence_of_customer(self):\n        customer = self.app.get_customer(id=9999)\n        self.assertEqual(customer.name, u\"バナナ\")\n        self.assertEqual(customer.address, \"10 Red Road, Akihabara, Tokyo\")\n\nif __name__ == '__main__':\n    unittest.main()\n```"},{"slug":"python-packaging","filename":"2023-10-13-python-packaging.md","date":"2023-10-13","title":".py | 让自己的代码可以被别人使用","layout":"post","keywords":["md","py"],"excerpt":"这里所说的“别人”，也包括6个月之后，已经不记得当初如何写出这段代码的自己。","content":"\n\u003e 这里所说的“别人”，也包括6个月之后，已经不记得当初如何写出这段代码的自己。\n\u003e \n\n很久以前写过一篇《**[import 引用现成的代码](https://mountaye.github.io/blog/articles/python-import-script-module-package)**》讲如何使用别人的代码，最后讲到一个模块的 `setup.py` 文件就没再往下写，这次继续。\n\n- [https://packaging.python.org/en/latest/tutorials/packaging-projects/](https://packaging.python.org/en/latest/tutorials/packaging-projects/)\n- [https://setuptools.pypa.io/en/latest/userguide/quickstart.html](https://setuptools.pypa.io/en/latest/userguide/quickstart.html)\n\n## 基本流程\n\n- 写代码，且让代码项目文件的结构符合一定的要求（见下一节）\n- 根据项目的文件结构，填写 `pyproject.toml`、`setup.cfg` **或** `setup.py` **文件**\n- 安装 build 这个库，然后运行 `python -m build`，产生 `dist/` 文件夹及下面的文件。（可选）将项目上传到 PyPi 或者 Conda\n\n## 项目文件结构\n\n常用的文件结构有两种：src-layout 和 flat-layout，另外一些小项目只有一个 python 文件。\n\n### src-layout\n\n在 src-layout 里，写有 package 源代码的文件夹上层还套了一个文件夹，这个文件夹习惯上命名为 src，当然也可以是别的。`pyproject.toml` 和 `src/` 文件夹同级。\n\n```\n\u003cproject_name\u003e\n├── LICENSE\n├── pyproject.toml\n├── README.md\n├── src/\n│   └── \u003cpackage_name\u003e/\n│       ├── __init__.py\n│       └── example.py\n└── tests/\n```\n\n### flat-layout\n\nflat-layout 指的是写有 package 源代码的文件夹直接作为开发项目的第一级子文件夹，和 `pyproject.toml` 处于同一级。\n\n这种结构比较古老，不太推荐\n\n```\n\u003cproject_name\u003e\n├── pyproject.toml  # and/or setup.cfg/setup.py (depending on the configuration method)\n├── \u003cpackage_name\u003e\n|   ├── __init__.py\n|   └── ... (other Python files)\n├── test\n|   └── ... (test files)\n├── # README.rst or README.md (a nice description of your package)\n└── # LICENCE (properly chosen license information, e.g. MIT, BSD-3, GPL-3, MPL-2, etc...)\n```\n\n### 单文件项目\n\n可以看作是 flat-layout 的一种特殊情况\n\n```\n\u003cproject_name\u003e\n├── pyproject.toml  # and/or setup.cfg/setup.py (depending on the configuration method)\n├── \u003cmy_module\u003e.py\n├── # README.rst or README.md (a nice description of your package)\n└── # LICENCE (properly chosen license information, e.g. MIT, BSD-3, GPL-3, MPL-2, etc...)\n```\n\n## 填写 `pyproject.toml`、`setup.cfg` **或** `setup.py` **文件**\n\n要想让构建程序把我们的代码打包成安装包，标题中的三个文件至少有一个要出现在 project 的根目录。\n\n文件中要按照各自拓展名对应的语法，填写项目的有关信息，绝大多数可以顾名思义。\n\n各参数的取值和代码文件结构相关，参数主要包括 `name`, `packages`, `package_dir`。如果文件结构完全满足上一节的结构，那么 `setuptools.find_packages()` 的[自动发现机制](https://setuptools.pypa.io/en/latest/userguide/package_discovery.html#automatic-discovery)就够用了。\n\n### `name`\n\n这是一个必填项。\n\n注意：上一节的文件结构中，有两个名字 `\u003cproject_name\u003e` 和 `\u003cpackage_name\u003e` ——\n\n`\u003cproject_name\u003e` 是整个开发项目的名字，如果用了类似 git 的版本控制的话，这个名字就是你的 repository 的名字。\n\n`\u003cpackage_name\u003e` 比较复杂，它可以是，但不一定是你在其他代码中 `import __` 的名字，import 的名字由 `pyproject.toml` / `setup.cfg` **/** `setup.py` 里面的 `name` 参数指定。不能有连字符，只能用下划线。\n\n如果你的 `name` 参数和 `\u003cpackage_name\u003e` 不同，还需要填写 `package_dir` 参数，\n\n此外还有第三个名字，就是 `pip install __` 时候的名字，上传到 PyPI 的时候填写，可以带有连字符，比如 scikit-image。\n\n### `packages`\n\n参数是一个 list，但是一般都使用 `setuptools.find_packages()` 的结果。\n\n该函数常用三个参数，都是可选的：\n\n- `where`: 一个路径，相对于 `setup.py`\n- `include`: 一个 list，元素是 glob patterns\n- `exclude`: 一个 list，元素是 glob patterns\n\n不指明任何参数 = 使用自动发现机制\n\n### `package_dir`\n\n参数是一个 dict，两种用法：\n\n- 标准的 src-layout，直接写 `{\"\": \"src/\"}`, 表示所有的代码都在这个文件夹里。\n- 当 python 模块的结构和代码的文件结构不同的时候，用这个 dict 指明 模块-文件夹 之间的关系。\n\n文件的路径相对于 `setup.py` 而言\n\n### `py_modules`\n\n参数是一个文件路径的列表，几乎专为单文件结构而存在。\n\n## 打包和上传\n\n安装 build 工具：`python3 -m pip install --upgrade build`\n\n运行 build：`python3 -m build`\n\n如此会生成一个 `dist/` 文件夹，里面包含打包的结果。\n\n要想让自己的程序可以被别人用 `pip install` 的方式安装，需要将打包成果上传到 PyPI，方法在[这里](https://packaging.python.org/en/latest/tutorials/packaging-projects/#uploading-the-distribution-archives)。\n\n## 安装\n\n### 静态安装\n\n已经上传到 PyPI 的包，可以直接用 `pip install \u003cpackage\u003e` 安装，这种方法叫做静态安装\n\n### 动态安装\n\n还在开发过程中的包，可以在 `setup.py` 所在的位置，运行 `pip install -e .` 这种安装方法叫做动态安装，因为代码的修改可以实时反映在引用的项目中。\n\n## 思考题\n\n上篇文章提到的一个[数据分析项目](https://gist.github.com/ericmjl/27e50331f24db3e8f957d1fe7bbbe510)，其文件结构是这样的（我稍微改动了一下）：\n\n```\n/path/to/project/directory/\n|-- notebooks/\n    |-- 01-first-logical-notebook.ipynb\n    |-- 02-second-logical-notebook.ipynb\n    |-- prototype-notebook.ipynb\n    |-- archive/\n\t      |-- no-longer-useful.ipynb\n|-- src/\n    |-- projectname/\n\t      |-- __init__.py\n\t      |-- config.py\n\t      |-- data.py\n\t      |-- utils.py\n    |-- setup.py\n|-- README.md\n|-- data/\n    |-- raw/\n    |-- processed/\n    |-- cleaned/\n|-- scripts/\n    |-- script1.py\n    |-- script2.py\n    |-- archive/\n        |-- no-longer-useful.py\n|-- environment.yml\n```\n\n问：\n\n1. 这是一个 flat-layout 还是 src-layout？\n2. setup.py 应该怎么写？执行动态安装时的 `pwd` 结果是什么？\n"},{"slug":"notes-on-TailwindCSS","filename":"2023-10-11-notes-on-TailwindCSS.md","date":"2023-10-11","title":".css | TailwindCSS 笔记","layout":"post","keywords":["md","js"],"excerpt":"一直听说“全栈项目 = Next.js + TailwindCSS + HeadlessUI”，但是 TailwindCSS 到底是啥，之前一直妹整明白","content":"\n\u003e 一直听说“全栈项目 = Next.js + TailwindCSS + HeadlessUI”\n但是 TailwindCSS 到底是啥，之前一直妹整明白\n\u003e \n\n## 思路：utility-first\n\n\u003e [https://tailwindcss.com/docs/utility-first](https://tailwindcss.com/docs/utility-first)\n\u003e \n\n传统设计需要根据 html 中的结构，在 CSS 中给相应的元素/class/id 定义所需要的所有样式 style。\n\n问题很明显：\n\n- 最低效的情况下，每个 `\u003cdiv/\u003e` 都要定义一个 class。\n- 每个定义里包含若干不同的性质，背景颜色、字体、边框样式等等都挤在一个大括号里，耦合过强。\n\nTailwindCSS 的思路名叫 utility-first, 预先定义一批“性质-取值”的组合，每个组合给出一个有规律命名的类。使用时，一个 `\u003cdiv/\u003e` 后面声明几个甚至几十个不同的 class。缺点就是不灵活了，每个性质只搭配有限几种取值，且类的数量很多。好处是——\n\n- 不用绞尽脑汁给类取名字\n- CSS 不会再变大了（也可以说已经大得不能再大了）\n- 修改视觉效果时更换一个类，而不是修改类的定义，也就不用担心对类的修改在自己不记得的地方生效。\n\n与之相对的另一种思路，是直接用 html 元素的 style 属性，或者用 module.css 让样式只对某一 component 生效。TailwindCSS 派对这种方法的批评是：\n\n- 每个取值都是设计者拍脑袋想出来的，一个项目要拍太多次脑袋，容易风格不统一。\n- 难以做 responsive design （真的吗？很怀疑）\n- 难以处理鼠标悬浮、聚焦等等状态（这玩意应该由 CSS 处理吗？）\n\nutility-first 在维护性方面收到批评的一点是，很多地方要不断重用相同的组合，少了一点封装和抽象。TailwindCSS 对此的辩护是，可以抽象出 components 和 partials（见下节），或者使用编辑器的多光标功能。（绷……）\n\n## 技术细节\n\n- 样式重用\n- 状态，比如鼠标悬浮、聚焦\n- Responsive design\n- 夜间模式\n- 添加自定义样式\n- 函数和 directives\n\n### 状态，比如鼠标悬浮、聚焦\n\n在正常的类名字之前添加 `\u003c状态\u003e:` 标记，用来指明在相应状态时的样式。这些状态可以叠加，之间用 `:` 分隔。比如 `\u003cbutton class=\"hover:bg-sky-700\"\u003e`\n\n可以标记的状态：[https://tailwindcss.com/docs/hover-focus-and-other-states#appendix](https://tailwindcss.com/docs/hover-focus-and-other-states#appendix)\n\n- Pseudo-classes\n    - 举例： [https://tailwindcss.com/docs/hover-focus-and-other-states#pseudo-class-reference](https://tailwindcss.com/docs/hover-focus-and-other-states#pseudo-class-reference)\n        - `hover:`, `focus:`, `active:`\n        - `first:`, `last:`, `odd:`, `even:`\n        - `required:`, `invalid:`, `disabled:`: 主要用在 `\u003cform\u003e` 中\n    - 需要父元素的状态信息时，\n        - 如果因为嵌套，存在多个 group 时，可以给每一个父元素的类命名 `group/\u003cname\u003e`, 子元素的类名需要写在伪类的后面，有点反直觉 `group-hover/\u003cname\u003e:`\n        - 给父元素添加 `group` 的 class，然后给需要变化的子元素添加 `group-\u003cpseudo-class\u003e:` 前缀。如 `group-hover:`\n        - 当需要更细致的选择时，可以在子元素的 group 后面添加自定义内容，如 `group-[.is-published]:`, `group-[:nth-of-type(3)_\u0026]:`\n    - 需要姊妹元素的状态信息时：\n        - 给被跟踪的姊妹元素添加 `peer` class, 被跟踪的元素只能在跟踪元素的前面。\n        - 其余特性类比 group\n- [Pseudo-elements](https://tailwindcss.com/docs/hover-focus-and-other-states#pseudo-elements), like `::before`, `::after`, `::placeholder`, and `::selection`\n    - 写作 `before:` 等等，默认相当于 `before:content-['*']`\n    - 当想要调整 content 以外的性质时，需要指明 `before:block`, `before:absolute`, `before:-inset-1` 等等\n    - `placeholder:` 用于调整表格中代填内容的样式\n    - `file:` 上传文件按钮的样式\n    - `list:` 列表开头的\n    - `selection:` （鼠标）选中文字之后的样式\n    - `first-line:`, `first-letter:` 杂志常用的首行、首字母的特殊样式\n- [Media and feature queries](https://tailwindcss.com/docs/hover-focus-and-other-states#media-and-feature-queries), like responsive breakpoints, dark mode, and `prefers-reduced-motion`\n    - 结合响应式设计 responsive design 一节，使用 `md:`, `lg:` 等前缀\n    - `dark:` 黑夜模式\n    - `motion-reduce:` 用户选择屏蔽动画效果时的样式，`motion-safe:` 只有不屏蔽动画才会生效的样式\n    - `portrait`,`landscape` 屏幕朝向\n    - `print:` 打印时的样式\n    - `supports-[...]` 当浏览器支持某种特性时启动。也可在 `tailwind.config.js` 文件中设置 `theme.supports` 变量\n- [Attribute selectors](https://tailwindcss.com/docs/hover-focus-and-other-states#attribute-selectors), like `[dir=\"rtl\"]` and `[open]`\n    - `aria-*` modifier to conditionally style things based on [ARIA attributes](https://developer.mozilla.org/en-US/docs/Web/Accessibility/ARIA/Attributes).\n    - `data-[key=value]` data 参数的值\n    - `ltr:` \u0026 `rtl:` 从右往左书写的文字\n    - `open:` \u0026 `close:` 用于可以展开的元素\n    - 自定义选择符：用中括号包围，`\u0026` 开头选择元素，下划线表示空格，如 `[\u0026:nth-child(3)]:`, `[\u0026_p]:mt-4`, `[@supports(display:grid)]:grid`\n\n### Responsive design\n\n| Breakpoint prefix | Minimum width | CSS |\n| --- | --- | --- |\n| sm | 640px | @media (min-width: 640px) { ... } |\n| md | 768px | @media (min-width: 768px) { ... } |\n| lg | 1024px | @media (min-width: 1024px) { ... } |\n| xl | 1280px | @media (min-width: 1280px) { ... } |\n| 2xl | 1536px | @media (min-width: 1536px) { ... } |\n\n移动端优先的思路，所有尺寸限定的都是大于该宽度时的样式。\n\n要限定上限，要用 `max-\u003csize\u003e:` 比如 `md:max-xl:flex`\n\n要想自定义 breakpoints，可以看 [customizing breakpoints documentation](https://tailwindcss.com/docs/breakpoints).\n\n也可以单独设定 `min-[320px]:`, `max-[600px]:` 等等\n\n### 夜间模式\n\n默认使用操作系统的设定。\n\n要想手动设定，须在 tailwind.config.js 中加入\n\n```jsx\n/** @type {import('tailwindcss').Config} */\nmodule.exports = {\n  darkMode: 'class',\n  // ...\n}\n```\n\n然后含有 `class=’dark’` 的元素的子元素都时夜间模式的效果\n\n这个[链接](https://tailwindcss.com/docs/dark-mode#supporting-system-preference-and-manual-selection)包含了同时兼容系统设置和手动设置的做法\n\n### 样式重用\n\n- 编辑器的多光标功能：[https://code.visualstudio.com/docs/editor/codebasics#_multiple-selections-multicursor](https://code.visualstudio.com/docs/editor/codebasics#_multiple-selections-multicursor)\n- 标记语言的循环语法\n- react 等框架的 component 概念\n- `@apply` and `@layer` in the [Functions \u0026 Directives](https://tailwindcss.com/docs/functions-and-directives#layer) documentation.\n- 避免提前过度抽象\n\n### 添加自定义样式\n\n- 编辑 `tailwind.config.js`, 文档在此：[https://tailwindcss.com/docs/theme](https://tailwindcss.com/docs/theme)\n- [Arbitrary properties](https://tailwindcss.com/docs/adding-custom-styles#arbitrary-properties) 和 [arbitrary variants](https://tailwindcss.com/docs/adding-custom-styles#arbitrary-variants)\n- [Using CSS and @layer](https://tailwindcss.com/docs/adding-custom-styles#using-css-and-layer), 使用多个 CSS 文件时，需在 postcss.config.js 文件中添加 `plugins: {’postcss-import’:  {},}` 字段\n- [Writing plugins](https://tailwindcss.com/docs/adding-custom-styles#writing-plugins)\n\n### 函数和 directives\n\ndirectives 是 CSS 文件中的 `@` 开头的语句\n\n`@layer` 用来把一些需要打包的样式绑在一起，后面三个取值：base, components, utilities\n\n`@apply` 后面接 TailwindCSS 已经定义的类，表示把类的定义移植于此处。\n\n**[`@config`](https://tailwindcss.com/docs/functions-and-directives#config)** 指定所在 CSS 文件需要使用的 TailwindCSS 配置文件，放在 @import 语句后面\n\n```css\n@tailwind base;\n@tailwind components;\n@tailwind utilities;\n\n@layer base {\n  h1 {\n    @apply text-2xl;\n  }\n  h2 {\n    @apply text-xl;\n  }\n}\n\n@layer components {\n  .btn-blue {\n    @apply bg-blue-500 hover:bg-blue-700 text-white font-bold py-2 px-4 rounded;\n  }\n}\n\n@layer utilities {\n  .filter-none {\n    filter: none;\n  }\n  .filter-grayscale {\n    filter: grayscale(100%);\n  }\n}\n```\n\nTailwindCSS 还自定义了一些 CSS 函数：\n\n- `theme()`: 返回 config 文件中的参数，比如\n    \n    ```css\n    .content-area {\n      height: calc(100vh - theme(spacing.12));\n    }\n    ```\n    \n- [`screen()`](https://tailwindcss.com/docs/functions-and-directives#screen): 以预定义的 breakpoint 为参数，避免代码中间出现硬编码的数值\n    \n    ```css\n    @media screen(sm) { /* ... */ }\n    ```\n    \n\n## 在 Next.js 项目中安装 TailwindCSS\n\n\u003e [https://nextjs.org/docs/pages/building-your-application/styling/tailwind-css](https://nextjs.org/docs/pages/building-your-application/styling/tailwind-css)\n\u003e \n\n在命令行\n\n```bash\nnpm install -D tailwindcss postcss autoprefixer\nnpx tailwindcss init -p\n```\n\n如此会在项目的根目录新建 `tailwind.config.js` \u0026 `postcss.config.js` 文件\n\n然后编辑 `tailwind.config.js` 文件，添加需要用到 TailwaindCSS 的路径\n\n```jsx\n/** @type {import('tailwindcss').Config} */\nmodule.exports = {\n  content: [\n    './app/**/*.{js,ts,jsx,tsx,mdx}', // Note the addition of the `app` directory.\n    './pages/**/*.{js,ts,jsx,tsx,mdx}',\n    './components/**/*.{js,ts,jsx,tsx,mdx}',\n \n    // Or if using `src` directory:\n    './src/**/*.{js,ts,jsx,tsx,mdx}',\n  ],\n  theme: {\n    extend: {},\n  },\n  plugins: [],\n}\n```\n\n在全局样式表 `styles/globals.css` 中引入 TailwaindCSS\n\n```css\n@tailwind base;\n@tailwind components;\n@tailwind utilities;\n```\n\n在 `pages/_app.js` 中引入全局样式表。`@` 的含义不明\n\n```jsx\n// These styles apply to every route in the application\nimport '@/styles/globals.css'\nimport type { AppProps } from 'next/app'\n \nexport default function App({ Component, pageProps }: AppProps) {\n  return \u003cComponent {...pageProps} /\u003e\n}\n```\n\n在项目的 components 中使用 TailwindCSS 的类：\n\n```tsx\nexport default function Page() {\n  return \u003ch1 className=\"text-3xl font-bold underline\"\u003eHello, Next.js!\u003c/h1\u003e\n}\n```"},{"slug":"afaik-generative-ai","filename":"2023-09-11-afaik-generative-ai.md","date":"2023-09-11","title":"·如是我闻 | 生成式人工智能","layout":"post","keywords":["md","ai","rss"],"excerpt":"","content":"\n## 生成式语言模型\n\n### 模型\n\n- OpenAI/GPT\n- Claude\n- `bloomchat`, 可以商用 [[GitHub](https://github.com/sambanova/bloomchat)]\n- `falcon40B`\n    - apache 2.0 许可证，可商用[[huggingface](http://huggingface.co/tiiuae)]\n    - gpt3 的性能，更少的运算资源，其中Falcon 7B可以跑在苹果Mac上 [[推特](https://twitter.com/rickawsb/status/1666148546285043714)]\n- `TigerBot`: 一款国产自研的多语言任务大模型，70亿参数和1800亿参数两个版本 [[GitHub](https://github.com/TigerResearch/TigerBot)]\n- `QLoRA`: 单个GPU，ChatGPT 99%的能力，消费级GPU微调12个小时就可以达到97%的ChatGPT水平，4B就可以保持16B精度的效果 [[论文](https://www.notion.so/Endocytic-trafficking-promotes-vacuolar-enlargements-for-fast-cell-expansion-rates-in-plants-6b8f0a313c184ccba9fb5a035bb04a0e?pvs=21)] [[GitHub](https://www.notion.so/pdf-14a94950d61c42d3b03bb132f7655589?pvs=21)]\n- `MBT 30B`: 开源商用模型为数不多的选择里出现了一个比Falcon 40B更好的模型 [[Twitter](https://twitter.com/fi56622380/status/1672137540281974784)]\n- `GLM-6B` \u0026 `GLM2-6B`: 智谱AI发布，对学术研究完全开放，并且在完成企业登记获得授权后，允许免费商业使用。[[Twitter](https://twitter.com/GanymedeNil/status/1679892021807550465)][[微信公众号@GLM大模型](https://mp.weixin.qq.com/s?__biz=MzkxNjMzMjM3NA==\u0026mid=2247484214\u0026idx=1\u0026sn=e42153f987a74d1ffc7882f7cc09670d)]\n- `Llama 2`: Meta开源大语言模型Llama 2，可免费商用. [[微信](https://mp.weixin.qq.com/s/9pcmrCEyp2AQsL3MbPYx-Q?utm_source=pocket_saves)介绍]\n    - Jim Fan 评论 [[推特，翻译](https://twitter.com/dotey/status/1681553916373135362?utm_source=pocket_saves)]\n    - 很多团队几乎都达成共识， RLHF 不重要，SFT 就够了。现在 Llama2 的论文说 RLHF 非常非常重要。[[推特](https://twitter.com/oran_ge/status/1681793774685659136?utm_source=pocket_saves)]\n    - `LLaMA-2-7B-32K`, context为32K的模型 [[推特](https://twitter.com/JefferyTatsuya/status/1685423475979325440)][[Twitter](https://twitter.com/togethercompute/status/1685048832168714240)]\n\n### 基于模型，直接可用的产品\n\n- OpenAI/GPT\n    - ChatGPT\n        - 2023年6月13日，GPT提供了函数调用，让ChatGPT来自己调用函数。[[Twitter](https://twitter.com/cryptonerdcn/status/1668733300070924288)][[OpenAI](https://openai.com/blog/function-calling-and-other-api-updates)][[用法 Twitter@宝玉](https://twitter.com/dotey/status/1668728109376450566)]\n    - ChatGPT - Code Interpreter\n        - 介绍 [[推特](https://twitter.com/fuyufjh/status/1684191835210809344)][[YouTube](https://www.youtube.com/watch?v=4wGlRrir_u4)]\n        - 《ChatGPT 探索：Code Interpreter 高级指南》[[微信@浮之静](https://mp.weixin.qq.com/s/K_csi1oWDv5tEaeeKSlvwA?utm_source=pocket_saves)]\n        - 源码可能被套出。[[Twitter](https://twitter.com/fuergaosi/status/1679457847237820416)]\n        - 对 code interpreter 的逆向工程 [[Twitter](https://twitter.com/Yampeleg/status/1678045605527003136)][[Mem](https://mem.ai/p/xyy8ULiAce1BecTxnU0M)]\n    - OpenAI API\n        - 2023年8月23日，OpenAI 开放了 GPT-3.5 的微调的API [[推特](https://twitter.com/dotey/status/1694207797351616703)]\n    - OpenAI on Azure 内置了一个内容过滤器 [[推特1](https://twitter.com/jw1dev/status/1666613728106938368)][[推特2](https://twitter.com/jw1dev/status/1666622878962548740)]\n    - `forefront`: 完全免费 GPT-4 的工具 [[登录](https://accounts.forefront.ai/)]，大概基于 `gptfree-ts` [[GitHub](https://github.com/xiangsx/gpt4free-ts)]\n    - `BratGPT`: ChatGPT的激进版本。[[官网](https://bratgpt.com/)]\n    - `SmartStudy`: 提供文本文档，创建10个问题的小测验。[[官网](https://smartstudy.streamlit.app/)]\n    - `XrayGPT`: 通过给定的 X 光片来促进围绕胸部 X 光片的自动化分析的研究。[[GitHub](https://twitter.com/CarsonYangk8s/status/1661588037892198401)]\n    - `FinGPT`: 类似BloomBerg的开源方案，RLHF 和 Lora 的低秩技术 [[Twitter](https://twitter.com/JefferyTatsuya/status/1668433680887615488)]\n- 微软\n    - BingAI\n        - 本地部署方案 [[推特](https://twitter.com/geekbb/status/1665692703055552513)][[GitHub](https://github.com/adams549659584/go-proxy-bingai)]\n    - VsCode Copilot\n    - Office 365 Copilot: 每月每名用户30美元. [[verge](https://www.theverge.com/2023/7/18/23798627/microsoft-365-copilot-price-commercial-enterprise)][[微信](https://mp.weixin.qq.com/s/9pcmrCEyp2AQsL3MbPYx-Q?utm_source=pocket_saves)]\n- Claude+\n    - 例子：阅读多份行业报告 [[推特](https://twitter.com/iamshaynez/status/1684398211958730753)]\n- `Llama`\n    - llama2.ai: 一个基于 llama 2 的聊天机器人，非官方。[[网站](https://llama2.ai/)]\n    - WizardCoder 34B based on Code Llama 写代码 [[推特](https://twitter.com/dotey/status/1696202647269785875)]\n- WebGLM: 清华开源的带网络搜索功能的 GLM 实现 [[GitHub](https://www.notion.so/pdf-14a94950d61c42d3b03bb132f7655589?pvs=21)]\n- mendable: 根据开发文档进行问答 [[官网](https://www.mendable.ai/usecases/documentation)]\n- 阅读 PDF 文档\n    - Humata.ai\n    - explainpaper\n    - ChatPDF\n    - [[对比](https://twitter.com/oran_ge/status/1683432444169711616?utm_source=pocket_saves)] Claude2支持超长上下文，摘要信息量更大，更适合长文提炼。ChatDOC 具有页码溯源、表格解析、原文定位功能，数据找得准，也方便二次验证，能够限制大语言的幻觉问题。\n- Obsidian-copliot: 快速获取文字的核心观点\n- 视频内容梗概\n    - Glarity: 浏览器插件，基于ChatGPT和字幕生成Youtube摘要，20秒看完梗概 [[Twitter](https://twitter.com/starzqeth/status/1640867876109422595)]\n    - summarize-tech: 5分钟了解长视频的要点. [[Twitter](https://twitter.com/starzqeth/status/1640867876109422595)]\n- webpilot: 可联网可读网页链接的插件 Webpilot 推出的 Chrome 版插件 [[chrome](https://chrome.google.com/webstore/detail/webpilot-copilot-for-all/biaggnjibplcfekllonekbonhfgchopo?utm_source=link)]\n\n### 模型教程、评论、二次开发\n\n- 一般性原理\n    - 《Prompt 编写模式》[[phodal](https://prompt-patterns.phodal.com)]\n    - 《LLM+Embedding构建问答系统的局限性及优化方案》[[知乎](https://zhuanlan.zhihu.com/p/641132245)]\n    - 基于检索的 LM，外挂一个数据库用来检索。[[推特](https://twitter.com/cosmtrek/status/1678077835418955781)][[GitHub.io](https://acl2023-retrieval-lm.github.io/)]\n    - 一篇泼冷水的论文 [[ACL Anthology](https://aclanthology.org/2023.findings-acl.426/)]\n    - 即刻出的Prompt调试工具。[[Twitter](https://twitter.com/vista8/status/1678784460786135040)][[官网](https://promptknit.com/)]\n- GPT\n    - GPT best practice [[OpenAI](https://platform.openai.com/docs/guides/gpt-best-practices?utm_source=pocket_saves)]\n    - Andrew Ng 吴恩达 \u0026 Isa Fulford from OpenAI 《Build system with [#ChatGPT](https://twitter.com/hashtag/ChatGPT?src=hashtag_click) API》[推特@**[金田達也](https://twitter.com/JefferyTatsuya)**]\n        - 借助 CoT 的思路，翻译字幕，返回正确的 JSON 格式 [[推特](https://twitter.com/dotey/status/1665476562219573249)]\n        - 同样的加入了CoT（Chain of Though）的Prompt，如果让GPT打印出来步骤，效果非常好，但是如果不让GPT打印（省点token，以及更容易解析），那么GPT就会偷懒 [[Twitter](https://twitter.com/dotey/status/1668736426286915590)1][[Twitter2](https://twitter.com/dotey/status/1664335473500626946)]\n    - 熊猫吃短信是 Twitter@威力狈 开发的垃圾短信过滤工具。将其与 GPT 结合的一些讨论\n        - [Twitter@威力狈](https://twitter.com/waylybaye/status/1664253928970788864)：尝试了下用 ChatGPT 自动标注数据，效果太差了。\n        - [Twitter@宝玉](https://twitter.com/dotey/status/1669028955842650139)：通常如果我写的话，会做一些小调整\n        - [Twitter@IIInoki](https://twitter.com/IIInoki)：是的，感觉八爷用 API 用得有点糙……就只是很简单的 prompt 达到的效果都还不错\n    - 《ChatGPT 越过山丘之后，再来谈谈 LLM 应用方向》[[橘子汽水铺](https://quail.ink/orange/p/chatgpt-cross-over-the-hills-and-discuss-llm-application-directions)]\n- `LangChain`:\n    - 官方教程 [[推特](https://twitter.com/LangChainAI/status/1665009694627250176)][[streamlit](https://blog.streamlit.io/langchain-tutorial-1-build-an-llm-powered-app-in-18-lines-of-code/)]\n    - 一个使用 LangChain 和 GPT Index 的教程 [[leanpub, 收费](https://leanpub.com/langchain)][[Pocket](https://getpocket.com/read/3839490971)]\n    - LangChain for LLM Application Development 基于LangChain的大语言模型应用开发 [[YouTube](https://t.co/JXV1SBI2OA)]\n        - 基于Embedding的文档问答。stuff, map reduce, refine, map rerank [[Twitter@宝玉](https://twitter.com/dotey/status/1667790801420558342)]\n    - Chanin Nantasenamat: LangChain tutorial #1: Build an LLM-powered app in 18 lines of code [[streamlit](https://blog.streamlit.io/langchain-tutorial-1-build-an-llm-powered-app-in-18-lines-of-code/?utm_source=pocket_saves)]\n    - 把一篇很长的 PDF 内容喂给 ChatGPT，然后向他提问\n        - 纯 JS 开源工具推荐 [[推特](https://twitter.com/Barret_China/status/1638119945749037056)]\n        - 用 `LangChain` 六七行代码就可以搞定了 [[LangChain](https://js.langchain.com/docs/get_started/introduction)]\n- `AutoChain`\n    - 介绍 [[推特](https://twitter.com/zhangjintao9020/status/1683996172980199429)][[GitHub](https://github.com/Forethought-Technologies/AutoChain)]\n    - 《我为什么放弃了 LangChain》[[推特](https://twitter.com/Barret_China/status/1683135367862718465)][[微信](https://mp.weixin.qq.com/s/jIbz9JYc8-_ua-QLENX__A)] 推友提出的 AutoChain 替代方案 [[推特](https://twitter.com/Barret_China/status/1684211570186887170?utm_source=pocket_saves)]\n- `OpenDAN`: 为各类 AI 模块提供运行环境，并提供它们之间的互操作性协议。可创建诸如律师、医生、教师，甚至男女朋友等角色 [[GitHub](https://twitter.com/Barret_China/status/1666455683758161920)]\n- “视频语音↔文字”任务相关\n    - 指定视频URL，识别文字，翻译 [[GitHub](https://github.com/lewangdev/autotranslate)]\n    - `WhisperX`: 按照单词对齐时间戳，生成的字幕都是完整的句子 [[GitHub](https://github.com/m-bain/whisperX)]。[[Twitter@宝玉](https://twitter.com/dotey/status/1667394662628204546)] 写了一个可以根据 YouTube Url 识别 YouTube 字幕的 [Jupyter Notebook](https://github.com/JimLiu/whisper-subtitles/blob/main/whisperx_youtube_subtitle.ipynb)\n    - `audiocraft`: audio processing and generation with deep learning. [[GitHub](https://github.com/facebookresearch/audiocraft)]\n    - [[推特]](https://twitter.com/Barret_China/status/1684218981639413760) 小作文\n    - `yt-dlp` 一行命令下载视频字幕的工具，不需 puppeteer 无头浏览器 [[推特](https://twitter.com/Barret_China/status/1684228477644570624)][[GitHub](https://github.com/yt-dlp/yt-dlp)]\n- ChatGPT + AI agent + ScholarAI + Noteable 写的小综述 [[链接失效](https://t.co/eqVc2LIfSz)]\n- `MusicGen`: 将文本和旋律转化为完整乐曲 [[Twitter](https://twitter.com/Fenng/status/1668141100610248705)][[ReadHub](https://www.notion.so/3753e42dc4204a99ab83a725b655a632?pvs=21)]\n- `MMS`: 一个声音模型 [[HuggingFace](https://huggingface.co/docs/transformers/main/en/model_doc/mms)]\n- `FRVR Forge`: AI-Powered End-to-End Game Creation [[Twitter](https://twitter.com/FRVRGames/status/1669758477789540365)][[官网](https://www.notion.so/ai-University-Cloud-8078b4682e454a5fba982f67e4530498?pvs=21)]\n\n### 开发平台\n\nRunpod: 租用 GPU 跑模型并创建 Serverless API 一站式服务，最低只要0.2刀/hr。[[官网](http://runpod.io)]\n\n### 杂项\n\n- 2023年5月27日、28日，OpenAI 使用 Sentry 审计工具封禁来自中国的用户，解决方案：\n    - 路由器 Clash 规则 [[推特](https://twitter.com/wey_gu/status/1663003950214438912?utm_source=pocket_saves)]\n    - 改用 Azure OpenAI service [[推特](https://twitter.com/zhangjintao9020/status/1662865819041402880)]\n    - Cloudflare WARP [[左耳朵](https://haoel.github.io/)]\n\n## 生成式图像模型\n\n2023年5月31日，Adobe 添加人工智能相关功能 generative fill。[[推特](https://twitter.com/CodeByPoonam/status/1663824055164887040)]\n\n- 配置要求极低，连Win掌机都能跑，但是不能断网。[[推特](https://twitter.com/OfflineHelper/status/1666042746866663424)]\n- 填充将横屏的视频转换为竖屏。[[推特](https://twitter.com/Alex_Cerrato/status/1681677307843432449)]\n\n`MidJourney`\n\n- 在提示词中添加相机镜头信息。[[推特](https://twitter.com/4rtofficial/status/1663310457854099458)]\n- zoom [[Twitter](https://twitter.com/jesselaunz/status/1674210886695923712)]\n\n`StableDiffusion`\n\n- Eric Fu: 训练指南. [[Coding Husky](https://ericfu.me/stable-diffusion-finetune-guide/?utm_source=pocket_reader)]\n- 文字或者符号融合生成图片 [[Twitter](https://twitter.com/op7418/status/1680223090138316800)][[微信](https://mp.weixin.qq.com/s/rvpU4XhToldoec_bABeXJw)]\n\n`StyleDrop`: Google 基于 MUSE 的样式迁移 transformer [[推特](https://twitter.com/recatm/status/1665056017107886080)][[GitHub.io](https://styledrop.github.io/)]\n\n`Redream`: 从视频到二次元动画 [[推特](https://twitter.com/heyBarsee/status/1665034805384290307)][[GitHub](https://github.com/Fictiverse/Redream)]\n\n`Runway` Gen-2: 文本生成视频和图片生成视频, 4 秒钟 [[推特](https://twitter.com/op7418/status/1666461595818504192)][[需注册](https://app.runwayml.com/login)]\n\n一个 AI 视频解决方案，来自南洋理工，代码尚未开源 [[Twitter](https://twitter.com/op7418/status/1669026494885285888)] [[GitHub.io](https://anonymous-31415926.github.io/)][[Twitter2](https://twitter.com/rickawsb/status/1672310994390126593)][[arxiv](https://arxiv.org/abs/2306.07954)]\n\n`AWPortrait1.1`: 图像生成 [[Twitter](https://twitter.com/dynamicwangs/status/1673730591462928385)][[LibLibai](https://www.liblibai.com/modelinfo/721fa2d298b262d7c08f0337ebfe58f8)] \n\n`Anything AI`: 可以取代照片中的任何物体。免费，不需要注册. [[官网](https://www.anything-ai.com/)]\n\n`PixelLab`: 草图创建2D图像. [[官网](https://www.pixellab.ai/)]\n"},{"slug":"what-is-intelligence-not-same-as-intelligence-is-what","filename":"2023-06-17-what-is-intelligence-not-same-as-intelligence-is-what.md","date":"2023-06-17","title":".tex | 什么是智能≠智能是什么","layout":"post","keywords":["tex","doc","md","ai"],"hasMath":true,"excerpt":"“什么是智能”的问题每每得不到回答，是因为它的逆问题“智能是什么”没有答案。","content":"\n## 0\n\n这是一篇酬和之作。\n\n徵文标题说的是：\n\n\u003e **機器會製造「內涵」嗎？**\n\u003e \n\n但是正文提出的问题是：\n\n\u003e AI透過程式組合出回答你問題的文字組合，有「涵義」嗎？\n\u003e \n\n可是，「內涵」和「涵義」两个词的——内涵/涵义——就不完全一样啊……\n\n“内涵”(connotation) 通常指词语或表达方式所隐含的情感、态度、暗示或附加的意义。它涉及到词语或表达方式所引起的情感、联想或隐含的观点。也就是弦外之音。\n\n而“涵义”(meaning) 一般指词语、表达方式或行为所传达的字面意义或字面上的定义。它强调的是直接的、明确的意义。\n\n看热闹不嫌事大，那我们再把问题搞复杂一点——在逻辑学里也有一个“内涵”(intension)，和“外延”(extension) 相对应。用**面向对象编程**的说法来理解，一个类里面定义的所有状态量和内部方法的集合，就构成这个类的“**内涵**”；所有（已经和将来能够）从这个类实例化出来的对象的集合，就构成这个类的“**外延**”。\n\n所以看起来，征文者想问的是日常“内涵”也就是言外之意，但是怕杠精（比如我）用有严格定义的逻辑“内涵”解构掉，所以换了“涵义”一词。\n\n这个问题很显然是因应最近大语言模型掀起的这一波 AI 浪潮。这个问题往前再问一句，就是“大语言模型是智能体/有智能吗？”\n\n- 前两年 DeepMind 的 AlphaGo/AlphaZero 系列 AI 在围棋中击败人类棋手时，人们也在问这个问题。\n- 上世纪四五十年代专家系统 (expert system) 刚刚开发的时候，人们也在问这个问题。\n- 从电子计算机往前追溯到机械计算机，甚至是巴比奇的差分机的时候，人们就已经开始问这样的问题了。\n\n这些问题求并集，然后在问题数量趋近于无穷下的极限，就是“什么是智能”。\n\n这样的问题每每得不到回答，是因为它的逆问题“智能是什么”没有答案。我们并没有智能的准确定义，只能一事一论。而之前的智能和非智能体的区别太明显，以至于作出判断也不能对智能的定义有所启发。\n\n## 1\n\n而对“智能是什么”的探究，哲学、逻辑学、计算机科学、生物学、管理学，不同领域的研究者有着不同的思路。\n\n### 古哲学·洞穴之壁与理念世界\n\n古希腊哲学家柏拉图在《理想国》里提到了“洞穴之壁”的寓言故事。\n\n有一群被囚禁在一个深洞的囚徒，从出生开始就被束缚在这个洞穴里，脖子和腿都被铁链锁住，没办法转身或离开。囚徒身后的洞穴入口处有一道火焰，火焰后有人持物体走过，物体的投射在洞穴内的墙壁上形成了影子。囚徒们就以为这些影子就是唯一的存在。\n\n这里的囚徒代表着人类，洞穴代表着世界，影子则代表着我们对于现象世界的感知和观念。人们的知识和信念往往受限于自己的经验和感知，就像囚徒们只看到了洞穴墙壁上的影子，而在影子之外还存在一个理想的理性世界。柏拉图用这个寓言故事表达了他对于人类认识和智慧的理解。所谓智慧，就是从洞穴的影子反过去推测火把前物体的能力。\n\n当然，这种思想被 Marx 主义定性为一种客观唯心主义、唯理论，是受其批判的。\n\n### 逻辑学·从命题到希尔伯特算符\n\n柏拉图的学生亚里士多德，今天在低年级的物理教科书里基本是个反面典型，但他对逻辑学进行了系统化和全面的研究，提出了许多逻辑学的基本概念和原理。这些成果后来成为了欧洲哲学和逻辑学的基石，对西方哲学和科学的发展产生了深远影响。\n\n所谓逻辑，就是研究命题的对错，以及如何判断命题对错的学问。而命题，就是能被判断对错的句子。但是句子显然可以再分成不同成分，于是就发明/发现了主体、客体、谓词、谓词的量词……等等概念，以及用这些概念构造命题的方法。\n\n但是要注意，虽然逻辑主要由语言来表达，但是逻辑还是和语言不同，主体、客体也不等于句子的主语、宾语。这两者的区别，基本可以类比于之前洞穴之壁寓言里的实体和影子。\n\n这种努力到目前为止的巅峰，基本上要数希尔伯特形式化逻辑系统了。感兴趣的朋友可以自行查阅戈得门特《代数学教程》的第一章，这玩意相当于思想界的引体向上，反正我是一个也拉不上去……\n\n### 计算机·从半导体到抽象语法树\n\n希尔伯特是德国的数学家，《代数学教程》也是数学而不是哲学教材。显而易见，逻辑虽然由哲学家奠基，但是主导权很快落到数学家，至少是哲学家兼数学家手里了。\n\n命题的“真”与“非真”同构于 {1, 0}，各种逻辑运算都可以分解成“或”与“非”两种基本逻辑运算的组合，这就是以数学家乔治·布尔 (George Boole) 命名的布尔代数。因为 {1, 0} 又可以同构于半导体电路的高低电位，和各种类似继电器的门电路组合，所以很容易用计算机在物理世界表示出这些逻辑运算。\n\n我们的电脑由上亿个这样的电位和逻辑门组成，一般的科普文章应该会去介绍芯片啊光刻机之类的东西，本文关注的是另一个方面：虽然生产电脑配件的厂商很多，不同的型号的元器件设计不同，组装出的成品应该千差万别，但是他们可以运行同样的程序，理想条件下（虽然实际工程中常常不理想）我们也可以期望他们跑出同样的结果。\n\n这说明所谓计算机科学，并不等同于研究计算机元件的电子科学和工程，这里电科和电子工程相当于洞穴岩壁上的影子，而计算机科学就相当于火光前的物体。这种超越物理的计算本质，一般用一种叫做“抽象语法树”的数据结构来表示。\n\n### 生物学·从神经元到神经网络\n\n人们发明计算机的时候，基本上还是把它当作工具，就没期望它有什么主体性和智慧。\n\n而随着生物学逐渐发现了神经系统及其作用，也随着物理学在二十世纪初的大发展之后的相对平静，很多物理学家开始插手其他学科。既然生命和非生命体的背后都服从同一套物理规律，既然物理学的众多成功经验说明，搞清楚构成系统的所有微观组成就可以理解宏观的系统，那么搞清楚人类的智力器官的基本单元以及相互作用，按理说也就能够理解什么是智慧。\n\n![a cartoon illustrating a neuron](/photos/2023-06-17-neuron.png)\n\n上图是一个神经细胞的结构示意图。从其他神经细胞释放出来的名为神经递质的化学物质，到达神经元左侧短且密集的树突之后，激活细胞膜表面的离子泵，主动运输离子跨过细胞膜，从而产生电信号。电信号沿细胞膜传导到右侧的树突，刺激凸触释放神经递质给下一个细胞。\n\n![a handdrawing style illustration of perceptron](/photos/2023-06-17-perceptron.png)\n\n上图就是根据神经元的工作原理抽象出的数学模型，名为 perceptron。一个 perceptron 就是一个函数，接受多个输入的自变量，加权求和之后套一个非线性的激活函数，得到一个输出。很多个这样的 perceptron 并连和串联，就构成下图，计算机算法中的神经网络。\n\n![a handdrawing style illustration of a neural network](/photos/2023-06-17-neural-network.png)\n\n而从实验方向研究神经系统，我们隔壁系就有，经常来我们系招人。基本上就是在小鼠的天灵盖上锯开一个天窗，然后给它带上个头盔，头盔上有能从天窗伸进去的电极，采集脑神经的电信号。以前头盔有网线伸到实验室天花板，实时传到数据中心的超算。现在好像进步了，改用 Wi-Fi 了。\n\n这实验怎么通过的伦理审查，咱也不知道，咱也不敢问……\n\n### 管理学·DIKW “数据-信息-知识-智慧”模型\n\n![a pyramid of DIKW model](/photos/2023-06-17-DIKW.png)\n\nDIKW 四个字母分别代表 data, information, knowledge, wisdom，即数据、信息、知识、智慧，是一种知识管理中的心智模型。\n\n四个层次，前一层都是后一层的基础，后一层都是对前一层的理解。\n\n如果是书面文字，数据就是笔画和字母；如果是语言，数据就是人声的响度、频率和音色。由笔画/字母/声音组成的有含义的字词就是信息。表示信息之间的关系的，可以判断对错的命题就是知识。包含和统摄各条知识的思想体系，就是智慧。\n\n反过来说，虽然智慧高于思想，但它仍需要通过把各条知识的表达汇总起来，才能被人感知。对知识的命题的理解依赖于构成名字的各个概念的涵义，属于信息水平的内容。而每个字都有不考虑其涵义的笔画字母构成。\n\n这层与层之间**看似**并没有插入额外的内容，智慧可以直接由笔画构成。但是我们一层层理解的深入，其实是不自觉地借用了我们当前社会约定俗成的解读方式。\n\n比如下面这个图片里的符号，对于现代人就只是数据，无法解读成信息。但是对于苏美尔人，这是用楔形文字表示的数字，是等腰直角三角形的腰和直角边的比值，也就是 $$\\sqrt{2}$$ 的近似值。\n\n![sumerian numerical approximation to square root of two](/photos/2023-06-17-ancient-root-2.png)\n\n约定俗成的数据解读方式，也就是关于**数据的数据**，根据西方的构词法，可以叫做“**元**数据”(meta-data)。\n\n数据和元数据一起构成信息，信息和元信息一起构成知识，知识和元知识一起构成智慧。俺坚持写博客的动机，就是用费曼学习法，把无意间使用的元知识显式地表达出来，而且记录下来，争取学而不退转。\n\n## 2\n\n回顾了这些，再来看大语言模型，就会发现它落在了各方努力的延长线的交点。\n\n大语言模型里有一个重要概念叫做“嵌入”(embedding)，就是把语言的基本字元 (token) 可逆地映射到一个超多维度的向量空间里。本来“国王”和“儿子”之间没办法加减乘除，但是嵌入后的向量空间里有加法和数乘，如果嵌入函数选得好，“国王”的向量 + “儿子”的向量，结果向量就约等于“王子”的向量。\n\n![illustration of vector addition from wikipedia](/photos/2023-06-17-vector-addition.png)\n\n生成式语言模型的核心就是一个超多元函数，接受前一个字嵌入后的向量作为输入，给出另一个向量作为输出，用嵌入函数的逆映射翻译成字元；再把旧的输出作为新的输入，直到输出结果是“语段结束”这样一个特殊字元为止。模型训练的过程，主要就是通过现成的语料，拟合这个超多元函数的参数。\n\n从 DIKW 模型来看，语言模型操作的是最基本的数据，它的输出究竟是什么信息，是不是正确的知识，体现了多少智慧，是人根据当下的社会文化来解读的。\n\n而实现 AI 的电子计算机，或是复杂生命的大脑，他们和智能之间的关系，应该就类似于具体的计算机电路和抽象语法树之间的关系。以此类比，未来的智能科学应该会成为一门独立的专业，它和计算机科学和神经生物学的区别，就像今天的电子科学与工程，和计算机科学之间的区别一样。当下神经生物学的热度，将来恐怕多半会被分流。\n\n这种对字符的计算不同于逻辑运算，语言模型不判断输出结果在逻辑上的正确与错误，这既给了他啥都能说几句的 feature，又给了它经常编假消息的 bug。\n\n想要改掉这种错误，引入对 AI 的纠错机制，治本之道恐怕还是诉诸于对世界的正确描述，与理论相关的还是要靠逻辑，与现实相关的还是要靠科学。\n\n只不过，大语言模型提供了一种数据结构，有希望把人类已知的真理储存在一起。对这种数据结构本身的研究，有可能反过来启发科学的发展。柏拉图的洞穴之壁可能不再是一个比喻，未来更大的语言模型的，亿万维度的参数空间有希望成为洞穴门口的那团火。\n\n只不过这一切都是“可能”，现在还只是 AI 的萌芽阶段，还没有足够的证据来证实或者证伪这种畅想。而且 AI 的参数量再大也是有限的，它所能表达的信息也就有限，而真理应当是无限的，就像科学一样，总要训练更新更大的模型，总要发现已知的未知，然后欣然接受更多未知的未知之存在。\n\n如果电子计算机实现的 AI 独立于人类产生了意识和超出人类的智慧，很难想象他们会继续用人类语言这种对他们来说很不方便的方式来交流。\n\n所以，哪怕是做个 AI 生成内容的质检员，科学家依然有事可做。这算是科学的堕落吗？当然不算，如果算的话，那从计算物理也被当作理论物理的那天起，人类就已经投降了（逃）\n\n## 3\n\n现在正面来回答问题：AI透過程式組合出回答你問題的文字組合，有「涵義」嗎？\n\n答：有。\n\n因为语言的「涵義」来自于语言的内容，和整个社会的文化，并不来自于这句话的作者的身份。即便是人与人之间的交流，诉诸身份也是一种非形式逻辑谬误，是理性不足的表现。只有在信息不足仍不得不下结论的时候才该使用，比如法律判决时的自由心证主义和/或法定证据主义。\n\n而鹿妈眼里真人鹿酱与 AI 鹿酱的区别，如果有的话，好像主要体现在动机的区别。动机这种东西，很多智慧不高的生物，比如小猫小狗都会有；而现在的 AI，似乎还没有展现出超出编程者设计的动机。编程写入的信息有限，现有 AI 的动机也就有限，鹿酱的赢面还是很大的。\n\n而动机是生物与非生物的区别吗？而什么是生物 ≠ 生物是什么，那就是另一个含混而复杂的问题了。\n\n## 4\n\n这篇博文发布的时候，高考应该已经结束了，马上该填报志愿了。\n\n那么，西元 2023 年，AI 来袭的当下，该选个啥专业在 AI 浪潮中幸存，或者选个啥专业给 AI 老爷带路呢？\n\n![a screenshot of a quotation from Three Body about attitudes towards aliens](/photos/2023-06-17-three-body-quotation.png)\n\n我的建议是，不要听别人的建议，按自己的兴趣来就好了。\n\n刚刚改开的时候，有一个超级热门的专业，叫科技英语。科技落下了好多年，对外开放需要语言交流，两者一结合应该是热门又稀缺了。结果呢，你现在还听说过这个专业吗？\n\n科技很重要是不错，语言很重要也不错，但是搞科技的人自己可以学英语，学英语的有几个搞得了科技？社会的进步主要靠创新，而创新的方向难以预测，不论这种预测分析听起来多有道理。\n\n如果真的找不到兴趣，那就在能力范围之内，找个难度最高的。如果想从事智力劳动，那数学含量是个不错的衡量标准；如果不排斥体力劳动，那训练时间越长越值得考虑。\n\n但这只是填志愿来不及时的权宜之计，发掘兴趣是人一生的课题。\n\n兴趣不是为了让你成功的时候更得意，毕竟成功的话不论做什么都很得意；\n\n兴趣是为了你不成功时也可以不失意，毕竟平凡才是人生的真谛。\n"},{"slug":"physics-based-neural-network-review-note","filename":"2023-03-20-physics-based-neural-network-review-note.md","date":"2023-03-20","title":".tex | 基于物理的神经网络 (PINN) 综述笔记","layout":"post","keywords":["tex","phy","md","ai"],"hasMath":true,"excerpt":"本文是《Scientific Machine Learning Through Physics–Informed Neural Networks: Where we are and What’s Next》这篇综述的读书笔记。","content":"\n\u003e 本文是《[Scientific Machine Learning Through Physics–Informed Neural Networks: Where we are and What’s Next](https://link.springer.com/article/10.1007/s10915-022-01939-z)》这篇综述的读书笔记。\n\u003e \n\n年前，今年新入职的天文学方面的一位老师给我们群发邮件，宣传某国家实验室超算的 GPU 编程马拉松活动，他可以担任指导老师。于是毫不意外地，我报了名。该编程马拉松项目还需要专门申请，申请材料里要写清楚打算干什么，于是报名的五六个人七嘴八舌地想创意。基于物理的神经网络 PINN 就是天文老师的点子。\n\n~~写到这里，我才意识到，老哥是不是想拿我们当免费劳动力啊~~~\n\n神经网络可以看作是一个复杂的非线性函数，接受一个（一般来说维度很高的）向量作为输入，一番计算后输出另一个向量。训练神经网络，就是找到这个函数的参数，绝大多数找参数的方法涉及计算网络输出对参数的偏导数，因此神经网络计算框架的核心功能就是自动微分 (auto-differentiation)。\n\n而很多物理问题，都可以用（偏）微分方程来描述，微分方程的解不是变量，而是函数，而且往往是复杂的非线性函数。所以基于物理的神经网络 (PINN) 就是以神经网络来表达这个函数，然后把这个函数带入到物理的微分方程中，把神经网络输出和真正的物理解之间的差距当作损失函数，反向传播回去来优化神经网络的参数。代入方程时的微分计算，正好可以利用现成框架的自动微分功能。\n\n在以 GPT 为代表的 transformer 类神经网络模型出现之前，自然语言处理类的机器学习项目，往往要在网络之外，利用人类的语法知识，对语段进行语义分割等等“中间任务”。Transformer 一出，算力出奇迹，中间任务逐渐变得没有必要了。\n\n在 GPT 崭露头角，并且越来越有迹象表明其将会涌现出通用人工智能的今天，这些基于物理的神经网络，会不会还未成熟就已过时？这种心情，就和《三体》第二卷开始，章北海和吴岳面对焊渍未漆的“唐”号航空母舰时差不多吧……\n\n\u003chr class=\"slender\"\u003e\n\n- Abstract\n    - PINNs are neural networks that encode model equations. a NN must fit observed data while reducing a PDE residual.\n\n1. Introduction\n    - The “curse of dimensionality” was first described by Bellman in the context of optimal control problems. (Bellman R.: Dynamic Programming. Sci. 153(3731), 34-37 (1966))\n    - Early work: MLP ([multilayer perceptron](https://en.wikipedia.org/wiki/Multilayer_perceptron)) with few hidden layers to solve PDEs. ([https://doi.org/10.1109/72.712178](https://doi.org/10.1109/72.712178))\n    - 感觉可能更全面的一篇综述：[https://doi.org/10.1007/s12206-021-0342-5](https://doi.org/10.1007/s12206-021-0342-5)。该文关注 what deep NN is used, how physical knowledge is represented, how physical information is integrated，本文只关于 PINN, a 2017 framework。\n\n    1. What the PINNs are\n        - PINNs solve problems involving PDEs:\n            - approximates PDE solutions by training a NN to minimize a loss function\n            - includes terms reflecting the initial and boundary conditions\n            - and PDE residual at selected points in the domain (called **collocation points**)\n            - given an input point in the integration domain, returns an estimated solution at that point.\n            - incorporates a [residual network](https://en.wikipedia.org/wiki/Residual_neural_network) that encodes the governing physical equations\n            - can be thought of as an **unsupervised strategy** when they are trained solely with physical equations in forward problems, but **supervised learning** when some properties are derived from data\n        - Advantages:\n            - [mesh-free](https://en.wikipedia.org/wiki/Meshfree_methods)? 但是我们给模型喂训练数据的时候往往已经暗含了 mesh 了吧\n            - on-demand computation after training\n            - forward and inverse problem using the same optimization, with minimal modification\n    2. What this Review is About\n        - 提到了一个做综述找文章的方法：本文涉及的文章可以在 Scopus 上进行高级搜索：`((physic* OR physical)) W/2 (informed OR constrained) W/2 “neural network”)`\n2. The Building Blocks of a PINN\n    - question:\n    \n    $$\n    F(u(z);\\gamma)=f(z),\\quad z\\ \\in\\ \\Omega \\\\ B(u(z))=g(z), \\quad z\\ \\in\\ \\partial \\Omega\n    $$\n    \n    - solution:\n    \n    $$\n    \\hat u_{\\theta}(z)\\approx u(z)\\\\ \\theta^* = \\arg\\min_{\\theta}\\left(\\omega_F L_F(\\theta)+\\omega_BL_B(\\theta)+\\omega_{data}L_{data}(\\theta)\\right)\n    $$\n    \n    1. Neural Network Architecture\n        - DNN (deep neural network) is an artificial neural network that is deeper than 2 layers.\n        \n        1. Feed-Forward Neural Network: \n            - $$u_{\\theta}(x) = C_{K} \\circ C_{k-1} ...\\alpha \\circ C_1(x),\\quad C_k(x) = W_k x_k + b_k$$\n            - Just change CNN from convolution to fully connected.\n            - Also known as multi-layer perceptrons (MLP)\n            \n            1. FFNN architectures \n                - Tartakovsky et al used 3 hidden layers, 50 units per layer,  and a hyperbolic tangent activation function. Other people use different numbers but of the same order of magnitude.\n                - A comparison paper: *Blechschmidt, J., Ernst, O.G.: Three ways to solve partial differential equations with neural networks –A review. GAMM-Mitteilungen 44(2), e202100,006 (2021).*\n            2. multiple FFNN: 2 phase [Stephan problem](https://en.wikipedia.org/wiki/Stefan_problem).\n            3. shallow networks: for training costs\n            4. activation function: the swish function in the paper has a learnable parameter, so — [how to add a learnable parameter in PyTorch](https://discuss.pytorch.org/t/how-could-i-create-a-module-with-learnable-parameters/28115)\n        2. Convolutional Neural Networks: \n            - I am most familiar with this one.\n            - $$f_i(x_i;W_i)=\\Phi_i(\\alpha_i(C_i(W_i,x_i)))$$\n            - performs well with multidimensional data such as images and speeches\n            \n            1. CNN architectures: \n                - `PhyGeoNet`: a physics-informed geometry-adaptive convolutional neural network. It uses a coordinate transformation to convert solution fields from irregular physical domains to rectangular reference domains.\n                - According to Fang ([https://doi.org/10.1109/TNNLS.2021.3070878](https://doi.org/10.1109/TNNLS.2021.3070878)), a Laplacian operator can be discretized using the finite volume approach, and the procedures are equivalent to convolution. Padding data can serve as boundary conditions.\n            2. convolutional encoder-decoder network\n        3. Recurrent Neural Network\n            - $$f_i(h_{i-1})=\\alpha\\left(W\\cdot h_{i-1}+U\\cdot x_i+b\\right)$$, where f is the layer-wise function, x is the input, h is the hidden vector state, W is a hidden-to-hidden weight matrix, U is an input-to-hidden matrix and b is a bias vector. 我认为等号左边的 $$h_{i-1}$$ 应当作为下标\n            - 感觉有点像 hidden Markov model，只不过 Markov 中间的 hidden layers 好像与序号无关（记不清了），~~RNN 看起来各个 W 和 H 似乎不同~~。**RNN cell is actually the exact same one and reused throughout.** (from [https://blog.floydhub.com/a-beginners-guide-on-recurrent-neural-networks-with-pytorch/](https://blog.floydhub.com/a-beginners-guide-on-recurrent-neural-networks-with-pytorch/)). Cartoon from Wikipedia:\n                \n                ![Untitled]({{ site.baseurl }}/assets/photos/2023-03-20-rnn-unit.png)\n                \n            - From [https://blog.floydhub.com/a-beginners-guide-on-recurrent-neural-networks-with-pytorch/](https://blog.floydhub.com/a-beginners-guide-on-recurrent-neural-networks-with-pytorch/):\n                \n                ![Untitled]({{ site.baseurl }}/assets/photos/2023-03-20-rnn-types.png)\n                \n            1. RNN architectures\n                - can be used to perform numerical Euler integration\n                - 基本上输出的第 i 项只与输入的第 i 和 i-1 项相关。\n            2. LSTM architectures\n                - 比 RNN 多更多中间隐变量，至于怎么做到整合长期记忆的，技术细节现在可以先略过\n        4. other architectures for PINN\n            1. Bayesian neural network: weights are distributions rather than deterministic values, and these distributions are learned using Bayesian inference. 只介绍了[一篇文章](https://doi.org/10.1016/j.jcp.2020.109913)\n            2. GAN architectures: \n                - two neural networks compete in a zero-sum game to deceive each other\n                - physics-informed GAN uses automatic differentiation to embed the governing physical laws in stochastic differential equations. The discriminator in PI–GAN is represented by a basic FFNN, while the generators are a combination of FFNNs and a NN induced by the SDE\n            3. multiple PINNs\n    2. Injection of Physical Laws\n        - 既然是要解常/偏微分方程，那么微分计算必不可少。四种方法：hand-coded, symbolic, numerical, auto-differentiation，最后一种显著胜出。所谓 auto-differentiation, 就是利用现成框架，框架自动给出原函数的导数的算法。\n        - Differential equation residual:\n            - $$r_F[\\hat u_\\theta](z)=r_\\theta(z):=F(\\hat u_\\theta(z);\\gamma)-f$$\n            - $$r_F[\\hat u_\\theta](z)=r_\\theta(x,t)=\\frac{\\partial}{\\partial t}\\hat u_\\theta(x,t)+F_x(\\hat u_\\theta(x,t))$$: 原文给出了来源，但是从字面上看不出来与前式的等价性\n        - Boundary condition residual: $$r_B[\\hat u_\\theta](z):=B(\\hat u_\\theta(z))-g(z)$$\n    3. Model Estimation by Learning Approaches\n        1. Observations about the Loss\n            - $$\\omega_F$$ accounts for the fidelity of the PDE model. Setting it to 0 trains the network without knowledge of underlying physics.\n            - In general, the number of $$\\theta$$ is more than the measurements, so regularization is needed.\n            - The number and position of residual points matter a lot.\n        2. Soft and Hard Constraints\n            - Soft: penalty terms. Bad:\n                - satisfying BC is not guaranteed\n                - assignment of the weight of BC affects learning efficiency, no theory for this.\n            - Hard: encoded into the network design. [Zhu et. al](https://doi.org/10.1007/s00466-020-01952-9)\n        3. Optimization methods\n            - minibatch sampling using the Adam algorithm\n            - increased sample size with L-BFGS (limited-memory Broyden-Fletcher-Goldfarb-Shanno)\n    4. Learning theory of PINN: roughly in DE, consistency + stability → convergence\n        1. convergence aspects: related to the number of parameters in NN\n        2. statistical learning error analysis: use *risk* to define *error*\n            - Empirical risk: $$\\hat R[u_\\theta]:=\\frac{1}{N}\\sum_{i=1}^N \\left\\|\\hat u_{\\theta}(z_i)-h_i\\right\\|^2$$\n            - Risk of using approximator: $$R[\\hat u_{\\theta}]:=\\int_{\\bar \\Omega}(\\hat u_{\\theta}(z)-u(z))^2dz$$\n            - Optimization error: the difference between the local and global minimum, is still an open question for PINN. $$E_O:=\\hat R[\\hat u_{\\theta}^*]-\\inf_{\\theta \\in \\Theta}\\hat R[u_\\theta]$$\n            - Generalization error: error when applied to unseen data. $$E_G:=\\sup_{\\theta \\in \\Theta}\\left\\|R[u_\\theta]-\\hat R[u_\\theta]\\right\\|$$\n            - Approximation error: $$E_A:=\\inf_{\\theta \\in \\Theta}R[u_\\theta]$$\n            - Global error between trained deep NN $$u^*_\\theta$$ and the correct solution is bounded: $$R[u^*_\\theta]\\le E_O+2E_G+E_A$$\n            - 有点乱，本来说 error 是误差，结果最后还是用 risk 作为误差\n        3. error analysis results for PINN\n3. Differential Problems Dealt with PINNs：读来感觉这一部分意义不大，将来遇到需要解决的问题时，回来看看之前有没有人做过就行了——另一方面看，一类方程就需要一类特殊构造的神经网络来解，那么说明神经网络解方程的通用性并不好~\n    1. Ordinary differential equations: \n        - Neural ODE as learners, a continuous representation of **ResNet**. [[Lai et al](https://doi.org/10.1016/j.jsv.2021.116196)], into 2 parts: a physics-informed term and an unknown discrepancy\n        - LSTM [[Zhang et al](https://doi.org/10.1016/j.cma.2020.113226)]\n        - [Directed graph models](https://doi.org/10.1016/j.compstruc.2020.106458) to implement ODE, and Euler RNN for numerical integration\n        - Symplectic Taylor neural networks in [Tong et al](https://doi.org/10.1016/j.jcp.2021.110325) use symplectic integrators\n    2. Partial differential equations: steady/unsteady的区别就是是否含时\n        1. steady-state PDEs\n        2. unsteady PDEs\n            1. Advection-diffusion-reaction problems\n                1. diffusion problems\n                2. advection problems\n            2. Flow problems\n                1. Navier-Stokes equations\n                2. hyperbolic equations\n            3. quantum problems\n    3. Other problems\n        1. Differential equations of fractional order\n            - automatic differentiation not applicable to fractional order → [L1 scheme](https://doi.org/10.1515/fca-2019-0086)\n            - [numerical discretization for fractional operators](https://doi.org/10.1137/18M1229845)\n            - [separate network to represent each fractional order](https://doi.org/10.1038/s43588-021-00158-0)\n        2. Uncertainty Estimation: [Bayesian](https://doi.org/10.1016/j.jcp.2020.109913)\n    4.  Solving a Differential Problem with PINN\n        - 1d non-linear Schrödinger equation\n        - dataset by simulation with MATLAB-based Chebfun open-source(?) software\n4. PINNs: Data, Applications, and Software\n    1. Data\n    2. Applications\n        1. Hemodynamics\n        2. Flows Problems\n        3. Optics and Electromagnetic Applications\n        4. Molecular Dynamics and Materials-Related Applications\n        5. Geoscience and Elastiostatic Problems\n        6. Industrial Application\n    3. Software\n        1. `DeepXDE`: initial library by one of the vanilla PINN authors\n        2. `NeuroDiffEq`: PyTorch based used at Harvard IACS\n        3. `Modulus`: previously known as Nvidia SimNet\n        4. `SciANN`: implementation of PINN as Keras wrapper\n        5. `PyDENs`: heat and wave equations\n        6. `NeuralPDE.jl`: part of SciML\n        7. `ADCME`: extending TensorFlow\n        8. `Nangs`: stopped updates, but faster than PyDENs\n        9. `TensorDiffEq`: TensorFlow for multi-worker distributed computing\n        10. `IDRLnet`: a python toolbox inspired by Nvidia SimNet\n        11. `Elvet`: coupled ODEs or PDEs, and variational problems about the minimization of a functional\n        12. Other Packages\n5. PINN Future Challenges and Directions\n    1. Overcoming Theoretical Difficulties in PINN\n    2. Improving Implementation Aspects in PINN\n    3. PINN in the SciML Framework\n    4. PINN in the AI Framework\n6. Conclusion\n"},{"slug":"parameters-in-convolution-in-neural-network-and-transposeconv","filename":"2022-12-29-parameters-in-convolution-in-neural-network-and-transposeconv.md","date":"2022-12-29","title":".ai | 神经网络中的卷积及其参数","layout":"post","keywords":["ai","py","md","m"],"hasMath":true,"excerpt":"在读 PyTorch 的文档和源码的时候，发现写文档的人也不怎么解释啥是卷积，卷积的各个参数是什么意思，只在文档里扔了个链接就完事了……","content":"\n在读 PyTorch 的文档和源码的时候，发现写文档的人也不怎么解释啥是卷积，卷积的各个参数是什么意思，只在文档里扔了个链接就完事了，链接那头是一个 GitHub 上的动图演示仓库，是一篇论文《A guide to convolution arithmetic for deep learning》（链接在文末）的附件。于是这篇文章，基本上就是论文的读书笔记了。\n\n## 数学的卷积：连续 vs. 离散\n\n### 定义\n\n连续的情况，两个单变量函数 $$f(\\cdot)$$ 和 $$g(\\cdot)$$ 的卷积，定义为：\n\n$$\n\\left(f*g\\right)(x):=\\int_{-\\infty}^{\\infty}f(\\tau)g(x-\\tau)d\\tau\n$$\n\n离散的情况，两个向量（也就是一阶张量） $$\\vec f$$ 和 $$\\vec g$$ 的卷积，定义为：\n\n$$\n\\left(\\vec f * \\vec g\\right)_i := \\sum_{j=-\\infty}^{\\infty} f_j g_{i-j}\n$$\n\n多变量函数/高阶张量的情况，只需要多加几重积分/求和号就可以类推了。\n\n看这两个定义——\n\n只看等号左边的话，可以把卷积看作是一种特殊的乘法，也就是一种**运算。**f 和 g 的地位是平等的，卷积甚至还满足交换律，你甚至可以把两者的顺序变一变；\n\n但是看等号右边的话，卷积就应该被看作是一种**变换**。f 和 g 的地位不再平等，f 是被变换的函数/向量，g 是变换的核 (kernel)。函数的情况里，g 把定义在 $$\\tau$$ 空间里的函数 f 变换成了 x 空间里的另一个函数；向量的情况里，g 把一个 J (j 所有可能取值的数量) 维向量 f 变换成了一个 I (i 所有可能取值的数量) 维向量。\n\n神经网络中的卷积，**借用**的主要是第二种**理解**。\n\n### 手算一个例子\n\n例如 $$\\vec f = (1,2,3,4)$$, $$\\vec g = (1,2,3)$$，而且约定下标从 0 开始的话——\n\n\u0026nbsp; $$(\\vec f*\\vec g)_0 = f_0g_0 = 1$$\n\n\u0026nbsp; $$(\\vec f*\\vec g)_1 = f_0g_1 + f_1g_0  = 4$$\n\n\u0026nbsp; $$(\\vec f*\\vec g)_2 = f_0g_2 + f_1g_1 + f_2g_0 = 10$$\n\n\u0026nbsp; $$(\\vec f*\\vec g)_3 = f_1g_2 + f_2g_1 + f_3g_0 = 16$$\n\n\u0026nbsp; $$(\\vec f*\\vec g)_4 = f_2g_2 + f_3g_1 = 17$$\n\n\u0026nbsp; $$(\\vec f*\\vec g)_5 = f_3g_2 = 12$$\n\n不想手算？\n\n```python\nimport numpy as np\nfrom scipy import signal\nsignal.convolve(np.array([1,2,3,4]),np.array([1,2,3]))\n```\n\n### 形象化表示\n\n上面的计算过程，可以看作是——\n\n1. 把 g 向量的**顺序反过来；**\n2. 把 g 的最右一个元素和 f 的最左元素对齐，\n3. 上下两行都有数字的列相乘（也就是把没有数字的地方看作 0），然后把所有乘积相加，得到 f*g 的第一项；\n4. 把 g 向右移动一格\n5. 重复第3、4步\n6. 直到 g 的最左项移动到 f 的最右一个元素。\n\n形如下列各表：\n\n| f |  |  | 1 | 2 | 3 | 4 |\n| g | 3 | 2 | 1 |  |  |  |\n| --- | --- | --- | --- | --- | --- | --- |\n| (f*g)(0) = 1 |  |  | 1 |  |  |  |\n\n\u003chr class=\"slender\"\u003e\n\n| f |  | 1 | 2 | 3 | 4 |\n| g | 3 | 2 | 1 |  |  |\n| --- | --- | --- | --- | --- | --- |\n| (f*g)(1) = 4 |  | 2 | 2 |  |  |\n\n\u003chr class=\"slender\"\u003e\n\n| f | 1 | 2 | 3 | 4 |\n| --- | --- | --- | --- | --- |\n| g | 3 | 2 | 1 |  |\n| (f*g)(2) = 10 | 3 | 4 | 3 |  |\n\n\u003chr class=\"slender\"\u003e\n\n| f | 1 | 2 | 3 | 4 |\n| g |  | 3 | 2 | 1 |\n| --- | --- | --- | --- | --- |\n| (f*g)(3) = 16 |  | 6 | 6 | 4 |\n\n\u003chr class=\"slender\"\u003e\n\n| f | 1 | 2 | 3 | 4 |  |\n| g |  |  | 3 | 2 | 1 |\n| --- | --- | --- | --- | --- | --- |\n| (f*g)(4) = 17 |  |  | 9 | 8 |  |\n\n\u003chr class=\"slender\"\u003e\n\n| f | 1 | 2 | 3 | 4 |  |  |\n| g |  |  |  | 3 | 2 | 1 |\n| --- | --- | --- | --- | --- | --- | --- |\n| (f*g)(5) = 12 |  |  |  | 12 |  |  |\n\n## 机器学习的卷积，是卷积吗？\n\n看论文给出的图 Figure 1.1，在卷积核是灰色 3\\*3 矩阵的情况下，对蓝色 5\\*5 矩阵的卷积就是直接把核对齐到蓝色矩阵上，**并没有把核的元素顺序颠倒过来**。\n\n这玩意能叫卷积吗？\n\n![convolution]({{ site.baseurl }}/assets/photos/2022-12-29-convolution.png)\n\n有人强行挽尊，说我们画图示的时候已经把核给颠倒过来了，想知道卷积核就把灰色小矩阵再颠倒回去——\n\n但是，不颠倒就对齐相乘的运算也是有名字的，叫 cross correlation。核有没有颠倒，convolution 还是 cross correlation 一组合，可以带来升维打击般的混乱，堪比高中化学的“还原剂被氧化，氧化剂被还原”……所以，对于计算机专业的数学水平，不予置评～\n\n（你这样纠缠有意思吗？.jpg）\n\n## 卷积`torch.nn.Conv` 及其各个参数\n\n### `in_channels` \u0026 `out_channels`\n\n“卷积”的意义在于用一种比较省内存的方式，考虑输入张量中各个元素，和空间上相近的邻居元素之间的关系。所以只需要在真的存在空间关系的维度做卷积，其他维度可以留着不动。\n\n比如一张彩色图片，是一个 (颜色*高度*宽度) 的 3 阶张量，我们只需要对高度和宽度两个维度做卷积，颜色就是不参与“卷积”的 channel。\n\n`in_channel` 就是被“卷积”的张量的 channel 数，`out_channel` 是“卷积”结果的 channel 数。比如我们想从一张 RGB 三色图片中分辨出前景和背景两种不同区域，`in_channel=3`, `out_channel=2`。\n\n而 `in_channel` 如何能够与 `out_channel` 取值不同，原理见 Figure 1.3。我们使用 `out_channel` 个不含 channel 维度的“卷积”核，每一个核都与每一个 in channel 做卷积，得到图中的蓝、紫色小矩阵，然后直接把不同的 in channel 暴力求和，得到的结果分别作为卷积结果的 out channel。（这个暴力求和与我以前想得不一样，我以为是什么每一元素都做了一个`in_channel`*`out_channel` 的全联通层）\n\n![channels]({{ site.baseurl }}/assets/photos/2022-12-29-channels.png)\n\n PyTorch 的习惯，对于一个 N 阶“卷积”，参与卷积的是张量的最后 N 阶，`in_channel` 和 `out_channel` 也就是被卷张量和卷积结果的 `Tensor.shape[-(N+1)]`\n\n后面图示的例子都没有考虑 `in_channel` 和 `out_channel` 的数量，也就是都当作 1 了。\n\n### `kernel_size`\n\n就是灰色矩阵“卷积”核，每边有几个数字。如果不同方向的边长不一，该参数就需要用一个 tuple 来表示。Figure 1.1 的灰色卷积核，`kernel_size=(3,3)`\n\n![kernel]({{ site.baseurl }}/assets/photos/2022-12-29-kernel.png)\n\n### `padding` \u0026 `padding_mode`\n\n前面手算例子的时候很鸡贼地把 0 作为向量下标的起点。如果采用日常 1 开头的下标来算，第 1 项结果为零，整个卷积结果的长度会长很多，而且多出来的后面几项也都是零。\n\n而且在这个过程中，我们实际上是把一个有限长度的向量，看作了一个以所有整数 $$\\Z$$ 为定义域的函数，除了那有限的几项之外，其余地方都定义函数值为 0。\n\n用计算机计算的话显然没法如此奢侈地谈“无限多个”，例子中实际用到的，在 $$\\vec f$$ 左右两边各需要 2 个 0，也就是说 `padding=2`, `padding_mode='zeros'`\n\nFigure 1.2 表示的就是 `padding=(1,1)` 的情况（蓝色是被卷张量，白色是 padding，灰色是卷积核，绿色是卷积结果）：\n\n![padding]({{ site.baseurl }}/assets/photos/2022-12-29-padding.png)\n\n既然神经网络中的卷积并不是真正的卷积，所以他们索性不装了——\n\n正常卷积的结果往往比被卷张量大一圈（具体大多少取决于  `kernel_size`, `padding`, `stride` 多个参数），但是图像处理的时候经常希望输出图片和输入图片一样大，此时可以用字符串 `“same”` 作为 `padding` 的参数，自动计算 padding 的大小。`“strict”` 则表示 `padding=0`, 这样输出图片尺寸会变小，但是没有 padding，也就没有往图片里掺杂研究者对图片边缘以外信息的臆测。\n\n同时 `padding_mode` 参数表示往被卷张量四周填充的数字也不一定是 0。比如对于图片，0 往往表示纯黑，而绝大多数图片的视野之外，往往是和图片边缘像素值相差不大的值。所以 `padding_mode` 除了 `zeros` 之外，还接受以下取值：\n\n- `reflect`: 以图片边缘为镜面，把边缘附近的像素值对陈反射出去；\n- `replicate`: 只取边缘的像素值作为常数，直接向外延拓；\n- `circular`: 类似于物理中的周期性边界条件，取对边附近的像素值作为 padding 内容。\n\n### `stride`\n\n前面手算卷积的第4步，把卷积核向右移动了1格，如果每次移动超过1格，就需要这个参数指定移动步长。如果不同方向的步长不同，也是用 tuple 来表示。\n\nFigure 1.4 表示的就是 `stride=(2,2)` 的情况（蓝色是被卷张量，蓝色中的深色块是卷积核，绿色是卷积结果）：\n\n![stride]({{ site.baseurl }}/assets/photos/2022-12-29-stride.png)\n\n### `dilation`\n\n这个参数把“卷积”核撑开，也就相当于在“卷积”核的相邻元素之间加 0。Figure 1.5 表示的就是 `dilation=(1,1)` 的情况（蓝色是被卷张量，蓝色中的深色块是卷积核，绿色是卷积结果）：\n\n![dilation]({{ site.baseurl }}/assets/photos/2022-12-29-dilation.png)\n\n比如 `dilation=1` 时，(1,2,3) 的卷积核就相当于 (1,0,2,0,3)\n\n比如 `dilation=2` 时，(1,2,3) 的卷积核就相当于 (1,0,0,2,0,0,3)\n\n这样可以让卷积核在尺寸比较小的情况下，覆盖到更大面积的被卷张量。当然具体实现时，不可能直接补 0 这么浪费内存。\n\n### `groups`\n\n该参数必须是 `in_channel` 和 `out_channel` 的公约数，当其不为 1 时，就相当于同时做 `groups` 个卷积，其中每个卷积的 `in_channel=in_channel/groups`, `out_channel=out_channel/groups`\n\n### `bias`\n\n该参数是一个布尔值，卷积类似于一种高维空间里的乘法，这个参数就决定是否要拟合 `y=kx+b` 中的 `b`\n\n## “卷积”的“逆运算”： `TransposeConv`\n\n卷积的结果比 padding 之后的被卷张量要小。尤其当“卷积”的 `stride` 约等于 `kernel_size` 时，卷积的就变成了某些池化 (pooling)（求最大值不是一种线性算子，所以最大值池化不能用卷积表示，但是平均值池化可以）。\n\n那么在类似 U-net 这样的模型里，右半边的数据升维（下图中的绿箭头），就需要一种“卷积”的“逆运算”。有人把这种运算叫做 deconvolution，有人叫做 transposed convolution，还有人叫做 convolution with fractional strides。\n\n![Unet]({{ site.baseurl }}/assets/photos/2022-12-29-unet.png)\n\nPyTorch 取的是第二种名字。论文解释了为什么这么取名字，笔记以后有时间再补上把……\n\n因为这个与运算本身就是作为“卷积”的逆运算出现的，所以 PyTorch 的文档里这么说：\n\n\u003e This is set so that when a `Conv2d` and a `ConvTranspose2d` are initialized with same parameters, they are inverses of each other in regard to the input and output shapes.\n\u003e \n\n也就是说，把 `ConvTranspose` 的输入和输出反过来，然后按照 `Conv` 的规则确定各个参数，填入 `ConvTranspose` 的括号里就可以了，除了 `output_padding`\n\n### `output_padding`\n\n`ConvTranspose` 的输出就是对应 `Conv` 的输入。看 Figure 2.7：\n\n![padding_output]({{ site.baseurl }}/assets/photos/2022-12-29-output-padding.png)\n\n当 $$(input+2*padding)/stride$$ 不能整除的时候，最右的几列最下的几行就被卷积核忽略掉了。那么在逆运算 `TransposeConv` 中，这就意味着同一个输入可能对应着 $$stride-1$$ 种可能的输出。`output_padding`参数就可以消除这种歧义，调整 `TransposeConv` 输出张量的尺寸。\n\n## 参考链接\n\n- 给卷积正名: [https://www.kaggle.com/general/225375](https://www.kaggle.com/general/225375)\n- PyTorch Conv2d 源码: [https://pytorch.org/docs/stable/_modules/torch/nn/modules/conv.html#_ConvNd](https://pytorch.org/docs/stable/_modules/torch/nn/modules/conv.html#_ConvNd)\n- 论文: [https://arxiv.org/abs/1603.07285](https://arxiv.org/abs/1603.07285)\n- 动图演示: [https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md](https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md)\n- U-net: [https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/](https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/)\n- PyTorch TransposeConv 文档: [https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html](https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html)\n"},{"slug":"python-decorator","filename":"2022-11-09-python-decorator.md","date":"2022-11-09","title":".py | Python decorator 装饰器","layout":"post","keywords":["md","py"],"excerpt":"所谓装饰器 (decorator)，就是函数前一行 @ 打头的一串字符，是 python 的一种语法糖。","content":"\n最近参加了一个关于如何在 Python 项目中利用 GPU 提高运算效率的培训，里面提到了 `numba` 这个加速科学计算的库，而 `numba` 发挥作用的主要工具就是各种装饰器。\n\n所谓装饰器，就是读一些网上现成的 python 代码的时候会看到的，函数前一行 `@` 打头的一串字符，一般是一个名字，偶尔会附带有参数：\n\n```python\n@decorator\ndef myfunction():\n    # do something...\n    return results\n```\n\n它的实际作用相当于：\n\n```python\ndef myfunction():\n    # do something...\n    return None\n\nmyfunction = decorator(myfunction)\n```\n\nPython 是一种[函数式编程语言](https://program-think.blogspot.com/2012/02/why-choose-python-4-fp.html)，函数和各种类型的变量一样，在 Python 都是一种对象，所以可以把函数赋值给一个变量，可以在函数里定义另一个函数，可以把函数作为参数传递给另一个函数，可以把函数名作为另一个函数的返回值。\n\n`myfunction = decorator(myfunction)` 就是装饰器的定义，是 Python 的一个[语法糖](https://zh.m.wikipedia.org/zh-hans/%E8%AF%AD%E6%B3%95%E7%B3%96)。也就是说装饰器本身也是一个函数，我们的函数被装饰器装饰之后，函数名称不变，在完整实现函数原有功能的同时，额外执行装饰器中的命令。\n\n### 装饰器是如何做到的\n\n要想自己写一个装饰器的话，需要了解一下装饰器的实现原理。一个最简单的装饰器可以这么写：\n\n```python\ndef decorator(func):\n    def inner():\n        # do something\n        func()\n        # do some more\n        return None\n    return inner\n```\n\n也就是在装饰器内部再定义一个函数，这个内部函数的函数体执行被装饰的函数，然后外层装饰器把内层函数名当作返回值。\n\n### 如果一个函数需要多个装饰器\n\n把前面装饰器的定义套在多个装饰器的情况里：\n\n```python\n@decorator1\n@decorator2\ndef myfunction():\n    return None\n\nmyfunction = decorator1(decorator2(myfunction))\n```\n\n### 如果被装饰的函数有传入参数\n\n装饰器不知道自己要装饰的函数长什么样，也就不知道函数接受多少个参数，其中有几个是位置参数，几个是关键词参数。所以需要用单星号打包/解包位置参数，双星号打包/解包关键词参数。`args` 和 `kwargs` 是变量名的代词，可以换成其他自己喜欢的名字。\n\n```python\ndef decorator(func):\n    def inner(*args,**kwargs):\n        # do something\n        func(*args,**kwargs)\n        # something else\n        return None\n    return inner\n\n@decorator\ndef myfunction(x,y,mode=\"normal\",strict=True):\n    # do something...\n    return None\n```\n\n### 如果被装饰的函数有返回值\n\n则装饰器的内层函数需要把被装饰的函数的返回值返回出来：\n\n```python\ndef decorator(func):\n    def inner(*args,**kwargs):\n        # do something\n        func(*args,**kwargs)\n        # something else\n        return func(*args,**kwargs)\n    return inner\n\n@decorator\ndef myfunction(x,y,mode=\"normal\",strict=True):\n    # do something...\n    return results\n```\n\n### 如果想让装饰器本身接受参数\n\n也就是想达到下面的效果：\n\n```python\n@param_decorator(param=\"neat\")\ndef myfunction(x,y,mode=\"normal\",strict=True):\n    # do something...\n    return results\n```\n\n也就是让 `param_decorator(param=\"neat\")` 返回一个装饰器函数，也就是在之前的装饰器外面再加一层：\n\n```python\ndef param_decorator(param):\n    def decorator(func):\n        def inner(*args,**kwargs):\n            if param==\"neat\":\n                print(\"neat\")\n                # ...\n            else:\n                print(\"not neat\")\n                # ...\n            func(*args,**kwargs)\n            return func(*args,**kwargs)\n        return inner\n    return previous_decorator\n```\n\n### 如果想让装饰器既可以接受参数，也可以不接受参数～\n\n实在是有点过于高级了，直接说答案：\n\n```python\ndef flex_decorator(_func=None,*,kw1=\"val1\",kw2=\"val2\"):\n    def decorator(func):\n        def inner(*args,**kwargs):\n            print(kw1,kw2)\n            # do something\n            func(*args,**kwargs)\n            # something else\n            return func(*args,**kwargs)\n        return inner\n    if _func is None:\n        return decorator\n    else:\n        return decorator(_func)\n```\n\n根据 [https://peps.python.org/pep-3102/](https://peps.python.org/pep-3102/)，`*`作为一个单独的函数参数，表示后面所有的参数都是关键词参数，用来限定星号前面位置参数的数量。\n\n- 当 `@flex_decorator` 不加参数使用的时候:\n    - 根据定义 `myfunction = flex_decorator(myfunction)`\n    - `_func=myfunction`\n    - 此时 `else` 生效，`myfunction = decorator(myfunction)`\n- 当 `@flex_decorator(kw1=\"val1\",kw2=\"val2\")` 加上参数使用的时候:\n    - 根据定义 `myfunction = flex_decorator(kw1=\"val1\",kw2=\"val2\")(myfunction)`\n    - `_func=None`\n    - 此时 `if` 生效，`myfunction = decorator(myfunction)`\n\n——**接受的参数必须是关键词参数**，否则和被装饰的函数名无法区分。\n\n### `@functools.wraps`：刻章、办证\n\n以上各节基本完成了常用场景下装饰器的功能。\n\n但是，Python 作为一种动态语言，一大特征就是可以在运行时进行[类型内省](https://en.wikipedia.org/wiki/Type_introspection)。而按照我们上面的写法，被装饰之后的函数，Python 认为它不再是原来的函数，而是装饰器里面定义的那个内部函数，这样可能会出现意想不到的问题。\n\n解决方法是使用一个专门的装饰器根装饰器定义的内部函数办个假身份：\n\n```python\nimport functools\n\ndef decorator(func):\n    @functools.wraps(func)\n    def inner(*args,**kwargs):\n        # do something\n        func(*args,**kwargs)\n        # something else\n        return func(*args,**kwargs)\n    return inner\n```\n"},{"slug":"pytorch-dataset-dataloader-sampler","filename":"2022-09-01-pytorch-dataset-dataloader-sampler.md","date":"2022-09-01","title":".py | PyTorch 数据处理方面的封装","layout":"post","keywords":["md","py","ai"],"excerpt":"一般监督学习的数据结构和处理过程，以及 PyTorch 对上述结构和处理过程的封装","content":"\n## 一般监督学习的数据结构和处理过程\n\n### 训练集、验证集、测试集\n\n所有数据整体构成一个大集合，这个集合的每一个元素都包含一个输入和一个目标，分别记作 x 和 y。\n\n把这个大集合分成互相没有交集的三个子集，分别是训练集 (training set)、验证集 (validation set)、测试集 (test set)。\n\n- 训练集和验证集在训练过程中使用。\n    - 训练集的数据带入模型时，模型处于训练模式，模型输出对参数的导数被记录。通过比较把“模型输出”和“训练目标 y”代入**损失函数**的损失，更新模型的参数。同时记录“模型输出”和“训练目标 y”带入验证函数的结果，和验证集比较。\n    - 验证集的数据代入模型时，模型处于求值模式，模型只根据输入计算输出，对参数的导数不记录。通过观察“模型输出”和“训练目标 y”带入**验证函数**的结果，观察训练是否陷入“过拟合”。当训练集的验证函数结果不断下降，但是验证集的验证函数结果几乎不变时，可以认为模型过拟合。\n- 测试集在训练完成之后使用，代入模型时，模型处于求值模式。用于评价训练结果的好坏。\n\n### epoch vs. batch\n\n如果把所有数据同时进行训练，所需要的空间一般都大于电脑内存。所以一般会将训练集随机分成若干批次 (batch)，一个批次的数据同时塞入模型进行训练，在一个 batch 里每一个模型输出对参数的导数累加在一起，整个 batch 结束后更新模型参数，同时导数清零。因为 batch 这个概念和内存有关，所以数值一般选择为 2 的指数。\n\n将训练集所有的 batches 跑完一次称为而一个 epoch。一次训练一般需要很多 epochs，直到损失函数结果足够低，或验证集显示出现过拟合。\n\n## PyTorch 对上述结构和处理过程的封装\n\n### `Dataset`\n\n前面已经说了，数据集包括输入和目标两部分，`Dateset` 及其子类的作用就是\n\n如果在把数据装入 `Dataset` 之前就已经是规整的两个张量了的话——\n\n```python\nimport torch\nfrom torch.utils import data\n\n# ...\n\nfor x,y in zip(train_x,train_y):\n    # do something with x and y\n\ntrainset = TensorDataset(train_x,train_y)\nfor x,y in trainset:\n    # do something with x and y\n```\n\n——这一步确实没什么意思。\n\n有意思的地方在于可以自己写一个数据集类，继承 `Dataset`，然后重载 `__getitem__()` 和 `__len__()` 方法，这样可以把一些不适合用张量表示的数据塞进 `Dataset` 里面，对图像进行学习的话可以在此处加入图像增强的步骤，并进一步用于 `DataLoader`\n\n### `DataLoader`\n\n`DataLoader` = `Dataset` + `Sampler`，因为一般的教程里只需要讲数据集进行简单随机划分，也就只用到了 `batch_size` 等等参数，用到 Sampler 的地方很少。\n\n最常见的用例就是 `WeightedRandomSampler` 。训练分类器的时候，有时其中一个类别的数据远少于其他，那么训练器就更难判断出这一分类（因为只要无脑排除这个类别就能获得不错的正确率），所以需要平衡不同组别之间的权重。\n\n```python\nlist(WeightedRandomSampler(weights=[0.1, 0.9, 0.4, 0.7, 3.0, 0.6], num_samples=5, replacement=True))\n# [4, 4, 1, 4, 5]\nlist(WeightedRandomSampler(weights=[0.9, 0.4, 0.5, 0.2, 0.3, 0.1], num_samples=5, replacement=False))\n# [0, 1, 4, 3, 2]\n```\n\n平衡完之后转化为 batch，搭配 `BatchSampler`：\n\n```python\nlist(BatchSampler(WeightedRandomSampler(weights=[0.1, 0.9, 0.4, 0.7, 3.0, 0.6], num_samples=5, replacement=True), batch_size=2, drop_last=False))\n# [[4, 4], [1, 4], 5]\nlist(BatchSampler(WeightedRandomSampler(weights=[0.1, 0.9, 0.4, 0.7, 3.0, 0.6], num_samples=5, replacement=True), batch_size=2, drop_last=True))\n# [[0, 1], [4, 3]]\n```\n\n### 汇总一下\n\n```python\nimport torch\nfrom torch.utils import data\n\ntrain_x = torch.rand((100,5))\ntrain_y = torch.rand((100,2))\ntrainset = data.TensorDataset(train_x,train_y)\n\n# either:\ntrainloader = data.DataLoader(\n    trainset,\n    batch_size=2,\n    drop_last=True,\n    sampler=data.WeightedRandomSampler(\n        weights=[0.1, 0.9, 0.4, 0.7, 3.0, 0.6], \n        num_samples=5, \n        replacement=True))\n# or:\ntrainloader = data.DataLoader(\n    trainset,\n    batch_sampler=data.BatchSampler(\n        data.WeightedRandomSampler(\n            weights=[0.1, 0.9, 0.4, 0.7, 3.0, 0.6], \n            num_samples=5, \n            replacement=True), \n        batch_size=2, \n        drop_last=True))\n\nfor epoch in range(100):\n    for x,y in trainloader:\n        train(model,x,y,loss_function)\n```\n\n需要注意的是，`for x,y in trainset` 的 x 和 y 的维度是单个数据的维度，最简单的情况就是是 P 和 Q 维向量，而此时如果把 batch_size 记作 B，`for x,y in trainloader` 中的 x 和 y 是维度分别为 (B,P) 和 (B,Q) 的矩阵。`train()` 函数里面的计算要考虑到多出的这一个维度。\n\n## 参考链接：\n\n- [https://pytorch.org/tutorials/beginner/basics/data_tutorial.html](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html)\n- [https://pytorch.org/tutorials/beginner/nn_tutorial.html](https://pytorch.org/tutorials/beginner/nn_tutorial.html)\n- [https://pytorch.org/tutorials/beginner/ptcheat.html](https://pytorch.org/tutorials/beginner/ptcheat.html)\n- [https://pytorch.org/docs/stable/data.html?highlight=dataset#torch.utils.data.Dataset](https://pytorch.org/docs/stable/data.html?highlight=dataset#torch.utils.data.Dataset)\n- [https://pytorch.org/docs/stable/data.html?highlight=dataloader#torch.utils.data.DataLoader](https://pytorch.org/docs/stable/data.html?highlight=dataloader#torch.utils.data.DataLoader)\n- [https://pytorch.org/docs/stable/data.html?highlight=dataloader#torch.utils.data.Sampler](https://pytorch.org/docs/stable/data.html?highlight=dataloader#torch.utils.data.Sampler)\n- [https://pytorch.org/docs/stable/_modules/torch/utils/data/dataset.html#Dataset](https://pytorch.org/docs/stable/_modules/torch/utils/data/dataset.html#Dataset)\n"},{"slug":"knowledge-structure-machine-learning-image-processing-summer-school","filename":"2022-08-17-knowledge-structure-machine-learning-image-processing-summer-school.md","date":"2022-08-17","title":".py | 深度学习暑期学校知识点","layout":"post","keywords":["md","py","ai"],"excerpt":"机器学习在图像处理当中的应用，知识结构树","content":"\n- 电脑设置、python 入门\n    - NoMachine, ssh, python, conda, jupyter\n    - 文件夹操作 `pathlib`, 图片操作 `skimage`\n    - 数据增强 (data augmentation): `imgaug`\n    - TensorBoard\n- 机器学习简介\n    - Linear Classifier\n    - `sklearn**.**model_selection**.**train_test_split()`\n    - Stochastic gradient descent\n    - TensorFlow:\n        - `tensorflow_addons as tfa`\n        - `tfa.image.rotate()`, `tf.image.random_flip_left_right()`\n        - **`from** tensorflow.keras **import** Model`, **`from** tensorflow.keras.models **import** Sequential`, **`from** tensorflow.keras.layers **import** Input, Flatten, Dense, Activation, BatchNormalization, Conv2D, MaxPool2D, Softmax`\n        - `tf**.**keras**.**losses**.**CategoricalCrossentropy()`, `tf**.**keras**.**optimizers**.**Adam(lr**=**1e-3, clipnorm**=**0.001)`\n        - `linear_classifier **=**``Model(...)`, `linear_classifier.compile()`, `linear_classifier.fit()`, `linear_classifier.predict()`\n- 深度学习简介\n    - Perceptron\n    - Perceptron-based XOR gate\n    - **decision boundary** of your model: `np.meshgrid`\n- 图像恢复 (image restoration)\n    - CARE network\n    - Noise2Nosie, Noise2Void\n- 图像翻译 (image translation)\n    - micro-DL: a tool to generate and train U-net from config files.\n- 图像语义分割 (image semantic segmentation): 比较详细，前两节有点水了。\n    - **`from** PIL **import** Image`, **`import** imageio`, **`from** torchvision **import** transforms`\n    - **`from** torch.utils.data **import** Dataset, DataLoader`, **`import** torch.nn **as** nn`, **`from** torch.nn **import** functional **as** F`,  **`from** torch.utils.tensorboard **import** SummaryWriter`,\n    - U-net on PyTorch\n- 图像实例分割 (instance segmentation)\n    1. Foreground segmentation: \n        - **Receptive Field of View**: The term is borrowed from biology where it describes the \"portion of sensory space that can elicit neuronal responses when stimulated\" (wikipedia). Each output pixel can look at/depends on an input patch with that diameter centered at its position. Based on this patch, the network has to be able to make a decision about the prediction for the respective pixel.\n        - **Early Stopping** to avoid overfitting: define an `EarlyStopping` class\n    2. Instance Segmentation:\n        - Ideas:\n            - Three-class model (foreground, background, boundary),\n            - Distance transform (label for each pixel is the distance to the closest boundary),\n            - Edge affinity (consider not just the pixel but also its direct neighbors, predicts the probability that there is an edge, this is called affinity.) 听的时候懂了，回来看的时候没太看懂\n            - Metric learning (learns to predict an embedding vector for each pixel.)\n    3. **Tile and Stitch：**\n        - 当需要处理的图片过大时，将图片切分成多个小图，分别预测之后拼接在一起。\n        - 文中说图片尺寸不是 某个参数的整数倍的时候拼贴结果会不连续，但是代码注释中说等于这个整数倍的时候会不连续，晕。\n        - [https://arxiv.org/pdf/2101.05846.pdf](https://arxiv.org/pdf/2101.05846.pdf)\n    4. 一个实例，epithelia cells\n- 失败模式：极其之水，就是科普了一下训练参数错误的后果，以及一点对抗学习的内容\n- 追踪：比较水，因为机器学习追踪的运算量极大，且主讲人感觉就是来做广告的，所以就直接用 CoLab 体验了一下就完事了。（就这还加州理工呢~）\n- 知识提取：\n    - 前面的基本上是从像素到像素的映射，这里的知识提取是从图片到标签的映射。\n    - CycleGAN\n    - **Create a balanced Dataloader**\n    "},{"slug":"what-a-PyTorch-project-looks-like","filename":"2022-08-17-what-a-PyTorch-project-looks-like.md","date":"2022-08-17","title":".py | 一个 PyTorch 机器学习项目长什么样","layout":"post","keywords":["md","py","ai"],"excerpt":"官网的一个pytorch教程的笔记，原文先按照第一性原理，尽量用原生 python 写了一遍，然后一步一步重构成接近生产环境的代码。这里我把顺序反过来，先放出重构之后的最终结果","content":"\n自学，或者说一切学习和教学，本质就是在已经掌握的知识和未知的目标知识之间修路。路有两种修法，一是理论或者说是第一性原理路线，从不证自明的公理或者已经掌握的知识出发，通过逻辑推理一步步得到新的知识；另一种是实践或者说工程师路线，拿到一个已经可以工作的产品，划分成各个子系统，通过输入的改变来观察输出的不同，直到子系统简化到自己可以理解的地步，不再是黑箱，借此了解整个系统的功能。\n\n但是当学习的对象复杂到一定程度之后，凭借一个人的自学能力，只用其中一种方法往往难以钻透。又或者两种方法学到的路线并非同一条路。对于机器学习，理论路线就是“让输入数据通过一个带有超多参数的函数，根据函数返回值和输出数据之间的差别修正参数，直到函数能够近似输入数据和输出数据之间的关系”；实践中代码往往会使用很多库作者封装好的函数，只读源码往往一头雾水。\n\n所以，看到 PyTorch 官网的这篇教程 **WHAT IS TORCH.NN *REALLY*?:** [https://pytorch.org/tutorials/beginner/nn_tutorial.html](https://pytorch.org/tutorials/beginner/nn_tutorial.html) 可以说是喜出望外，把两种路线写出的代码都给了出来，对于自学者来说，就像罗塞塔石碑一样可以互相对照。这里我把 CNN 相关的部分抽掉了，毕竟 CNN 只是深度学习的一个子集，深度学习只是机器学习的一个子集，和这篇文章的主题关系不大。\n\n原文先按照第一性原理，尽量用原生 python 写了一遍，然后一步一步重构成接近生产环境的代码。这里我把顺序反过来，先放出重构之后的最终结果：\n\n```python\nfrom pathlib import Path\nimport requests\nimport pickle\nimport gzip\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\nfrom torch import optim\nfrom torch.utils.data import TensorDataset,DataLoader\n\n# Using GPU\n\nprint(torch.cuda.is_available())\ndev = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n\n# Wrapping DataLoader\n# https://pytorch.org/tutorials/beginner/basics/data_tutorial.html?highlight=dataloader\n# https://pytorch.org/tutorials/beginner/data_loading_tutorial.html?highlight=dataloader\n\ndef preprocess(x, y):\n    return x.view(-1, 1, 28, 28).to(dev), y.to(dev)\n\ndef get_data(train_ds, valid_ds, bs):\n    return (\n        DataLoader(train_ds, batch_size=bs, shuffle=True),\n        DataLoader(valid_ds, batch_size=bs * 2),\n    )\n\nclass WrappedDataLoader:\n    def __init__(self, dl, func):\n        self.dl = dl\n        self.func = func\n\n    def __len__(self):\n        return len(self.dl)\n\n    def __iter__(self):\n        batches = iter(self.dl)\n        for b in batches:\n            yield (self.func(*b))\n\n# Define the neural network model to be trained\n\n# # If the model is simple:\n# model = nn.Sequential(nn.Linear(784, 10))\n\n# generally the model is a class that inherites nn.Module and implements forward()\nclass Mnist_Logistic(nn.Module):\n    def __init__(self):\n        super().__init__()\n        # self.weights = nn.Parameter(torch.randn(784, 10) / math.sqrt(784))\n        # self.bias = nn.Parameter(torch.zeros(10))\n        self.lin = nn.Linear(784, 10)\n\n    def forward(self, xb):\n        # return xb @ self.weights + self.bias\n        return self.lin(xb)\n\n# Define the training pipeline in fit()\n\ndef loss_batch(model, loss_func, xb, yb, opt=None):\n    loss = loss_func(model(xb), yb)\n\n    if opt is not None:\n        loss.backward()\n        opt.step()\n        opt.zero_grad()\n\n    return loss.item(), len(xb)\n\ndef fit(epochs, model, loss_func, opt, train_dl, valid_dl):\n    for epoch in range(epochs):\n        model.train()\n        for xb, yb in train_dl:\n            loss_batch(model, loss_func, xb, yb, opt)\n\n        model.eval()\n        with torch.no_grad():\n            losses, nums = zip(\n                *[loss_batch(model, loss_func, xb, yb) for xb, yb in valid_dl]\n            )\n        val_loss = np.sum(np.multiply(losses, nums)) / np.sum(nums)\n\n        print(epoch, val_loss)\n    return None\n\n# __main()__:\n\n# data\nDATA_PATH = Path(\"data\")\nPATH = DATA_PATH / \"mnist\"\n\nPATH.mkdir(parents=True, exist_ok=True)\n\nURL = \"https://github.com/pytorch/tutorials/raw/master/_static/\"\nFILENAME = \"mnist.pkl.gz\"\n\nif not (PATH / FILENAME).exists():\n        content = requests.get(URL + FILENAME).content\n        (PATH / FILENAME).open(\"wb\").write(content)\nwith gzip.open((PATH / FILENAME).as_posix(), \"rb\") as f:\n        ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding=\"latin-1\")\n\nx_train, y_train, x_valid, y_valid = map(\n    torch.tensor, (x_train, y_train, x_valid, y_valid)\n)\n\ntrain_dataset = TensorDataset(x_train, y_train)\nvalid_dataset = TensorDataset(x_valid, y_valid)\ntrain_dataloader, valid_dataloader = get_data(train_ds, valid_ds, bs)\ntrain_dataloader = WrappedDataLoader(train_dataloader, preprocess)\nvalid_dataloader = WrappedDataLoader(valid_dataloader, preprocess)\n\n# hyperparameters/model\nlearning_rate = 0.1\nepochs = 2\nloss_function = F.cross_entropy # loss function\nmodel = Mnist_CNN()\nmodel.to(dev)\noptimizer = optim.SGD(model.parameters(), lr=learning_rate , momentum=0.9)\n\n# training\nfit(epochs, model, loss_function, optimizer, train_dataloader, valid_dataloader)\n```\n\n可以看到，一个项目主干可以分成4部分：\n\n1. 准备数据\n2. 定义模型\n3. 描述流程\n4. 实际运行\n\n下面把各部分拆分开来，把两种思路的代码进行对比。\n\n## 1. 准备数据\n\n### 重构之前\n\n```python\nDATA_PATH = Path(\"data\")\nPATH = DATA_PATH / \"mnist\"\n\nPATH.mkdir(parents=True, exist_ok=True)\n\nURL = \"https://github.com/pytorch/tutorials/raw/master/_static/\"\nFILENAME = \"mnist.pkl.gz\"\n\nif not (PATH / FILENAME).exists():\n        content = requests.get(URL + FILENAME).content\n        (PATH / FILENAME).open(\"wb\").write(content)\nwith gzip.open((PATH / FILENAME).as_posix(), \"rb\") as f:\n        ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding=\"latin-1\")\n\nx_train, y_train, x_valid, y_valid = map(\n    torch.tensor, (x_train, y_train, x_valid, y_valid)\n)\nn, c = x_train.shape\n```\n\n### 重构以后：\n\n```python\n# Wrapping DataLoader\n# https://pytorch.org/tutorials/beginner/basics/data_tutorial.html?highlight=dataloader\n# https://pytorch.org/tutorials/beginner/data_loading_tutorial.html?highlight=dataloader\n\ndef preprocess(x, y):\n    return x.view(-1, 1, 28, 28).to(dev), y.to(dev)\n\ndef get_data(train_ds, valid_ds, bs):\n    return (\n        DataLoader(train_ds, batch_size=bs, shuffle=True),\n        DataLoader(valid_ds, batch_size=bs * 2),\n    )\n\nclass WrappedDataLoader:\n    def __init__(self, dl, func):\n        self.dl = dl\n        self.func = func\n\n    def __len__(self):\n        return len(self.dl)\n\n    def __iter__(self):\n        batches = iter(self.dl)\n        for b in batches:\n            yield (self.func(*b))\n```\n\n## 2. 定义模型\n\n### 重构之前\n\n```python\nweights = torch.randn(784, 10) / math.sqrt(784)\nweights.requires_grad_()\nbias = torch.zeros(10, requires_grad=True)\n\ndef log_softmax(x):\n    return x - x.exp().sum(-1).log().unsqueeze(-1)\n\ndef model(xb):\n    return log_softmax(xb @ weights + bias)\n\ndef nll(input, target):\n    return -input[range(target.shape[0]), target].mean()\nloss_func = nll\n\ndef accuracy(out, yb):\n    preds = torch.argmax(out, dim=1)\n    return (preds == yb).float().mean()\n```\n\n### 重构以后\n\n```python\n# If the model is simple:\nmodel = nn.Sequential(nn.Linear(784, 10))\n\n# generally the model is a class that inherites nn.Module and implements forward()\nclass Mnist_Logistic(nn.Module):\n    def __init__(self):\n        super().__init__()\n        # self.weights = nn.Parameter(torch.randn(784, 10) / math.sqrt(784))\n        # self.bias = nn.Parameter(torch.zeros(10))\n        self.lin = nn.Linear(784, 10)\n\n    def forward(self, xb):\n        # return xb @ self.weights + self.bias\n        return self.lin(xb)\n\n```\n\n## 3. 描述流程\n\n### 重构之前\n\n```python\nlr = 0.5  # learning rate\nepochs = 2  # how many epochs to train for\n\nfor epoch in range(epochs):\n    for i in range((n - 1) // bs + 1):\n        #         set_trace()\n        start_i = i * bs\n        end_i = start_i + bs\n        xb = x_train[start_i:end_i]\n        yb = y_train[start_i:end_i]\n        pred = model(xb)\n        loss = loss_func(pred, yb)\n\n        loss.backward()\n        with torch.no_grad():\n            weights -= weights.grad * lr\n            bias -= bias.grad * lr\n            weights.grad.zero_()\n            bias.grad.zero_()\n```\n\n### 重构以后\n\n```python\n\ndef loss_batch(model, loss_func, xb, yb, opt=None):\n    loss = loss_func(model(xb), yb)\n\n    if opt is not None:\n        loss.backward()\n        opt.step()\n        opt.zero_grad()\n\n    return loss.item(), len(xb)\n\ndef fit(epochs, model, loss_func, opt, train_dl, valid_dl):\n    for epoch in range(epochs):\n        model.train()\n        for xb, yb in train_dl:\n            loss_batch(model, loss_func, xb, yb, opt)\n\n        model.eval()\n        with torch.no_grad():\n            losses, nums = zip(\n                *[loss_batch(model, loss_func, xb, yb) for xb, yb in valid_dl]\n            )\n        val_loss = np.sum(np.multiply(losses, nums)) / np.sum(nums)\n\n        print(epoch, val_loss)\n    return None\n```\n\n## 4. 实际运行\n\n### 重构之前\n\n```python\n# __main()__:\nprint(loss_func(model(xb), yb), accuracy(model(xb), yb))\n```\n\n### 重构以后\n\n```python\n# __main()__:\n\n# data\nDATA_PATH = Path(\"data\")\nPATH = DATA_PATH / \"mnist\"\n\nPATH.mkdir(parents=True, exist_ok=True)\n\nURL = \"https://github.com/pytorch/tutorials/raw/master/_static/\"\nFILENAME = \"mnist.pkl.gz\"\n\nif not (PATH / FILENAME).exists():\n        content = requests.get(URL + FILENAME).content\n        (PATH / FILENAME).open(\"wb\").write(content)\nwith gzip.open((PATH / FILENAME).as_posix(), \"rb\") as f:\n        ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding=\"latin-1\")\n\nx_train, y_train, x_valid, y_valid = map(\n    torch.tensor, (x_train, y_train, x_valid, y_valid)\n)\n\ntrain_dataset = TensorDataset(x_train, y_train)\nvalid_dataset = TensorDataset(x_valid, y_valid)\ntrain_dataloader, valid_dataloader = get_data(train_ds, valid_ds, bs)\ntrain_dataloader = WrappedDataLoader(train_dataloader, preprocess)\nvalid_dataloader = WrappedDataLoader(valid_dataloader, preprocess)\n\n# hyperparameters/model\nlearning_rate = 0.1\nepochs = 2\nloss_function = F.cross_entropy # loss function\nmodel = Mnist_CNN()\nmodel.to(dev)\noptimizer = optim.SGD(model.parameters(), lr=learning_rate , momentum=0.9)\n\n# training\nfit(epochs, model, loss_function, optimizer, train_dataloader, valid_dataloader)\n```"},{"slug":"python-subprocess-run-bash","filename":"2022-07-08-python-subprocess-run-bash.md","date":"2022-07-08","title":".py | python.subprocess执行bash命令","layout":"post","keywords":["md","py"],"excerpt":"让 python 读 bash 的命令结果，写 bash 的命令语句。","content":"\n笔记本的触摸屏被我摔了道裂纹，一开始还不影响使用，但是最近几周情况恶化，有时鼠标光标会突然暴走，不听指挥。所以需要禁用触摸屏作为输入设备。\n\n在 xorg 的桌面环境之下，可以用 `xinput list` 显示出所有输入设备，以及对应的 id 号码。然后把找到的 id 填入 `xinput disable ##` 就可以了。一般来说这个 id 的数值是稳定的，所以我就直接把禁用命令写到 `~/.bashrc` 里面去了。\n\n然而，最近把吃灰很久的树莓派拿出来玩了，所以败家买了个60%布局的小机械键盘，小键盘往笔记本一插，诶，您猜怎么着，新买的键盘输完密码登陆之后就不能用了，着实吓了一跳。\n\n所以需要每次检查一遍输入设备的 id，然后把和触摸屏有关的 id（不止一个）从 `xinput list` 的输出里摘出来赋值给一个变量，然后把变量带入 `xinput disable #` 里面。这一套操作已经超出我的 bash 能力了，所以考虑用 python 完成中间步骤，也就是需要让 python 读 bash 的命令结果，写 bash 的命令语句。\n\nGoogles搜索给到了这个结果：[https://stackoverflow.com/questions/4256107/running-bash-commands-in-python](https://stackoverflow.com/questions/4256107/running-bash-commands-in-python)，稍微看了一下 subprocess 的官方文档，写了下面的一段，存到 `~/disable_touchscreen.py`, 然后在 `~/.bashrc` 里加一句 `python ~/disable_touchscreen.py`\n\n```python\nimport subprocess\ncheck = subprcess.run([\"xinput\",\"list\"],capture_output=True)\nfor line in check.stdout.decode(\"utf-8\").split(\"\\n\"):\n    if \"touchscreen\" in line:\n        device = line.partition(\"id=\")[2].partition(\"\\t\")[0]\n        disable = subprocess.run([\"xinput\",\"disable\",str(device)])\n        if disable.returncode==0:\n            print(f\"Successfully disabled touchscreen device {device}\")\n        else:\n            print(f\"Failed to disable touchscreen device {device}\")\n```\n\n- 这段代码的核心是 `subprocess.run()` ，第一个参数就是传给 bash 的命令，这是一个list，其中每个元素就是 bash 语句用空格分割开的每一部分。要想得到命令的执行结果，需要添加参数 `capture_output=True`\n- 上面函数的返回值是一个特殊的数据结构，命令顺利执行的话，结果会写在 `.stdout` 里，这是一个二进制串 `b'xxx...'`，所以需要 `.decode(\"utf-8\")` 转化成字符串。\n\n如果只用bash的话，\n\n- 我的 bash 水平可以做到 `xinput list | grep \"touchscreen\"`, 这里的 `|` 是一个管道，也就是将前一条命令的输出传递给后一条作为输入。要想在 python 里使用管道，可以看这个问答：[https://stackoverflow.com/questions/13332268/how-to-use-subprocess-command-with-pipes](https://stackoverflow.com/questions/13332268/how-to-use-subprocess-command-with-pipes)\n- 上面管道的结果还需要裁剪出 id 号，也就是 `line.partition(\"id=\")[2].partition(\"\\t\")[0]`，估计需要用到 awk, 虽然难，但可以学，至少知道该学什么；\n- 但是把 id 号传递给一个变量，然后把这个变量填进 `xinput disable`, 这一步就连该学什么也不知道了。\n"},{"slug":"mimic-mathematica-with-wolfram-engine-and-vscode","filename":"2022-06-19-mimic-mathematica-with-wolfram-engine-and-vscode.md","date":"2022-06-19","title":".nb | Mathematica 入门：免费正版、vscode、近似原生体验","layout":"post","keywords":["md","nb","m"],"excerpt":"不用算号器，完全合法的免费手段搭建一个免费的 Wolfram Languange 运行环境，效果尽可能贴近 Mathematica。","content":"\n## Mathematica 的原生体验暨山寨目标\n\n本科的时候老师总是跟我们念叨，让我们学点科学计算软件，可学的不多，不过 MatLab, Mathematica, Maple, Origin 和 Labview.  ~~作为编程语言的 MatLab 是世界上语法最垃圾的（没有之一）~~ ，Maple 实在是太小众了，Origin 和 Labview 不仅应用场景有限而且繁琐还有版权问题，于是 MMA 就成了我主要的折腾对象。不敢说拿手，起码是略懂。\n\nWolfram 和 python 一样也是[动态语言和解释型语言](python-interpreter-editor-virtualenv)，而且默认的新建文件类型就是 `.nb` 笔记本文件，命令按块执行，输出结果直接显示在代码块下方，强烈怀疑 Jupyter Notebook 就是山寨了 Mathematica。当然另有一种 `.m` 文件，用于执行文件内的所有命令，适用于比较大型的独立应用。\n\n![]({{ site.baseurl }}/assets/photos/2022-06-19-mathematica-notebook.gif)\n\n虽然现在的学校给学生买了正版许可证，但是只能用在一台电脑上，所以笔记本上就安装不了。虽然百度贴吧的精华帖里有传统艺能算号器教程，但是现在 Wolfram 开放了免费的 Wolfram Engine，所以我们还是来点正大光明的，用完全合法的免费手段搭建一个免费的 Wolfram Languange 运行环境，效果尽可能贴近 Mathematica。\n\n需要用到的工具有：\n\n- Wolfram Engine\n- Wolfram Script\n- Wolfram Engine For Jupyter\n- jupyter\n- vscode\n\n## Wolfram Engine 和 Wolfram Script 下载和安装\n\n《How do I install Mathematica on Linux?》：[https://support.wolfram.com/12453](https://support.wolfram.com/12453)\n\n在 Google 上搜索“Wolfram Engine”后可以找到官网的下载地址：[https://www.wolfram.com/engine/](https://www.wolfram.com/engine/)\n\n根据自己的操作系统点击下载之后会弹出获取许可证的页面 ([https://account.wolfram.com/access/wolfram-engine/free](https://account.wolfram.com/access/wolfram-engine/free))，没有 Wolfram 账号的需要注册一个账号。\n\n完成之后在下载文件夹打开 terminal, 输入以下命令，其中 xyz 是下载文件名中的版本号：\n\n```python\nsudo bash WolframEngine_xxx.yy.zz_LINUX\n```\n\n强烈建议按照默认设置完成安装，不做要任何个性化的调整，理由见下方引用块。\n\n\u003e 我把第二个选项，也就是 Wolfram Engine 可执行文件的路径设置成了自己 home 下安放一般独立软件的文件夹，结果激活过程出现了问题：输入 `wolframscript` 之后说找不到 WoflramEngine，填入自己的路径之后提示 Wolfram Engine 尚未激活，手动启动 `WolframEngine` 之后提示输入激活密钥 activation key，但是各处遍寻不得。\n解决方法来自以下 StackOverflow 回答：[https://mathematica.stackexchange.com/questions/198822/the-wolfram-kernel-must-be-activated-for-wolframscript-to-use-it](https://mathematica.stackexchange.com/questions/198822/the-wolfram-kernel-must-be-activated-for-wolframscript-to-use-it)\n在wolfram官网登陆自己的账号之后，在一个新的标签页输入以下网址 [https://www.wolframcloud.com/users/user-current/activationkeys](https://www.wolframcloud.com/users/user-current/activationkeys)，即可看到自己的 activation key，在 terminal 中打开 Wolfram Engine，根据提示把 activation key 复制粘贴到指定位置，即可完成激活。\n但是第二天配置好 vscode 和 Jupyter 之后，再次在命令行打开 WolframScript 的时候提示激活失败，重新按照上述方法操作后，显示 activation key 已被使用。即便是删除后按照默认设置重装，也依然会提示超过许可证限制。\n后来在官网给出的联系方式给客服发了消息，客服回信给了新的激活码。\n\u003e \n\n再下载 [WolframScript](https://account.wolfram.com/products/downloads/wolframscript)，这是 Wolfram 的前端。然后按照各个操作系统自己的规矩安装 WolframScript，我的 fedora 就是双击 rpm 文件然后根据提示操作。完成后在命令行输入 `wolframscript`, 根据提示输入 Wolfram 账号和密码，Wolfram Engine 就会联网激活自己。\n\n激活成功之后，在 terminal 输入 `wolframscript`, 显示的结果如下，即说明 Wolfram Engine 和 Wolfram Script 配置成功。\n\n```bash\nWolfram Language 13.0.1 Engine for Linux x86 (64-bit)\nCopyright 1988-2022 Wolfram Research, Inc.\n\nIn[1]:=\n```\n\n在 `In[1]:=` 处输入 `Exit[]` 并按回车，即可退出 Wolfram 回到命令行。\n\n## 将 Wolfram Engine 设为 Jupyter 的后端\n\n[https://github.com/WolframResearch/WolframLanguageForJupyter](https://github.com/WolframResearch/WolframLanguageForJupyter)\n\n根据上面网址的指示，将官方 repo 克隆到本地，因为我们的 python 分隔成了多个[虚拟环境](https://virtual.env)，所以比官网教程多一步 `workon base`:\n\n```bash\n[me@my_computer dev]$ git clone https://github.com/WolframResearch/WolframLanguageForJupyter.git\n# Cloning into 'WolframLanguageForJupyter'...\n# remote: Enumerating objects: 649, done.\n# remote: Counting objects: 100% (140/140), done.\n# remote: Compressing objects: 100% (52/52), done.\n# remote: Total 649 (delta 93), reused 126 (delta 88), pack-reused 509\n# Receiving objects: 100% (649/649), 321.55 KiB | 2.23 MiB/s, done.\n# Resolving deltas: 100% (411/411), done.\n[me@my_computer dev]$ cd WolframLanguageForJupyter\n[me@my_computer WolframLanguageForJupyter]$ workon base\n(base) [me@my_computer WolframLanguageForJupyter]$ ./configure-jupyter.wls add\n(base) [me@my_computer WolframLanguageForJupyter]$\n```\n\n## 用法和效果\n\n打开 vscode，按 `Ctrl+Shift+P` 呼出命令搜索框，找到 \"Jupyter: Create Interactive Window\":\n\n![]({{ site.baseurl }}/assets/photos/2022-06-19-jupyter-start.png)\n\n单击 Jupyter 后端内核的图表（下图中右上角的\"base(Python 3.9.12)\"字样），把内核切换为 \"Wolfram Language ##\"\n\n![]({{ site.baseurl }}/assets/photos/2022-06-19-switch-kernel.png)\n\n等待后端内核切换完成，就可以输入 Mathematica 命令查看效果了：\n\n![]({{ site.baseurl }}/assets/photos/2022-06-19-final-result.png)\n\n完成！\n"},{"slug":"R-install-and-simple-syntax","filename":"2022-04-29-R-install-and-simple-syntax.md","date":"2022-04-29","title":".r | R语言入门笔记","layout":"post","keywords":["md","r","mpipks-note"],"excerpt":"本文也是马克思·普朗克复杂物理研究所《非平衡态集体过程》第9讲的笔记。","content":"\n\u003e 本文也是马克思·普朗克复杂物理研究所《非平衡态集体过程》第9讲的笔记。\n\u003e \n\n最近在蹭一门数学系的读书课，题目叫“非线性时间序列分析”。本来以为会是数学系的人抢物理系动力学方向的饭碗用的（可能实际上也确实是，或者更有可能是反过来），但是老师在前几讲一直把重点放在何种定理如何证明上面。数学系的嘛，这种对公理化系统的喜爱可以理解。但是发现我们几个上课的对这些证明都不太感兴趣之后，~~直接一个发卡弯漂移，教我们用 R 语言分析现实数据，现在新课还没上，不过可能是以股票数据做例子，我的老天鹅，这也太“经世济民”了吧……~~ 最后期末作业发现大家居然又都选择做 PPT 讲中心极限定理的证明，经世济民计划无疾而终。\n\n以下是为了新课程，我自己提前做的准备。\n\n## 下载、安装、环境配置（精神病版）\n\n- [https://cloud.r-project.org/](https://cloud.r-project.org/)\n- 看这一个链接就够了：[https://marketplace.visualstudio.com/items?itemName=Ikuyadeu.r](https://marketplace.visualstudio.com/items?itemName=Ikuyadeu.r)\n- [https://github.com/randy3k/radian](https://github.com/randy3k/radian)\n- [https://github.com/nx10/httpgd](https://github.com/nx10/httpgd)\n\n`Fedora Linux` + `R` + `radian` + `vscode` + `R extension for vscode`\n\n1. 在官网（[https://cloud.r-project.org/](https://cloud.r-project.org/)）下载并安装 R [解释器](https://python-interpreter)。\n2. 在命令行输入 `R` 打开解释器，在 R 中输入 `install.packages(\"languageserver\")` \n3. 在 vscode 的市场页面搜索 “R”，安装 R 语言插件。\n4. 安装 radian：\n    1. 在合适的 python [虚拟环境](https://pyhton-virtualenv) 里输入 `pip intall radian`\n    2. 用 radian 取代 R：编辑 `~/.bashrc` , 在其中加入一句`alias r=\"radian\"` ，重启命令行\n    3. 在 vscode 的设置中找到 `R\u003eRterm: Linux`, 输入 radian 的安装路径（在命令行中输入 `which R` 可以找到）\n5. 在 vscode 的市场页面搜索 “R debugger”，安装 `R debugger for vscode`\n6. 在命令行输入 `R` 打开解释器，在 R 中输入 `install.packages(\"httpgd\")`，安装可视化工具 `httpgd`\n\n## 简单语法\n\n来自马克思·普朗克复杂物理研究所《非平衡态集体过程》第9讲，这老师的讲课顺序简直了……\n\nR 的语法也简直了……\n\n### 简单数据结构 Some data types\n\n```r\n# vector\na \u003c- c(1,2,3,NA)\nb \u003c- c(\"m\",\"f\",\"f\",\"m\")\n# using a value\nb[2]\n\n# list\nlist.ab \u003c- list(number=a, gender=b)\nlist.ab$number\nlist.ab[[1]]\n\n# factors (categorial variable)\nf \u003c- factor(b)\nf\n# [1] m f f m f m m\n# levels: f m\nlevels(f) \u003c- c(\"female\", \"male\")\n\n# dataframe (lists of vectors of the same length)\nx \u003c- c(1,2,3)\ny \u003c- c(\"aa\",\"bb\",\"cc\")\nz \u003c- c(TRUE, FALSE, TRUE)\ndf \u003c- data.frame(first=x, second=y, third=z)\nView(df)\n# |      | first | second | third |  \n# |    1 |     2 |     aa |  TRUE |\n# |    2 |     3 |     bb | FALSE |\n# |    3 |     5 |     cc |  TRUE |\n```\n\n### 控制流 Loops, conditional statements\n\n```r\n# for loop\nfor(i in 1:5) {\n\tprint(i)\n}\n# while loop\nwhile(!finished) {\n\tprint(\"Hello\")\n}\n# if statement\nif(i\u003c5) {\n\tprint(\"Hello\")\n} else {\n\tprint(\"Not Hello\")\n}\n```\n\n### 函数 Functions\n\n```r\n# calling a function\nrnorm(5,mean=1,sd=1)\n\n# defining a function\nmysum \u003c- function(a,b,c=1) { a + b + c }\nmysum(1,1)\n```\n\n### 复杂数据类型 `data.table`\n\nSome comparison:\n\n- `data.table`: R package, fast and memory efficient\n- `python.pandas`: python implementation of data frames\n- `dplyr`: highly popular, easy to learn\n\n```r\nlibrary(data.table)\ndt \u003c- as.data.table(df)\n```\n\n```r\n# read and write\nflights \u003c- fread(\"path/to/your/flights.txt\")\nweather \u003c- fread(\"path/tp/your/weather.txt\")\n```\n\nMaking data tidy can simplify the following analysis.\n\n- Every column is an observable.\n- Every row is an observation.\n\n```r\n# making data tidy\nmelt( dt,\n      id.vars = \"ID\",\n      value.name = \"expression\",\n      variable.name = \"cell\"\n)\n# making data messy\ndcast(dt, ID ~ cell+expression) # not understand\n```\n\nGet item\n\n```r\nd[i, j, by] # take `d`, subset rows using `i`, then **calculate** `j` grouped by `by`\n# examples\nplanes[engines == 4]\n\n# slice the 2 columns\nplanes[, .(tailnum, year)] \n# groupby and calculate\nflights[, .(mean_delay = mean(dep_delay, na.rm=T)), by=carrier]\nflights[time_hour\u003e20, .(mean_delay=mean(dep_delay,na.rm=T)), by=.(month, origin)]\n# math calculations\nflights[, speed_kmh := 60*1.61*distance/air_time]\nflights[, resc_distance := distance/mean(distance), by=carrier]\n```\n\nMerge 2 tables\n\n```r\n# left join\nmerge(a,b,all.x=T)\nb[a] # same as above\n\n# right join\nmerge(a,b,all.y=T)\na[b] # same as above\n\n# inner join\nmerge(a,b, all=F)\na[b, nomatch=0]\n\n# full join\nmerge(a,b,all=T)\n```\n\nChaining operations\n\n```r\n# data.table way\nweather[, ws_kmh:=1.61*wind_speed][, .(mean_ws=mean(ws_kmh)), by=month]\n# use operator %\u003e% to take the result on the left as the 1st argument on the right\nlibrary(magrittr)\nweather[, ws_kmh:=1.61*wind_speed] %\u003e%\n.[, .(mean_ws = mean(ws_kmh)), by=month] %\u003e%\nhead()\n```\n"},{"slug":"rust-simple-syntax","filename":"2022-04-29-rust-simple-syntax.md","date":"2022-04-29","title":".rs | Rust 入门笔记","layout":"post","keywords":["md","rs"],"excerpt":"introduction to Rust and some simple syntax","content":"\nOn youtube.\n\n## What is Rust\n\nfast and powerful, system language\n\nweb development through webAssembly\n\nNo garbage collection, also no need to manege memory, which makes language more tedious\n\nUse Cargo to manage package\n\n## Install\n\nWindows: .exe\n\nLinux: run the curl script.\n\n```bash\nrustup --version\nrustc --version \ncargo --version\n```\n\nvscode Rust(rls) plugin\n\n## empty folder and compile\n\n```bash\nmkdir rustsandbox\ncd rustsandbox\ntouch hello.rs\n```\n\n```bash\nfn main() {\n\tprintln(\"Hello World!\");\n}\n```\n\n## initialize a project with cargo\n\n```bash\ncargo init\n```\n\nbettertmil plugin for highlight of `.tmil` file.\n\n```bash\ncargo run # compile and debug\ncargo build\ncargo build --release # for production\n```\n\n## print line\n\n```bash\ntouch src/print/rs\n```\n\n```rust\n// in print.rs\npub fn run() {\n\tprintln!(\"Hello World!\");\n\t// Basic formatting\n\tprintln!(\"Number: {}\",1); \n\tprintln!(\"{} is from {}\",\"Brad\",\"Mass\");\n\t// Positional arguments\n\tprintln!(\"{0} is from {1} and {0} likes to {2}.\",\"Brad\",\"Mass\",\"code\");\n\t// Named argument\n\tprintln!(\"{name} likes to play {activity}.\", name=\"John\", activity=\"baseball\")\n\t//Placeholder traits\n  println!(\"Binary: {:b} Hex: {:x} Octal: {:o}\", 10,10,10);\n\t//Placeholder for debuging\n  println!(\"{:?}\", (10,True,\"hello\"));\n\t//Basic Math\n\tprintln!(\"10+10={}\",10+10)\n}\n```\n\n```rust\n// in main.rs\nmod print;\n\nfn main() {\n\tprint::run();\n}\n```\n\n## Variable\n\n```rust\n// var.rs\npub fn run() {\n\tlet name=\"Brad\";\n\tlet age = 37;\n  age = 38; //cannot assign twice\n\tlet mut age = 37;\n\tage = 38;\n\tprintln!(\"My name is {} and I am {}\",name, age);\n\t// constant\n\tconst ID:i32 = 001;\n\tprintln(\"ID: {}\",ID);\n\t// assign multiple variables\n\tlet ( my_name, my_age ) = (\"Brad\", 37);\n\n}\n```\n\n## Primitive Types\n\n```rust\n// type.rs\n/*\nPrimitive Types:\nIntegers: u8,i8,u16,i16,...u128,i128\nFloats: f32,f64\nBoolean: bool\nCharacters: char\nTuples\nArrays\n*/\n\n// Rust is a static type language, but compiles usually can infer\n \npub fn run() {\n\t// default is i32, f64\n\tlet x = 1; \n\tlet y = 2.5;\n\tlet z: i64 = 454544445554;\n\t// find max size\n\tprintln!(\"Max i32:{}\", std::i32::MAX);\n\tprintln!(\"Max f64:{}\", std::f64::MAX);\n\t// boolean\n\tlet is_active = true;\n\t// get boolean from expression\n\tlet is_greater = 10 \u003e 5;\n\t// char\n\tlet a1 = 'a'; // has to be single quotes\n\tlet a2 = 'ab'; // error, can only be one character\n\tlet a3 = '\\u{1F600}';\n}\n```\n\n## Strings\n\n```rust\n// string.rs\n\n// primitive str = immutable fixedlength string\n// String = growable heap-allocated data structure - use when I need to modify or own srtring data\n\npub fn run() {\n\tlet mut hello = String::from(\"Hello \");\n\t//get length\n\tprintln!(\"Length: {}\", hello.len());\n\thello.push('W');\n\thello.push_str(\"orld\");\n\t// immutable\n\tlet hello2 = \"Hello\";\n\t// methods\n\tprintln!(\"Capacity {}\", hello.capacity());\n\tprintln!(\"Is empty {}\", hello.is_empty());\n\tprintln!(\"Contains 'World' {}\", hello.contains(\"World\"));\n\tprintln!(\"Replace {}\", hello.replace(\"World\",\"there\"));\n\t// loop through string by whitespace\n\tfor word in hello.split_whitespace() {\n\t\tprintln!(\"{}\",word);\n\t}\n\t// Create string with capacity\n\tlet mut s = String::with_capacity(10):\n\ts.push('a');\n\ts.push('b');\n\t// Assertion testing\n\tassert_eq!(2,s.len());\n}\n```\n\n## tuple\n\n```rust\n//tuples.rs\n\n// can be different types\npub fn run() {\n\tlet person: (\u0026str,\u0026str,i8) = (\"Brad\",\"Mass\",37);\n\tprintln!(\"{} is from {} and is {}\", person.0, person.1, person.2);\t\n}\n```\n\n## Array\n\n```rust\n// array.ts\n\nuse std::mem\n\npub fn run() {\n\tlet numbers: [i32,5] = [1,2,3,4,5]; // length has to be exact\n\tprintln!({:?},numbers);\n\t// get single value\n\tprintln!(\"Single value{:?}\",numbers[0]);\n\tlet mut numbers1: [i32,5] = [1,2,3,4,5];\n\tnumbers1[2] = 20;\n\tprintln!(\"Array length: {}\", numbers1.len());\n\t//arrays are static allocated\n\tprintln!(\"Arry occupies {} bytes\", mem::size_of_val(\u0026numbres1));\n\t// get slice\n\tlet slice: \u0026[i32] = \u0026numbres[0..2];\n\tprintln!(\"\")\n}\n```\n\n## Vector\n\n```rust\n//vector.rs\n\n// vectors are resizable arrays\npub fn run() {\n\tlet mut numbers: Vec\u003ci32\u003e = Vec![1,2,3,4];\n\t// add \n\tnumbers.push(5);\n\tnumbers.push(6);\n\t//pop off las values\n\tnumbres.pop();\n\t// loop through vector values\n\tfor x in numbers.iter(){\n\t\tprintln!(\"Number: {}\", x);\n\t}\n\tfor x in numbers.iter_mut(){\n\t\t*x *= 2; // no idea what the 1st * does\n\t\tprintln!(\"Number: {}\", x);\n\t}\n}\n```\n\n## conditional\n\n```rust\n//conditional.rs\n\npub fn run() {\n\tlet age: u8 = 18;\n\tlet check_id: bool = false;\n\tlet knows_person_of_age = true;\n\n\t// if/else\n\tif age \u003e= 21 \u0026\u0026 check_id || knows_person_of_age {\n\t\tprintln!(\"Bartender: what would like to drink?\");\n\t} else if age \u003c 21 \u0026\u0026 check_id {\n\t\tprintln!(\"Bartender: sorry you have to leave.\");\n\t} else {\n\t\tprintln!(\"Bartender: I'll need to see your ID.\");\n\t}\n\n\t// short if\n\tlet is_of_age = if age\u003e=21 {true} else {false};\n\tprintln!(\"Is of age: {}\", is_of_age);\n}\n```\n\n## loop\n\n```rust\n//loop.rs\n\npub fn run() {\n\tlet mut count = 0;\n\t//infinite loop\n\tloop {\n\t\tcount += 1;\n\t\tprintln!(\"Number: {}\", count);\n\n\t\tif count \u003e 20 {\n\t\t\tbreak;\n\t\t}\n\t}\n\t// while loop (FizzBuzz)\n\twhile count \u003c=100 {\n\t\tif count%15 === 0 {\n\t\t\tprintln!(\"fizzbuzz\");\n\t\t} else if count%3 == 0 {\n\t\t\tprintln!(\"fizz\");\n\t\t\telse if count%5 == 0 {\n\t\t\tprintln!(\"buzz\");\t\n\t\t} else {\n\t\t\tprintln!(\"{}\",count)\n\t\t}\n\t\tcount += 1;\n\t}\t\n\t// for loop\n\tfor x in 0..100 {\n\t\t// ...\n\t}\n}\n```\n\n## function\n\n```rust\n// function.rs\npub fn run() {\n\tgreeting(\"Hello\",\"Jane\");\n\tlet get_sum = add(5,5);\n\t//closure\n\tlet n3 = 10;\n\tlet add_sums = |n1: i32, n2: i32| n1+ n2 + n3; //need to find out more\n}\n\nfn greeting (greet: \u0026str, name: \u0026str) {\n\tprintln!(\"{} {}, nice to meet you.\", greet, name);\n}\n\nfn add(n1: i32, n2: i32) -\u003e i32 {\n\tn1 + n2 // no semi-colum here\n}\n```\n\n## Pointer/Reference\n\n```rust\n// pointer.rs\n\npub fn run() {\n\t// primitive\n\tlet arr1 = [1,2,3];\n\tlet arr2 = arr1;\n\tprintln!(\"Values: {:?}\", (arr1,arr2));\n\t// with non-primitive, if you assign another variable to a piece of data, the 1st variable will no longer hold that value. You'll need to use a reference (\u0026) to point to the resource\n\tlet vec1 = Vec![1,2,3];\n\tlet vec2 = \u0026vec1;\n\tprintln!(\"Values: {:?}\", (\u0026vec1, vec2))\n}\n```\n\n## Struct\n\n```rust\n// struct.rs\n\n// used to create custome data types\n\n// traditional struct\nstruct Person {\n\tfirst_name: String,\n\tlast_name: String\n};\n\nimpl Person{\n\tfn new(first: \u0026str, last: \u0026str)-\u003e Person {\n\t\tPerson {\n\t\t\tfirst_name: first.to_string(),\n\t\t\tlast_name: last.to_string()\n\t\t}\n\t}\n\tfn full_name(\u0026self) -\u003e String {\n\t\tformat!(\"{} {}\", self.first_name, self.last_name)\n\t}\n\t// set last name\n\tfn set_last_name(\u0026mut self, last: \u0026str){\n\t\tself.last_name = last.to_string();\n\t}\n\t// name to tuple\n\tfn to_tuple(self)-\u003e(String,String){\n\t\t(self.first_name,self.last_name)\n\t}\n}\n\nstruct Color {\n\tred: u8,\n\tgreen: u8,\n\tblue: u8,\n};\n\n// tuple struct\nstruct ColorTuple(u8,u8,u8); \n\npub fn run() {\n\tlet mut c = Color{\n\t\tred: 255,\n\t\tgreen: 0,\n\t\tblue: 0,\n\t};\n\tc.red = 200;\n\tprintln!(\"Color: {},{},{}\",);\n\t\n\tlet mut ct = ColorTuple(255,0,0);\n\tct.0 = 200;\n\tprintln!(\"Color: {},{},{}\",ct.0,ct.1,ct.2);\n\n\tlet mut p = Person::new(\"John\",\"Doe\");\n\tprintln!(\"Person {} {}\", p.first_name, p.last_name)\n\tp.set_last_name(\"Williams\");\n\tprintln!(\"Person {}\", p.full_name());\n\tprintln!(\"Person {:?}\", p.to_tuple());\n}\n```\n\n## enumerate\n\n```rust\n//enum.rs\n\n// enum is a type with a few definitive values\n\nenum Movement {\n\t// variants\n\tUp, \n\tDown,\n\tLeft, \n\tRight\n}\n\nfn move_avatar(m:Movement) {\n\tmatch m {\n\t\tMovement::Up =\u003e println!(\"Avatar Moving Up\"),\n\t\tMovement::Down =\u003e println!(\"Avatar Moving Down\"),\n\t\tMovement::Left =\u003e println!(\"Avatar Moving Left\"),\n\t\tMovement::Right =\u003e println!(\"Avatar Moving Right\")\n\t}\n}\n\npub fn run() {\n\tlet avatar1 = Movement::Up;\n\tlet avatar2 = Movement::Down;\n\tlet avatar3 = Movement::Left;\n\tlet avatar4 = Movement::Right;\n\n\tmove_avatar(avatar1);\n\tmove_avatar(avatar4);\n\tmove_avatar(avatar2);\n\tmove_avatar(avatar3);\t\n}\n```\n\n## Command line interface\n\n```rust\n// cli.rs\n\nuse std::env;\n\npub fn run() {\n\tlet args: Vec\u003cString\u003e = env::args().collect();\n\tlet command = args[1].clone();\n\tlet name = \"Brad\";\n\tlet status  = \"100%\"\n\n\tprintln!(\"Args: {:?}\", args);\n\tprintln!(\"Command: {}\", command)\n\n\tif command == \"Hello\" {\n\t\tprintln!(\"Hi {}, how are you?\", name);\n\t} else if command == \"status\" {\n\t\tprinln!(\"Status is {}\", status);\n\t} else {\n\t\tprintln!(\"That is not a valid command.\");\n\t}\n}\n```"},{"slug":"typescript-simple-syntax","filename":"2022-04-29-typescript-simple-syntax.md","date":"2022-04-29","title":".ts | TypeScript 入门笔记","layout":"post","keywords":["md","js","ts"],"excerpt":"An introduction to Typescript and some simple syntax","content":"\nWhat is TypeScript:\n\n- static types\n- types from 3rd parties can be added with type definition\n\nDynamic vs Static types\n\n- dynamic: types associated with run-time values, and not explicitly in code\n- static: explicitly assign types\n\nPros and Cons\n\n- pros: more robust, debug, predictability\n- cons: more codes to write, requires compilation, **not true typing**\n\nCompiling\n\n- `.ts` or `.tsx` extension\n- `TSC` is used\n- `tsconfig.json`\n\n# Hands on\n\n## Install\n\n```bash\nsudo npm i -g typescript\ntsc -v\ntouch index.ts\n```\n\n```tsx\nlet id: number = 5 \nid = `5` // error\n\n```\n\n```bash\ntsc index\ntsc -- watch index\n```\n\n```bash\ntsc --init # produces tsconfig.json\n```\n\n```json\n{\n\t\"outDir\": \"./dist\",\n\t\"rootDir\": \"/src\"\n}\n```\n\n## Types\n\n```tsx\nlet id: number = 5\nlet company: string = \"Media\"\nlet isPublished: boolean = true\nlet x: any = \"hello\"\nx = true // no errors\n\n// array\nlet ids: number[] = [1,2,3,4,5]\nids.push(\"hello\") // error\n\nlet arr: any[] = [1, true, 'hello']\n\n//tuple\nlet person: [number, string, boolean] = [1, 'Brad', true]\n\n// tuple array\nlet employee: [number, string][]\nemployee = [\n\t[1, 'Brad'],\n\t[2, 'Sam']\n]\n\n// union\nlet pid: string | number = 22\n\n// enum\nenum Direction1 {\n\tUp, // default is 0\n\tDown,\n\tLeft,\n\tRight\n}\n\nenum Direction2 {\n\tUp   = \"Up\", // default is 0\n\tDown = \"Down\",\n\tLeft = \"Left\",\n\tRight= \"Right\"\n}\n\n//objects\nconst user: {\n\tid: number,\n\tname: string\n} = {\n\tid: 1,\n\tname: 'John'\n}\n\ntype User = {\n\tid: number,\n\tname: string\n}\n\nconst user: User\n\n// type assertion\nlet cid: any = 1\nlet customerId = \u003cnumber\u003ecid\nlet customerId = cid as number\n\n```\n\n## function\n\n```tsx\nfunction addNum(x: number,y: number): number {\n\treturn x+y\n}\n//void\n\nfunction log(message: number | string): void {\n\tconsole.log(message)\n}\n```\n\n## Interfaces\n\n```tsx\ninterface UserInterface {\n\treadonly id: number,\n\tname: string,\n\tage?: number // optional property\n\tregister(): string\n}\nconst user1: UserInterface = {\n\tid: 1, // error , read only\nname: \"brad\"\n}\n\ninterfce MathFunc {\n\t(x: number, y: number): number\n}\n\nconst add: MathFunc = (x: number, y: number): number =\u003e x+y\n```\n\n## Class\n\n```tsx\nclass Person implements UserInterface{\n\tprivate id: number\t // within the class\n\tprotected name: string // within class or extended classes\n\tsalary: number\n\n\tconstructor(id: number, name: string) {\n\t\tthis.id = id\n\t\tthis.name = name\n\t}\n\tregister() {\n\t\treturn `${this.name} is registered.`\n\t}\n}\n\nconst brad = new Person(1,'Brad')\n```\n\n## Subclass\n\n```tsx\nclass Employee extends Person {\n\tposition: string\n\n\tconstructor(id: number, name: string, position: string) {\n\t\tsuper(id,name)\n\t\tthis.position = position\n\t}\n}\nconst emp = new Employee(3,\"Shawn\",\"developer\")\nconsole.log(emp.register())\n\n```\n\n## Generics\n\n```tsx\nfunction getArray(items: any[]): any[] {\n\treturn new Array().concat(items)\n}\nlet numArray = getArray([1,2,3,4])\nlet strArray = getArray(['a','b','c','d'])\n\nnumArray.push('hello') // no error, but not what we want\n\nfunction getArray\u003cT\u003e(items: T[]): T[] {\n\treturn new Array().concat(items)\n}\nlet numArray = getArray\u003cnumber\u003e([1,2,3,4])\nnumArray.push('hello') // error\n```\n\n## With React\n\n```bash\nnpx create-react-app . --template typescript\nnpm start\ntouch header.tsx\n```\n\n```tsx\nexport interface Props{\n\ttitle: string\n\tcolor?: string\n}\nconst Header = (props: Props) =\u003e {\n\treturn (\n\t\t\u003cheader\u003e\n\t\t  {% raw %}\n\t\t\t\u003ch1 style={{ color: prps.color ? props.color: \"blue\" }}\u003e\n\t\t\t\t{props.title}\n\t\t\t\u003c/h1\u003e\n\t\t  {% endraw %}\n\t\t\u003c/header\u003e\n\t)\n}\n```"},{"slug":"tensorboard-on-pytorch","filename":"2022-04-08-tensorboard-on-pytorch.md","date":"2022-04-08","title":".py | TensorBoard 笔记（PyTorch 版）","layout":"post","keywords":["md","py","ai"],"excerpt":"TensorBoard 是 TensorFlow 团队开发的一款可视化工具，方便观察和调整机器学习的数据集、模型、超参数和训练结果等等。但是不知道为什么，PyTorch 调用 TensorBoard，要比 TensorFlow 方便简单得多，\u003cdel\u003e这何尝不是一种 NTR\u003c/del\u003e……","content":"\n- 官网教程：[https://pytorch.org/tutorials/intermediate/tensorboard_tutorial.html](https://pytorch.org/tutorials/intermediate/tensorboard_tutorial.html)\n- 官方文档：[https://pytorch.org/docs/stable/tensorboard.html](https://pytorch.org/docs/stable/tensorboard.html)\n\nTensorBoard 是 TensorFlow 团队开发的一款可视化工具，方便观察和调整机器学习的数据集、模型、超参数和训练结果等等。但是不知道为什么，PyTorch 调用 TensorBoard，要比 TensorFlow 方便简单得多，~~这何尝不是一种 NTR~~……\n\n---\n\n## 用法和效果\n\n一个使用了 TensorBoard 的 torch 项目的主文件一般是这样的（把和 TensorBoard 无关的部分都注释掉了）：\n\n```python\n# imports\n# import matplotlib.pyplot as plt\n# import numpy as np\n\n# import torch\n# import torchvision\n# import torchvision.transforms as transforms\n\n# import torch.nn as nn\n# import torch.nn.functional as F\n# import torch.optim as optim\n\nfrom torch.utils.tensorboard import SummaryWriter\n\n# # transforms\n# transform = transforms.Compose(\n#     [transforms.ToTensor(),\n#     transforms.Normalize((0.5,), (0.5,))])\n\n# # datasets\n# trainset = torchvision.datasets.FashionMNIST('./data',\n#     download=True,\n#     train=True,\n#     transform=transform)\n# testset = torchvision.datasets.FashionMNIST('./data',\n#     download=True,\n#     train=False,\n#     transform=transform)\n\n# # dataloaders\n# trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n#                                         shuffle=True, num_workers=2)\n# testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n#                                         shuffle=False, num_workers=2)\n\n# # constant for classes\n# classes = ('T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n#         'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle Boot')\n\n# class Net(nn.Module):\n#     def __init__(self):\n#         super(Net, self).__init__()\n#         self.conv1 = nn.Conv2d(1, 6, 5)\n#         self.pool = nn.MaxPool2d(2, 2)\n#         self.conv2 = nn.Conv2d(6, 16, 5)\n#         self.fc1 = nn.Linear(16 * 4 * 4, 120)\n#         self.fc2 = nn.Linear(120, 84)\n#         self.fc3 = nn.Linear(84, 10)\n#     def forward(self, x):\n#         x = self.pool(F.relu(self.conv1(x)))\n#         x = self.pool(F.relu(self.conv2(x)))\n#         x = x.view(-1, 16 * 4 * 4)\n#         x = F.relu(self.fc1(x))\n#         x = F.relu(self.fc2(x))\n#         x = self.fc3(x)\n#         return x\n# net = Net()\n\n# criterion = nn.CrossEntropyLoss()\n# optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n\n# default `log_dir` is \"runs\" - we'll be more specific here\nwriter = SummaryWriter('runs/fashion_mnist_experiment_1')\n\nrunning_loss = 0.0\nfor epoch in range(1):  # loop over the dataset multiple times\n    for i, data in enumerate(trainloader, 0):\n        # # get the inputs; data is a list of [inputs, labels]\n        # inputs, labels = data\n\n        # # zero the parameter gradients\n        # optimizer.zero_grad()\n\n        # # forward + backward + optimize\n        # outputs = net(inputs)\n        # loss = criterion(outputs, labels)\n        # loss.backward()\n        # optimizer.step()\n\n        # running_loss += loss.item()\n        if i % 1000 == 999:    # every 1000 mini-batches...\n            # ...log the running loss\n            writer.add_scalar('training loss',\n                            running_loss / 1000,\n                            epoch * len(trainloader) + i) # 注意这一行！\n# print('Finished Training')\n```\n\n正常情况下，使用了tensorboard 的项目在训练的过程中，可以用网页浏览器打开网址 `localhost:6006`，应该可以看到和下图类似但不同的画面：\n\n![tensorboard](/photos/2022-04-08-tensorboard.png)\n\n下面来仔细分解。\n\n## 代码分解\n\n[引入](python-import-script-module-package) TensorBoard 需要下面一行代码：\n\n```python\nfrom torch.utils.tensorboard import SummaryWriter\n```\n\n从名字就能看出来，`SummaryWriter` 是一个 class。粗略用了一下文档页面的业内搜索，好像整个 `torch.utils.tensorboard` 就只有这一个 class。\n\n新建一个这个类的实例：\n\n```python\nwriter = SummaryWriter('runs/fashion_mnist_experiment_1')\n```\n\n这一步会在当前工作环境下新建一个 `/runs` 文件夹，\n\n要想显示导航栏上的“SCALARS”、“IMAGES”等等选项卡，并且让自己想观察的数据显示在各自类别的选项卡里，需要调用 `SummaryWriter` 下面的各种方法，比如 `add_scalar()`, `add_image()`.\n\n### 各种方法\n\n用法和效果举例如下：\n\n- `add_scalar()`: 在一张图中画出**一个标量**指标随学习迭代的**变化曲线**。\n    - `tag`: 图片的标题。\n    - `scalar_value`: 指标的值，也就是纵坐标。\n    - `global_step`: 全局迭代次数，也就是横坐标。\n- `add_scalars()`: 在一张图中同时画出**多个指标**随学习迭代的**变化曲线**。\n    - `main_tag`: 图片的标题\n    - `tag_scalar_dict`: 一个字典，字典的键是变量的名字，值是各个变量的纵坐标。\n    - `global_step`: 全局迭代次数，也就是横坐标。\n- `add_custome_scalars()`: 没有用过，也没见到例子，不太明白。根据描述像是把之前 `add_scalar()` 和 `add_scalars()` 的结果重新组合，对 SCALARS 选项卡重新排版。每个 `SummaryWriter` 只能运行一次，可以在训练开始前运行，也可以在之后。\n    - `layout`: 只有一个这参数，是一个字典，字典的键像是新图片/章节的名字，值是下一级字典或者是 `add_scalar()` 出现过/将要出现的 `tag` 参数。\n- `add_figure()`: 显示 `matplotlib` 画出的**图表**。\n    - `tag`: 标题\n    - `figure`: 图表，要求类型为 `matplotlib.pyplot.figure`\n    - `global_step`:迭代次数，效果如何 没试过。\n- `add_histogram()`: 在一张图中画出一个样本的**直方图**，以及这个直方图随迭代变化的规律。这是个三维图片，x 轴是直方图的取值范围，y 轴是迭代次数，z 轴是直方图的频率值。\n    - `tag`: 图片标题。\n    - `values`: 一个 `torch.Tensor` 或者 `numpy.array` ，用于绘制直方图的样本.\n    - `global_step`: 迭代次数，y 轴分量。\n    - `bins`: 取样间隔参数，`numpy.histogram()` 中用到的。 ****\n- `add_graph()`: 一般用于在训练前画出神经网络的**图状结构**。\n    - `model`: 要画的模型，类型是 `torch.nn.Module`\n    - `input_to_model=None`: （可选）输入模型的变量。\n- `add_mesh()`: **三维点云**。\n    - `tag`: 表格标题。\n    - `vertices`: 顶点三维坐标的列表。\n    - `colors`: 顶点的颜色。\n    - `faces`: （可选）没看懂 (Indices of vertices within each triangle.)\n    - `config_dict`: 用于画图的 ThreeJS 的参数。\n    - `global_step`: 迭代次数。\n- `add_embeddding()`: 神经网络的输入一般是高维向量，此工具将高维数据**投影到三维**空间，然后画出图像，方便我们感知训练集内样本之间的关系。\n    - `mat`: 一个矩阵，每一行都是一个要处理的向量。\n    - `metadata`: 标记文字，一般是列表。\n    - `label_img`: 标记图片，显示在每个数据点旁边的\n    - `global_step`: 迭代次数，一般没人用。\n    - `tag`: 图片标题。\n- `add_pr_curve()`: 准确率 (precision) -召回率 (call back) 曲线。准确率=真阳性/(真阳性+假阳性)，召回率=真阳性/(真阳性+假阴性)\n    - `tag`: 图片标题。\n    - `labels`: Ground truth 数据，每个数据点对应一个布尔值。\n    - `predictions`: 模型的输出，每个数据点对应一个 [0,1] 之间的实数。\n    - `global_step`: 迭代次数。\n    - `num_thresholds`:用于画出 PR 曲线的阈值的数目。\n- `add_hparams()`: 比较不同次运行之间的超参数。没太看懂。\n    - `hparam_dict`: 超参数的名称和取值\n    - `metric_dict`: metric （不知道怎么翻译）的名称和取值\n    - `hparam_domain_discrete`: （可选）字典，超参数的名称和有限个可能的取值。\n    - `run_name`: 运行的名称，将会成为 `logdir` 的一部分。\n- `add_image()`: 在 IMAGES 选项卡中显示**一张图片**。\n    - `tag`: 图片名称。\n    - `img_tensor`: 一个 `torch.Tensor` 或者 `numpy.array`，被显示的图片。对应于下面的 `dataformats` 参数。\n    - `global_step`: 迭代次数，没见有人在显示图片的时候用过。\n    - `dataformats=’CHW’`: 图片各维度的顺序。默认是“颜色-高度-宽度”。\n- `add_images()`: 并列显示**多张图片**。\n    - `tag`: 图片组的标题。\n    - `img_tensor`: 一个 `torch.Tensor` 或者 `numpy.array`，被显示的图片。图片个维度的含义由 `datadormats` 给出。\n    - `global_step`: 迭代次数，没见有人在显示图片的时候用过。\n    - `datadormats=’NCHW’`: 图片各维度的顺序。默认为“图片序号-颜色-高度-宽度”。\n- `add_video()`: 略\n- `add_audio()`: 略\n- `add_text()`: 略"},{"slug":"git-multiple-users","filename":"2021-12-26-git-multiple-users.md","date":"2021-12-26","title":".git | 管理 GitHub 不同用户身份的仓库","layout":"post","keywords":["md","git"],"excerpt":"多用户多账户，如何告诉 GitHub 某个项目文件夹该由哪个账号来做版本管理。","content":"\n很显然，我不可能把“阿掖山”这个名字写到论文里，与研究相关的项目、组里的代码和数据，都由另一个实名的 GitHub 账号来处理。这就有个问题——如何告诉 GitHub 某个项目文件夹该由哪个账号来做版本管理。\n\n于是 STFW，结果看到了GitHub  官方的回答：[“这边建议您把两个账号合并呢～”](https://docs.github.com/en/account-and-profile/setting-up-and-managing-your-github-user-account/managing-user-account-settings/merging-multiple-user-accounts)\n\n其实这个问题在建立这个博客站之前就解决了，但是因为很长时间我的研究代码都是自己在用，而且只在台式机上用，一直没有推到 GitHub 上去，实名账号一直没在笔记本上用过。具体细节忘得差不多了，这次复习一下。\n\n作为例子，两个账号的用户名分别是 `USER1` 和 `USER2`，注册邮箱分别是 `USER1@EMAIL.com` 和 `USER2@EMAIL.com`。\n\n## 多个计算机用户\n\n当然了，最简单的方法就是新建一个操作系统用户，每个用户登录一个 GitHub 账号。这种方法好处很多：\n\n- 不需要特殊操作。\n- 适用于不同操作系统。\n- 切换身份需要专门切换账号，有助于防止操作者忘记自己所处的身份。\n\n但是这篇文章不会涉及这种方法，因为当初买电脑的时候并没有注意到需要做身份隔离，等到发现事情不妙的时候已经混装和两个身份需要的不同软件，积重难返。于是采用了以下两节的解决方案。\n\n## 单个计算机用户@Windows: GitHub Desktop\n\n![](/photos/2021-12-26-github-desktop.png)\n\n- 在官网下载、安装、打开 [GitHub Desktop](https://desktop.github.com/) 客户端。\n- 在 GitHub 网页版上切换到新的账户。\n- 点击左上角 `File \u003e Options`，默认界面就是账户信息。点击 `Sign Out` 退出登录，然后再点击 `Sign In`，根据弹出窗口的提示操作，就来到了新的账户。\n\n## 单个计算机用户@Linux : `.ssh/config`\n\n具体操作看以下两个连接就够了：\n\n- [https://gist.github.com/JoaquimLey/e6049a12c8fd2923611802384cd2fb4a](https://gist.github.com/JoaquimLey/e6049a12c8fd2923611802384cd2fb4a)\n- [https://docs.github.com/en/authentication/connecting-to-github-with-ssh](https://docs.github.com/en/authentication/connecting-to-github-with-ssh)\n\n~~但是为了水字数，~~ 还是写得详细一点……\n\n### 本地：生成并启用 SSH key\n\n打开命令行，输入以下命令，生成 SSH key：\n\n```bash\n\nssh-keygen -t rsa -b 4096 -C \"USER1\" \nssh-keygen -t rsa -b 4096 -C \"USER2\"\n```\n\n其中 `-t` 指定加密算法，`-b` 指定密钥的位数，`-C` 相当于注释。\n\n然后命令行会弹出几个选项，可以一路按回车使用默认值。\n\n上述命令完成后，在 `~/.ssh` 文件夹应该会有两对四个密钥文件：\n\n- `~/.ssh/USER1`\n- `~/.ssh/USER1.pub`\n- `~/.ssh/USER2`\n- `~/.ssh/USER2.pub`\n\n然后输入以下命令，启用刚刚生成的密钥。\n\n```bash\n\neval \"$(ssh-agent -s)\" # 启动 ssh-agent\nssh-add ~/.ssh/USER1   # 添加 USER1 的密钥\nssh-add ~/.ssh/USER2   # 添加 USER2 的密钥\n```\n\n### 网页：把密钥添加到对应的账号\n\n在网页端以 `USER1` 身份登录 GitHub 之后，在 \"Settings\" 页面找到 \"SSH and GPG keys\" 选项卡，点击绿色的 \"New SSH key\" 按钮之后，将 `~/.ssh/USER1.pub` 中的内容复制到 \"Key\" 填空区，然后起一个名字，点击 \"Add SSH key\" 按钮。\n\n![]({{ site.baseurl }}/assets/photos/2021-12-26-github-key.png)\n\nUSER2也照此办理。\n\n**千万要注意**复制的应该是 `.pub` 后缀的文件！\n\n### 本地：编辑 `~/.ssh/config` 文件\n\n在 ~/.ssh/ 找到或者新建一个名为 config 的文本文件。打开之后，将以下内容添加到文件中：\n\n```\n\nHost github.com-user1\n\tHostname github.com\n\tUser git\n\tIdentityFile ~/.ssh/user1\nHost github.com-user2\n\tHostname github.com\n\tUser git\n\tIdentityFile ~/.ssh/user2\n```\n\n### `git clone` 时 repo 地址的改动\n\n一般的 git clone, 直接把 GitHub 提供的命令复制粘贴到命令行就行了。\n\n![]({{ site.baseurl }}/assets/photos/2021-12-26-git-clone.png)\n\n但是我们这个不同，首先是只能选择 SSH 模式，然后是需要在域名`github.com` 后面加上`-user1`:\n\n```bash\n\ngit clone git@github.com-user1:User/Repo.git\n```\n\n这里就体现出之前在 `~/.ssh/config` 把 Host 命名为 `github.com-****` 的好处了。\n\n### `git commit` 前填写在 repo 中填写账户信息\n\n 第一次做完改动推送到 GitHub 之前，需要专门在 repo 级别写明自己的身份，也就是在命令行输入：\n\n```bash\n\ngit config --local user.name  \"USER1\"\ngit config --local user.email \"USER1@EMAIL.com\"\n```\n\n### **注意：** 为了防止操作者忘记自己所处的身份\n\n强烈建议去掉用户信息的全局设置：\n\n```bash\n\ngit config --global --unset user.name\ngit config --global --unset user.email\n```\n\n这样假如忘记之前的 `git config --local`, 第一次 git commit 的时候会报错，提示信息缺失。\n\n这样操作一次之后，之后的操作几乎感受不到账户的不同。\n"},{"slug":"python-import-script-module-package","filename":"2021-12-11-python-import-script-module-package.md","date":"2021-12-11","title":".py | import 引用现成的代码","layout":"post","keywords":["md","py"],"excerpt":"正常的编程语言教程，教人配置完开发环境之后就应该进入正题，开始讲语法了。但是咱不正常，所以先来谈谈怎么用别人已经写好的代码。","content":"\n以官网给出的文件结构为例来说明：\n\n```\n\nsound/                          Top-level package\n      __init__.py               Initialize the sound package\n      formats/                  Subpackage for file format conversions\n              __init__.py\n              wavread.py\n              wavwrite.py\n              aiffread.py\n              aiffwrite.py\n              auread.py\n              auwrite.py\n              ...\n      effects/                  Subpackage for sound effects\n              __init__.py\n              echo.py\n              surround.py\n              reverse.py\n              ...\n      filters/                  Subpackage for filters\n              __init__.py\n              equalizer.py\n              vocoder.py\n              karaoke.py\n              ...\n```\n\n## 使用现成的 python 代码\n\n正常的编程语言教程，教人配置完开发环境之后就应该进入正题，开始讲语法了。但是咱不正常，所以先来谈谈怎么用别人已经写好的代码。其中最简单的，就是可以直接通过包管理程序安装的：\n\n```bash\n\npip install sound\n```\n\n然后想要使用某个文件中的函数，比如假装 `wavwrite.py` 中有个函数叫 `write()`，以下写法都是可以的，注意不同 import 方法对应不同的函数调用写法：\n\n```python\n\nimport sound\nsound.formats.wavwrite.write()\n\nfrom sound import formats\nformats.wavwrite.write() \n\nfrom sound.formats import wavwrite\nwavwrite.write()\n\nfrom sound.formats.wavwrite import write\nwrite()\n```\n\n但是，不是所有的 python 代码都可以直接安装，比如一篇论文的研究成果发表之后，处理数据的代码也往往开源，但是这些作者基本上就只是把自己写代码的文件夹公开出来而已，我们把文件夹下载下来，然后直接 `import sound`, 会报错，提示找不到名为 sound 的库。\n\n## python 如何读取代码文件\n\n仔细想想，找不到才是正常的，之前轻轻松松的一句 `import sound`就解决问题，这才不简单——不同的库往往位于文件系统的不同位置，但我们只要写出他们的名字就行了，不需要指定文件路径。电脑硬盘那么大，找到库却几乎是瞬间完成的。\n\n这是因为 python 并没有搜索整个硬盘。有一个变量，一般名为 `PYTHONPATH`，其变量值是一个列表，表中成员是含有 python 库文件夹的路径。当我们在命令行输入命令的时候，电脑会：\n\n- 搜索当前所在的文件夹，也就是在命令行输入 `python` 时终端所在的文件夹。\n- 遍历 `PYTHONPATH` 中的文件夹。\n- python 包管理程序默认的位置，一般是 `\u003cpath to python\u003e/site-package`。\n\n看看有没有我们要引用的库，找到了就引入，找不到就报错。\n\n上一节的错误中，如果我们恰好位于 sound 所在的文件夹，然后运行 python，此时第一条生效， `import sound` 不会报错，但在其他位置就不行了。\n\n## 名词解释：interactive, script, module, package\n\n可执行的 python 命令可以出现在以下四个地方，第一种是接受键盘输入的程序，后三种都是文件：\n\n1. interactive: python **交互式界面**，也叫做 calculator mode，也就是在命令行输入 `python`之后出现的界面。每次输入一句，结果在命令行上显示出来。当 python 退出之后，输入过的命令就消失了。\n2. script: python **脚本**文件，也就是在命令行输入 `python somefile.py`里面的那个`somefile.py`。\n    1. 毕竟 python 是一种很轻量化的语言，在一定程度上可以起到 shell 的作用，有些命令我们并不想要用完就扔，而是保存起来以便以后重复执行，另外很多命令的组合组合成函数也可以极大地简化工作。在这种语境之下, interactive 和 script 的关系，就好像 Linux 命令行和 bash script 的关系一样。\n    2. 但同时 python 又是一种功能很全面的语言，完全可以胜任复杂的面向对象编程。在这种语境之下，script 也可以用来指代 main module，也就是程序执行的主文件和入口，和下面的一般的 module 相区分。\n3. module: python **模块**文件，也就是在命令行输入 `python -m another` 里面的那个 `another`（注意这里不写拓展名 `.py`）。按照官方文档的说法，所有 `.py` 文件都是 module。但是实际上这句话很有误导性，上一节的 main module 和一般的 module 非常不同，下一节会详细展开讲。一般提到 module，都是在强调这个文件定义的变量和函数可以被其他的 python 文件引用。\n4. package: python **包**，互相关联的 modules 构成的更高一级的可供引用的结构，简单理解就是含有 `__init__.py` 的文件夹，但是 python 并不是根据文件夹和文件之间的从属关系来确定 package 和 module 之间的关系的，下一节会详细展开讲。\n\n## script vs. module\n\npython 同时兼具脚本语言的灵活性，和各种重型语言的功能全面性。因为前者，所以它并不要求程序作者一定要在一个叫做 `main.py` 的文件里写一个名叫 `Main` 的类, 然后在里面实现一个 `main()` 方法。但是因为后者，没写不代表 python 不需要知道一个复杂程序执行的起点。\n\n这个起点就是不带有 `-m` 参数的 `python` 命令后面跟着的 `.py` 文件，这就使得这个文件变得比其他 `.py` 文件特殊。底层表现就是 python 会不管这个文件的名字叫什么，都将它的 `__name__` 属性赋值为 `\"__main__\"`。这样，即便这个文件可能是一个大型库中间的一个模块，运行的时候 python 连它的真名都不知道，就更找不到它同级和上下级的其他模块了。\n\n各种普通模块被 python 用到的方法就是通过在主模块 main module（或者说 script）中 import。经过“python 如何读取代码文件”一节中的搜索过程之后找到了所需模块或包，模块的名字、模块之间的关系、模块里定义了哪些属性和函数，就被 python 了解了，从而当主模块召唤他们的时候就知道去哪里找相应的代码。除了在被 import 的时候，`python -m` 命令的宾语也可以告诉 python 被运行的模块和包的相对关系：`python -m sound.formats.wavwrite` ，此时 python 执行了 `wavwrite.py` 中的所有可执行的命令，同时知道从 `sound/` 到 `wavwrite.py` 的各个包之间的关系。\n\n## absolute import vs. relative import\n\n开头使用已经安装过的包使用的语法全都是绝对引用 (absolute import)，表现就是 import 语句里面没有以 `.` 作为开头的。\n\n另外一种 import 方法叫相对引用 (relative import)，`.` 表示模块所在的文件夹，`..` 表示模块的上一级文件夹。主要用在各种明确知道自己是工具代码，而且是一个更高层次结构的组成部分，几乎永远不需要被作为主模块运行的代码。\n\n回到开头例子里的文件结构，假如 sound/effects/surround.py 中想要使用 sound/formats/wavwrite.py 和 sound/effects/echo.py 中的函数，可以写成：\n\n```python\n\n# in sound/effects/surround.py\nfrom ..formats import wavwrite\nfrom . import echo\n```\n\n## 如何组织代码，以便自己重用\n\n研究终于推进到了准备写论文的阶段了（学渣本质暴露了），写草稿之余，之前几年时间里做过的处理和分析，接下来的一两个月里需要把工作流程规范化之后迅速重做一遍确认。\n\n随手写散落各处的分析代码需要整理到一起，之前试图统一到一个项目之下，结果总是在某个模块引用其他模块的时候遇到报错。于是才有了这篇文章。\n\n以下是 [这篇文章](https://gist.github.com/ericmjl/27e50331f24db3e8f957d1fe7bbbe510) 给出的一个推荐的项目文件结构：\n\n```\n\n|- notebooks/\n   |- 01-first-logical-notebook.ipynb\n   |- 02-second-logical-notebook.ipynb\n   |- prototype-notebook.ipynb\n   |- archive/\n\t  |- no-longer-useful.ipynb\n|- projectname/\n   |- projectname/\n\t  |- __init__.py\n\t  |- config.py\n\t  |- data.py\n\t  |- utils.py\n   |- setup.py\n|- README.md\n|- data/\n   |- raw/\n   |- processed/\n   |- cleaned/\n|- scripts/\n   |- script1.py\n   |- script2.py\n   |- archive/\n      |- no-longer-useful.py\n|- environment.yml\n```\n\n学过这篇笔记包含的内容，我才理解作者这样的安排。既然主文件 ~~很难~~ 没办法通过相对引用来找到工具代码，索性就把工具代码写成一个完整可安装的库，然后就像 `numpy`, `pandas` 一样在独立的 notebook 和 scripts 中引用。\n\n实际使用的时候，需要安装 `projectname` 下的代码：\n\n```bash\ncd projectname\npip install -e .\n```\n\n要知道为什么这样做，需要理解 `setup.py` 这个文件。这篇文章已经够长了，所以这个话题还是下次再说吧。\n\n## 参考链接\n\n- [What's the difference between a Python module and a Python package?](https://stackoverflow.com/questions/7948494/whats-the-difference-between-a-python-module-and-a-python-package). all python files re modules, while package is a specific kind of modules. It is a subsection of module in the python documentation.\n- Official explanation of python module: [https://docs.python.org/3/tutorial/modules.html](https://docs.python.org/3/tutorial/modules.html)\n- [This stackoverflow  answer: \"run as module\" is different from \"run as script\".](https://stackoverflow.com/questions/14132789/relative-imports-for-the-billionth-time) run as module sets the \"name\" to the module's name, while running as script sets it to `__main__` . Here we use \"name\" instead of `__name__` because it also contains `__path__` in newer versions.\n- A tutorial for project organization: [https://realpython.com/python-application-layouts/](https://realpython.com/python-application-layouts/)\n- Official about packaging: [https://packaging.python.org/en/latest/tutorials/packaging-projects/](https://packaging.python.org/en/latest/tutorials/packaging-projects/)\n- Gist: How to organize data science project: [https://gist.github.com/ericmjl/27e50331f24db3e8f957d1fe7bbbe510](https://gist.github.com/ericmjl/27e50331f24db3e8f957d1fe7bbbe510)\n- From the gist there is a link: [http://drivendata.github.io/cookiecutter-data-science](http://drivendata.github.io/cookiecutter-data-science)"},{"slug":"blog-update-theme-materalize","filename":"2021-12-06-blog-update-theme-materalize.md","date":"2021-12-06","title":".md | 博客外观现代化升级","layout":"post","keywords":["md"],"excerpt":"\u003cs\u003e本期博文点赞过 5 亿，下篇文章写用 jekyll 在 GitHub Pages 上搭建静态博客的教程。\u003c/s\u003e","content":"\n今年感恩节三天假期加上周末，一共五天的时间，别的啥也没干，憋在家里给博客换了个主题模板。现在新版本已经上线，基本上已经能用，明显的 bug 都已经解决了。主要的工作内容如下：\n\n- 从 Tufte 风格转换到 material 风格\n- 评论区迁移到现成的 ~~[utterance](https://utteranc.es/)~~  [giscus](https://giscus.app/)。\n- 将旧模版中的侧边栏注记功能移植到新的模板。\n- 调整了 CSS，包括字体、代码模块、博客标题限制高度、博客博客实现类似纸张的卡片效果。\n- 重写了 index.html, /History.html, /Links.html 等页面，尤其是 /Topics.html，实现了一个响应式的两列结构。\n- 重写了博文前面的元信息。\n- 将之前的 repo 重命名并且归档，新建一个同名的 repo，推送上线。\n\n下面挨个来说。\n\n## 风格转换\n\n![](/photos/2021-12-06-blog-old.png)\n\n之前的风格名字叫做 Tufte，好像是根据一个美国教授的设计，主要设计元素包括一套自己加载的衬线字体（很漂亮，但是对中文没用），还有博文占据页面宽度的大约 60%，右侧剩下的空间可以做边注，有一个设计很精美的横向分割线 `\u003chr class=\"slender\"\u003e`。\n\n这个模板很漂亮，但是衬线字体搭配背景色，就跟人一种上个世纪古董网站的感觉（没有对卢昌海老师不敬的意思）。\n\nMaterial Design 是 Google 推出的一个组件库，提供类似纸张和卡片的视觉效果，就给人一种很现代的感觉，内心觊觎已久。[GitHub 的这个 repository](https://github.com/naveenshaji/material) 就用了这套设计风格，虽然已经不再更新维护了，但是已有的功能，比如点击链接之后的加载页面，页面顶端的阅读进度条，我觉得让我自己来的话，十年之内都不一定能学到做出这些效果的技术，于是就直接拿来用了。\n\n## 评论区\n\n### 旧版：jQuery 搭配 GitHub Milestones\n\n之前的评论区是根据 [farseerfc](https://farseerfc.me/zhs/pages/about.html) 大佬的[博文](https://farseerfc.me/zhs/github-issues-as-comments.html)自己仿写的 JavaScript 函数。原文是把 issue 作为一个博文的评论区，issue 下面的 comment 作为评论，这样每一条评论都是平级的，没办法实现对某一条评论的回复，只能在评论内容中指明回复的对象。\n\n除了像博文里一样自己手搓代码，也有现成的第三方工具 [utterance](https://utteranc.es) 来完成这一工作，但是我还是不喜欢这种单层评论系统，于是决定自己仿写一个类似的系统，但是让一个 milestone 对应一篇文章，一个 issue 对应一条评论，一个 comment 对应一条回复。\n\n这就需要我在博文的 Markdown 页面注明对应的 milestone 的编号，然后在网页加载完成之后向 GitHub 对应的 milestone 发送一个 GET 请求，询问是否存在 issues。 得到肯定的回答之后，再依次发送请求 GET 每一条 issue 的内容，显示在博文下方。代码位于 `[/_includes/comments.html` 页面](https://github.com/MountAye/blog-tufte/blob/source/_includes/comments.html)，当年从异步编程开始学起，颇费了我几个周末。\n\n问题出在 GitHub 的用户权限上，只有作者和管理员才有权给某条 issue 指定一个 milestone，所以读者建立 issue 进行评论之后并不会直接显示在博文下面，还需要我回到 repo 把那条 issue 手动挪到对应的文章，几乎不可用。\n\n### 新版：giscus 搭配 GitHub Discussions\n\n这次改版的时候觉得用户体验比“老子可以手搓评论区代码”的自我满足更重要，直接换用了 utterance。评论区在博客页面就提供了编辑区，新版本第一次上线不久就有朋友留言，说明这个评论区还是很好用的。\n\n后来GitHub推出了 Discussion 功能，每一条 discussion 下方可以有不同评论，评论下面可以有针对的回复，这就和一般的评论区和BBS 的“楼层-单元”同构了。\n\n然后又在阮一峰老师的博客看到了 [giscus，](https://giscus.app/)就是在 Discussions 架构下和 [utterance](https://utteranc.es/) 类似的一个工具，只需要在官网按照流程配置，然后把生成的几行代码嵌入到自己的页面里就好了，比原来方便太多了：\n\n```html\n\u003cscript src=\"https://giscus.app/client.js\"\n        data-repo=\"[ENTER REPO HERE]\"\n        data-repo-id=\"[ENTER REPO ID HERE]\"\n        data-category=\"[ENTER CATEGORY NAME HERE]\"\n        data-category-id=\"[ENTER CATEGORY ID HERE]\"\n        data-mapping=\"pathname\"\n        data-reactions-enabled=\"1\"\n        data-emit-metadata=\"0\"\n        data-theme=\"light\"\n        data-lang=\"en\"\n        crossorigin=\"anonymous\"\n        async\u003e\n\u003c/script\u003e\n```\n\n之前 utterance 创建的评论也可以直接从 Issue 移动到 Discussion 板块，所以刚刚那条评论可以正常显示。但是因为换了新的 repo，旧版本的两条评论就移植不过来了，请萌狼和 HK 兄弟包涵。 \n\n## 旧版特有功能：侧边栏注记、正文图片\n\n早就想给博客改版了，一直拖着不办的原因主要是直接把博文复制粘贴到新模板的 `/_posts` 文件夹之后会报错，不知道排错需要多长时间，于是压根就不动手了。因为旧版通过 jekyll 实现了一些正常 markdown 文档没有的功能，我用到的主要就是侧边栏的边注 `sidenote`，以及在正文内部插入图片的 `maincolumn`。\n\njekyll 是建立在 ruby on rail 上的一个软件，所以这些功能也是用 ruby 语言写成的。本以为会很难，结果认真一看，其实根本没什么工作量，就直接把旧模板的 `/_plugins`文件夹复制到新模板就不再报错了，当然显示效果需要调整 CSS，下一节会讲到。`{% raw %}{% sidenote %}{% endraw %}` 就可以继续用了。\n\n 至于正文图片，这个功能和 markdown 已有的图片功能重复了，我觉得不值得为了一点点显示效果牺牲可移植性，于是直接把相关组件删除了，然后把博客文章中用到的地方换成了 markdown 插入图片的语法，重复性体力劳动，不提了。\n\n## 调整 CSS\n\n### 新建 `.paper` 取代 `.cover`\n\n![](/photos/2021-12-06-material-card-cover.png)\n\n这个模板主打的一个内容卡片如图所示，很漂亮，但是整个元素的宽度是由封面图片 `.cover` 决定的：\n\n```css\n@media (min-width: 1600px) {\n    .scroll-1 {\n        width: 1200px;\n        margin-left: -600px;\n    }\n    .scroll-1 .card .cover {\n        width: 1200px;\n    }\n}\n```\n\n而我的很多博文都是纯文字的，直接不添加图片会有大半个卡片没什么内容，去掉图片区域之后整个卡片的宽度会缩水到文字的最大宽度，丑。解决方法是新建了一个新的类 `.paper`，把 `.cover` 中和宽度相关的设置移动到新的类，然后把新类应用在整个卡片的 `\u003cdiv /\u003e` 上。\n\n```css\n@media (min-width: 1600px) {\n    .scroll-1 {\n        width: 1200px;\n        margin-left: -600px;\n    }\n    .paper {width: 1065px;}\n    /* .sidenote {width: 300px;} */\n}\n```\n\n### 考虑侧边栏的页面宽度响应式布局\n\n响应式布局指的是同一个网页，在不同的终端设备上都又能够有适合的视觉效果，主要方法就是根据不同的屏幕宽度设定某些元素的不同取值：\n\n```css\n@media (min-width: 1600px) { /* ... */ }\n@media (max-width: 1600px) { /* ... */ }\n@media (max-width: 1200px) { /* ... */ }\n@media (min-width: 768px) and (max-width: 979px) { /* ... */ }\n```\n\n具体到这个博客：\n\n- 屏宽大于一个很大的值之后，博客正文和侧边栏的宽度固定，不再增加；\n- 屏幕小于这个值，但是大于能够正常显示侧边栏的宽度时，博客正文和侧边栏都占据给定的百分比；\n- 屏幕小于能够正常显示侧边栏的宽度时，侧边栏不再显示，正文宽度固定在之前最小的像素值；\n- 正文像素数占据屏幕全部宽度之后，宽度设定为 100%。\n\n## 复习 Liquid，重写非博文页面\n\n非博文页面和之前的布局基本上相同，除了 `/Topics` 从一个无序号列表变成了响应式的两个竖栏。这是得益于模板引用的 `materialize.css`采用了 [flexible grid](https://materializecss.com/grid.html)。\n\n具体来说，一个`container` 可以动态调整内部元素的占宽，以适应不同大小的设备，包含两级子元素，两级子元素顾名思义，分别显示为行和列的元素：\n\n```html\n\u003cdiv class=\"container\"\u003e\n\t\u003cdiv class=\"row\"\u003e\n\t\t\u003cdiv class=\"col s12 m6 l4\"\u003e\u003c/div\u003e\n\t\t\u003cdiv class=\"col s12 m6 l4\"\u003e\u003c/div\u003e\n\t\u003c/div\u003e\n\u003c/div\u003e\n```\n\n然后将父元素的宽度分成 12 个基本单位，方便指定某一个元素所占的宽度和高度。比如 `class = \"col s12 m6 l4\"` 表示该元素在小设备 (small) 上占据 12/12=100% 屏宽，在中设备 (medium) 上占据 6/12 = 50% 屏宽，在大设备 (large) 上占据 4/12=1/3 屏宽。 \n\n### Liquid\n\nJekyll 本身的编程语言是 Ruby，但是被 Jekyll 编译之前的网页文档中的特殊标记，是一种叫做 Liquid 的领域专用语言。\n\n一般的命令和控制流结构用 `{% raw %}{% %}{% endraw %}` 括起来 \n\n```html\n for i in (1..len1) %}\n\t assign idx = i | times: 2 | minus:2 %}\n   assign cate = category[idx] %}\n endfor %}\n```\n\n直接在网页中显示内容和变量使用 `{% raw %}{{  }}{% endraw %}`\n\n```html\npost.title }}\n```\n\n函数调用的语法最奇葩，是 `input | function` 或者 `input | function: parameter`：\n\n```html\nassign category = site.data.category %}\nassign length = category | size %}\nassign   len1 = length | plus: 1 | divided_by: 2 %}\nassign   len2 = length | divided_by: 2 %}\n```\n\n## 调整博文元信息\n\n每篇博文的开头两道分割线之间的 YAML 是绑定在页面上的变量，可以被 Liquid 引用 `{% raw %}{{ post.VAR_NAME }}{% endraw %}`。\n\n```yaml\n---\nlayout:  post\ntitle:   .html | 翻译：为什么说物理不是一门学科\nkeywords: html\nexcerpt: 一篇稍微硬核的科普文章，讨论物理在生物学当中的可用性。\ndate:    2019-06-19\ncategories: post\nmilestoneID: 5\n---\n```\n\n新版本简化了很多，删掉了没有用的 `date`, `categories`, `milestoneID` 字段，同时把 `keywords` 变成了一个数组，也就是说同一篇文章可能出现在多个 `/Topics` 页面的卡片中。\n\n```yaml\n---\nlayout:  post\ntitle:   .en | 翻译：为什么说物理不是一门学科\nkeywords: [html,inter]\nexcerpt: 一篇稍微硬核的科普文章，讨论物理在生物学当中的可用性。\n---\n```\n\n## 下一步计划\n\n- 进一步优化排版:\n    - 给博文的标题添加视觉效果，现在的一二级标题之间很难区分。\n    - 超链接的样式不够明显，一眼看不出哪里有链接。\n    - 侧边注距离正文的距离不合适，CSS 相关参数的效果很奇怪。\n    - 现在的 /Links 页面在宽度缩小之后用户头像会和介绍卡片挤到一起去。\n    - 网站整体字号在笔记本上比较合适，在台式机上看起来太大了，移动端更大。\n- 加入 google analytics\n- 欢迎大家提建议。\n- 欢迎有独立博客的朋友互相链接。\n- ~~本期博文点赞过 5 亿，下篇文章写用 jekyll 在 GitHub Pages 上搭建静态博客的教程。~~\n\n"},{"slug":"install-pytorch-cpu-on-fedora","filename":"2021-10-13-install-pytorch-cpu-on-fedora.md","date":"2021-10-13","title":".py | Fedora 上安装 CPU 版 pytorch","layout":"post","keywords":["md","py","ai"],"excerpt":"import torch (as tf)","content":"\n马上要参加一个暑期学校，关于深度学习在显微图像处理当中的应用。\n\n深度学习是机器学习的一个分支，机器学习中的绝大多数数据都可以抽象为向量（一阶张量），绝大多数的算法都可以分解为向量之间的运算，或者对向量的变换，表示为矩阵（二阶张量）。这就对张量计算相关算法的库函数产生了很大的需求。PyTorch 和 TensorFlow，还有其他的一些库，比如 Keras，Caffe 等等等等，都是为此而生。早期版本的 pytorch 和 tensorflow 有很大的区别，但是随着版本的迭代，两者逐渐兼并和挤掉了其他的竞争者，两者的相似之处也越来越多，“变成了自己曾经最讨厌的样子”。lol\n\n不知道课上究竟要使用哪种机器学习的框架，所以决定把 PyTorch 和 TensorFlow 全都安装了（摊手）。正好可以接着上一篇的 [python 教程](python-interpreter-editor-virtualenv) 往下写。\n\n先说一下自己的软硬件环境：Intel 家的 CPU 和集成显卡（玩不了《文明6》）。虽然不在官方支持 Linux 的名单上，但是自己安装了 Fedora，内核更新了几次之后已经没有了兼容性问题。python 版本 3.9.6，包管理器是 pip，编辑器是 vscode。\n\n## 建立虚拟环境\n\n为什么要建立虚拟环境的问题本系列的前一篇已经回答过了，这次直接开干。我给两个虚拟环境分别取名为 `torch` 和`tf` 。关于命令行部分的代码，为了表示各个虚拟环境，特别加上了命令提示符`(env)[me@mycomputer]$`，抄代码的时候注意去掉。\n\n```bash\n\n[me@mycomputer]$ mkvirtualenv torch\n(torch)[me@mycomputer]$\n```\n\n## 安装\n\n在 PyTorch 官网，找到自己的硬件配制对应的安装命令：[https://pytorch.org/get-started/locally/](https://pytorch.org/get-started/locally/)。比如我的就是 `Stable`\u003e`Linux`\u003e`Pip`\u003e`Python`\u003e`CPU`。把生成的命令复制到命令行：\n\n```bash\n\n(torch)[me@mycomputer]$ pip3 install torch==1.9.1+cpu torchvision==0.10.1+cpu torchaudio==0.9.1 -f https://download.pytorch.org/whl/torch_stable.html\n```\n\n等待各种提示信息显示安装完成。\n\n## 验证和退出\n\n按照 [官网给出的方法](https://pytorch.org/get-started/locally/#linux-verification)，验证安装是否成功：\n\n```python\n\nimport torch\nx = torch.rand(5, 3)\nprint(x)\n\n# tensor([[0.3799, 0.4494, 0.4296],\n#       [0.5800, 0.0180, 0.3110],\n#       [0.9847, 0.0125, 0.2648],\n#       [0.0296, 0.3142, 0.9266],\n#       [0.3192, 0.9645, 0.5545]])\n```\n\n为了下一步安装 tensorflow，先要退出到默认的虚拟环境：\n\n```bash\n\n(torch)[me@mycomputer]$ deactivate\n[me@mycomputer]$\n```\n\n## 说好的 TensorFlow 呢\n\n本来这篇文章是打算把  pytorch 和 tensorflow 一起写了，结果 tensorflow 实在是不给力。\n\n- 直接安装\n\n在 [TensorFlow 的官网](https://www.tensorflow.org/install)上方导航栏找到 install 按钮，然后在页面左侧找到 package/pip，[安装命令](https://www.tensorflow.org/install/pip#3.-install-the-tensorflow-pip-package)也是只有一句话\n\n```python\n\npip install --upgrade tensorflow\n```\n\n然而不行，虽然安装过程中没有报错，但是验证安装的时候报出一堆错误。\n\n报错信息里有一句 `Could not load dynamic library 'libcudart.so.11.0'`，怀疑上面命令安装的是 GPU 版本。\n\n- 安装 CPU 版本\n\n在网页正文的“Package Location”一节找到了 CPU 版本的安装文件：`https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow_cpu-2.6.0-cp39-cp39-manylinux2010_x86_64.whl`，于是删除虚拟环境、重建虚拟环境、重新安装。\n\n```bash\n\n(tf)[me@mycomputer]$ deactivate\n[me@mycomputer]$ rmvirtualenv tf\n[me@mycomputer]$ mkvirtualenv tf\n(tf)[me@mycomputer]$ pip install --upgrade pip\n(tf)[me@mycomputer]$ pip install https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow_cpu-2.6.0-cp39-cp39-manylinux2010_x86_64.whl\n```\n\n运行官方提供的测试之后依然会有警告信息：\n\n```bash\n\n(tf) [shixing@yoga-laptop ~]$ python -c \"import tensorflow as tf;print(tf.reduce_sum(tf.random.normal([1000, 1000])))\"\n20XX-XX-XX XX:XX:XX.XXXXXX: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\ntf.Tensor(-1338.4773, shape=(), dtype=float32)\n```\n\n[stackoverflow 的这个回答](https://stackoverflow.com/questions/47068709/your-cpu-supports-instructions-that-this-tensorflow-binary-was-not-compiled-to-u) 说，需要从源码编译 tensorflow，具体的方法在[官网也有](https://www.tensorflow.org/install/source#linux)，但是实在是太麻烦了，~~（还是鸽了）~~ 下次单独写成一篇吧。"},{"slug":"python-interpreter-editor-virtualenv","filename":"2021-06-29-python-interpreter-editor-virtualenv.md","date":"2021-06-29","title":".py | 笔记：python 编辑器、解释器、虚拟环境","layout":"post","keywords":["md","py"],"excerpt":"","content":"\n上一篇 《[在 Windows 10 上配置 python 开发环境](python-installation-and-configuration)》 很惭愧，只做了一点微小的工作，大概三件事：\n- 一个，安装了 python 的解释器；\n- 第二个，把 vscode 安装进了电脑；\n- 第三个，就是我们知道的 virtualenv 虚拟环境。\n\n如果说还有一点什么要讲的，就是在 how 之外，讲一点 what。为了让文章更通顺一点，我打算调整一下顺序，先讲编辑器，再讲解释器，最后再说虚拟环境。虽然文章是立足于 python 来谈，但是这些知识适用于几乎所有通用编程语言。因为我也不是计算机专业出身，这篇文章只是我的学习笔记，如果有不对的地方欢迎大家指出。\n\n## 编辑器\n\n首先我们来做一个实验，把上次教程里创建的 python 文件 `hello.py` 重命名，把拓展名 `.py` 改成 `.txt`, 然后双击鼠标打开文件，会发生什么？在 Windows 系统里，会弹出最最普通的记事本窗口，窗口里是白底黑字的 python 语句，不像 vscode 里面不同语句有不同颜色，但是内容完全一样。\n\n甚至更进一步，你可以在命令行里直接让 python 编译器执行 `.txt` 文件的内容，（`py -m hello.txt`）效果和 `.py` 也是一样的。\n\n`.py` 这个拓展名什么也没有做，和 `.txt` 文件一样，内容就是一串我们人类能够读懂的字符，这样的文件叫做[文本文件](https://zh.wikipedia.org/wiki/%E6%96%87%E6%9C%AC%E6%96%87%E4%BB%B6)。处理文本文件的程序，就叫做[编辑器](https://zh.wikipedia.org/wiki/%E6%96%87%E6%9C%AC%E7%BC%96%E8%BE%91%E5%99%A8)。记事本就是一种编辑器，vscode 也是。\n\n既然记事本就可以编写 python 代码，那我还费劲安装 vscode 干嘛？确实有人真的只用记事本或其他操作系统自带的编辑器写 python 代码，（油管上还有一个视频是用 Microsoft Word 写代码；）然后用命令行调试程序，但是 vscode 毕竟是专为程序员而开发，提供了很多默认编辑器不具备的功能，比如前面提到将函数、变量、保留词显示成不同的颜色的语法高亮功能。\n\n## 编译器 / 解释器\n\n除了文本文件，这篇文章里涉及的第二种文件是[二进制文件](https://en.wikipedia.org/wiki/Binary_file)。\n\n电脑并不能直接看懂人类认识的字符，在一切的最底层，经典计算机认识的是以不同方式表示的 0 和 1。虽然说所有文件在底层都是二进制的文件，但是“二进制文件”这个名词一般专门用来表示除了文本文件之外的文件，又因为图片文件、视频文件啥的都有自己的名字，所以这个词用来指代的文件，基本上都和软件程序，可以让计算机执行的文件有关。\n\n人类只认识字符，计算机只认识 0 和 1，那么最直觉的思路就是把文本文件翻译成二进制文件。这个翻译过程叫做[编译](https://en.wikipedia.org/wiki/Compiler)，能够完成这一过程的软件就叫做编译器。编译器编译完成之后就退出了，要想执行程序，电脑直接执行编译之后的可执行文件就可以了。\n\n但是编译存在一个问题，就是整个软件需要在所有的源代码文本文件都写好的情况下才能被编译成软件，编译耗费的资源和时间随着软件规模的增长而扩大；一旦修改某处，整个项目又要重新编译。很多时候我们只想快速地知道某个大型项目中的某一句命令的效果是什么，编译这种方法就不适合这种场景了。\n\n于是就出现了[解释器](https://en.wikipedia.org/wiki/Interpreter_(computing))，这种程序比编译器复杂的多，在我们执行这种编程语言命令的时候始终运行，允许我们一句一句地输入命令，记得代码的上下文，还记得我们之前命令的结果，代价就是对于大型项目也需要一句一句地分析解释，计算资源的开销和速度都不如编译。\n\npython 就是一种（官方实现）使用解释器的编程语言，我们在官网下载的那个 `python-***.exe` 文件就是 python 的解释器。这也导致了 python 程序的性能往往不如同水平的 C/C++ 程序员写出来的程序，但是由于单句执行适合试错，所以在 ~~经常犯错的~~ 科研领域还挺流行的。\n\n当然了，python 的特点远不仅仅是解释型语言这么简单，它还是一种：\n- [脚本语言](https://program-think.blogspot.com/2009/08/why-choose-python-1-script.html)\n- [动态语言](https://program-think.blogspot.com/2009/08/why-choose-python-2-dynamic.html)\n- [面向对象语言](https://program-think.blogspot.com/2010/08/why-choose-python-3-oop.html)\n- [函数式编程语言](https://program-think.blogspot.com/2010/08/why-choose-python-3-oop.html)\n- [……](https://program-think.blogspot.com/2010/08/why-choose-python-3-oop.html)\n\n## 虚拟环境\n\n有一次我问我女朋友，她写 python 用什么 IDE，她很自豪地回答，她的 Macbook 自带 python，直接在命令行就可以运行……答非所问还不是最大的问题（仔细想想好像也不是答非所问，不过只回答了问题的一部分），而是直接在命令行运行系统自带的 python，或者其他编程语言的解释器或者编译器，本身就是编程初学者常干的一种危险行为。\n\nWindows 还好，毕竟这是一个面向广大家用消费者的操作系统，防呆设计还是挺多的，没有原装的 python。更加极客向的操作系统，比如 Linux 和 BSD 家族可就不一样了，这些操作系统（的发行版）往往预装了 python。这个 python 可不是给用户拿来开发自己的程序用的，而是用来让很多 python 语言写成的操作系统工具调用的。既然如此，这个 python 的版本一般由发行版的安装包管理者来控制，往往落后最新的 python 版本一段时间，为了避免新版本 python 有什么 bug，也为了让操作系统工具的开发者有时间更新自己的代码。所以如果直接用这个版本的 python 做开发，而且不小心自己升级了 python 的版本，很有可能导致系统的某些功能失常。\n\n虚拟环境就是 python 对这个问题的解决方案。我们可以安装不同于原装 python 的版本，但是并不将这个解释器加入系统路径，操作系统也就不知道这个版本 python 的存在。创建虚拟环境的时候，我们指定使用这一特定版本的 python，这样在虚拟环境激活之后就是我们开发需要的 python，退出虚拟环境就是系统工具使用的 python。\n\n另外，即便两个项目适用于同一个 python 版本，而且都是系统自带的这个版本，虚拟环境也有用武之地。绝大多数程序都需要依赖别人写好的工具代码，这些代码叫做库 (library)，不同的项目可能依赖不同的库代码，或者同一个库的不同版本。这个时候，可以创建不同的虚拟环境，并在其中安装各个项目对应的库，项目之间可以互不影响。\n\n## 编辑器 + 解释器 / 编译器 + 虚拟环境管理 + …… = IDE\n\n为了开发 python 程序要安装这么多不同的程序，太麻烦了，就不能一键安装全搞定吗？当然可以了，这种集成了开发过程中用到的各种工具的程序，就叫做[集成开发环境 (IDE)](https://zh.wikipedia.org/zh-cn/%E9%9B%86%E6%88%90%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83)。对于 python，最有名的 IDE 当属 [Anaconda](https://www.anaconda.com/) 了。\n\n那我为什么不用呢？当然用过，但是听说了 vscode 的大名，而且尝鲜之后，就再也回不去了。写 python 需要编辑 `.py` 文件，写博客需要编辑 `.md` 文件，博客的一些功能需要 JavaScript 实现，这些事情本质上都是编辑文本文件这一件事，在 vscode 这样的编辑器里全搞定就再自然不过了，那么 Anaconda 里的 Spyder 和 Jupyter Lab 就显得多余了。\n\n“把一件事做好”，这也是 [Unix 哲学](https://zh.wikipedia.org/wiki/Unix%E5%93%B2%E5%AD%A6)的一部分。但是问题在于，不同的人对于“一件事”的定义是不同的，有些人觉得做早饭是一件事，有些人觉得是热牛奶煎鸡蛋烤面包等等好几件事，谁是对的？\n\n也许都对，但是 编辑器 + 解释器 + …… 比起 IDE 就是处在鄙视链的上游。这一点可以不同意，但是应该要知道，不然别人抖包袱的时候你没捧上哏，挺尴尬的。\n"},{"slug":"qc-hackathon-write-up","filename":"2021-04-22-qc-hackathon-write-up.md","date":"2021-04-22","title":".qs | QC Hack 量子编程马拉松","layout":"post","keywords":["md","phy"],"hasMath":true,"excerpt":"4月初的时候，系秘书转发了一封邮件，耶鲁和斯坦福有两个关于量子计算的学生社团，打算举办为期一周的线上训练营,然后在周末举办一个24小时的编程马拉松","content":"\n\n## 一\n\n4月初的时候，系秘书转发了一封邮件，耶鲁和斯坦福有两个关于量子计算的学生社团，打算举办为期一周的[线上训练营](https://www.quantumcoalition.io/)，然后在周末举办一个24小时的编程马拉松 ([hackathon](https://en.wikipedia.org/wiki/Hackathon)) 的活动。只要年满18岁就可以参加，并不限定本科生。\n\n整个活动由几家从事量子计算的科技公司赞助，前面的线上训练营基本就是各家轮流上来介绍一下自己家的量子计算平台的使用方法，最后的编程马拉松也由他们每家出一套题，所以这个活动也有在学生和公司之间搭桥，给参与者争取实习机会的目的在里面。参与者可以自由组队，但是在项目提交的的时候每个人只能属于一支队伍。虽然参与者可以参加任意数量的题目，但是每一名参与者最终只能成为一家公司的优胜者。如果预感到自己在某一个项目的赢面比较大，可以在提交之前通知自己参加的其他队伍把自己除名。24小时的时间限制还是比较紧迫的，所以基本上认准一家答题就可以了。\n\n女朋友也收到了一样的邮件，所以理所当然地一起组队。我之前在本科阶段上过一门一学期的量子信息和量子通信课程，内容约等于在量子力学之后再上一个学期的习题课，以及在不讲群论的情况下应用 SU(2) 群，并没有接触过这个活动中会用到的编程语言。女朋友没有上过这门课，基本就是物理专业普通研究生的量子力学水平。周中的线上训练营，我只参加了第一天的，是 Microsoft 的 Quantum Development Kit (QDK) 和 Q# 编程语言相关的，顺便介绍了一下量子计算中很有名的 Deutsch 算法。剩下的讲座我基本上都没有参加，一方面是知道前面的规则之后就懒下来了，另一方面是实验室的工作仍然需要继续，再有就是线上活动实在是太容易摸鱼了没有效率。周五的晚上，女朋友看了一晚上我的量子信息笔记，我看了看 Q# 的语法规则，在台式机上安装了开发环境。以上就是我们参加编程马拉松之前的基础和准备。\n\n\n## 二\n\nHackathon 美东时间周六上午10点开始，周日上午10点结束。因为我们只看了 Microsoft 相关的内容，所以直奔[相关题目](https://github.com/quantumcoalition/qchack-microsoft-challenge)。\n\n题目一共分为两部分。\n\n第一部分一共四道题，就像是一般的计算机课程的作业一样，参赛者只需要在举办方写好的主程序文件里的指定区域填入代码，然后运行主办方写好的测试文件检查结果，测试通过即可得分。四道题目要求如下：\n\n1. 判断一个3-5位的2进制数能否被4整除。\n2. 判断一个3比特位当中是否至少有两位不同。\n3. 同第2题，但是要求量子比特门最多只能使用 3-比特，而且 3-比特门最多使用一次。\n4. 给定一个有两种颜色的无向图，判断图当中不含有任何单一颜色的三角形。\n\n第二部分内容比较自由，要求用 Grover's 算法解决一个自己感兴趣的问题，打哪指哪，然后写一篇文章介绍自己的这个项目，并提交相关的代码。根据问题深度(6分)、工具使用(5分)、创新性(4分)、教育价值(5分) 四方面进行评分。\n\n\n## 三\n\n### I.1.\n\n第一道题最简单，但是我们当时约等于0基础，所以做起来也颇费了一些时间。不过由于我听过第一天的课，知道 `oracle` 在 Q# 编程语言中是一个很重要的概念，所以在题目给出的参考教程 [Quantum Kantas](https://github.com/Microsoft/QuantumKatas/) 里找到了[oracle 相关的教程](https://github.com/microsoft/QuantumKatas/tree/main/tutorials/Oracles)。里面有个名为 `ControlledOnBitString` 的 function，可以根据一串量子比特的取值是否等于一个特定的二值串而对另外一个比特做一个特定的操作。前一天晚上又知道了 `Microsoft.Quantum.Convert` 的 namespace 里有各种数据类型转换的函数，搭配 `IntAsBoolArray`，就做出了第一题的初版。后来看到了更简单的 `ControlledOnInt` 函数，就直接用上了。\n\n### I.2.\n\n第二题的初版是女朋友做的。题目要求是找出是否至少两位不同，这一判断的否定就是三位比特全部相同，所以同样用 `ControlledOnBitString` 函数，然后判断一次全 `true` 一次全 `false`，再把最终结果取反就可以了。但是在做第三题的时候，因为两个题目长得太像了，中间不小心把一个能通过第二题测试但是通不过第三题测试的答案直接覆盖在了第二题上面，懒的改回去了，于是就成了最后提交的版本。\n\n### I.3.\n\n第三题和第二题非常不同。第二题的解决思路中，判断全 `true` 和全 `false`有3个控制位1个输出位，这里用了两次 4-量子比特门，所以第三问需要全新的思路。另外我曾经试过在一个 `operation` 里申请一个新的 `Qubit()` 结果测试报错，因为误解了报错信息，所以误认为除了程序的主 operation 之外不能创建新的 qubit，于是被卡住了。这时候已经来到了下午，实在想不出来又很困，于是去床上躺了一会。半睡半醒之间想到，题目虽然要求输入的量子比特不变，但是我们仍然可以直接改动输入，只要在函数结束之前把对输入的改动全部复原就可以了。于是用 CNOT 门分别作用在 1-2, 1-3 对输入的量子比特上，两个门分别以第2、3号比特为输出。然后用一个 3-bit 门判断2、3号比特是否相同，并输出到结果位上。为了复原第2、3号比特，只需要把 CNOT 在两对比特上分别再用一次就行了。\n\n但是这个结果还是无法通过测试（后来成为了第二题的提交版本），报错的提示信息是使用了超过一次 3-量子比特门——这不是开玩笑吗？于是打开了官方提供的测试文件，发现测试代码计算 3-量子比特门的使用次数的时候，会把用户定义的 3-量子比特门的数量，和 `CCNOT` 门的数量做加法，于是看文档，我们定义的那个 “用一个 3-bit 门判断2、3号比特是否相同，并输出到结果位上” 的操作和 `CCNOT` 门是等价的，于是直接换用 `CCNOT` 门，问题解决。\n\n### I.4.\n\n第四题看起来复杂，但是可以分成三个部分：\n\n1. 找出图中所有的三角形，确定每个三角形的三条边，这一步完全可以用经典算法完成；\n2. 创建一个和三角形相同数量的量子比特数列，对每个三角形，把三条边直接带入第二/三题的操作里，结果输入创建的量子比特列中；\n3. 判断量子比特列是否全为 `true`，结果输出到整个程序的结果位上。\n\n第一步由女朋友来想我来写（毕竟只有一台电脑有开发环境），难点在于：\n\n1. Q# 语法改变数列值的语法十分难受\n    \u003cbr\u003e`mutable points = [-1,-1,-1,-1,-1,-1];`\n    \u003cbr\u003e`set points w/=0..1 \u003c- [0,1];`；\n2. 作为一种强类型语言对元组和数列的区分让我这个 python 选手十分蛋疼\n    \u003cbr\u003e`(Int,Int)`/`Int[]`；\n3. 求数列中不重复的值居然不排序不能给出正确结果。\n    \u003cbr\u003e`let uniquePoints = Arrays.Unique(EqualI,Arrays.Sorted(LessThanI,points));`\n\n这也是唯一一段用上了 `Message()` 函数来 debug 的部分。\n\n第三步就重新回到了第三题暂时敷衍掉的问题：对于在操作中创建的 `Qubit()`/`Qubit[]`，`Reset()`/`ResetAll()` 函数相当于测量，会破坏操作的 adjoint 性质，不测量则（当时的我）没有办法将这个量子比特列复原。\n\n此时已经午夜，我来解决这个问题，女朋友去看第二部分，后来她看完 Grover‘s 算法的教程去睡了，我还在想这个问题。直接把报错信息复制到 Google，找到了一个[论坛里的问答]()，好像是去年微软在其他地方举办的类似活动的。里面只是提到要“uncompute the qubits”，给出的例子用的是旧版本 Q# 的语法，~~没法直接抄~~ 。最终不抱希望地把之前对那个 `Qubit[]` 做过的循环顺序倒过来重做了一遍，诶，您猜怎么着，还是没通过！绝望了！正序重做一边，诶，通过了！为什么为什么为什么？到现在也没弄清楚。\n\n### II.\n\n然后把女朋友叫醒，让她来讲一讲 Grover's 算法。听完之后我的理解是，对于一个 $$f:(0,1)^N \\rarr (0,1)$$ 的函数，这个算法可以大概率地找到一个解 $$S\\in(0,1)^N$$ 满足 $$f(S)=1$$. \n\n至于这个函数 $$f$$，之前每一道题都是这样一个函数，当时已经夜里两三点了，实在是没时间再想一个新函数了，于是我们直接就拿复杂度最高的第4题来换个皮。换个什么皮呢？为了这个活动翘掉了这周的[《文明6》联机游戏](barrier-forward-keyboard-mouse-to-another-computer)，然后之前看 YouTuber [\"PotatoMcWhiskey\"](https://www.youtube.com/user/PotatoMcWhiskey)介绍过[一个 Mod](https://steamcommunity.com/sharedfiles/filedetails/?id=1753346735\u0026searchtext=diplomacy)，里面可以将文明之间的外交关系可视化为无向图，所以，诶嘿嘿嘿……\n\n女朋友写完文稿就睡了，我把文稿改了改，然后和官方对 Grover's 算法的实现缝合了一下。提交的时候，距离截止时间大约还有一个小时。\n\n\n## 四\n\n之后的周五的时候收到了消息，我们得奖了。优胜者一共6支队伍。从活动结束之后公布的结果看，要想成为优胜，第一部分的4道题必须全部正确，然后第二部分得分在 8-20 分之间。\n\n这个成绩是个什么水平？截止到写这篇文章的此刻，官方题目的 Github 仓库有 80 份 fork，有少数几份 fork 是针对已有的 fork，有可能来自同一队伍，再考虑到可能有些队伍的不同成员分别 fork 了主项目，所以估测 60 支队伍应该是有的，官方给出 6 组优胜者这么一个不零不整的数字，个人猜测是取了前 10%？据主办方在 discord 提供的消息，有一支队伍的第二题成绩高于8分，但是前面没有全对，所以没有得奖；其余队伍的第二题都不超过6分；并不清楚有多少队伍第一题全对，主办方也不打算公布各队的详细成绩。\n\n这大约说明活动的参与者，其成绩基本上符合二八原理——少数人得到的分数，占据了所有参赛者全部得分的大多数。\n\n参加过这个活动之后，我们一下子就从量子计算小白摇身一变，成了优秀人才了？实际上，直到现在，我还是搞不太清楚 oracle 到底是个什么东西，女朋友对量子计算的理解估计比我还差（逃）。美国哲学教授约翰·希尔勒提出过一个叫做[“中文房间”](https://zh.wikipedia.org/wiki/%E4%B8%AD%E6%96%87%E6%88%BF%E9%97%B4)的思想实验，说一个只会说英语的人被关在一间满是汉字字块的房间里，不断从房间外收到写着中文问题的纸条。房间里有一本英文写成的手册，指示如何对输入的汉字进行回复。凭借这个手册，房中人可以在完全不会中文的情况下，与外界进行交流。希尔勒类比外人、房中人、手册，与程序员、计算机、计算机程序，认为房中人不会中文，进而论证计算机不可能通过程序来获得理解力。\n\n希尔勒教授想论证啥是他的事，我倒是对这个类比的本体很感兴趣——如果一个人已经能够熟练运用那个英文写成的汉字使用手册了，我们还能不能，能在多大程度上说他不懂中文呢？就说一般的程序员，工作时间能保证不看 stack overflow 的有几个，所以他们都不会编程？反对中文房间思想实验结论的人，很多都支持用图灵测试超过某一阈值来作为有智能的标志，但是我觉得，智能本身就不是一个非有即无的性质，而是一个连续分布，没有上限的谱。\n\n另一方面，得分名列前茅，和能力名列前茅，又是两回事。本科的时候做建模美赛，我们学校数理金融的一个学神前一年成绩“略有不佳”，没拿到 M 奖，于是我们那年找到了我和风神俩学物理的，准备再次冲击荣誉。巧了这一年的题目正好有一道浴缸放热水的问题，这不就是物理中的扩散方程嘛，那得奖还不是手拿把掐的？结果呢，H 奖，丢人丢到姥姥家去了。合着我们两个成绩还都不错的物理专业学生，在自己的专业里，打不过那么多同龄的非物理专业本科生？\n\n两相对照之下，我想起了很久之前看过的一篇博客文章，文章以一个问题开头——“熟练”的反义词是什么？当然说“生疏”这文章就写不下去了，作者给出的答案是——“应变”。熟练意味着，你对于问题、选项、最优解已经有了充分且完备的了解，只需要重复自己的经验就可以了，但是在自己不了解的战场上，经验至少不能直接派上用场，这时候，脱离具体环境的应变能力就成了生存和取胜的关键，我们当时的专业水平高不成低不就，反而成了掣肘我们的桎梏。\n\n读到这篇文章的时候，我被这种剑走偏锋的观点击中了，从那以后，一直都在注意培养自己的应变能力——如果明天我所研究的这个领域消失了，我还有没有谋生的能力？如果自己正在解决的问题被上帝或者 Matrix 作弊修改成一个新问题，我能不能看到连作弊都改动不了的题眼，然后一击命中？在凌晨两三点的时候，我也没有放弃解决第一题第 4 问的 Qubit 复位问题，虽然当时我并不知道评分标准，但是内心非常确定，这个问题必须解决。\n\n以上两次活动的成绩差别，也可以从得奖难度来看。建模美赛的 M 奖，得奖率应该远小于 10%，即便考虑到二八原理中绝大多数参赛者都只是凑数，而且样本越大凑数者越多，这个差距也还是无法忽略。我们能够得奖，和量子计算领域才刚刚萌芽，连“方兴未艾”都算不上，因此竞争并不激烈也有很大关系，应变能力是切入这些蓝海领域的必要条件，是躲避内卷的利器。我们现在对“内卷”人人喊打，但是培养应变能力是需要牺牲相当多本可以精进专业的时间和精力的。当社会中的大多数人向往着逃离内卷的时候，真的不需要有人咬定一个领域不断深耕？我现在的选择真的正确吗？我不知道。我是打算留在当前的领域继续熟练，还是换个领域应变，抑或是虚掷 PhD 光阴换一张工作签证？我也不知道。\n\n## 五\n\n哦对了，我有女朋友了，而且在 hackathon 的过程中把女朋友惹哭了……问题是我现在已经不记得具体是怎么把人家惹哭的了，连道歉都显得很不诚恳……我确实是一个不擅长合作的人，或者说跟别人说话的我，和想问题的我并不是同一个人，之前本科 CUPT 和建模的时候也一样，需要和人打交道的时候就几乎干不了活儿，严重的时候自己就退化成了鼓励师……总之一切错误在我，希望她不要记仇…… \u003cbr\u003e（。・＿・。）ﾉ\n"},{"slug":"python-installation-and-configuration","filename":"2021-03-07-python-installation-and-configuration.md","date":"2021-03-07","title":".py | 在 Windows 10 上配置 python 开发环境","layout":"post","keywords":["md","py"],"excerpt":"截图截到哭……","content":"\n我的 python 版本之前一直停留在 3.7，目前最新已经到了 3.9。最近研究上要用一段别人写的 python 的代码，原作要求 3.6 版本的 python，所以卸载了 3.7 版本，重新安装了 3.9 和 3.6 两个版本的 python。\n\n煽动爸妈学编程很久了，欠他们一篇教程。不论是打算不断精进者的第一门编程语言，还是浅尝辄止者打算学的唯一一门语言，python 都是一个很不错的选择。所以即便爸妈对编程可以说是毫无兴趣，我也还是把这篇教程写完了，希望能帮到其他人。因为是面向纯新手的，所以基于 Windows 10 操作系统。因为这套设置我自己也要用，出于个人偏好，所以没有直接使用 `conda`。\n\n## ~~别看你今天闹得欢~~ 先拉个清单\n- python\n    - 在官网下载 `python` 解释器\n    - 安装 `python` 解释器\n- 虚拟环境\n    - 设置环境变量\n    - 用 `pip` 安装 `virtualenv` \n    - 用 `pip` 安装 `virtualenvwrapper`\n    - 创建一个虚拟环境，安装 `jupyter`\n- 编辑器\n    - 在官网下载 `vscode`\n    - 安装 `vscode` 和相关插件\n    - 测试一下效果\n\n## 教程本体\n\n### 在官网下载 `python` 解释器\n\n在 python 的官网上 ([https://www.python.org/downloads/](https://www.python.org/downloads/)) 下载\n\n![](/photos/2021-03-07-python-official-website.png)\n\n### 安装 `python` 解释器\n\n执行下载的文件，可以看到如下图所示的界面，点击红框中的选项，因为我想更改安装路径。此处似乎也可以勾选最下面的 \"Add Python 3.9 to PATH\"，我安装的时候没有选，后面才发现还要手动完成这一工作。\n\n![](/photos/2021-03-07-python-install.png)\n\n接下来的界面里每个选项都勾选，next。\n\n![](/photos/2021-03-07-python-next.png)\n\n在接下来出现的界面中，选择 \"Install for all users\"，然后选择一个自己喜欢的安装路径。我之所以选择 `C:\\Python39` 这个位置，是因为:\n1. C 盘是系统盘，而且是固态硬盘，比较快；\n2. 默认路径在个人文件夹里，非常难找\n3. `C:\\Program Files` 需要管理员权限，用 `pip` 做软件包管理不方便。\n\n![](/photos/2021-03-07-python-install-path.png)\n\n安装完成后进入命令提示符，输入 `py`，应该可以看到一串 python 版本信息，然后命令提示符变成 `\u003e\u003e\u003e` 字样，此时就已经进入了 python 环境，可以按行运行 python 命令。输入 `exit()` 就可以退出 python。\n\n### 设置环境变量\n\n在 windows 的搜索框里输入 \"environment variables\"，没等你输完，应该就可以看到下图中的联想结果：\n\n![](/photos/2021-03-07-environment-variable-search.png)\n\n再按照下图，依次点击按钮，一共要做两件事:\n1. 给 `Path` 变量添加 `C:\\Python39` 和 `C:\\Python39\\Scripts` 两个新值。\n2. 新建一个文件夹，用于存放 python 的虚拟环境（比如 `C:\\PythonEnvs`）。创建一个名为 `WORKON_HOME` 的新变量，并将其值设为 `C:\\PythonEnvs`.\n\n![](/photos/2021-03-07-environment-variable-box.png)\n\n### 用 `pip` 安装 `virtualenv` 和 `virtualenvwrapper`\n\n按照刚才的步骤，此时 `pip` 应该已经安装，此时在命令行输入途中的命令，应该可以看到类似的返回信息：\n\n![](/photos/2021-03-07-pip-check.png)\n\n如果命令行说没找到 `pip`，可能需要按照[这个链接](https://pip.pypa.io/en/stable/installing/#installing-with-get-pip-py)里的方法重新安装。\n\n在命令行中输入 `py -m pip install virtualenv`， 安装虚拟环境管理器：\n\n![](/photos/2021-03-07-virtualenv-install.png)\n\n显示安装成功之后，在命令行输入 `py -m pip install virtualenvwrapper-win`，结果应该和上图差不多，忘了截图。\n\n### 创建一个虚拟环境，安装 `jupyter`\n\n`virtualenv` 和 `virtualenvwrapper` 安装完成之后，在命令行中输入 `mkvirtualenv base`，新建一个名为 base 的虚拟环境。\n\n然后输入 `workon base`，此时命令提示符的行首应该会多出一个 `(base)` 字样，这说明我们已经工作在了 base 这个虚拟环境里。此时 `pip --version` 的结果说明我们的 python 的地址已经和之前不同了。\n\n![](/photos/2021-03-07-virtualenvwrapper-check.png)\n\n在这个环境之下，安装 jupyter，输入命令 `pip install jupyter`，安装 jupyter。这一步不是必须的，但是这样我们将来安装 vscode 之后，可以获得和 conda 自带的 spyder 类似的体验，菜鸟逐行调试的时候非常方便。\n\n### 下载和安装 `vscode`\n\n从 vscode 的官网下载安装包（[链接在此](https://code.visualstudio.com/)），可以看到大大的下载按钮：\n\n![](/photos/2021-03-07-vscode-download.png)\n\n下在之后的安装过程就和一般的程序安装一样（安装很久了，没有截图）。\n\n安装完成之后打开程序，点击左侧工具栏红框中的那个图标，搜索 python 的相关 vscode 插件。python 开发一般都需要 `python`, `pylance`, `jupyter` 这三个：\n\n![](/photos/2021-03-07-vscode-plugin.png)\n\n以后要运行 vscode 的时候，在你想开始写代码的文件夹里，鼠标右键单击，选择 \"Open with Code\":\n\n![](/photos/2021-03-07-vscode-folder-open.png)\n\n然后在 vscode 的界面，在文件树上单击红框中的按钮，新建一个文件，命名为 `*.py`，这里随便写了个 \"hello\".\n\n![](/photos/2021-03-07-vscode-new-file.png)\n\n在随后打开的文档编辑区输入 `print(\"Hello World\")`，将光标停留在这一行，按 `Shift`+`Enter` 执行这一行：\n\n![](/photos/2021-03-07-vscode-run.png)\n\n因为是第一次执行，又因为安装了 `jupyter`，这时候 vscode 的右下角应该会出现一个提示框，询问是否用 jupyter interactive window 执行代码。然后关掉下方的这个 terminal (下半区右上角的叉号)。重新按 `Shift`+`Enter` 执行，此时的结果应该如下图，点击红圈中的变量浏览器，这就几乎和 spyder 的体验一样了。\n\n![](/photos/2021-03-07-vscode-interactive.png)\n"},{"slug":"barrier-forward-keyboard-mouse-to-another-computer","filename":"2020-12-22-barrier-forward-keyboard-mouse-to-another-computer.md","date":"2020-12-22","title":".md | 扫盲Barrier，用笔记本键鼠无线控制台式机","layout":"post","keywords":["md"],"excerpt":"你看我这个标题它像不像编程随想:-p","content":"\n## 为啥要用 `Barrier`\n\n圣诞节前的周五，我们系在线上办了一个聚会。和学姐闲聊的时候，听说他们高年级的几个朋友每周一起联机玩《文明6》，于是一番理(卑)直(躬)气(屈)壮(膝)的要(恳)求之后，成功加入了组织，等到了 steam 的圣诞特惠（没错我一直没买 new frontier pass，我是云玩家），准备以物理学工作者的严谨态度，研究《文明6》诱导的基于回合的反常时间平移对称现象。\n\n![](/photos/2020-12-22-livingroom.png)\n\n我现在公寓房间的结构如图所示，前任房客走的急，把她的几乎全部家具低价甩卖给我了，四舍五入相当于白捡了一台大电视。电视用 HDMI 线连接电脑，当作第二显示器使用。躺在沙发上，看着大屏幕，玩着文明6，[运用自如](https://www.bilibili.com/video/BV1g441187zq)的话说——“那岂不是非！常！爽！”\n\n所以说，我的要求是：\n\n- 在笔记本和台式机处于同一 Wifi 的情况下——\n- 笔记本电脑和台式机之间不需要任何数据线连接\n- 使用 Linux 笔记本上的触控板、鼠标和键盘\n- 控制 Windows 台式机\n- 正常运行和操作《文明6》，不会因为延迟卡顿被人喷（卡顿是不存在的，只可能掉线lol）\n\n一番搜索之后，锁定了 `Barrier` 这款软件。它可以设定一个 server 和多个 client，用 server 的输入设备控制各个 client。切换方法也很方便，只要提前约定好各个设备的“相对位置”，当 server 的鼠标光标跨过 server 屏幕的边角处，就开始控制对应方向上的设备。（具体可以查看下一节。）\n\n之所以选择它，主要原因如下：\n\n- [开源软件](https://github.com/debauchee/barrier)，堂堂正正地不用花钱。（我知道开源软件≠免费软件，但是……）\n- 目前依然有人维护，GutHub 上还有上百个 open issues，当然 closed issues 更多，有问题可以查，提问也有人回答。\n- 跨平台，Windows 和 Linux 都能用。主流 Linux 发行版都可以在仓库里找到，也可以通过 snap 安装。\n\n## 如何配置 `Barrier`\n\n### 下载和安装：\n\n因为我的发行版是 Fedora，在笔记本的 terminal 上任选一句运行：\n\n```shell\n\nsudo dnf install barrier\nsudo snap install barrier\n```\n\n台式机是 Windows 10，在GitHub 项目（[链接在此](https://github.com/debauchee/barrier/releases)）里找到最新一期的发布版本，找到 `.exe` 结尾的文件（可以用浏览器的页内搜索），下载，双击下载后的文件安装。\n\n### 配置 server （笔记本电脑）：\n\n首先要确定 linux 的桌面环境是基于 xorg 的，当前版本的 GNOME 默认使用的是 Wayland，所以需要 log out 之后重新选择带有 xorg 字样的环境，如下图：\n\n![](/photos/2020-12-22-gnome-environments-new.png)\n\n安装完成之后，在 app 界面（`Win`+`A`）应该可以找到 barrier 的图标，单击即运行。一般来说之后会有相当长一段时间电脑没有反应，这个时候 **千万不要重复点击图标**，会导致无法连接。\n\n![](/photos/2020-12-22-barrier_linux_desktop.png)\n\n之后会看到软件的主界面，选中图中所示的选项：\n\n![](/photos/2020-12-22-barrier_linux_start.png)\n\n然后单击 `configure server` 按钮，进入下图的界面，拖动右上角的电脑屏幕图标，拖到中间屏幕周围的任意一格。我选了右边一格，因为这样在笔记本上向右滑鼠标会进入台式机的显示器，从显示器右边框向右会进入电视，一路反过来就回到了笔记本屏幕，操作比较自然。\n\n![](/photos/2020-12-22-barrier_linux_config.png)\n\n双击图标后会弹出一个新窗口，在最上面的 \"screen name\" 栏填入 client 上显示的本机名称，现在我们还没有配置 client，所以需要等到配置完之后回来填写。\n\n![](/photos/2020-12-22-barrier_linux_naming.png)\n\n注意主窗口的左下角，如果不是 “barrier is running” 的话，需要点击右下方的 reload，还是不行的话就要准备 debug 了。\n\n### 配置 client （台式机）：\n\n安装完成之后，应该能在开始菜单里找到 barrier 的图标，点击运行，应该会看到和笔记本上面差不多的界面。\n\n![](/photos/2020-12-22-barrier_windows_main.png)\n\n选择图中的选项，填入笔记本窗口中显示的本机 IP 地址，选中 `auto config` 选项，然后点击右下方的 `start`，然后窗口左下角同样应该有 \"barrier is running\" 字样。点击菜单栏里的  -\u003e `show log` 打开日志，应该可以看到下图里的 \"connected to server\" 字样。\n\n![](/photos/2020-12-22-barrier_windows_log.png)\n\n### 几个雷点：\n\n- Fedora 的显示管理器默认并不使用 xorg，需要专门切换。\n- Linux 上启动较慢，如果不耐烦多点了几次，可能会重复打开多个实例，造成端口被占用，无法使用。\n    + 诊断方法：打开 log 界面，会发现 \"ERROR: cannot listen for clients: cannot bind address: Address already in use\"  字样。\n    + 解决方法：杀掉多余的进程，不会杀的话就重启电脑吧。\n- server 和各个 client 对是否使用 SSL 的选择必须是一致的。\n- 官方没有按步骤来的配置说明，这么多雷点，我居然看到好几篇博客都在夸 barrier 的界面多么通俗易懂，简直了。 \n\n## `Barrier` 之外的其他方案\n\n- `synergy`：成熟的商业软件，好像要花钱。\n- `Microsoft Garage Mouse without Borders`：要求所有设备都使用 Windows 系统，不适用于我的笔记本电脑\n- `usbip`：来自 farseerfc 大神的[这篇博文](https://farseerfc.me/zhs/usbip-forward-raspberrypi.html)，原文说“设置好的话，就像是一台 PC 多了几个位于树莓派上的 USB 端口，插上树莓派的 USB 设备统统作为 PC 的设备”。\n    + 和 `Barrier` 相比，配置的步骤更繁琐；\n    + 好像一次只能控制一台设备（不太懂，如果转发给多个设备的话可能会一起执行相同的操作？）；\n    + 好像也不适用于非 USB 接口的设备，比如笔记本的原生键盘和触屏笔；\n    + 一旦运行起来，server 的鼠标就不再能够控制自己，所以原文使用了几乎注定吃灰的树莓派，相当于给键盘加了个广播天线。\n\n\u003c!-- 用的是一款叫作 [Gather](https://gather.town/) 的 web 应用，就像 90 年代的 RPG 游戏一样，参加者每人指挥一个像素很糊的人物，在一个像素很糊的房间里走来走去，不同之处在于当角色相互靠近到一定距离之内的时候，可以像 Zoom 一样视频连线。 --\u003e"},{"slug":"py-matplotlib-two-api","filename":"2020-10-04-py-matplotlib-two-api.md","date":"2020-10-04","title":".py | matplotlib笔记：两种API","layout":"post","keywords":["md","py"],"excerpt":"import matplotlib.pyplot as plt","content":"\n“图”这个字在英语中可以对应好几个词，picture, image, figure, plot... 其中的 plot，意思是展示两组或两组以上的数据之间关系的图像。用时髦一点的话说，就是数据可视化的产物。  \n\n所谓`matplotlib`，顾名思义，~~`mat` 表示山寨 MATLAB~~，`plot` 的含义如上所述，`lib` 表示这是 python 的[一个第三方库 (library)，而不是某种领域专用的编程语言 (domain specific languange, DSL)](http://www.yinwang.org/blog-cn/2017/05/25/dsl)。\n\n所谓 API，全称是 application programming interface, 应用程序接口，约等于在你有了自己的数据，想调用 matplotlib 来画图的时候，那些需要写在你自己代码里的语句的语法规则。\n\n因为是代码库，所以在一切开始之前，需要在你的 python 代码开头声明引入\n\n```python\nimport matplotlib.pyplot as plt\n```\n\n`plt` 可以换成你喜欢并且不和其他代码冲突的名字，但是这三个字母是大家的约定俗成的，网上的绝大多数示例代码都这么写，~~照抄就完事了。~~\n\n## `matplotlib` 的两种 API\n\n`matplotlib` 有两种 API，（其实还有第 3 种 `pylab`，但它没能经得起时间的检验，已经处于官方极不推荐的状态），分别是：\n\n- 基于状态的 (state-based)\n- 面向对象的 (object-oriented)\n\n两种风格混用的话大概率没法玩得转，会产生各种出人意料的输出结果，新手 debug 的能力又比较差，所以最好先选边站队，有时间再学剩下的一个。\n\n对于有 MATLAB 基础的朋友，基于状态的 API 语法和 MATLAB 几乎一模一样，几乎可以直接上手，当年 python 算是后起之秀，这一招当初就是为了从 MATLAB 那里吸引用户， ~~相当歹毒。~~ 这套接口本身也比较简单，适合在调试程序的时候快速看一下结果，检查错误。\n\n对于一般的初学者，matplotlib 的代码本身就是用面向对象的编程范式写成的，学习这套 API 可以更好的理解代码，知道自己究竟在干什么，顺便还可以熟悉一下面向对象的编程范式。现在学 python 之前就会 MATLAB 的人越来越少，网上 ~~可供复制粘贴~~ 的示例代码越来越多地使用面向对象的语法，学习面向对象的接口也更加实用。\n\n## 两种 API 的相同任务\n\n![](/photos/2020-10-04_figure-and-axes.png)\n\n上图来自网上随便找的一篇论文，可以看到，一般我们会把信息相关的几幅小图放在一起，在文章排版的时候，这张组合在一起的图片算作一个单位。在 matplotlib 里面，这样一个基本单位叫做 `figure`，而每一幅小图叫做 `axis` （变量名常简写作 `ax`）。平时的单图可以看作只有一个 `axis` 的 `figure`，多图的时候往往用一个 tuple `axes` 的 `__getitem__()` 方法来控制每个子图。\n\n![](/photos/2020-10-04_anatomy-of-figure.png)\n\n上图[来自官网](https://matplotlib.org/gallery/showcase/anatomy.html#anatomy-of-a-figure)，图中的蓝字就是 matplotlib 认为的一张只有一个 axis 的 figure 所包含的元素。\n\n两种 API 要做的事情，就是建立 `figure` 和 `axis`，然后提供函数/方法来生成或者改变各个元素。\n\n## 基于状态 (state-based)\n\n所谓基于状态的 API，不太好解释，前面已经说过，在每个函数前面加上 plt，剩下的就和写 MATLAB 几乎完全一样。\n\n看[官网给出的教程](https://matplotlib.org/tutorials/introductory/pyplot.html#sphx-glr-tutorials-introductory-pyplot-py)，可以观察到两个有趣的现象：\n\n- 几乎没有赋值运算符 `=`\n- 几乎所有的 `.` 前面都是 `plt`\n\n也就是说，与 matplotlib 相关的命令都是函数，而且不需要将返回值赋给任何变量。`figure` 和 `axis` 的概念被隐藏起来了，`plt.figure()` 建立一个 figure；`plt.subplot()`建立多个 axes，并且将程序的注意力放到函数参数指定的子图上；紧跟着的设定各种元素的函数都会作用到之前最新一个 `plt.subplot()` 所指定的子图上。\n\n没有赋值说明函数的返回值并不重要，这些函数都会作用在后台维护的 figure 和 axis 的状态机上面，也就是说这些函数都有副作用，不是纯函数。\n\n## 面向对象 (object-oriented)\n\n[作为对比](https://matplotlib.org/gallery/showcase/anatomy.html#anatomy-of-a-figure)：\n\n- 头几句会有赋值运算符 `=`，被赋值的变量名一般就是 `fig` 和 `ax`。\n- `.` 前面都是 `fig` 和 `ax`，其中 `ax` 居多。\n\n`fig` 和 `ax` 分别是 `matplotlib.figure.Figure` 和 `matplotlib.axes.Axes` 两种对象的实例，画图和调整都是在调用两种对象的方法，主要是 `ax` 的方法。\n\n## 不同之处 Cheat Sheet\n\n绝大多数命令，在两种 API 之下的名字都一样，差别就在于开头究竟是 `plt.` 还是 `ax.`，但是少数命令不同，下面做了一个表格，进行一个不完全的列举：\n\n| State-Based | 任务 | Object-Oriented |\n|---|---|---|\n|`plt.figure(**args)`|__新建 figure__|`fig = plt.figure(**args)`|\n|`plt.subplot(**args)`|__新建 axis__|`ax = fig.add_subplot(**args)`|\n|好像没有|__同时新建 figure 和复数 axes__|`fig,axes = plt.subplots(**args)`|\n|`plt.title(**args)`|__设置 figrue 标题__|`ax.set_title(**args)`|\n|`plt.xlabel(**args)`|__设置 x 轴名称__|`ax.set_xlabel(**args)`|\n|`plt.ylabel(**args)`|__设置 y 轴名称__|`ax.set_ylabel(**args)`|\n|`plt.xlim(**args)`|__设置 x 轴范围__|`ax.set_xlim(**args)`|\n|`plt.ylim(**args)`|__设置 y 轴范围__|`ax.set_ylim(**args)`|\n\n其他不同的命令，以后用到的时候会随手更新。\n"},{"slug":"measure-david-with-imagej","filename":"2020-04-29-measure-david-with-imagej.md","date":"2020-04-29","title":".ijm | 用 ImageJ 给大卫量尺寸","layout":"post","keywords":["md"],"excerpt":"\u003cs\u003e用 ImageJ 测量大卫雕像的丁丁长度\u003c/s\u003e","content":"\n前两天看果壳，推送的头条是《古代雕塑的丁丁真的都（像大卫）那么小吗？》，为了立论，作者根据图片测量了大卫像的丁丁长度，虽不十分精确，但是好歹有个定量的结果——“4厘米不到”，还给出了使用的工具——ImageJ。\n\n![](/photos/2020-04-25_guokr-quote.png)\n\n怎么做到的，咱也来试试？\n\n## `ImageJ` 是啥？\n\n上一篇文章（[《“你这是不是原图直出啊？”》]({{ site.baseurl  }}raw-image-or-not)）我们提到了，专业相机可以记录 RAW 格式的图片，这类图片包含的信息比平时常见的图片更多，也需要专门的软件进行处理。不只是消费者水平的相机，实验室里的显微镜、CT机都可以看作是特殊的照相机，它们产生的图像自然也需要专业的软件来读取和处理。\n\nImageJ 就是这样一款软件。相比于直接使用编程语言（比如 MATLAB，python 的 scikit-image 模块），它提供的方框、多边形、椭圆选区工具，直接用编程语言来替代的话要麻烦许多；而且各种操作的结果都可以几乎实时地反馈在画面上，不需要一遍又一遍地 `imshow()`。\n\nImageJ 用 Java 写成，所以跨平台的表现一致，~~用户界面有一股浓郁的 Windows 95 风味，而且在 MacOS 上也一样，Windows 用户幸灾乐祸中~~。代码开源在 GitHub，免费，能让用户知道自己的每个操作究竟在干啥，而且可以自己开发拓展功能，打包成第三方的发行版。我们今天要用的 Fiji，就是一个集成了很多常见拓展功能的 ImageJ 发行版。\n\n## 具体怎么量？\n\n先简单列举一下步骤，详细介绍在后面：\n\n- 下载大卫像的正面照。\n- 运行 `ImageJ`，打开图片。\n- 复制图片，防止我们的操作改变原始数据。\n- 用矩形选择框选定大卫像从头到脚的区域，记录选择框的高度。\n- 用 `Analyze -\u003e Set Scale` 工具确定像素和厘米的换算关系。\n- 用直线工具画出要测量的距离，从主面板读取长度。\n- 用 `Analyze -\u003e Tools -\u003e Scale Bar...` 绘制比例尺。\n\n如果你看到这么简略的介绍就能脑补出如何操作，那么就可以 ~~（关掉这篇文章）~~ 跳过下面的详细介绍了。\n\n我们使用的照片来自维基百科的“大卫像”汉语词条，图片作者是 Jörg Bittner Unna，根据 CC-BY-3.0 协议共享。我们下载的是中等尺寸（480×720）的图片：\n\n![](/photos/2020-04-25-david.jpg)\n\n鼠标右键单击画中的任意一点，在弹出的菜单中选择 `Duplicate`，在对话框中选确定，这样我们就得到一张原图的副本。之后的所有操作都在这个副本上进行，这样即便有任何操作失误，对副本做出了不可逆转的伤害，都不会影响到我们的原始数据。如果数据的安全和完整得不到保证，无意之失叫做学术错误，有意为之叫做学术造假。\n\n选择工具栏中的矩形选择框（图中主面板左一的阴影按钮），在图中画出一个矩形，上下边分别是大卫像的头顶和脚底，**画完后鼠标不要乱动**，然后在主面板的底部读出高度 `h=595`，此处的单位是像素：\n\n![](/photos/2020-04-25_find-height.png)\n\n点击主面板的 `Analyze` 按钮，在下拉菜单中找到 `Set Scale`，在弹出的对话框中，`Distance in pixels` 填入我们读出的 595 像素，`Known distance` 和 `Unit of length` 填入作者假设的 150 cm：\n\n![](/photos/2020-04-25_set-scale.png)\n\n然后就可以开始正式的测量了。再次使用矩形选择工具，**画完后鼠标不要乱动**，主面板底端的 `length=20.93` 就是以 cm 为单位的距离了：\n\n![](/photos/2020-04-25_measure-head.png)\n\n丁丁可以选中直线工具（左起第五个），用鼠标拖拽的方法画线，主面版的底端也会显示结果。\n\n量完之后，要想方便他人，可以在图上画出比例尺。再次点击主面板的 `Analyze` 按钮，在下拉菜单中找到 `Tools`，然后找到 `Set Scale`，在弹出的对话框里选择合适的选项，然后点击确定，就可以在画面中添加一个比例尺：\n\n![](/photos/2020-04-25_menu-scale-bar.png)\n\n如果对操作满意的话，就可以保存图片了。~~（我对这个结果不满意，所以直接没有保存就退出了。）~~ 原图没有改动，可以直接关闭，软件应该也不会发表反对 ~~（通知书）~~ 对话框。\n\n## 量出来个几？\n\n我们的截图显示，按照 150 厘米的身高，大卫的头颅对应的长度是 20.92 厘米。\n\n对于丁丁，由于图片中的阴影部分对起止点的认定有很大的干扰作用，几次测量的结果都不相同，最小值是 3.91 厘米，最大值是 4.45 厘米。\n\n## 这么量对吗？\n\n我们的结果和原文有一定不同，但处于同一数量级 ~~（废话）~~。由于原文并没有介绍自己使用的数据来源，原文中的图片本身并不适合测量这个数据（下面会讨论），而且还给关键区域加了遮挡，所以我们无从比较两个结果的差异。\n\n对于我们自己的测量，我们可以对如下几个误差来源进行讨论：\n\n- 相机视差\n- 深度误差\n- 关于大卫身高的假设\n\n### 相机视差\n\n我们在几何光学里学的相机成的是倒立的实像，物高和像高之间的关系按照牛顿公式取决于物距、像距、透镜的焦距等等知识，都用到了“傍轴条件”这一假设。换句话说，考虑的都是理想状态，要想满足这种理想状态，基本相当于要求镜片的直径无限大，厚度无限小，焦距还可以是任意值。\n\n我们的相机显然不满足这种理想状态（简单证明，理想状态下大卫像和后面的墙壁显然不可能同时被同一个透镜组严格地在同一个平面上成像）。景深范围内，不同深度的物体都可以在图中成像，但是存在着透视关系。同样长度的物体，在不同的深度下，在图片上的成像的长度不同；在画面中央的长度，和在画面边缘时的成像长度也不同。B 站里的摄影师们拍小姐姐的时候，脸尽量靠近画面中间，腿一般出现在画面边缘，就是利用广角镜头的透视效应。\n\n我们的照片目测是用中长焦镜头拍摄的，透视变形不是很明显，但是在测量雕像身高的时候还是遇到了困难，我们用的是脚尖到头顶的距离，如果取脚的中间的话结果就会短一些。\n\n### 深度误差\n\n我们的照片是二维的，所以之前的测量，相当于假设被测距离的起点和终点在同一深度上。如果两点之间有一个深度差，那么测量线、深度差、实际距离三条线会构成一个直角三角形，实际距离需要按照勾股定理进行计算。我们这里得不到深度差，所以这一误差就成了一个系统误差，而且是有偏 (biased) 的，真实结果不小于测量值。\n\n我们选取的照片几乎平视着整座雕像，尤其是我们比较关注的部位，所以深度差几乎可以忽略；果壳提供的图片是仰视的，所以如果这就是测量所用的照片的话，误差应该比我们更大。当然了，还是由于作者没有公布数据和方法，以及原文中图片有马赛克，这种比较没有意义。\n\n### 关于大卫身高的假设\n\n开头的文章截图已经给出了将大卫像对应的身高定为 150 cm 的理由，我们来看一下，这个理由成立吗？\n\n文中说“这相当于现代12岁男童的平均身高”，但是米开朗基罗真的是以 12 岁的人体为原型来创作的吗？我们为什么量了一下大卫的头高呢？是为了算头身比。大卫像头身比是 1:7.5，这已经几乎是一个成年人的比例了。艺术作品为了凸显对象的某些特点和气质，某些特点不符合原型的设定，这在艺术上是完全可能而且可以接受的。《红楼梦》书中的绣像，后来的电视剧中演员的服化道，几乎都比原书的年龄设定更成熟。\n\n像是某一器官的大小，其衡量标准应该是相对于某个基准的（比如体长）的比值，也就是一个无量纲的量。像是4厘米这种带有长度量纲的量，虽然的确是能比一个比例更容易让人有直观的感受，但是结论很容易因为对身高的质疑而动摇。\n\n最后，我觉得最致命的一个问题是，大卫像其实是文艺复兴时期的作品，和罗马仿希腊的艺术品一样，是一个其他文明模仿希腊的产物，而且模仿的过程中融入了很多近代人文主义的精神。这一题材也并非来自于希腊文化，大卫击杀歌利亚是犹太教-基督教文化中的故事。在这种情况下，作者应该额外论证文艺复兴对丁丁的描写照搬了希腊文化的习俗。既然如此，为什么不直接找一个古希腊时期的古希腊作品呢？\n\n而且原文中有“你也许会说大卫像也许只是一个个例，万一只是米开朗基罗的恶趣味呢？一个例子不能说明古希腊雕塑里的小丁丁都很小”字样，不好意思，米开朗琪罗的作品不是古希腊雕塑的“一个例子”。\n\n## 课后作业\n\n![](/photos/2020-04-25_horseman.jpg)\n\n上图是法国画家雅克-路易·大卫的作品《跨越阿尔卑斯山圣伯纳隘道的拿破仑》，请根据该作品完成下列题目：\n\n1. 请寻找资料估测马的体长，并据此估测拿破仑的身高。\n2. 请寻找拿破仑身高相关的史料，并据此估测马的体长。\n3. 比较前两题的结果。\n\n啥？你说拿破仑当时骑的是一头驴？哦，那没事了，下课！\n"},{"slug":"Arch-Linux-in-VirtualBox","filename":"2019-09-03-Arch-Linux-in-VirtualBox.md","date":"2019-09-03","title":".sh | 在 VirutalBox 中安装 Arch Linux","layout":"post","keywords":["md"],"excerpt":"这是一篇速记，相当于实验的原始记录。日后有可能根据这些记录，整理出一些心得体会，或者全流程的教程。","content":"\n\u003e 这是一篇速记，相当于实验的原始记录。日后有可能根据这些记录，整理出一些心得体会，或者全流程的教程。\n\n第四代树莓派（[Raspberry Pi 4](https://www.raspberrypi.org/products/raspberry-pi-4-model-b/)）刚刚推出的时候，脑子一热买了一台。作为一台计算机，只用原装的操作系统有点咸鱼了，所以打算安装 Linux 的一款发行版。既然要追求刺激，那就贯彻到底咯，于是就决定挑战一下 Arch Linux。 ~~（反正早晚也要吃灰）~~\n\n既然要挑战一个很有挑战性的操作系统，那么先在虚拟机里试试水就很有必要了。（后来才发现用这个虚拟机往树莓派里写操作系统，也是官方比较推荐的一种安装方式。）于是就开始了这篇速记中的一系列折腾。\n\n## 我的硬件和软件配置\n\n* **硬件**：蓝厂 CPU、绿厂显卡、2*8 GiB 内存、小 SSD + 大 HDD 硬盘、iTX 主板，机龄不到两年，日常可以流畅使用。\n* **物理机操作系统**：Windows 10 Pro\n* **虚拟机控制软件**：VirtualBox 6.10，短暂用过 5.12 版本\n* **虚拟机镜像文件**：Arch Linux 2019.08.01, Windows 10, Fedora 30 Workstation\n\n## 虚拟机托管软件 VirtualBox\n\n可以看一下编程随想的博客里的 [系列文章](https://program-think.blogspot.com/2012/10/system-vm-0.html)。\n\n我的手头一直有虚拟机托管软件，但是没有编程随想那样的需求，平时几乎用不到。（用虚拟机学 Linux？你可拉倒吧）不过之前很轻松地装过一些 Linux 的发行版，主要是 Fedora，所以没觉得这部分会有什么问题，直接开始了主线任务：下载 Arch Linux 的安装 ISO 镜像、检查校验和（FCIV，教程可以看 [这里](https://program-think.blogspot.com/2013/02/file-integrity-check.html)）、按照官网 [安装指南](https://wiki.archlinux.org/index.php/Installation_guide)（[中文](https://wiki.archlinux.org/index.php/Installation_guide_(%E7%AE%80%E4%BD%93%E4%B8%AD%E6%96%87))）操作，然后发现虚拟机连不上网 :-(\n\n### 问题：虚拟机无法通过 NAT 或 NAT Network 模式联网\n\n具体表现为 `ping www.google.com` 报错：\n\n``` bash\n\u003e ping archlinux.org\n# Temporary failure in name resolution\n```\n\n此时的网络设置没有修改过，就是几乎没有可选参数的 NAT 模式。\n\n第一反应当然是把报错的信息直接复制粘贴到 Google 里，然后看到了 [这个帖子](https://bbs.archlinux.org/viewtopic.php?id=237461)。\n\n回答用一种很有礼貌的方式表述了 RTFM，并给出了官方的 [Network Configuration 页面](https://wiki.archlinux.org/index.php/Network_configuration)。\n\n按照这个页面的步骤，因为我的 ISP 给的是动态的 IP，需要用DHCP，DHCP 是啥我都不知道，搞得我跪着下载了编程随想书单里的《Richard Stevens TCP-IP 详解》，解压后看着 3 卷一百多个 PDF 文件，默默关上了资源管理器……\n\n继续变换关键词搜索解决方案，在这期间改动过防火墙设置，又重置了一次；重装了两次旧版本 VirtualBox，因为第一次重装忘记删除配置信息（Windows 10 Pro 在 `%userprofile%\\.virtualbox`，见[这个帖子](https://superuser.com/a/1429931)）；另外装了 Windows 10 和 Fedora 30 两个虚拟机；找来一个玩过 VirtualBox 而且打包票能解决问题的同学。确定了和虚拟机中的操作系统无关，VirtualBox 的 Bridge 桥接模式是可以联网的，但是总不能放任虚拟机的重要联网功能残废着啊 ~~（何况 NAT 模式在多重代理中不可或缺）~~，还是一筹莫展。\n\n就在折腾了这么几天之后，Windows 推送了一次更新，神奇的一幕发生了！更新之后，不论是 NAT 还是 NAT Network 模式，虚拟机都能正常上网，所以，可能真的是“天下本无事，庸人自扰之”吧……\n\n**解决方法：~~吃饭睡觉打豆豆~~**\n\n## 安装 Arch Linux\n\n继续按照官方指南进行安装，不得不说这份指南写的真的不好，至少对于新手是不友好的。一个典型的例子就是下面这句：\n\n\u003e 你需要安装 Linux 引导程序以在安装后启动系统，你可以使用的的引导程序在 [启动加载器]() 中，请选择一个并且安装并配置它，比如 [GRUB]()。\n\n你该怎么办？公布一下正确答案：点击 `GRUB` 那个链接，在一众分类中找到适用于你的计算机配置的那一节，目力忽略各种介绍，找到那一两行有用的命令。ε=( o｀ω′)ノ\n\n反正单看这一句，以我的理解能力，我是没办法正常通关，于是就有了下面硬盘格式化的问题。\n\n### 问题：硬盘格式化不正确\n\n我是用 fdisk 进行的硬盘分区，在正常模式之下似乎找不到选择分区类型的选项，所以我的 EFI 分区也被当作了 Linux 文件系统，所以在很多个步骤之后报错，具体的报错信息已经不记得了。\n\n还好在编程随想的博客的评论区里见到过萌狼，一个 Arch Linux 用户，顺手去他家的博客逛了逛，知道他写过一系列关于 Arch 的博客。所以找到了《给 GNU/Linux 萌新的 Arch Linux 安装指南》，于是直接重建了一个空白虚拟机，按照新教程的流程走了一遍。\n\n**解决方法：看[萌狼的安装教程](https://blog.yoitsu.moe/arch-linux/installing_arch_linux_for_complete_newbies.html)，用 cgdisk 分区**\n\n### 问题：开机后显示 UEFI interactive shell\n\n本以为万事大吉，结果卸载掉安装 ISO 之后开机，总是会进入 UEFI interactive shell。输入 `exit` 之后重新开始倒计时，然后再次进入这个 shell……\n\n就在写这篇速记的时候，我找到了官方 wiki 的 [这个条目](https://wiki.archlinux.org/index.php/GRUB_(%E7%AE%80%E4%BD%93%E4%B8%AD%E6%96%87)#%E5%90%AF%E5%8A%A8%E6%97%B6%E8%BF%9B%E5%85%A5%E4%BA%86%E6%95%91%E6%80%A5%E6%8E%A7%E5%88%B6%E5%8F%B0)，理论上应该是针对这个问题的，但是看一下就知道，并没有提供可操作的解决方案。\n\n通过 STFW，找到了这篇文章：《[如何在 VirtualBox 内安装 Arch Linux](https://cli.ee/archlinux-virtualbox)》，里面提到这个问题是由 VirtualBox 的 UEFI 引起的，同时提供了解决方法：\n\n```bash\n\nShell\u003e bcfg boot dump -v              # 查看启动的菜单\nShell\u003e bcfg boot rm 0                 # 删除光驱启动目录\nShell\u003e fs0:                           # 进入 EFI 分区\nFS0:\\\u003e ls                             # 查看目录，可以看到 EFI 目录\nFS0:\\\u003e mkdir EFI\\boot                 # 在 EFI 目录下创建子目录\nFS0:\\\u003e ls EFI                         # 查看 EFI 目录，确认是否已创建子目录 boot\nFS0:\\\u003e cp EFI\\grub\\grubx64.efi EFI\\boot\\bootx64.efi # 复制 efi 文件并重命名\nFS0:\\\u003e exit                           # 退出\n```\n\n执行完这一步，开机之后就可以看到 GRUB 的选择操作系统的界面了，因为只安装了 Arch Linux，我们只能看到两个选项，一个 `Arch Linux` 正常启动，一个 `Advanced Options for Arch Linux`。\n\n### 问题：开机后无法进入图形界面\n\n本以为万事大吉，结果开机之后选择 `*Arch Linux` 选项，屏幕上只剩下两行字，然后没有其他反应。\n\n同样是上网搜了一下，发现如果按下 `Ctrl + Atl + F2` 是可以进入命令行模式的，随便执行了几个命令，发现系统工作正常。用 `journalctl` 看一下系统日志，发现在最后一段里经常出现高亮的几行里面，都有 `drm` 这个关键词，再次搜索，发现是和显卡相关的。因为是在虚拟机里，所以就去 VirtualBox 里康康有啥可能会导致问题的选项。\n\n**解决方法：在 VirtualBox 的虚拟机显示设置界面，禁用3D加速**\n\n\u003chr class=\"slender\"\u003e\n\n至此，所有的坑都已经踩完了，开机之后可以看到正常的登录界面，Duang~Duang~~~\n\n![](/photos/arch.png)"}]],["tex",[{"slug":"bayesian-equation-and-view-of-world","filename":"2024-09-27-bayesian-equation-and-view-of-world.md","date":"2024-09-27","title":".m | Bayesian 贝叶斯，从公式到世界观","layout":"post","keywords":["tex","m"],"excerpt":"我们老板真是太能吹了，Bro 居然跟隔壁真的在研究物理的课题组 brag abou 我会贝叶斯参数估计，yo know wat ur sayin? 赶紧来补课～","content":"\n\n## 公式\n\n我上学的时候，贝叶斯公式是概率论里面，少数高中完全不涉及，到了本科才第一次见的公式，所以我从来没背下来过。不过也用不着背，根据条件概率里面的一个平凡结果：\n\n$$\n\\Pr(A|B)\\ \\Pr(B) = \\Pr(B|A)\\ \\Pr(A)\n$$\n\n可以得到 $$\\Pr(A|B)$$ 和 $$\\Pr(B|A)$$ 之间的关系\n\n$$\n\\Pr(A|B) = \\frac{\\Pr(B|A)\\ \\Pr(A)}{\\Pr(B)}\n$$\n\n这就是贝叶斯公式本体。\n\n分母没什么意思，所以一般我们要用全概率公式替换，也就是把 $$A$$ 划分为全覆盖但是不相交的 $$\\{A_i | \\ A_i \\cap A_{j \\neq i}=\\varnothing,\\ \\bigcup_i A_i=A\\}$$\n\n$$\n\\Pr(A|B) = \\frac{\\Pr(B|A)\\ \\Pr(A)}{\\sum_i \\Pr(B|A_i) \\Pr(A_i)}\n$$\n\n其中任意一个子事件 $$A_j$$\n\n$$\n\\Pr(A_j|B) = \\frac{\\Pr(B|A_j)\\ \\Pr(A_j)}{\\sum_i \\Pr(B|A_i) \\Pr(A_i)}\n$$\n\n### 根据实验结果筛选理论模型\n\n以上是数学。在科学中，令\n\n- A 为一族理论模型的一组参数取值，记为 $$Param_k$$，下标可任意选取。\n- B 为实验观测数据，记为 *Ob*\n\n$$\n\\Pr(Param_j|Ob) = \\frac{\\Pr(Ob|Param_j)\\ \\Pr(Param_j)}{\\sum_i \\Pr(Ob|Param_i) \\Pr(Param_i)}\n$$\n\n其中 \n\n- $$\\Pr(Param_j)$$ 表示第 j 组参数是模型的正确参数的，未经实验验证，根据零假设计算的 **先验 (prior) 概率；**\n- $$\\Pr(Param_j|Ob)$$ 叫做经过实验观测修正之后的，第 j 组参数正确的 **后验 (posterior) 概率**。\n- $$\\Pr(Ob|Param_j)$$ 在之前的文章中讲过，是当前测量数据下，模型参数的 **似然性 (likelihood)**。\n\n$$\nposterior \\propto likelihood \\cdot prior\n$$\n\n### 举个例子\n\n隔壁组的问题可以简化为下图：\n\n![](/photos/2024-09-27-two-gaussian.png)\n\n- 有两组数据 (x, y1), (x, y2) 可以用同一族函数来拟合。（假设为两个高斯函数的叠加，$$y=f_{A_1,A_2,\\mu_1,\\mu_2,\\sigma_1,\\sigma_2}(x) = A_1e^{-\\frac{(x-\\mu_1)^2}{\\sigma_1^2}} + A_2e^{-\\frac{(x-\\mu_2)^2}{\\sigma_2^2}}$$\n- 两组数据的误差不同。（红色数据点显然比蓝色数据点，相对于理论值偏离得更远一些）\n- 问有没有一个数值，可以衡量每组数据的误差程度。\n\n我给他们的建议是\n\n- 根据自己的专业知识指定先验概率 $$\\Pr(param_j)=\\Pr(A_1,A_2,\\mu_1,\\mu_2,\\sigma_1,\\sigma_2)$$。比如选定一个参数空间的范围，范围之外概率为零，范围之内均匀分布。\n    - $$A_1,A_2 \\in \\left[\\min(\\{Y_1\\}\\cup \\{Y_2\\}),\\max(\\{Y_1\\}\\cup \\{Y_2\\}\\right]$$\n    - $$\\mu_1\\in[\\min\\{X\\},\\max\\{X\\}],\\ \\mu_2\\in[\\mu_1,\\max\\{X\\}]$$\n    - $$\\sigma_1,\\sigma_2\\in[0,\\ \\Sigma_i\\sqrt{|X_i-\\bar X|^2/N}]$$\n- 根据一些假设和统计规律计算 $$\\Pr(Ob|Param_j)$$\n    - 假设误差与 x 变量无关，服从期望为 0 的高斯分布，$$[y_i-f(x_i)]\\sim N(0,\\sigma^2)$$，标准差根据各数据点减去模型预测值的残差估计。\n    - 假设每个数据点的观测相互独立，$$\\Pr(Ob)=\\Pr(\\bigcap_i Ob_i)=\\prod_i\\Pr(Ob_i)$$\n    - 对于模型的每一组参数 ，$$\\Pr(Ob_i|param_j)$$ 取上述高斯分布的绝对值大于残差绝对值的部分，就是钟形曲线两侧尾巴的线下面积。\n- 对参数空间中的每一组值都算出一个后验概率之后，计算整个空间的信息熵（方法见之前的文章）。误差较大的一组数据，应当有更多组参数可以获得类似的拟合结果，从而信息熵更大。\n\n## 世界观\n\n对于概率，有三种理解：\n- 古典的 (classical)、\n- 频率学派的 (Frequentist)、\n- 贝叶斯的 (Bayesian).\n\n### 古典\n\n就是将古典概型推广，成为一种关于可能性的普遍观点——一个随机空间里的随机事件可以分解成若干子事件，子事件还可以再分，直到每个基本事件的概率相等，都等于基本事件总数的倒数，而要计算人们感兴趣的某一事件，只需要数出其包含的基本事件的数量就行了。\n\n让人联想到古希腊古典时代的原子论。时人认为物质世界也不是无限可分的，将任意一种材料打碎研磨，这一过程最终会有一个终点，最终的产物就是这种物质的“原子”。一块材料的大小，就是其所含原子数量的多少。\n\n有人批评这种观点用可能性去定义可能性，有循环论证谬误之嫌。但是看现代化了的概率论，概率被定义成了满足某些条件的函数，公理化是公理化了，逻辑链条是有了坚实的起点，但是那里的概率还能不能被当作可能性的度量，实在是不好说。\n\n有人批评这是机械唯物主义，这种人批判的武器一般是武器的批判，别争辩，先活下来再说。\n\n### 频率学派\n\n这种观点一言以蔽之：概率是频率在样本量趋于无穷时的极限。\n\n科学中（日常生活中也一样，只是人们通常没这么精确），测量误差不可避免，我们每一次的测量哪怕正确，互相之间也会有细微的差别，更不用说和待测的真实值不同了。\n\n解决方法在初中物理实验里学过：多次测量，把平均值当作真值（的估计量），根据标准差计算误差（置信区间、p 值等等……）。\n\n不同的人（假设有 M 个）可以对同一个可观测量进行 N 次测量，对于一个确定的 N，不论这个可观测量本身服从何种概率分布，这 N 个测量值的平均数 $$\\bar X_N$$ 都服从正态分布，这就是中心极限定理（注意不是大数定律）。\n\n当可观测量本身也服从正态分布的时候，就会导致标准差 (standard deviation) 和标准偏误 (standard error of the mean, 常简称为 standard error) 容易让初学者混淆。\n\n而按照这种世界观，所谓一个物理量的真值，就是所有可能的（所有已经发生过的+思想实验中可能发生的）测量的均值 $$\\bar X_\\infin$$。\n\n因为包括可能发生还未发生的测量，所以哪怕我们面对的问题是纯决定论的，客观存在一个确定的真值，无论我们已经进行过多少次测量，都无法保证得到真值。\n\n有人批评这是客观唯心主义，这种人批判的武器一般是武器的批判，别争辩，先活下来再说。\n\n### 贝叶斯\n\n前述世界观好歹还认为真值客观存在——\n\n贝叶斯世界观则直接不再对真值的客观存在下断言，不论先验还是后验，科学理论里的每一条命题，都不再孤单，而是要和所有可能的替代理论打包在一起；也不再“正确”，而是具有一个以概率衡量的可信程度。\n\n实验的作用不再是判断对错，而是在有限的先验知识（现存的科学理论）下，判断新取得的实验结果在多大程度上，更新了旧知识里每条命题的可信权重。\n\n而且每个人掌握的知识不同，先验概率不同，在同样的实验数据面前，所更新出来的知识体系也会不同。\n\n再者，如果先验概率为 0，任你实验数据如何显著，后验概率也一定为 0，所以对“未知的未知”无能为力。实践中，再离谱的先验假设，只要能想到，也要赋一个小而不为 0 的初值。\n\n有人批评这是主观唯心主义，这种人批判的武器一般是武器的批判，别争辩，先活下来再说。\n\n## 送分题\n\n已知本省不超过二十个地级行政单位。一中是本市最好的高中，本科过线人数年年创新高。\n\n已知本市报纸会公布喜报，上有全市前若干名学生的姓名、分数、录取学校等信息。省招办有根据成绩取得全省排名的服务。比如某年本市第十名，全省排名两千名开外。\n\n你能否据此评价母校和家乡的教学质量，以及本省各地区之间教育水平的平均程度？\n\n你该如何评价，从定义原假设和备择假设，到用何种概率分布对先验概率建模？\n\n你有资格评价吗？"},{"slug":"nick-lane-the-vital-question","filename":"2024-07-29-nick-lane-the-vital-question.md","date":"2024-07-29","title":".tex | Nick Lane \"The Vital Question\" 读书笔记","layout":"post","keywords":["tex","bio"],"excerpt":"顾简体中文名思义，这本书把真核细胞组成的多细胞生命体称为“复杂生命”，认为这种生命形式起源于一件“在历史上只发生过一次”的事件——线粒体“内共生”(endosymbiosis)——一个古菌 (archaea) 细胞吞噬了一个有氧呼吸的细菌，然后两者一起存活了下来。前者和后者一起演化成了现代真核细胞，其中后者演化成了真核细胞中的线粒体。","content":"\n往年暑假，隔壁理论组都会发起一个不务正业系列演讲，每周五抓壮丁讲点和科研本职工作无关的话题，例如美国央行的历史、自然利率、公园里的松鼠、以 stroad (street-road) 为代表的傻逼郊区规划……\n\n结果今年，隔壁组的老板度假去了，学弟参加暑期学校去了，本科学妹到德国交流去了，只剩下一个键政人，于是我们老板非常轻松地篡组夺权，把这个活动改造成了非不务正业的读书会。在股肱之臣（我）的坚持下，免费披萨和啤酒的共和传统保留了下来。\n\n![这已经不是一般的博士后了，必须要出重拳！](/photos/2024-07-29-coup.png)\n\n这已经不是一般的博士后了，必须要出重拳！\n\n---\n\n读的书目是 Nick Lane 的《The Vital Question》。名字里的 vital 很难翻译，字面意义上是重要的意思，但 vi- 前缀本身就有和生命相关的联想。以至于台版直接起名叫《生命之源》，陆版起名叫《复杂生命的起源》。\n\n顾简体中文名思义，这本书把真核细胞组成的多细胞生命体称为“复杂生命”，认为这种生命形式起源于一件“在历史上只发生过一次”的事件——线粒体“内共生”(endosymbiosis)——一个古菌 (archaea) 细胞吞噬了一个有氧呼吸的细菌，然后两者一起存活了下来。前者和后者一起演化成了现代真核细胞，其中后者演化成了真核细胞中的线粒体。\n\n但是此事毕竟发生于几十亿年前，并没有监控录像证明事情的始末经过，所以这种说法只能算是一种有证据支持的科学假说。\n\n所谓证据，包括数量相对少的古老岩石的成分、生物化石证据，和数量非常多的现存生物体的细胞结构、不同物种间的遗传信息的对比结果等等。\n\n所谓支持，按卡尔·波普尔的科学哲学观点，指的是这些事实能够证伪 { [ (原命题) 的否定 ] 的一个子集 }。即便自身言之成理，也无法否认其他可能性的成立。持不同观点的科学家以“谁主张谁举证”的原则立论，学界根据互相竞争的理论形成的不同预测构造实验，证伪错误的主张。\n\n作者认为这一内共生事件意义非凡，线粒体这样一个专门产生能量的结构，导致真核细胞有了远多于原核细胞的能量预算（以平均每个基因所能支配的能量来衡量），从而在结构和功能的复杂度上有了重大“进步”。\n\n不仅如此，作者更进一步认为很多并不直接和线粒体关联的现象，也是这一内共生事件的结果，比如核膜的出现、性别和有性生殖的出现、物种的产生、物种寿命的长短等等。不过这一部分的证据不太充足，就连作者对自己的论证也不很确信。\n\n---\n\n虽然作者的核心观点是关于真核生物的出现，但是我反而觉得全书写作的高光，出现在对细胞从无到有的过程的推断。其因果链条极长，但又有事实作为支撑，即便怀疑，也难以否认这一理论在智力上的精妙。\n\n作者根据以下事实：\n\n- 今天细胞生物系统普遍的能量通货——ATP ↔ ADP + Pi ；\n- 这一分子普遍的产生方式——ATP 合成酶在氢离子（质子）跨过生物膜上的过程中催化，也就需要在线粒体内膜两侧存在质子的浓度梯度；\n- 现存生物系统中梯度的来源是一系列氧化还原反应，电子从食物分子转移到以 Complex I 为代表的呼吸链分子上，最后结合到氧气分子，这些分子位于线粒体内膜上，电子转移的过程会将质子逆梯度跨膜输送；\n- 细菌和古菌两种原核生物的磷脂膜成分不同。\n- 生物出现前，古代地球大气中氧气浓度很低，二氧化碳浓度高。\n- 海底板块扩张中心附近的海床上，地壳运动从地幔中露出表面的岩石中富含橄榄石，橄榄石富含亚铁离子和镁。亚铁离子被水氧化是一个放热反应，伴随着大量氢气的释放，氢气溶于碱性海水。由此形成碱性热液喷口。\n- 碱性热液喷口，喷出60℃至90℃的温水，形状是布满互相通连、迷宫一样复杂的微孔结构。\n\n给出了细胞出现过程的推测：\n\n- 生物出现前，古代地球二氧化碳浓度高，海洋呈现酸性；碱性热液喷口附近海水为碱性，于是存在一个相对稳定的氢离子/质子梯度。\n- 热力学上：碱性环境中，氢气分子氧化的反应可以自发进行；酸性环境中，二氧化碳还原成有机分子的反应可以自发进行。（详见上一篇文章中的氧化还原电位问题。）两者合并，在质子梯度存在的前提下，氢气将二氧化碳还原为有机物的反应可以放能。\n- 动力学上：碱性热液喷口附近岩石表面的金属离子可以充当催化剂，显著降低活化能。这些金属离子的催化效率虽然低，但是随着进化，金属离子周围逐渐有蛋白质修饰，进而演化为今天的生物酶分子。\n- 碱性热液喷口的微孔结构存在热泳效应，可以让有机分子高度浓缩，有助于反应速率。微孔也可以让脂类分子自发形成囊泡，包裹各类有机分子，形成细胞的雏形。\n- （此时尚无遗传信息及其复制，此套理论也没有解释遗传信息分子的进化，但是可以推论，生化反应随着进化而不断复杂，出现了 DNA、RNA、核糖体等生物信息相关的物质和结构。）\n- 其中氢气还原二氧化碳的催化剂，进化成为了能量转换氢化酶 (energy-converting hydrogsase，简称*Ech*)，负责碳代谢。它嵌在原始细胞膜上，在质子从膜外进入膜内的过程中完成催化。这一机制直到今天的产甲烷细菌仍在使用。\n- 类似地，ATP 合成酶也镶嵌在原始细胞膜上，在质子从膜外进入膜内的过程中催化合成 ATP，负责能量代谢。\n- 此时的膜结构无法阻止质子跨膜，也就无法依靠自身维持质子梯度，于是只能存在于碱性热液喷口附近。\n- 后来早期细胞进化出了一个反向转运蛋白 (antiporter)，用一个钠离子交换一个水合氢离子跨膜。膜外的氢离子浓度高于膜内，于是氢离子从膜外进入膜内；钠离子离开原始细胞，并无法通过扩散透过原始细胞膜，但是他的尺寸类似水合氢离子，于是可以通过 Ech 和 ATP 合成酶回到膜内，驱动碳代谢和能量代谢。\n- 这种方式减弱了对外界氢离子浓度的依赖，扩大了原始细胞的生存范围。\n- 然后出现了细菌和古菌的分化：\n    - 产乙酸菌（细菌）的祖先倒转了 Ech 的方向，让其成为了维持质子浓度并消耗能量的质子泵，利用其他机制进行谈代谢。\n    - 产甲烷菌（古菌）的祖先演化出一个新的质子泵来运出质子，仍用质子通过原来的 Ech 来进行碳代谢。\n- 两种生物采取了不同的方式维持质子浓度，也就导致膜的选择性也是独立演化出来的，磷脂分子的结构不同，细胞壁的成分也不同。\n\n---\n\n看得出来，我们老板对这套理论应该是非常买账的。实验室刚创建还在施工，只能带着我们做 journal club 的时候就给我们安利过这套假说，之后也集中读过几篇反驳书中观点的论文，以及作者及其支持者们的回应，以及进一步的反驳。\n\n没错，学界存在很大的一批势力，对这一理论大加挞伐：\n\n首先是这个从原核细胞到真核细胞是一种进步的观点，已经不太符合现代生物学对 evolution 的理解了，甚至这个词现在都已经普遍不再翻译成“进化”，而代之以更中性的“演化”。\n\n同一环境下能够幸存的所有物种，在演化看来都是成功的，并不存在谁比谁更优越，即便有，衡量标准也是该种群遗传信息的拷贝数，从这个意义上看，细菌和古菌往往比同一环境中的单细胞真核生物还更成功些。（我对拷贝数这个衡量标准倒是也有些保留意见。）\n\n其次是这个“平均每个基因所能支配的能量”作为真核细胞优越性的标准，有点先射箭再画靶的嫌疑。\n\n虽然正方的主要观点结集在这本书里，而反方主要发表在学术期刊上，给人一种民科大战官科的观感，但是两方曾在一些顶级生物期刊上正面笔战，双方也都是生物学研究的业内人士。\n\n那些反驳论文是很久以前读的了，所以这里只是凭回忆概括一下，如果将来对这个问题还感兴趣的话，可能会再回去读一下。\n\n---\n\n由于内共生事件是本书的主角，而它在时间上位于生物演化中间，本书的行文顺序容易让人丧失时间观念。\n\n本来打算重新整理一下自己的笔记，写成一个编年史的体例，但是本书的很大篇幅用来讨论原因、机制、后果等等，试了一下发现效果也很不好。\n\n而且作者的行文思路也很跳脱，经常跑一段题，然后后面无缝接回刚才的话题……\n\n斗胆揣测一下原因，可能是是作者先写成了初稿，然后在编辑的威逼利诱下灌了水～\n\n![对于编辑第二重要的工作是……](/photos/2024-07-29-editor-job.png)\n\n对于编辑第二重要的工作是……\n\n这就导致在每周的讨论时间，很多时候他们提出的问题，都是作者在书里已经写过的，需要我对照自己读汉语版做的笔记，倒回去找到英文版的章节，指给他们看。\n\n当然了，也有可能是我个人的汉语阅读能力超出了其他人的英语阅读能力，但是这里的其他人包括了我老板，所以我觉得不太可能。\n\n# 读书笔记\n\n\n- Introduction: Why is life the way it is?\n- Part I: The Problem\n    - `1.` What is life?\n    - `2.` What is living?\n- Part II: The Origin of Life\n    - `3.` Energy at life’s origin\n    - `4.` The emergence of cells\n- Part III: Complexity\n    - `5.` The origin of complex cells\n    - `6.` Sex and the origins of death\n- Part IV: Predictions\n    - `7.` The power and the glory\n    - Epilogue: From the deep\n\n## Introduction: Why is life the way it is?\n\n开头，作者似乎甚至要把细菌开除生物户籍？好吧，是用 complex life 称呼以真核细胞为单元的复杂生命体。（那酵母呢？后文说单细胞阿米巴虫也算 complex life。）这个定义和 multicellularity 还不一样，也就是说，本书的内容范围远小于生物学研究对象的全体。\n\n细菌之于生物，类似原子之于物质——Kluyver’s students Cornelis van Niel and Roger Stanier: “Bacteria like atoms could not be broken down any further.”\n\n1. 线粒体和叶绿体起源的 endosymbiosis 假说。作者认为这两件事历史上各自只发生了一次，我有点怀疑。\n2. 核糖体基因在不同种生物间的基因差异推断出的演化距离，从而把古菌 archaea 从 bacteria 中独立出来。\n3. 基因对比表明内吞线粒体祖先的是一种 archaea. Bill Martin \u0026 Miklós Müller 认为真核生物的出现和线粒体祖先被内吞是一回事。\n\n作者的观点是：这一内吞给真核生物带来了能量上的优势。呼吸作用的能量基本用于质子泵产生跨膜质子梯度。\n\n## Part I: The Problem - 1. What is life?\n\n### ===\n\n人对于地外生物的想象，其实是反映了对地球生物特征的模糊总结，和无意识的刻板印象。\n\n物理学试图回答为什么物理定律是我们所知道的样子。（呵呵。）但是生物学很少有这方面的 predictability. （可预测性和为什么之间能划等号吗？）\n\nJacques Monod 在《Chance and Necessity》中认为，生命的产生来自于偶然。\n\n其他人反对道，（实际作者也持这种观点，）生命是宇宙化学的必然产物。一些物理定律可能会限制生物进化，导致趋同演化。\n\n但是由于只有一个地球，从统计学上来看，没办法判定到底有没有这样的定律，以及是哪些定律。进化论擅长总结历史，很难预测未来。作者认为能量的限制就是一个物理上的限制，也就是这本书的核心主张。\n\n薛定谔的《啥是生命》里认为，遗传物质是一种 aperiodic crystal，并非严格重复的结构，从而可以成为代码本。\n\n不同物种的基因量很反常识。洋葱、小麦、阿米巴虫的基因数量和DNA量比人类多。不同种类的两栖类的基因量跨度在两个数量级，其中能达到人类的 40 倍。\n\n基因组结构和尺寸没有明显的物理限制，其携带的信息就也不受限制。生物本应该可以在自然选择之下长成相应环境的最优解，但历史上的古生物显然不是这样的。所以这种限制不来自基因组。\n\n### A brief history of the first 2 billion years of life\n\n早期地球：\n\n诞生于 45 亿年前，开始的约 7 亿年不断受到小行星撞击，其中一个有火星的大小，并导致了月球的产生。早期岩石在地球上已找不到，但是在月球上可以找到。\n\n锆石晶体络合的分子表明：地球很早就出现了水，而且温度不高，和地球早期是到处都有沸腾岩浆的炼狱的传统观点不符；气体方面，传统观点认为空气中充满甲烷、氢气、氨气，新发现认为氧化物较多，如二氧化碳、水蒸气、氮气、二氧化硫，除了没有氧气和今天的大气层很像。结论：细菌可以在这样的环境中演化。\n\n早期生命：\n\n格陵兰西南的 Isua 和 Akilia 是已知最早的岩石，可追溯到 38 亿年前。生命的证据是岩石中碳颗粒中各同位素的比例，因为生物基本上只利用较轻的同位素。有人不同意这是生命的证据，作者认为即便不是，这种地球化学作用也是非生命到生命连续谱上的一环。\n\n澳大利亚和南非的古岩石包含了细胞化石，这是生物存在的确定证据。\n\n32 亿年前的地层出现了铁和碳的富集，这是原始光合作用的产物。\n\n29 — 24 亿年前，制氧的光合作用出现，至于 22 亿年前的大氧化事件，导致了地球的冰川化。至此生物的绝大多数生化反应都以出现，由细菌和古菌完成。\n\n（下一节中提到）16 —12 亿年前开始出现真核细胞的化石。7.5—6 亿年前，在一段地质活跃和冰球期之后，氧气含量再次快速上升，大到直径 1 米的生物化石开始出现，随后再一次大灭绝，5.41 亿年前寒武纪物种大爆发，大量动物开始出现。\n\n### The problem with genes and environment\n\n最传统观点认为氧气是个好东西，它的出现为进化松开了刹车，因为细胞有氧呼吸产生的能量（以能量而非功率衡量？按细胞平均还是按化学反应平均？）比其他呼吸方式高 1 个数量级。\n\n稍新一点的观点认为氧气不是好东西，过于活泼且对无氧呼吸的物种有毒。生命是靠共生和内共生才克服了氧气的存在，进化出了今天有氧呼吸的复杂物种。\n\n作者不同意后一种观点的潜台词——基因和环境构成了生物学所研究的一切，而氧气是环境的核心变量，塑造了进化图景。作者认为基因和环境之外，细胞和物理限制也是生物学的研究对象，而且后者很少能从前者直接得到答案。\n\n如果有无氧气是导致进化爆发与否的原因的话，作者认为应该看到今天的好氧生物进化自多种细菌祖先，即 polyphyletic radiation。而如果进化的瓶颈是生命本身的结构的话，我们就会看到有着“正确”结构的少数甚至单一谱系的后代占据大量生态位。\n\n这正是真核细胞的出现时的情况，而从单细胞到多细胞反而有大约 30 个独立的进化路径。\n\n### The black hole at the heart of biology\n\n前一节的 polyphyletic radiation 可以来自于自然选择，也可以是内共生的结果。但是现实并非如此。真核细胞生命体的细胞之间几乎看不到（内共生理论描述的现象之外的）区别。（那细胞壁呢？）\n\n内共生理论本身不排除 polyphyletic radiation. Margulis 的原教旨内共生理论甚至认为所有细胞器都是内共生而来。已知超过一千种的没有线粒体的真核生物似乎支持了这一理论，其中 Giardia 甚至有两个细胞核，Cavalier-Smith 把它们叫做 archezoa，主张这些物种是内吞线粒体之前的真核细胞，因此真核细胞在内共生之前就有真核细胞的各种特征，并非来自外部。（这个结论认为线粒体只能是第一种被内吞的细胞器，理由呢？其他细胞器不是内吞而来的，证据是基因吗，可靠吗？）总之这是现代真核细胞起源的教科书级观点。\n\n现代基因组研究发现，这些物钟并非原核生物和真核生物的过渡，而是曾有包括线粒体在内的真核生物的一切结构，然后在进化中把线粒体简化成了 hydrogenosomes 或 mitosomes。\n\n（解决了刚听说这本书的时候我的质疑：）That does not in itself mean that the origin of complex cells was a rare event. In principle, complex cells could have arisen on numerous occasions, but only one group persisted – all the rest died out for some reason. 但是作者又加强了自己的观点：这种其他合并事件的后代被淘汰的场景也没有发生。（为什么）\n\n据共同祖先，可以将真核细胞根据形态分为 5 组，各组组内的基因差异大于五组原始祖先的基因差异。符合 early radiation 尤其是 monophyletic radiation 的结果。除了叶绿体之外，原始祖先几乎啥都有了。\n\nAll are sexual, with a life cycle involving meiosis (reductive division) to form gametes like the sperm and egg, followed by the fusion of these gametes. The few eukaryotes that lose their sexuality tend to fall quickly extinct (quickly in this case meaning over a few million years). （等一下，酵母呢？）\n\n问题来到了几乎什么都有了的初代真核生物，它的各部件是哪里来的呢？细菌没有任何往这方向演化的迹象。\n\n### The missing steps to complexity\n\n作者也承认大量中性进化的积累本身会筛选掉进化的中间态，这符合原核生物和真核生物之间没有过渡物种的发现。（把我好闪，看了两遍才明白。）\n\n但是这解释不了内共生是一个一次性事件。（前面也说了可能是多次发生，但是其他事件的后代没有留存下来，主张了这种情况没有发生，但是至今未举证。）\n\n靶观点是两者之间的竞争，真核生物的适应性如此之强，以至于往真核生物某些特征发展的原核生物都被淘汰了。问题在于原核生物的数量众多，且基因可以横向转移，很难在演化中灭绝。所谓 oxygen holocaust，其实至今没有证据证明存在。\n\n而且之前提到的 archaezoa，虽然他们不是演化上的中间物种，但却是生态位上的中间物种，说明这些生态位不会被淘汰。真核细胞可以再简化，但是原核细胞无法复杂化，可能的解释来自于他们的结构。\n\n（作者说的细菌在有些情况下同时指真细菌 (bacteria, eubacteria) 和古菌 (archaea, archaebacteria)）两者基因和生化反应不同，但是形态相似。再次说明结构上的物理限制。\n\n### The wrong question\n\n要回答复杂生命产生的原因，其答案需要强到足够发生，但有没有强到会在演化历程中多次发生。\n\n答案并不能只从生物信息中寻找，需要演化的具体历史。\n\n扯了扯熵和自由能。说下一章要讲自由能里“自由”的意思。\n\n薛定谔的书名问错了问题，不应问 what is life, 而是 what is living.\n\n## Part I: The Problem - 2. What is living?\n\n### ===\n\n作者认为病毒不 alive，因为没有自有的 active metabolism。\n\n但随即自问，难道复杂生物不也是环境的寄生物吗。\n\n区别在于量的不同，病毒存活的环境里有他们需要的一切，而植物几乎不太需要从环境摄取有机物，我们处于两者之间。\n\nNASA 对生命的定义：a self-sustaining chemical system capable of Darwinian evolution\n\n而一个生命体的生死状态也可能是连续的，比如病毒、孢子、水熊虫的休眠状态。为什么他们的这些状态不会按热力学第二定律衰变呢？这说明生命和活着还有区别。生命是关于结构的，而活着是关于生长、繁殖，和环境互相关联。\n\n### Energy, entropy and structure\n\n反直觉的现象：孢子和把孢子磨碎到分子成分的水平，两者的熵变化不大。According to the careful measurements of the bioenergeticist Ted Battley, entropy barely changed.\n\n（~~我感觉这一节完全扯淡，事实错误。~~）像磷脂双分子膜可以在水中自发形成，因为其分子有疏水基团，所以双分子膜的自由能比这些分子散落在水中的自由能更低。（我依然感觉这一节完全扯淡，作者在谈自由能和熵的时候没指明自己所说的对象。如果说打碎的孢子是水油混合的，那就说明系统不在热力学平衡态，静置等待油分子再成膜，（这些成分不可能自发变回原来的孢子，）然后计算熵值呢？）\n\n（~~后面对于活化能的描述也很离谱，感觉连《细胞生物学精要》都没看过的水平。~~）\n\n（）\n\nEverything that happens in a living cell is spontaneous, and will take place on its own accord, given the right starting point.（这话不对吧）\n\n### The curiously narrow range of biological energy\n\n\u003e A single cell consumes around 10 million molecules of ATP every second! … There are about 40 trillion cells in the human body, giving a total turnover of ATP of around 60–100 kilograms per day. … In fact, we contain only about 60 grams of ATP  we know that every molecule of ATP is recharged once or twice a minute.\n\u003e \n\n\u003e We use about 2 milliwatts of energy per gram – or some 130 watts for an average person weighing 65 kg, a bit more than a standard 100 watt light bulb. That may not sound like a lot, but per gram it is a factor of 10,000 more than the sun.\n\u003e \n\n\u003e Bacteria such as E. coli can divide every 20 minutes. To fuel its growth E. coli consumes around 50 billion ATPs per cell division, some 50–100 times each cell’s mass.\n\u003e \n\n生物的单位体重功率极高，但不违背物理定律，因为洒在地球上的太阳能的总功率更高。（@钱院士）\n\n尽管如此，生物系统却及其受限于能量。\n\n（下面的基本上就是高中生物奥赛的分子生物学部分～）\n\n然后讲了讲 ATP 和 ADP，呼吸作用中的电子传递链，iron-sulphur cluster。就是说生物系统利用的是化学能，而不是热能、机械能、定向电流……（但是你说 UV radiation 就有点难绷。）另一个特殊之处在于 ATP 和 ADP 的转化是通过跨膜的质子泵完成的。\n\n然后作者开始水字数。ATP 大小比作人的话，Complex I 就有一座摩天大楼那么大。将质子泵过膜有两个结果：一是膜两侧的质子浓度差，二是电荷浓度差/电势差。\n\n\u003e The electric field you would experience in the vicinity of the membrane – the field strength – is 30 million volts per meter, equal to a bolt of lightning,\n\u003e \n\n电势差驱动 ATP 合成酶。\n\n\u003e For every ten protons that pass through the ATP synthase, the rotating head makes one complete turn, and three newly minted ATP molecules are released into the matrix.\n\u003e \n\n### A central puzzle in biology\n\nPeter Mitchell 的化学渗透假说 (chemiosmotic hypothesis): [https://pubmed.ncbi.nlm.nih.gov/13771349/](https://pubmed.ncbi.nlm.nih.gov/13771349/), 上文所说的过程叫做氧化磷酸化，1970s 才被解开。\n\n\u003e John Walker’s Nobel Prize in 1997 for the structure of the ATP synthase\n\u003e \n\nMitchell 给出一个更深刻的问题：生物是怎么让内部不同于外部的？\n\n不同于试管化学，生物体中的化学是向量化的，沿着膜的法线方向。\n\n生物使用氧化还原反应提供自由能，（电子的空间分布）；使用质子在膜两侧的渗透压储能，（质子的空间分布）。为什么要这么做？于是引出下两节的内容：\n\n### Life is all about electrons\n\n为什么要用氧化还原反应，答案比较简单，生物是碳基的，碳的最终来源主要是空气中的二氧化碳，正4价，而大分子有机物中的碳原子普遍低于 4 价，而改变化合价的反应自然就是氧化还原反应。\n\n之所以要以碳为骨架，是因为碳有 4 个成价电子，且比硅电负性更高；有气体形态的氧化物。\n\n但是现代物种中，碳代谢和能量代谢基本都独立开了，只由 ATP, thioesters 等少数几种分子链接，而且这些分子不需要生物通过氧化还原反应合成。\n\n而且也有其他的反应被认为是生物最初的反应，比如氮气和甲烷在紫外线下合成氰化物。这样的反应为什么没有在今天为生物供能呢？\n\n用电子传递链的好处是，这一链条的起始端并没限定反应物，不仅食物可以，一些细菌可以将矿物质和气体作为呼吸作用的反应物，而整个链条不会有太大变化。\n\n链条的终点也不一定是氧气，而可以是很多其他氧化物，甚至一些细菌可以呼吸产生矿物质。\n\n### Life is all about protons\n\n化学渗透现象的普遍性表明，这一机制在生物体中出现得非常早。与之同样普遍的是 DNA 转录成 RNA 再翻译成蛋白质的过程。\n\nArchaea 和 bacteria 的区别主要在于内部的生化反应种类，比如 DNA 复制用到的酶，细胞壁的化学成分，发酵作用的生化通路，细胞膜的化学成份。在如此基本的地方都有区别的前提下，化学渗透却是普遍的。\n\n这些异同对两者的共同祖先构成了疑问。\n\n考虑膜，立了一个靶子：可能原始祖先有细菌一样的膜，然后 archaea 替换成了自己的膜——以适应高温环境。（这个归因哪来的？）后面就是对高温的反驳——细菌也能生活在高温里，而绝大多数环境下两者没有演化优势。\n\n然后是，膜的具体成分区别（为什么不早说，是不是因为这会导致前面的疑问很傻逼？）是，细菌和古菌使用的是甘油的两种镜像异构体。但是两种合成甘油的酶在演化上并非很近的亲缘关系，也就是说 archaea 重新发明了一种酶，就为了利用同一种分子的异构体。因此祖先细胞膜成分应该和现代细胞很不一样。\n\n化学渗透出现得很早这一观点也有问题，因为 ATP 合成酶的结构十分精巧。现代细胞的 ATP 合成酶只在膜不允许质子透过的情况下才能运转正常，而古细胞膜是可以允许质子透过的。在没有质子浓度差的情况下，如何演化一种需要质子浓度差的酶，就成了一个鸡生蛋生鸡的问题。\n\n所以作者把这个问题和复杂生命的起源问题合并回答——化学渗透耦合将细菌和古菌困在他们的结构长达几十亿年的时间，直到一个偶然的内共生事件使得形成的真核细胞突破了能量限制。\n\n## Part II: The Origin of Life - 3. Energy at life’s origin\n\n### ===\n\n（相对来说读起来质疑最小的一章～）\n\n长链代谢路径+特异性的酶的作用是提高产物分子的特异性，从而避免生成不需要的副产物，节省能量。反过来说，在进化早期，产生同样的目标产物的过程会生产比现在多得多的副产物，对能量的需求反而更高。\n\n\u003e In 1953, Stanley Miller was an earnest young PhD student in the lab of Nobel laureate Harold Urey. In his iconic experiment, Miller passed electrical discharges, simulating lightning, through flasks containing water and a mixture of reduced (electron-rich) gases reminiscent of the atmosphere of Jupiter… Amazingly, Miller succeeded in synthesizing a number of **amino acids**… Miller, in contrast, featured on the cover of Time magazine in 1953.\n\u003e \n\n米勒-尤里实验和沃森-克里克解析 DNA 结构发生在同一年，但当时获得了更大的名声。两个发现其实都迷惑了生物学家：前者巩固了原始汤理论，后者让人将生命等同于信息，从而生命的起源等同于信息的起源，进一步编程信息分子自我复制的起源。\n\n因为结构的复杂度，最早的自我复制因子不太可能是 DNA，RNA 比较符合原始信息分子的设定。于是传统观点认为原始汤产生了 RNA 世界，然后逐渐演化成今天的复杂世界。\n\n但是能量上说不通，米勒-尤里实验即便在早期地球上能发生，闪电的能量和产物的数量很难满足生命产生的需要。紫外线更有希望一点，但是紫外线对现存的生物物质都是有害的，会导致其分解；且紫外线催化的有机反应的产物是氰化物，也是现存生物不利用的有毒物质。\n\n所以支持这一观点的科学家在地质学上寻找海洋超量蒸发的事件，认为这些事件导致的有机物浓度升高，从而使早期生命活动得以发生。但是这样的剧烈事件对生物本身却是灾难性的。\n\n生命是远离平衡态的 (equilibrium)，但也是稳态的 (steady state)，不断自我更新的同时维持不变的结构形式，这需要持续的能量流，来自俄裔比利时物理学家伊利亚·普利高金（Ilya Prigogine）提出的“耗散结构”（dissipative structure). （作者说这个过程和信息无关，我觉得不对，但是不影响其他结论。）\n\n### How to make a cell\n\n细胞的特点：\n\n1. 持续的活化碳供应，用来合成新的有机物\n2. 自由能供应，用来驱动代谢生化反应，包括新的蛋白质和DNA等物质的合成；\n3. 催化剂，用来加速和引导代谢反应；\n4. 泄废弃物，遵循热力学第二定律，驱使化学反应以正确的方向进行；\n5. 区隔化 (compartmentalization)，细胞式的结构，把内部和外部分隔开；\n6. 遗传物质，即RNA、DNA或同等物质，用来承载信息，规定具体的结构和功能。\n\n（然后他开始退回去谈上一节的信息分子复制了。~~感觉这本书的叙述顺序十分离谱，以至于各章节标题都有点问题，感觉像是被编辑无理裁切过一样。~~依然是在谈上面的六个特点，但不是从 1 到 6 的顺序～）（第 6 点：）新陈代谢和复制哪一个先出现？复制的指数性质使得代谢很难跟上遗传分子的增长速度。\n\n（第 6 点：）Graham Cairns-Smith 主张最早的信息分子是矿物质而不是有机分子。比较离谱，因为晶体的编码量太小。\n\n（第 1, 2 点：）脱水缩合反应在水溶液中为什么可行——与 ATP 水解耦合。\n\n（第 5 点：）囊泡可以自然形成，因为会增加总体熵。（不同意。）这样可以在有限空间里获得较高的分子浓度。表面积和体积的量纲不同也会导致自发分裂。\n\n\u003e 没有细胞壁的 L 型细菌 (L-form bacteria) 正是使用这种出芽生殖方式来分裂\n\u003e \n\n（第 4 点：）表面积与体积的比例问题，同时也限制了细胞的大小。这个问题的实质是反应物的供给与废物排泄的速度问题。原始汤理论对废物的移除又是一个根本的缺陷。因为在原始汤场景中，反应物和废物全都“腌”在一起。\n\n（第 3 点：）现代生物都以蛋白质（酶）为催化剂，但RNA也有一定的催化能力。最初的生化反应应该由金属离子催化，这些离子直到今天也是一些酶的配体 ligand。\n\n总而言之：大量的有机碳与能量流被引导流过无机催化剂。下一节是作者给出的具体答案：碱性深海热液喷口。这顺便也回答了上一章结尾的问题。\n\n### Hydrothermal vents as flow reactors\n\n深海热液喷口（黑烟囱）：\n\n水和岩浆的反应。\n\n微生物学家约翰·鲍罗什 (John Baross) 开始研究。\n\n金特·瓦赫特绍泽 (Günter Wächtershäuser) 提出了黄铁矿拉力反应机制来解释生命起源，但是问题很多：\n\n- 这一机制需要一氧化碳而非二氧化碳做反应物。\n- 温度太高，有机物会迅速降解为二氧化碳。\n- 强劲水流会让有机物快速扩散，且黑烟囱本身寿命也只有几十年。\n\n\u003e 黄铁矿常由亚铁离子（Fe^2+）和硫离子（S^2–）形成不断重复的晶格。直到今天，很多酶的核心仍然存在亚铁与硫化物形成的铁硫簇，包括一些呼吸蛋白都是这样。这些作为酶核心的铁硫簇结构，本质上与铁硫矿物的晶格结构完全一样.\n\u003e \n\n碱性热液喷口：\n\n迈克·拉塞尔（Mike Russell）在一篇1988年发表在《自然》上的短文章中提出碱性热液喷口与生命起源的关系。（预言性的学说，而非观测结果。）Bill Martin 以微生物学家的视角加入研究。\n\n（后面的内容就需要和下一节合并了，讲的都是这一学说的合理性，碱性只是合理性之一。）\n\n### The importance of being alkaline\n\n- 通过地壳运动从地幔中露出表面的岩石中富含橄榄石。橄榄石与水反应，会生成一种名为蛇纹岩（serpentinite）的水合矿物。\n- 大多发生在板块扩张中心附近的海床上，靠海水渗透到海床以下的岩层，有时深达数公里。60℃至90℃ 的温水。\n- 它们的形状不是大张口、直接向海洋喷出强力水流的烟囱，而是布满互相通连、迷宫一样复杂的微孔结构。\n    - 通过热泳（thermophoresis）过程，有机分子的浓度能达到起始浓度的数千甚至数百万倍。\n    - 它可以让脂肪酸自发形成囊泡，还有可能让氨基酸聚合成蛋白质，让……任何增加分子浓度的过程，都会促进分子之间的化学作用。\n- 它们不是酸性，而是强碱性。40亿年前的冥古宙，\n    - 水中没有氧气，铁会以亚铁离子的形式溶解在水中。\n    - 大气和海洋中的二氧化碳浓度比现在高得多。\n        - 这么多无机碳供应使得这些远古的喷口结构发展不受限制；\n        - 高浓度的二氧化碳使原始海洋酸化，让碳酸钙不容易沉淀\n- 2000年第一个海底碱性热液喷口区被发现，并命名为“失落之城”。\n\n高浓度的二氧化碳、微酸的海洋、碱性热液，再加上含有铁硫矿的喷口薄壁，这一组合——\n\n- 从热力学角度看，\n    - 二氧化碳会与氢气反应生成甲烷。这是一个放热反应，如果有机会，这个反应会自动发生。\n    - 这里的特定条件包括适中的温度，以及不能有氧气。\n    - 这些条件下，且环境温度在25℃至125℃之间，以二氧化碳和氢气为原料合成全套细胞生命物质（包括氨基酸、脂肪酸、碳水化合物、核苷酸，等等）是放热反应\n- 二氧化碳和氢气之间还有一道动力学障壁需要应对两道能量障壁。\n    - 第一道需要跨过去，达到甲醛和甲醇的还原程度；\n    - 第二道则绝不能跨过，细胞现在绝不能让反应进行到底，生成甲烷。\n\n\u003e 从氧化还原程度来看，生命大致相当于甲醛与甲醇的混合物。\n\u003e \n\n而让上述反应在动力学上可行的，就是质子浓度梯度，也就是下一节的内容。\n\n### Proton Power\n\n还原电位和电负性两个概念之间是什么关系？【Link】\n\n中性环境中，氢气的还原电位是 -414mV，甲酸盐的还原电位是 -430mV，甲醛还原电位是 -580mV——氢气不可能还原二氧化碳。\n\n分子的还原电位经常随着pH值改变，也就是随质子浓度的变化而变化。但是方向对于上述三者是一样的，单纯改变pH值没有任何作用，氢气仍然不可能还原二氧化碳。\n\n现在考虑隔着一层膜的质子梯度，膜两边的质子浓度（酸性）不同。在 pH 值为 10 的环境中，氢气的还原电位是 -584mV；在 pH 值为 6 的环境中，甲酸盐的还原电位是 -370mV，甲醛是 -520mV——氢气还原二氧化碳非常容易。\n\n碱性热液喷口就存在这样的现象。\n\n电子究竟是如何从氢气传递到二氧化碳的？答案就在薄壁结构中，那些嵌在微孔薄壁上的硫铁矿物质。它们虽然远没有铜导线那么好用，但还是会导电。\n\n（然后作者开始把命题加强，从“碱性热液喷口可以是生命起源”也就是充分性，试图加强为几乎只能是这一路径，甚至外星生命也是。）\n\n- 橄榄石是宇宙中最丰富的矿物之一，也是星际尘埃和吸积盘（accretion disc）的主要成分之一。包括地球在内的所有行星都是由吸积盘形成的。甚至太空中也可能发生橄榄石的蛇纹岩化作用，其实就是星际尘埃的水合。\n- 另一种丰富的物质是二氧化碳。太阳系大多数行星大气中，甚至在其他星系的行星大气中也探测到了二氧化碳。\n- 在几乎所有存在水的岩石行星上，我们都能找到它们。根据化学和地质学规律，它们会形成温暖的碱性热液喷口，会在催化性微孔系统的薄壁两侧形成质子梯度。\n\n## Part II: The Origin of Life - 4. The Emergence of Cells\n\n### ===\n\n种系发生学：phylogenetics\n\n生命树只是根据核糖体次基团 RNA 的编码基因序列的相似性进行的绘制的。而用其他 48 种保守基因对 50 种细菌和古菌进行绘制的话，每一种都会得到一个不同的树图，仅有的共性是源头细菌和古菌的分叉，以及末尾的分类。\n\n细菌和古菌的共性在于转录和翻译过程用到的酶、ATP 合成酶.\n\n差异在于 DNA 复制过程中用到的酶、细胞膜磷脂的手性。\n\n靶观点包括：\n\n- 共同祖先同时拥有两套，细菌和古菌各自丢弃了其中一套。\n- 细菌继承了共同祖先，古菌为了适应极端环境独立进化出一套，取代了共同祖先的。\n\n作者认为：共同祖先并不能执行不同之处所完成的生物功能，依赖环境提供，然后细菌和古菌各自进化出一套方案，取代从环境中获取。\n\n### The rocky road to LUCA\n\n乙酰辅酶A 是生物世界六条固定碳元素的化学反应途径种，唯一同时存在于细菌和古菌的。\n\n酶本身很复杂，但是活性基团只是乙酰基，而酶基团替换成甲基的硫代乙酸甲酯，可能在碱性热液喷口附近合成，催化剂是金属矿物，质子梯度由量转换氢化酶 (Ech) 驱动反应。\n\n### The problem of membrane permeability\n\n今天的细胞膜对质子来说几乎不可渗透。\n\n早期的细胞膜并非如此。这导致了古细胞无法自己生成质子梯度，但是古细胞直接利用碱性热液喷口的质子梯度。但是直接利用反而要求膜对质子的透过率要高，进化上不支持不可渗透膜的出现。\n\n解释利用到水合氢离子和钠离子的尺寸相似性，共同祖先有一个反向转运蛋白 (antiporter)，用一个钠离子交换一个水合氢离子跨膜，且钠离子也可以驱动质子泵。这使得古细胞可以扩大生存范围，在质子梯度不那么大的地方，自发产生质子梯度和膜的选择性就有了演化优势。\n\n### Why bacteria and archaea are fundamentally different\n\n产乙酸菌（细菌）与产甲烷菌（古菌）都利用氢气和二氧化碳制造乙酰辅酶A，通过电子歧化 (bifurcation) 机制。\n\n细菌和古菌的区别，就在于反向转运蛋白运输钠离子和质子的方向。\n\n- 始组细胞中，天然质子流从外向内通过 Ech，用来还原铁氧环蛋白，铁氧环蛋白还原二氧化碳。\n- 细菌也就是产乙酸菌的祖先，倒转了 Ech 的方向，铁氧环蛋白被用来泵出质子，借电子歧化还原二氧化碳。\n- 古菌也就是产甲烷菌的祖先，演化出一个新的质子泵来运出质子，仍用质子通过原来的 Ech 还原铁氧环蛋白，铁氧环蛋白还原二氧化碳。\n\n两个分类的生物独自维持质子梯度，导致膜的选择性也是独立演化出来的，磷脂分子的结构不同，细胞壁的成分也不同，（取决于细胞膜的性质。）\n\n## Part III: Complexity - 5. The Origin of Complex Cells\n\n### ===\n\n细菌和古菌的基因多样性高于真核生物，但是结构复杂性不如：体积、基因组尺寸\n\n具体的结构限制众说纷纭：\n\n- 失去细胞壁：卡瓦里耶·史密斯假说。但是蓝藻和蓝细菌都有细胞壁，复杂度依然差很多\n- 没有棒状染色体：环状 DNA 只能在一个地方开始复制，棒状有多个启动子平行工作。但是无法解释为什么没有演化出棒状，且有些细菌有棒状，依然结构简单。\n- 作者支持的观点：线粒体提供的能量\n\n### The chimeric origin of complexity\n\n大约 1/3 真核基因在细菌和古菌种都有等价基因，但是对应的物种在演化树中的位置却相距甚远。其中 3/4 来自细菌，1/4 来自古菌。\n\n而那些没有对应的基因，被称作真核生物的“识别基因”。\n\n- “古老真核假说”的解释：这些识别基因才是古老的基因，始于生命诞生之初，与其他分支之间的相似性随着时间流逝而磨灭。推论为真核细胞通过某些机制收纳了原核细胞的基因，比如内共生。然后是作者的反对：\n    - 先是诉诸动机，略幼稚。\n    - 化石记录中的原核生物比真核生物古老。现在没有找到内共生线粒体之前的真核生物。\n- 作者支持的解释，“嵌合起源假说”：\n    - 真核生物的识别基因还是来自细菌或古菌的某些祖先基因，只是因为它们演化得比其他基因快得多，对新功能的适应性演化完全抹去了过往历史，所以失去了与原核生物亲缘基因序列的相似性。\n    - 真核生物的细菌型基因来源远不仅限于 α-变形菌。根据已有研究的大致推测，至少有25种不同的现代细菌类群都为真核生物提供过基因。\n    - 在生命树的真核生物分支内部，所有这些细菌型和古菌型基因都一起分支、共同演化。很明显，真核生物在演化早期就获得了它们。排除了在真核生物的后续演化途中，持续的水平基因转移的影响。\n    - 一下子就从原核生物身上取走了好几千个基因，然后就再也没与原核生物进行过基因交流。对此最简单的解释，不是细菌式的水平基因转移，而是真核细胞式的内共生。\n\n引出“嵌合起源假说”的困难：曾经有25种不同的细菌和7～8种不同的古菌在演化早期全体参与了一场基因乱交派对，或者说一次单细胞生物的共生狂欢节，而在之后的整个真核生物史中，彼此都不再联系，这实在令人难以置信。\n\n- 很可能是真核生物最初是从某一群细菌中一次性获取了大量基因，而这群细菌后来慢慢分化了。剩下的仍然保持细菌的自由生活，在之后的15亿年间奉行水平基因转移，四处散播它们的基因。所以，它们古老的基因组合，现在会散布在很多现代细菌种群之中。\n\n### Why bacteria are still bacteria\n\n为什么它会永远保留在所有的细菌、古菌和真核生物身上。难道没有某种生物丢弃化学渗透偶联，代之以其他更好的能量机制？没有这种可能吗？\n\n有，发酵作用。速度较快，对原料的使用效率却较低。是生物除了化学渗透偶联之外唯一已知的产能方式。\n\n化学渗透偶联的好处：\n\n- 第二章\n- 灵活，它像是一个公共操作系统，支持多种电子供体和受体即插即用，还允许小范围的改装来产生更好的效果。\n- 相关基因可以通过水平基因转移在种群之间交流，就像把新的应用程序安装到其他兼容系统中。\n- 可以从任何环境中挤出最后一滴能量。它让细胞可以把能量“零钱”储存起来。如果需要10个质子才能合成1个ATP分子，而某个化学反应释放的能量只够泵出4个质子，那只需要把反应重复3次，泵出12个质子，再抽出其中10个就可以用来制造ATP分子。\n\n化学渗透偶联的其他作用\n\n- 摄取营养和排泄废物；\n- 转动细菌的鞭毛（一种可以旋转的外部推进结构），让细菌可以自由运动；\n- 被故意耗散，用来产生热量，褐色脂肪细胞就会这样做。\n- 质子梯度的崩解还被用来启动细菌种群突然的“程序性死亡”。\n\n### Energy per gene\n\n比较生物的能量水平的方法\n\n- 平均每克物质的能量。一克细菌和一克真核细胞的代谢率（测量标准是氧气的消耗速率，又称呼吸速率）：细菌的呼吸速率通常比单细胞真核生物更快，平均快3倍左右。\n- 单个细胞的代谢率：50种细菌和20种单细胞真核生物，这些真核生物的平均体积大约是细菌的15,000倍⑥。已知它们的呼吸速率是细菌的1/3，那么平均每个真核生物细胞每秒消耗的氧气是细菌细胞的5,000倍。真核细胞的体积比细菌大得多，DNA也多得多。即便如此，单个真核细胞还是比细菌细胞多了5,000倍的能量。\n- 每个基因平均分配到的能量：真核生物比原核生物基因多了1,200倍。\n- 把细菌基因组的大小（5,000个）比例放大到真核生物基因组的水平（20,000个），那么细菌每个基因的平均能量：就只有真核生物的1/5,000。\n    - 真核生物要么能够负担比细菌大5,000倍的基因组，\n    - 要么能为每个基因的表达提供比细菌多5,000倍的能量\n- 把细菌放大到真核细胞的大小，虽然ATP合成增加了625倍，但能量开销却增加了15,000倍，每份基因拷贝的平均能量反而减少为原来的1/25。另外，细菌和真核细胞每基因平均能量差距本来就有5,000倍之大（校正基因组大小之后的数字），5,000除以1/25，125,000倍！\n\n多出的能量的用途：\n\n- 单细胞生物复制DNA，大概只用到总能量的2%。\n- 80%的能量用于合成蛋白质。每连接一个肽键至少要花费5个ATP，5倍于聚合核苷酸生成DNA花费的能量。\n- 一个基因组中基因越多，合成蛋白质所需的能量就越多。这一结果可以通过简单统计核糖体数量来估算。（我有疑问）\n    - 大肠杆菌这样的普通细菌，平均有13,000个核糖体；而一个肝脏细胞至少有1,300万个核糖体，数量是细菌的1,000～10,000倍。\n    - 大肠杆菌这样的普通细菌，平均有13,000个核糖体；而一个肝脏细胞至少有1,300万个核糖体，数量是细菌的1,000～10,000倍。\n\n不只来自于尺寸的差异——把细菌的体积也增大到真核生物的平均尺寸，再来计算每个基因要花费多少能量。\n\n- 更大的细菌可以制造更多的ATP\n- 更大的体积也需要合成更多的蛋白质，消耗更多的ATP\n- 如果存在大如真核细胞的细菌，它每个基因的平均能量，比起同体积的真核细胞只有二十万分之一\n    - 表面积与体积之比：细菌放大到真核生物的尺寸，它的细胞半径会增加25倍，表面积则会增加625倍。\n    - 小小的基因组孤单地坐在细胞核里，现在要负责生产数量暴涨625倍的核糖体、蛋白质、RNA和脂质，还要在扩张到这么大的细胞空间内运送它们，成果仅仅是与从前一样的每单位面积ATP合成速率\n    - 要让蛋白质合成增长625倍，就需要625份完整的细菌基因组，而且每个基因组都以同样的方式运作。这样对每个细菌成员来说，每个基因的平均能量就跟原来一模一样。\n    - 真实情况比这还糟糕。细胞的内部空间现在可是放大了15,000倍。内部的代谢活动还没有明确；我们把它当成空白处理，即能量需求算成零。\n        - 这个放大的细菌就无法与真核细胞相提并论，毕竟真核细胞不只在体积上增加了15,000倍，其内部还充满了各种复杂的生化机器。\n        - 把细菌放大到真核细胞的大小，虽然ATP合成增加了625倍，但能量开销却增加了15,000倍，每份基因拷贝的平均能量反而减少为原来的1/25。另外，细菌和真核细胞每基因平均能量差距本来就有5,000倍之大（校正基因组大小之后的数字），5,000除以1/25，125,000倍！\n    - 例证：巨大的细菌\n        - 刺骨鱼菌（Epulopiscium）是一种厌氧菌，仅见于刺尾鱼（surgeonfish）后肠的无氧环境中。狭长身躯大约有半毫米长，肉眼可见。刺骨鱼菌的基因组拷贝多达20万份\n        - 嗜硫珠菌（Thiomargarita），还要更大，是直径接近一毫米的球菌，其绝大部分体积都由一个硕大的液泡占据。单独一个嗜硫珠菌可以长到果蝇的头那么大！嗜硫珠菌细胞虽然大部分都是液泡，也有18,000份基因组拷贝。\n        - 两种巨型细菌的基因组位置都很靠近细胞膜，分布在细胞膜内侧附近（图23）。细菌的中心位置则没有什么代谢活动。\n\n### How eukaryotes escaped\n\n细菌受到的严重能量限制倒是强有力的证据，可以证明嵌合起源是复杂生物诞生的必备条件。只有两种原核生物之间的内共生作用，才能打破加在细菌和古菌身上的能量桎梏。\n\n巨型细菌面临的问题是，要保持巨大的形态，它们就必须把整个基因组复制上几千次。但是一旦复制出来，基因组就无所事事了。细胞内所有的基因组是彼此完全一样的拷贝。即使有些细微的差异，也不受制于自然选择，因为它们不是自我复制的个体。同一细胞内的众多基因组即使出现变异，经过数代之后，也会像杂音一样消失。\n\n环境在不断改变，刚才那个无用的基因，现在可能又有用了。缺了它，细菌现在无法生长，除非通过水平基因转移再次获取。这种丢弃又重新获取基因的动态变化不断轮回，主导了细菌种群的构成。\n\n随着时间推移，细胞基因组的大小会逐渐稳定为一个最小可行的尺寸，而单个细菌可以随时从一个大得多的宏基因组（metagenome，整个种群的基因总和，另外还包括可以进行基因交流的亲缘种群）中获取基因。\n\n- 几代之后，快速分裂的细菌就会在种群中取得绝对优势。要获得生长速度上的微小优势，一个办法就是从基因组中丢弃一些DNA\n- 环境在不断改变，刚才那个无用的基因，现在可能又有用了。需要通过水平基因转移再次获取。\n- 一个大肠杆菌有4,000个基因，但大肠杆菌的宏基因组大概有18,000个基因。\n\n细菌类的内共生体，丢弃不必要基因的细菌会稍许提高复制速度，逐渐成为主流。关键差异在于环境的稳定性。不需要的基因就永远都不需要了。内共生体可以永久性地丢弃它们，基因组单向萎缩。\n\n内共生体的共同发展趋势，都是丢弃自己的基因。\n\n- 原核生物没有吞噬作用，无法吞噬其他细胞，所以它们之间的内共生作用很罕见。但是在细菌中，我们又确实知道几个现成的例子（图25）。这些例子的意义很清楚：原核生物的内共生确实会发生。\n- 几种真菌也有内共生体，虽然它们和细菌一样，也不会进行吞噬作用。\n- 而那些真正有吞噬作用的真核生物经常包含内共生体，已知的例子就有好几百个。\n\n最小的细菌基因组通常都发现于内共生体。\n\n- 比如立克次体，这种曾经毁灭了拿破仑侵俄大军的斑疹伤寒病原体，其基因组只有100万个碱基对，不到大肠杆菌基因组的1/4。\n- 另一种细菌Carsonella是寄生于木虱科昆虫的内共生体，它有目前已知最小的细菌基因组，只有大约20万对碱基，比某些植物的线粒体基因组还小。\n\n内共生体丢弃基因的好处：\n\n- 加快复制，还可以节省ATP。\n    - 考虑5% 的能量节省，理论上每秒省下的58万个ATP可以用来制造4.5微米长的肌动蛋白。\n    - 人类以及所有动物的线粒体，只保留了13个能够编码蛋白质的基因。经过漫长的演化，它们的基因组丢弃了99%以上的基因。\n- 这些多余的DNA片段，就成为真核生物演化的基因原材料。与细菌相比，最早的真核生物多了大约3,000个新的基因家族。新的基因可以被改造来执行各种各样的新功能，而且没有新增的能量开销。\n\n上述理论的问题：\n\n- 一些细菌（例如蓝细菌）会内化自己的生物能量膜，把细胞膜向内折叠成繁复的盘绕结构，这样可以大幅增加膜面积。为什么细菌不能通过这样的膜内化作用脱离化学渗透偶联的限制呢？\n- 为什么线粒体没有完全丢弃整个基因组，让能量收益达到极致呢？\n\n### Mitochondria — key to complexity\n\n线粒体为什么始终保留着一小群基因？\n\n- 也许并没有什么必然的生物物理原因，让这些基因非得留在线粒体中不可。它们之所以没有转移到细胞核中，并不是因为不行，只是因为演化史发展到现在，它们还没有转移。有些学者正努力尝试。\n- 也许如果没有保留这些基因，线粒体就不能存在。这些线粒体基因必须坚守现场，紧挨它们为之服务的生物能量膜。\n\n在战争中，“黄金控制”指中央政府，负责制定长远战略；“白银控制”指军队的指挥层，负责人员和武器调度；但战争的胜负是在战场上决定的，掌握在“青铜控制”的手中；他们是那些真正与敌军交锋的勇士。他们做出战术决定，激励手下部队，作为伟大的战士被历史铭记。线粒体基因就是青铜控制，现场的决策者。\n\n- 线粒体内膜的两侧有大约150～200毫伏的电位差，而膜的厚度只有5纳米。\n- 如果呼吸链不能好好地把电子传递给氧气（或其他任何电子受体），那就会导致类似短路的状况，即电子逃逸后，直接与氧气或氮气分子发生反应，形成反应性很强的自由基（free radicals）。\n- 如果线粒体基因被移到细胞核内，那么当发生氧气浓度改变、基质缺乏或自由基泄漏等严重情况时，线粒体几分钟内就会失去对膜电位的控制，细胞就会死亡。\n- 极少数真核生物丢掉了全部的线粒体基因，它们也失去了呼吸能力。比如氢酶体和纺锤剩体（发现于源真核生物体内、由线粒体特化形成的细胞器），一般都失去了所有基因，代价就是失去了化学渗透偶联的能力。\n- 前面讨论过的巨型细菌，总是把基因（应该说整个基因组）保留在生物能量膜旁边。\n- 拥有盘曲折叠内膜的蓝细菌。如果这些基因确实有必要留在现场才能控制呼吸作用，那么虽然蓝细菌小得多，也应该与巨型细菌一样，把整个基因组复制很多份，放在膜附近。它们的确是这样的。\n\n替代方案：利用细菌的质粒（plasmid）\n\n- 对原核生物来说，变大本身并没有优势，生产过剩的ATP也不会增加什么好处。\n- 单纯变大的第二个缺点，是细菌需要建立补给线来支持细胞内更远的代谢活动。\n\n真核生物如何突破尺寸的约束而演化出复杂的运输系统？他们的“氢气假说”认为，\n\n- 宿主和内共生体之间是一种新陈代谢的“互养”（syntrophy）关系，意义在于彼此交换生长所需的基质，而不只是能量。\n- 宿主体内的内共生体越多，就可以获得更多的基质，生长越快，内共生体的生存条件也越好。所以，在内共生作用的影响下，细胞越大越有好处\n- 内共生体开始丢弃基因时，它们对ATP的需求也随之降低。那么当所有的ADP都转换成ATP后，呼吸作用就会停止。呼吸链就会开始累积电子。演化提供了一个关键的蛋白质来救场：ADP-ATP转运蛋白（ADP-ATP transporter）\n\n## Part III: Complexity - 6. Sex and the Origins of Death\n\n### ===\n\n内共生作用促使真核生物崛起不是达尔文式的演化，因为它不是一系列逐代继承的微步改变，而是突然一跃进入未知领域，这种事件无法用标准的分支生命树图来展示。内共生作用是反向的树状图，它的树枝不是分叉，而是融合。\n\n但是内共生作用也是一次单一事件，发生于演化史中的一个时间节点，并不能一下子制造出细胞核，或者真核生物的任何其他主要特征。它的作用是触发了一系列后续事件，而这些事件的发展过程是标准的达尔文式演化。所以，并不是说真核生物的起源不符合达尔文式的演化理论。我认为，原核生物之间的单一内共生事件，使自然选择的整个场景彻底改观。\n\n理查德·戈尔德施密特（Richard Goldschmidt）提出的演化假说.\n\n以内共生作用为起点，立即对各个事件的发生顺序产生了一些限制。例如，细胞核与内膜系统必定出现在内共生作用之后；演化的实际发生速度也受到了一定限制。\n\n达尔文式的演化与渐变论（gradualism）经常被混为一谈，\n\n- 渐变的意思很简单：演化不会大跨步飞跃进入未知领域。所有的**适应性**变化，都应由微小而不连续的分步构成。\n- 纯粹的达尔文式演化。但这并不等于是说，这个过程在地质时间上一定会很慢。\n\n眼睛出现于寒武纪大爆发时期，在大约两百万年之内就演化出来了。数学模型曾经计算过，某种蠕虫身上原始的感光眼点演化成眼睛，假设平均生命周期为一年，每一代形态改变都不超过1%，答案是只需要50万年。\n\n演化史上的原核生物到真核生物的空白：\n\n- 要跨越原核生物与真核生物之间艰险的鸿沟，基因组合上可行的路径并不多，大部分探索者都中途而亡。\n- 这意味着最初的种群应该很小，最初的真核生物基因都不稳定，它们在一个很小的种群内快速演化，挣扎求生。\n- 所有的真核生物都拥有众多完全相同的特征。真核生物诞生之初，生殖隔离似乎并未发生，因为所有的真核生物都有一样的基本特征，很像是一个可以互相交配生殖的种群。有性生殖。\n\n有性生殖 vs. 无性生殖\n\n- 无性生殖（即“克隆”）会导致深远的发散演化，因为不同种群内的不同突变都会累积下来。这些突变在不同的环境中接受自然选择，面对的优势和劣势也迥然各异。\n- 有性生殖在种群内部形成基因池，不断地混合匹配各种特征，从而阻止分化。\n- 水平基因转移也涉及基因重组，也会变换不同的基因组合，造成“流动”的染色体。但并不是对等交换基因，也没有细胞融合或全基因组的系统性重组。水平基因转移是零敲碎打，而且是单向的，它无法对种群中的个体特征进行各种组合，反而会造成个体之间的分化。水平基因转移盛行的结果，是同一种细菌的不同菌株之间可能有多达一半的基因都不一样\n\n**两个原核生物之间的内共生作用是否有某种特殊效应，推动了有性生殖的演化？当然是的。**\n\n### The secret in the structure of our genes\n\n真核生物有着“破碎的基因”。\n\n- 我们一度被早期的细菌基因研究误导，认为人类染色体上的基因也应该像漂亮的珠串一般，按照有意义的顺序排列。\n- 它们由好几个较短的序列组成，每一段编码蛋白质的一部分；\n- 这些编码区域之间插入了长长的非编码DNA序列，我们称之为内含子（introns）。通常都比真正的编码序列长得多。\n- 每个基因中通常都插入了好几段内含子（基因通常的定义是，编码一整个蛋白质的DNA序列）\n- 内含子也会被转录到RNA上。不过在抵达核糖体之前，RNA上的内含子就已经被全部剪切掉。靠另一种精巧的纳米蛋白小机器——剪接体（spliceosome）来执行。\n\n好处：同一个基因可以通过不同的剪接方式拼出不同的蛋白质。比如抗体\n\n来源：\n\n- Ford Doolittle “内含子早现理论”：\n    - 早期的基因因为缺少现代基因复杂的修复机制，在复制过程中一定会迅速累积许多错误。DNA的长度会决定DNA上累积的突变数量，所以只有很短的基因组才可能避开突变熔毁的命运。\n    - 假说给出的最重要一项预测，就是“真核生物最先演化出现”，因为只有真核生物才有真正的内含子。当代的全基因组测序无可争辩地显示，真核生物起源于古菌宿主和细菌内共生体。\n- 内共生体。\n    - 细菌没有“真正的内含子”，但内含子的前身必定来自细菌，更准确地说，来自细菌的基因寄生物 (bacterial genetic parasites)，正式名称是移动II型自剪接内含子 (mobile group II self-splicing introns)。\n        - 这些移动内含子很可能非常古老，在三大域生物的基因组中都存在。\n        - 而它们又与逆转录病毒不同，从不需要离开宿主基因组这个安乐窝。\n        - 它们存在于基因之间的非编码区域，而且密度很低。\n    - 剪接体并非完全由蛋白质组成，其核心是一把RNA剪刀，和移动内含子的剪刀完全一样。它们剪切真核生物内含子的方式暴露了其来源，即细菌的自剪接内含子\n        - 内含子不会编码逆转录酶等蛋白质，也不能把自己切入或切出宿主DNA；\n        - 它们不是活动的基因寄生物，而是DNA序列上的赘疣，无所事事地待在那里。\n        - 但这些已经死亡的内含子，被累积的突变完全侵蚀，衰退得不成形状，却远比那些活着的寄生物更危险，因为它们再也无法剪切自己，宿主细胞必须主动移除它们。\n        - 宿主的办法就是从它们还活着的亲戚那里征用RNA剪刀。剪接体就是一种用细菌基因寄生物改造而成的真核生物机器。\n    - Eugene Koonin 和马丁：在真核生物诞生之初，内共生体在毫无防备的宿主体内放出了一群基因寄生物。内含子的入侵扩散到整个宿主基因组，塑造了真核生物基因组的基本结构，同时也推动了真核生物某些基本特征的形成，比如细胞核。我再补充一点：性。\n\n### Introns and the origin of the nucleus\n\n许多内含子在真核生物基因组中的位置都固定不变. 柠檬酸合酶（citrate synthase）基因总是含有2～3个内含子，而且插入位置几乎总是完全一样。\n\n- 内含子是各自独立插入这些位置的，出于某种原因，这些位置受到自然选择的青睐。\n- 过去某个时刻插入了共祖的基因组，只发生了一次，并传给了所有后代。\n\n直系同源基因 (ortholog) 和旁系同源基因 (paralog):\n\n- 直系同源基因基本是继承自共祖的共同基因，在不同物种的体内执行一模一样的功能。\n- 旁系同源基因，同样来自一个共同的祖先，但那个祖先基因却在同一个祖先细胞中经历了多次复制，形成了一个基因家族。\n\n旁系同源基因家族区分为“古老”和“近代”两类:\n\n- 古老：存在于所有真核生物中，但没有在任何原核生物中分化复制过的基因家族\n- 近代：只有在某些特定的真核生物种类中才有的基因家族，比如动物或植物。这种基因家族内的复制发生得较晚，是在那个特定生物种类的演化中发生的\n\n与近代旁系同源基因相比，古老旁系同源基因中内含子的位置应该更不规则。基因组测序的分析结果表明，库宁的预测非常准确。\n\n为什么这些内含子在细菌和古菌体内受到严格的控制，在真核生物细胞内却大肆扩散呢？\n\n- 最早的真核细胞（其实那时基本上还是个原核细胞，底子是一个古菌）基因组遭到了细菌内含子的大轰炸。如果宿主体内有很多内共生体，其中一个死亡并无大碍，但是这个已经死亡的内共生体，会把自己的DNA释放到胞质溶胶中。这些跳船的DNA很可能通过标准的水平基因转移方式，与宿主细胞的基因组发生重组。\n- 没有什么自然选择压力来限制早期内含子扩散。对细菌来说内含子在能量和基因方面是双重负担。真核生物的基因组可以自由扩充核基因组，正是因为内共生体的基因组不断缩小。宿主细胞并不会有计划地扩充基因组；之所以会扩充，只是因为更大的基因组也不会受到自然选择对细菌那样的惩罚。\n\n细胞核膜的出现\n\n- 剪接体还是要花几分钟时间才能切掉一段内含子。偏偏核糖体的工作速度奇快，每秒钟可以组装10个氨基酸，制造一个标准的细菌蛋白质（长度约为250个氨基酸）只需不到半分钟\n- 细胞核膜就是这道障碍，可以把转录和转译两个过程分开。\n- 这就解释了为什么真核生物需要细胞核，而原核生物不需要。原核生物根本没有内含子的麻烦。\n\n基因分析表明，宿主细胞是一个货真价实的古菌，所以它的细胞膜必然含有古菌脂质。但是，今天的真核生物膜却含有细菌脂质。\n\n- 可行性：各种细菌脂质和古菌脂质混合形成的嵌合膜，其实都是稳定的。\n- 动力：内共生体到宿主的混乱基因转移，一定包括了负责合成细菌脂质的基因。\n\n核膜产生的过程\n\n- 如果诱发某种突变，导致细菌的脂质合成速度加快，多余的脂质就会积聚形成内膜。脂质在自己合成之处附近积聚，结果是围绕着基因组形成了一堆堆脂质“小袋子”。\n- 一堆堆脂质小袋子也可以在DNA和核糖体之间临时拼凑起一道不太完美的障碍，减轻一点内含子带来的麻烦。\n- 这道障碍其实必须有缺口。完全封闭的膜反而会让RNA无法接触核糖体。\n- 核膜的形态非常符合这个假设。脂质小袋子就像塑料袋一样，可以被压扁。一个压扁的袋子，其横截面是两片紧贴且平行的膜，即双层膜结构。\n- 当细胞分裂时，核膜会散开，还原成分离的小囊泡；分裂完成后，这些小囊泡会生长并再次融合，重新形成两个子细胞的核膜。\n- 所有这些部件都由嵌合来源的蛋白质组成，一些蛋白质由细菌基因编码，少数由古菌基因编码，剩下的编码基因只有真核生物才有。除非细胞核是在获取线粒体后才演化出现的，是那次基因大规模混乱迁移的后续事件，否则我们根本无法解释这种基因组合模式。\n\n### The origin of sex\n\n染色体这种排列整齐进行基因重组的现象，在细菌和古菌的水平基因转移过程中也有出现，但一般不是对等进行的。细菌只是用这种机制修复受损的染色体，或是重新纳入以前丢弃的基因。两类基因重组用到的分子机器基本相同。\n\n有性生殖的特别之处在于重组的规模和对等基因交换。有性生殖可以打破原本固定的基因组合，让自然选择可以“看见”单独的基因，把我们的特质逐个分列出来。让真核生物拥有“流动”的染色体，组合中的基因版本不断变动（同一个基因的不同版本用专业术语说就是等位基因）。\n\n设想基因排列在一条染色体上，从不进行重组。\n\n- 自然选择只能鉴别整条染色体的适应能力。假设这条染色体上有几个非常重要的基因，稍有突变就会导致个体死亡。然而，对其他不太重要的基因突变，自然选择几乎无动于衷。轻微却有害的突变会在这些基因上逐渐积累，因为它们导致的小麻烦，会被保留几个关键基因带来的重大利益抵消。\n- 积极地作用于固定基因组合，结果可能更糟。这条染色体上的其他99个基因也会搭上优秀基因的便车\n\n缺点：\n\n- 打破在特定环境中已经获得成功的等位基因组合。\n- 无性生殖的种群每一代的数量都可以翻倍；而有性生殖的种群数量，还是和原来一样（从细胞的角度来计算）。\n- 必须先找到一个配偶：雄性代价、性病、基因寄生体\n\n有性生殖在真核生物中的完全普及，远远超过了“合理”的程度。原因很可能在于，真核生物的最后共祖已经是有性生殖。\n\n萨莉·奥托（Sally Otto）和尼克·巴顿（Nick Barton）：当基因突变率很高、自然选择压力很大，以及种群中充满基因多态性时，有性生殖的优势最大。\n\n- 无性生殖，高突变率意味着轻微有害的突变累积得更快，在发生选择性清除时损失也更大。基因组大到一定程度，突变熔毁就不可避免。基因组越大，就越难通过水平基因转移获取“正确”的基因。\n- 选择压力，来自寄生感染与变化的环境。只有全基因组范围内的重组才能有效增加基因多态性。会有一些细胞的内含子插在糟糕的位置上，也会有一些细胞的内含子插在较为无害的位置。接下去，自然选择会淘汰那些最不幸的细胞。\n- 基因多态，细菌和古菌通常都有单条环状染色体，而真核生物则有多条棒状染色体。\n    - 为什么是这样？最简单的答案是，当内含子切进切出基因组时，它们可能导致染色体形状出错。\n    - 这种通过有性生殖和基因重组来累积新基因的倾向，能很好地解释早期真核生物的基因组为什么会膨胀。\n\n如何\n\n- 分离染色体：普通细菌那样用细胞膜附着染色体。细菌分离大型质粒的机制 → 真核生物细胞分裂时使用的纺锤体。早期真核生物为了分开那些杂乱的染色体，很可能沿用了细菌的质粒分离机制。\n- 细胞融合：没有在有细胞壁的细菌中发现。L型细菌因为没有细胞壁就很容易融合。早期真核祖先很可能会积极主动地进行细胞融合。尼尔·布莱克斯通（Neil Blackstone）认为，早期的细胞融合可能由线粒体推动。（但是没说线粒体是怎么做到的。）\n\n### Two sexes\n\n为什么生物的性别总是两种\n\n不是两种的好处：\n\n- 同一种性别，那我们可以与任何人交配。我们选择伴侣的机会一下子增加了一倍\n- 三种、四种都比两种更好。就算限定只能与不同性别的个体交配，那我们可以和种群中2/3或3/4的人交配，而不仅仅是一半\n\n双方都不愿意承担充任“雌性”的代价。雌雄同体的生物，比如扁形虫，交配时会竭尽全力防止自己受精。\n\n最深刻的差异之一，在于线粒体的遗传。只有一种性别会把线粒体传给下一代，\n\n而单细胞藻类尽管会产生完全相同的配子（即同形配子，isogametes），结合时也只有一个配子可以把线粒体传下去；另一个配子的线粒体会从内部被消化掉。准确地说，只有线粒体DNA会被消化掉，看来问题在于容不下这些线粒体的基因.\n\n前面说线粒体煽动了有性生殖（并没给出很坚实的证据。），但是有性生殖并不会让线粒体传播得更快，有一半的线粒体传不下去。（然后是类似《自私的基因》一书的论证，但是这种因果关系的命题，有实验验证吗，什么样的实验构成验证呢？）（而且前面似乎暗示内吞线粒体活动实际上一次吞了好多个，那么当时线粒体祖先之间的竞争岂不是更加激烈吗？）\n\n作者的新观点：线粒体基因必须适应细胞核基因。线粒体的单亲遗传原因很可能在于改善两个基因组之间的相互适应。取样效应，将每一个线粒体弄进新细胞然后繁殖，结果是一群线粒体基因不同的细胞，如果一起弄进去，结果是一堆线粒体基因十分相似的细胞。多态性 (variance)\n\n数值模拟表明，单亲遗传的基因在种群中很难扩散，更不可能达到 fixed point。突变线粒体越多，维持品质的代价越高；突变线粒体越少，单亲遗传的好处越小。\n\n如果两种交配型（出于其他原因）已经存在，那么在某些条件下，单亲遗传突变基因会在种群中趋于固定。条件就是细胞内有大量线粒体，且线粒体基因的突变率很高。\n\n交配型 vs. “真正的性别”\n\n### Immortal germline, mortal body\n\n在很多代的时间跨度上，线粒体基因的演化速度比核基因快 10~50 倍。但最早出现的动物和我们大不一样：它们的形态类似于海绵或者珊瑚，是固着型的滤食动物，不会四处移动（至少在成年阶段不会）。它们的细胞没有很多线粒体也在意料之中，线粒体基因的突变率也很低，可能比核基因的突变率更低。\n\nArunas Radzvilavicius: 多细胞生物普通细胞的分裂效果与单亲遗传相似，它们都会增加细胞之间的多态性。因为普通分裂大概率没办法完美地均分线粒体的突变基因。\n\n两种策略：\n\n- 没有特化组织，全身到处都有干细胞，随机选择干细胞成为生殖细胞。这种策略下线粒体的多态性高，作为应对，此类生物线粒体数量少，突变率低，自然选择淘汰糟糕的生殖细胞糟糕个体的后代，但是个体的品质取决于最差的器官。\n- 一开始卵细胞就有更多线粒体，这样分给多个接收者时，差异比倍增复制再分给后代更小。而精子不遗传自己的线粒体，其多态性不减少，自然选择的效果依然明显。此类生物组织类型众多，且组织之间互相依赖。生殖细胞在胚胎发育之初就被藏匿起来，应对线粒体基因的高突变率。\n\n明确的、可以验证的预测，我们很想付诸实验。首选的实验生物就是海绵和珊瑚。二者都有精子与卵子，但都没有隔离的种系。如果我们不断选择线粒体突变率高的个体，它们会发展出隔离的种系吗？\n\n突变率为何升高：物活动增加，细胞和蛋白质的物质周转量随之增加。寒武纪大爆发前夕的海洋充氧事件，催生了活跃的两侧对称动物（Bilateria）\n\n“动物隔离出专门的种系。”中的“种系”到底是什么意思？\n\n死亡是身体预先计划好的、命中注定的终点。种系的不朽在于它们可以不断分裂下去，既不会衰老，也不会死去。这些特化的生殖细胞被藏匿起来，身体的其他部分就可以为了其他的专门用途而各自特化，首次出现了不能自我再生的组织。\n\n以这个视角看待整整40亿年的生命史，线粒体就处在真核生物演化的中心。\n\n（那这样看的话，非真核细胞就没有死亡的概念？也没有种系的概念？后者显然不能成立。）\n\n## Part IV: Predictions - 7. The Power and the Glory\n\n### ===\n\n呼吸蛋白具有独特的线粒体基因与核基因双重性质，互相完美镶嵌，犹如天作之合。线粒体表面上有自主性，好像随时想分裂就分裂，但这只是假象。事实上，它们功能的正常运转依赖于两个不同的基因组。\n\n（这种嵌合难道不和之前线粒体基因的“青铜控制”的功能相矛盾吗？）\n\n氧化还原两个中心之间的距离每增加1埃，电子的传递速率就会降低为原来的1/10。\n\n有性生殖对维持大型基因组中个体基因的正常运作是必需的，而两性有助于维持线粒体的质量。这导致的意外后果就是，两个基因组的演化方式完全不同。线粒体基因的演化速度比核基因快了10～50倍\n\n（这个说法似乎意味着两性首先是由是否将线粒体（尤其是其基因）传递给后代来定义的，而不是由性染色体的不同而定义的，但是这样不意味着性染色体的出现应当和线粒体的基因转移高度相关吗，事实上线粒体转移到细胞核的基因有这种分布的集中性吗？）（后文有提到，但是并没完全解决我的疑问。）\n\n大概是演化的短视最好的例子，最简单的原因就是繁殖速度，基因组越小的细菌繁殖得越快，长此以往就会成为主流。\n\n因为一些线粒体死亡后，其DNA释放到宿主胞质溶胶中，然后被细胞核纳入。这个过程会持续进行。一些迁移到细胞核的DNA后来获得了一段导向序列，就像一个地址代码。\n\n演化中必定有一个过渡阶段，在细胞核和幸存的线粒体中同时存在同一个基因的拷贝。除了我们线粒体中保留的13个蛋白质编码基因（只占原先基因组的不到1%），都是核基因组的拷贝被保留，线粒体的拷贝被丢弃。这种明确的趋势看起来不像是随机作用。\n\n一个可能的影响因素是雄性的品质。线粒体是母系遗传，从母亲传给女儿，所以不可能对有利于男性的线粒体基因变体进行选择。男性线粒体中即使突变出有利于男性的基因，也不会传下去。\n\n另一个可能的原因是，线粒体的基因很占空间，腾出来就可以放置进行呼吸作用或其他过程的结构。\n\n### On the origin of species\n\n直接互相关联的基因改变速度大致相同，比如编码呼吸链蛋白的各个基因；而其他核基因的改变（演化）速度则慢得多。线粒体基因的变化会导致与它们互动的核基因发生代偿性改变（compensatory change），反之亦然。\n\n如果线粒体基因组与核基因组配合不佳，就是细胞程序性死亡（或称细胞凋亡，apoptosis）的触发机制。\n\n- 各个氧化还原中心塞满了电子，前几个氧化还原中心都是铁硫簇，其中的铁在高还原态下会从Fe3+变成Fe2+（被还原）。\n- Fe2+可以直接与氧气反应，生成带负电的超氧自由基O2·–。\n- 自由基数量超过一定的阈值，就会氧化附近的内膜脂质，尤其是心磷脂。造成一种呼吸蛋白——细胞色素c脱离内膜。\n- 失去了细胞色素c，电子再也无法到达呼吸链的终点，电子流也就完全中断。\n- 不会简单地分解成碎片，而是会从内部释放出一大群蛋白质刽子手：半胱天冬酶（caspase enzyme）。它们会把细胞中的DNA、RNA、碳水化合物和蛋白质等大分子切成碎片。\n- 这些碎片会用小块的细胞膜包起来，形成一个个囊泡，再喂给周围的细胞。\n\n杂种衰退：线粒体基因组与核基因组的不兼容，是否造成了物种起源中更普遍的杂种衰退现象？分化（speciation）是真核生物不可避免的发展趋势。而且它的影响有时比其他机制更明显，原因就在于线粒体基因的演化速度。\n\n道格·华莱士（Doug Wallace）认为，线粒体处于生物适应的最前线。线粒体基因的快速演变，让动物能够迅速适应食物和气候变化。这是适应的第一步，之后才是更缓慢的形态适应。\n\n### Sex determination and Haldane’s rule\n\nHaldane’s rule：异种动物相互交配产生的杂种一代中，如果有一种性别缺失、稀少或者不育，那么这一性别就是杂合（heterozygous，也可称为异配heterogametic）性别。解释：性选择对雄性的影响更大，无法解释为什么雄鸟反而比雌鸟更不容易受到杂种衰退的影响。\n\n寄生虫、染色体数量、荷尔蒙、环境因素、压力、种群密度，甚至线粒体，都可能决定动物的性别。有更深刻的机制在发挥作用。决定性别的具体机制如此多样，而两种性别的发育又如此一致，这也意味着性别决定（即雄性发育或雌性发育的过程）应该有一个非常根本的共同基础，不同基因的作用只是表层的细节点缀而已。\n\n厄休拉·密特沃克（Ursula Mittwoch）提出性别由代谢率决定的假说。\n\n人类Y染色体上的SRY基因，能够提高这些生长因子活性的突变，就能诱发性别转换，让本来没有Y染色体（或SRY基因）的雌性胚胎发育成雄性。相反，降低生长因子活性的突变会有反向的效果，会让Y染色体功能正常的雄性胚胎发育成雌性。\n\nHaldane’s rule 和性别由代谢率决定的假说的关系：\n\n不育和无法存活都代表着某种功能缺陷。能量需求最高的细胞会最先发生能量短缺而死亡。这正是线粒体疾病的问题所在。假设两个细胞有相同的线粒体，如果两个细胞的代谢需求不同，那么结果也会不同。生物的大脑很大但不可替换能留下更多的健康后代，那当然它们就会繁衍兴盛。只有当生殖细胞与体细胞存在根本差异时，自然选择才能这样运作。但这也意味着肉体成了用完即弃的载体，寿命有限。最终，那些无法满足自身代谢需求的细胞会终结我们的生命。\n\n以上的探讨，为霍尔丹法则提供了一个简单明快的解释：代谢率最快的性别，最容易发生不育或者无法成活。\n\n例子：谷物害虫赤拟谷盗与近亲物种弗氏拟谷盗杂交、植物中细胞质雄性不育、果蝇胞质杂合细胞、雌鸟必须精心挑选交配对象\n\n### The threshold of death\n\n光靠堆积线粒体无法提升功率。有氧运动的肌肉中，最优化的空间分配大概是肌原纤维、线粒体和毛细血管各占1/3。拥有强大有氧代谢能力的代价，就是低生殖力。\n\n我把死亡的门槛提高，也就是说，我可以承受更多的自由基泄漏，而不至于启动凋亡。生殖力得到提升。问题是，我需要为此付出什么代价？后代只有很小的概率能拥有完美匹配的线粒体与核基因，这会直接导致另一种利弊平衡：适应性与疾病。\n\n作者怀疑，导致早期隐性流产中很大一部分是缘于线粒体－核不兼容。\n\n高死亡门槛会带来一个间接的，然而也是终极的代价：更快地衰老，以及更容易罹患各种老年病。\n\n### The free-radical theory of ageing\n\n自由基老化理论，源于20世纪50年代的辐射生物学（radiation biology）研究。电离辐射可以分解水分子，生成各种高反应性的“碎片”，带有一个不成对的电子，这就是氧自由基。\n\n蕾韦卡·格施曼（Rebeca Gerschman）、丹汉姆·哈曼（Danham Harman）等研究者都认识到：在线粒体中，同样的自由基可以直接从氧气产生，不需要电离辐射。\n\n一些著名的科学家，尤其是诺贝尔奖得主莱纳斯·鲍林（Linus Pauling），都相信抗氧化剂的神话；他们采取超量维生素C疗法，每天吃好几匙的剂量。\n\n《自由基生物学和医学》（*Free Radicals in Biology and Medicine*），这是哈利维尔（Barry Halliwell）和古特利基（John Gutteridge）编写的经典教科书：“到90年代已经很清楚，抗氧化剂绝不是抗衰老和疾病的万灵药。只有边缘保健产业还在推销这种观念。”\n\n研究人类的衰老过程时，我们没有测量到线粒体的自由基泄漏出现任何系统性的增加。线粒体的突变数量会有少许增加，但除了极少数组织区域，整体的突变比例低得惊人，远低于可能引发线粒体疾病的程度。促氧化剂（pro-oxidants）反而能够延长动物的寿命。\n\n安东尼奥·恩里格斯（Antonio Enriques）与同事通过细胞培养实验表明，使用抗氧化剂阻断自由基信号相当危险，可能会**抑制**ATP合成。看来，自由基信号可以通过增加呼吸蛋白复合体的数量来加强线粒体的呼吸能力，从而分别优化每个线粒体中的呼吸作用。\n\n自由基信号的根本意义在于：线粒体现在有问题，呼吸能力低于任务需求。\n\n雷蒙德·珀尔（Raymond Pearl）的“生命率理论”（rate-of-living theory）：代谢率较低的动物（通常是大象等大型物种），一般比代谢率较高的动物（比如大鼠和小鼠）寿命长\n\n\u003e 自由基老化理论原始的假设认为，自由基是呼吸作用不可避免的副产物，参与呼吸作用的氧气中大约有1%～5%一定会转化为自由基。\n\u003e \n\n所有传统实验测量的都是细胞或组织暴露在大气氧浓度下的情况，这个浓度远高于体内细胞接触到的实际氧气浓度。因此，实际的自由基泄漏速率可能比测量值低好几个数量级。\n\n其次，自由基泄漏**不是**呼吸作用中不可避免的副产物，而是故意释放的信号；而自由基的泄漏率，在不同物种、不同组织、每天的不同时间、不同的荷尔蒙状态、不同的热量摄取、不同的运动水平之间都存在天壤之别。\n\n真正的相关性，其实是在自由基泄漏与寿命长短之间。\n\n衰老过程中，一些线粒体确实会发生突变，让细胞中的线粒体种群成为不同类型的混杂，有些与核基因配合较好，有些较差。想想这会带来什么问题。最不兼容的线粒体通常会泄漏最多的自由基，因此自我复制的拷贝会多于其他线粒体。\n\n（自由基能在细胞内扩散吗？如果能的话，难道不是能促进所有的线粒体复制更多份吗？）\n\n细胞要么死于凋亡，比如脑细胞或心肌细胞的情况呢？这个组织会逐渐流失，这些变化都是衰老的表现。\n\n细胞没有死于凋亡，我们就能在“年迈”的细胞中发现线粒体突变的累积。经常引起慢性发炎和生长因子失调。刺激附近本来就有生长倾向的细胞，发展成癌症。\n\n一定程度的卡路里限制和低碳水化合物饮食，才能有效延缓衰老。它们都会促进生理压力反应（就像促氧化剂的作用），能够清除一些有缺陷的细胞和线粒体，短期内有利于生存，不过，代价通常是降低生育力。\n\n我们的祖先增加了有氧代谢能力、降低了自由基泄漏、降低了生育力，但同时延长了寿命。\n\n## Epilogue: From the Deep\n\n明神海丘附着在深海热液喷口附近的多毛纲蠕虫。蠕虫身上的微生物——其实只是其中一个细胞。明神海丘准核细胞（*Parakaryon myojinensis*）\n\n如果他们进行了全基因组测序，甚至只需要测定核糖体RNA的特征，就能更深刻地揭示这个细胞的真实身份，也能让这篇被人忽视的科学文献，变成《自然》级别的高影响力论文。但是他们把唯一的样本做成了电子显微镜切片。\n\n- 一个经过高度演变的真核细胞，为了适应不寻常的生活方式改变了正常的构造，才能寄生在深海热液喷口的蠕虫身上。但这种可能性不高。因为很多其他的细胞都在类似的环境中生存\n- 真的是一个中间型活化石，“真正的源真核生物”，不知如何能幸存至今。在稳定的深海环境中，它无法演化出现代真核生物的特征。它们并不是生活在从不改变的环境中，而是依附在多毛纲蠕虫的背上。蠕虫在真核生物演化初期显然尚不存在于世。\n- 这是一个原核细胞，获取了一些内共生体，正在变成一个类似于真核生物的细胞，重演演化之路。\n\n## 主线之外\n\n### 一些散碎知识\n\n核糖体在细胞器中算小的，人体肝脏细胞中约有千万量级，但从分子角度层面，这个结构又是一个大分子。\n\n核糖体的转录错误率大概在 1/10,000.\n\n古菌 archaea 并不比细菌更古老，反而比细菌更接近真核生物\n\nAllen Telescope Array 射电望远镜阵列在北加州的山地上，听命于旨在搜寻地外生命的 SETI\n\n人类 DNA 约有 30 亿字母，其中编码蛋白质的只有 2%，更大一部分用来进行基因调控，剩下的部分其功能科学家正在争论。\n\n洋葱、小麦、阿米巴虫的基因数量和DNA量比人类多。不同种类的两栖类的基因量跨度在两个数量级，其中能达到人类的 40 倍。\n\nLife, as biochemist Albert Szent-Györgyi observed, is nothing but an electron looking for a place to rest.\n\n从单细胞到多细胞反而有大约 30 个独立的进化路径。\n\nSuch ideas trace their roots back to the early twentieth century to Richard Altmann, Konstantin Mereschkowski, George Portier, Ivan Wallin and others, who argued that all complex cells arose through symbioses between simpler cells.\n\nmitochondria derive from α-proteobacteria; chloroplasts (the photosynthetic machinery of plants), deriving from cyanobacteria.\n\n端粒：telomeres\n\nEukaryotes have ‘genes in pieces’, in which short sections of DNA encoding proteins are interspersed by long non-coding regions, called introns.\n\n### 一些时间段/时间点\n\n- `5亿年`/`距今40亿年前`：地球产生 5 亿年后，生命出现：\n- `20亿年`：生命体的形态 (morphology) 进化停留在细菌水平\n- `距今15—20亿年前`：动、植物、单细胞原生生物的共同祖先出现\n- `5亿年前`：Cambrian explosion\n- `1670s`/`距今约350年前`：细胞被 Antony van Leeuwenhoek 发现\n- `1677`：Robert Hooke 也观察描述了细胞\n- `1944`：Schorödinger 发表 What is Life：1. 生命对抗熵增。2. 生命成功的关键在基因\n- `1953`：Crick \u0026 Watson 的第二篇 Nature 文章：\n- `1958`：Francis Crick 提出研究氨基酸序列信息 “protein taxonomy”\n- `1961`：Peter Mitchell 提出跨膜质子泵具有类似水库的储能作用。\n- `1967`：Lynn Margulis 提出线粒体和叶绿体的 endosymbiosis 假说\n- `1960s—`：Carl Woese 对比不同物种的编码同一核糖体结构的基因序列\n- `40年前`：Jacques Monod 写作了 Chance and Necessity\n- `1998`：Bill Martin 提出复杂生命来自 archaeon 宿主和后来成为线粒体的细菌的内共生合并。\n- `2011`：Lynn Margulis 去世\n\n\n# 讨论手记\n\n\n## June 7, 2024 ’s discussion: prologue to chapter 1\n\nmitochondrion 的基因组在内膜里面，我本以为在外面。在外面意味着假设今天的线粒体外膜是细胞膜，从而他自己有膜结构。在里面意味着内膜才是原始生物的细胞膜，线粒体外膜是胞吞时的囊泡膜。（维基百科有图）\n\n叶绿体 DNA 在什么位置？第 5 章有提及\n\nEndosymbiosis 在实验室里就能做。我问实验条件是否严格，是想问在自然环境下有多大概率发生，感觉老板答非所问。\n\nEndosymbiosis 之后的问题才更难，两个分裂周期不同的指数生长的生命体，怎么同步生长率，等等类似问题。\n\n老板推荐的文章：[https://www.cell.com/cell/pdf/S0092-8674(24)00182-X.pdf](https://www.cell.com/cell/pdf/S0092-8674(24)00182-X.pdf)\n\nSex: [https://www.pnas.org/doi/full/10.1073/pnas.1501725112](https://www.pnas.org/doi/full/10.1073/pnas.1501725112); 自然环境中的酵母细胞一般是二倍体，实验室用的单倍体是特殊处理后获得的。\n\ncell wall, 把老板难住了。（好像在第三章提到了）\n\n我们的呼吸作用前几步发生在细胞质，而这在需氧细菌中发生在它们的细胞质，等效于我们的线粒体内膜里面，把老板难住了\n\n被认为内吞线粒体的那种 archaea 最近在实验室养殖出来了。有 histone \n\n## June 14, 2024’s discussion: chapter 2 \u0026 3\n\nATP ↔ ADP+Pi 的好处已经说了，可以灵活地耦合在各种 ΔG \u003e 0 的化学反应里，使总反应的 ΔG \u003c 0，让他们在热力学和动力学上可行。\n\n为什么一定要用 ATP ↔ ADP+Pi？\n\n我们讨论的结果是理想状况下其实可以不一定用（比如燃料电池的电极），但是—— the ability to wait, and the tolerance to environmental noise\n\nGeometry problem：这些反应总需要一定的空间来发生，（还是说需要电极/天线来耦合不同的反应。）\n\n## June 21, 2024 discussion: chapter 2, 3 again\n\n老板的观点：DNA 或者遗传信息分子基本上充当了电池的作用，里面的信息转化成了自由能，驱动了细胞的能量变化的各种活动。\n\n我的问题：逆向的中心法则，在原始细胞里，先有原始的 DNA 分子，进行信息→物质的或称\n\nPURE system in a test tube\n\nError catastrophe of replicators in a German journal\n\n## July 5, 2024 discussion: chapter 4\n\nStarted with Figure 19.\n\nHuge debate of ocean mixing\n\na prediction: modern does not have a proton gradient, but would a sodium ion gradient compensate this lack of gradient?\n\npage 151 on english pdf\n\n## July 19, 2024 discussion: chapter 5\n\ncell cycle synchronization\n\npathways only exists in bacteria but not eukayrote, TSZ\n\ncytoskeleton are also found in bacteria\n\nescort machinery,  membrane shape in complex cells are also found in archae\n\ntriangle archae\n\nforward scattering to select vell size\n\nheteroplasmi\n\nlocalization tags onto pop tags\n"},{"slug":"reduction-potential-and-electronegativity","filename":"2024-06-16-reduction-potential-and-electronegativity.md","date":"2024-06-16","title":".tex | 还原电位和电负性","layout":"post","keywords":["tex","chem","bio"],"hasMath":true,"cover":"2024-06-16-chemical-battery.png","excerpt":"最近在看 Nick Lane 的《The Vital Question》，里面提到了“还原电位”的概念，就此复习一下高中学过的电化学知识。","content":"\n\u003e 最近在看 Nick Lane 的《The Vital Question》，里面 Part II, Chapter 3, proton power 一节讲地球上早期生命的生化反应的时候，提到了“[还原电位](https://zh.wikipedia.org/wiki/%E8%BF%98%E5%8E%9F%E7%94%B5%E4%BD%8D)”的概念，就此复习一下高中学过的电化学知识。\n\u003e \n\n### 还原电位 (Reduction Potential)\n\n维基百科：\n\n\u003e **还原电位**是**氧化还原电位 (Redox potential)** 的一种，指的是电活性物质发生电还原反应时的[电极电位](https://zh.wikipedia.org/w/index.php?title=%E7%94%B5%E6%9E%81%E7%94%B5%E4%BD%8D\u0026action=edit\u0026redlink=1)。\n\u003e \n\n（对应地，氧化电位就是电活性物质发生电氧化反应时的[电极电位](https://zh.wikipedia.org/w/index.php?title=%E7%94%B5%E6%9E%81%E7%94%B5%E4%BD%8D\u0026action=edit\u0026redlink=1)。）\n\n所以这一性质针对于一种反应物分子（个别情况下也可以是原子和离子，这些原子和离子也是化学反应的基本参与者，所以从语文上来讲也“是”分子）。\n\n这不是某一元素的性质。但是在一个还原反应中，真正化合价减少的一般也只有其中的一种元素的原子，所以也可以侧面体现出相应元素（高中好像管这个叫呈价元素是吧，记不清了）的性质。\n\n这也不是某一特定化学反应的性质。所以还原电位的测量是把待测的分子加入标准溶液中，在标准温度、标准气压下，测量标准电极的电位。\n\n\u003e Each species has its own intrinsic redox potential; the more positive the reduction potential (reduction potential is more often used due to general formalism in electrochemistry), the greater the species' affinity for electrons and tendency to be reduced.\n每种电活性物质有其特定的还原电位，还原电位值越正，代表该物质具有更强的得电子能力，即氧化性越强。\n\u003e \n\n于是就想起来高中的时候念“氧化剂被还原，得电子，化合价降低，发生还原反应，生成还原产物”的经，然后把氧化/还原、得/失、降低/升高对换再念一边，然后加上原电池/电解池、正极负极/阴极阳极再念两遍……\n\n然后老师就开始让你背，还说什么化学是理科中的文科，想想就头皮发麻。\n\n化学不该是这么学的。（如果你要高考还没高考的话当我没说，这么学确实是做题最快的。）\n\n拿书中的反应试着推理了一下，大约想通了还原电位和氧化性的关系。\n\n书中的反应是二氧化碳被氢气还原成甲烷，副产物是水，GPT 说这个反应叫 Sabatier reaction：\n\n$$\n\\text{CO}_2 + 4\\text{H}_2 \\rightarrow \\text{CH}_4 + 2\\text{H}_2\\text{O}\n$$\n\n还原反应：$$\\text{CO}_2 + 8\\text{H}^+ + 8\\text{e}^- \\rightarrow \\text{CH}_4 + 2\\text{H}_2\\text{O}$$，C 元素从 +4 价被还原成了 -4 价\n\n氧化反应：$$4\\text{H}_2 \\rightarrow 8\\text{H}^+ + 8\\text{e}^-$$，H 元素从 0 价被氧化成了 +1 价\n\n我把它画成了下图的形式。据书的作者说，这个反应可以在生命诞生的地方自发进行，所以这应该是一个原电池，导线中间的黑圈表示电流表。\n\n![原电池](/photos/2024-06-16-chemical-battery.png)\n\n左侧的电极周围发生的是氧化反应，会生成游离的电子。这些电子靠近电极时，进入导体的导带；右侧电极周围的还原反应正好需要电子作为反应物——电子的流向从左往右。\n\n电流方向和电子的流向相反，所以发生还原反应的右侧电极是原电池的正极，相对于氧化电极也就是电池负极，有一个正的电势差。此时反应自发进行，将化学能转化为电能。电势差正值越大，说明参与者的化学性质活泼，氧化剂的氧化性很强，被还原的趋势很大。\n\n这个例子作弊的地方在于反过来也能用，因为作者又说了，这个反应在今天的常见环境下是不能自发进行的，所以可以再画一个电解池的版本～\n\n![电解池](/photos/2024-06-16-electrolytical-cell.png)\n\n图中装置上唯一的变化就是把电流表换成了一个电源。电解液中发生的反应也不变，电子在两个的电极之间的流向也不变。\n\n但是这是一个电解池，发生还原反应电极的不再是电池的正极，而是电解池的阴极，相对于氧化电极也就是电解池阳极，有一个负的电势差。此时反应靠外来的电能驱动，电能转化成化学能。电势差的负值越大，说明参与者的化学性质越不活泼，氧化剂的氧化性很弱，被还原的趋势小。\n\n举这个化学反应做例子只是便于自己理解，还原电位的测量是在特定\n\n而且以上说法把电子当成了经典粒子，所以在量子力学的视角下不严格正确，但是图像也差不了太多～\n\n### 电负性 (Electronegativity)\n\n维基百科：\n\n\u003e 以一组数值的相对大小表示[元素](https://zh.wikipedia.org/wiki/%E5%85%83%E7%B4%A0)原子在分子中对成键电子的吸引能力，称为相对电负性，简称为电负性。元素电负性数值越大，原子在形成[化学键](https://zh.wikipedia.org/wiki/%E5%8C%96%E5%AD%A6%E9%94%AE)时对成键电子的吸引力越强。\n\u003e \n\n这个性质针对于某一种元素，不限于反应，不限于状态。\n\n好像不同的人给出了不同的定义和计算方法，感觉有点随意啊……\n"},{"slug":"information-entropy-kl-divergence-cross-entropy-mutual-information","filename":"2024-05-14-information-entropy-kl-divergence-cross-entropy-mutual-information.md","date":"2024-05-14","title":".tex | 比较两个概率分布/两条信息","layout":"post","keywords":["tex","phy","m"],"excerpt":"自信息、信息熵、KL Divergence、交叉熵、互信息","hasMath":true,"content":"\n\u003e 自鸣得意了半天，发现这篇文章基本就是维基百科 [Quantities of Information](https://en.wikipedia.org/wiki/Quantities_of_information) 词条英文版的翻译。但是对应的中文词条没有覆盖英文版那么多的内容，所以也不完全是无用功。\n\u003e \n\n## 信息和概率\n\n一条信息由一个命题来表达。（这一个命题可以是对多个命题进行逻辑演算的一个表达式。）\n\n而这个命题解答了人心中的某个疑问。既然这是个疑问，那么在得到确切的信息之前，有众多其他命题，和这条消息一样有可能是问题的答案。既然是有可能，那就是概率论可以派上用场的对方。所有这些可能成为答案的命题一起，构成一个随机变量空间。\n\n比如说一道有 ABCD 四个选项的选择题，如果是单选题，那么答案的随机变量空间就是 {A, B, C, D}，如果是多选题，则是 {A, B, C, D, AB, AC, AD, BC, BD, CD, ABC, ABD, ACD, BCD, ABCD}，如果是排序题、不定项排序题、答案出错了的题……\n\n## 描述一个概率分布的信息量\n\n### 自信息：Self Information\n\n自信息是一个随机事件的性质，也就是针对一个随机变量的**某一个可能取值**而言的。表达式为 \n\n$$\nI(m) = -\\log_n\\left(p(M=m)\\right)\n$$\n\n这是一个无量纲量，但是公式中指数的底数可以任意选择——\n\n- *n* = 2 的时候自信息的单位是 bit，也叫香农 (shannon), 这里的 bit 和二进制位 bit 不完全相同，一个香农是一个二进制位所能表示信息的**上限**：当一个二进制位完全取决于其它位时，这个位不包含任何额外信息，香农数为 0，但这个二进制位依然物理上存在；\n- *n* = *e* 的时候单位是 nat, 因为 $$\\log_e\\equiv\\ln$$ 叫做自然对数；\n- *n* = 10 的时候单位叫 hartley\n\n——单位之间的换算关系由对数的换底公式给出。\n\n这个量在信息论中的意义是，这条消息作为一个不方便问的问题的**答案**，**最少可以**用多少个 n 个选项的单选题套出答案。当 n=2 的时候，每个问题就是一个是非题，也就是一般疑问句。\n\n码农面试的时候经常问一类问题：一堆看起来相同的东西里面有一个不一样，你有一种不能直接测出答案的测量工具，最少需要测量几次才能辨别出来……但是自信息的计算不能提供具体的辨别方法，具体方法还是需要你自己去凑，而面试刷人很多都是在刷这种细枝末节。\n\n当然了，前提是你的面试官懂他自己在问什么，而不是相信美剧《硅谷》里压缩算法可以突破信息论极限的计算机民科～\n\n当 *p* = 0 时，自信息发散为无穷大。不过问题不大，原因在下一节。\n\n### 信息熵：Entropy\n\n信息熵是一个随机变量的概率分布的整体性质。\n\n算法很简单，就是自信息的概率期望，也就是按照随机变量每个取值的概率加权平均：\n\n$$\nS(p(M))=\\mathbb{E}_p[-\\log_n p(M)]=-\\sum_{m\\in M}p(m)\\log_n p(m)\n$$\n\n当 *p* = 0 时，自信息发散，但是概率为零，强行定义两者的积也为零，对信息熵不构成贡献。\n\n当我们只对某一特定的随机事件信息感兴趣，除此以外的所有事件合并为目标事件的补集，就得到二项信息熵 binary entropy:\n\n$$\nS_{binary} = -(1-p)\\log(1-p)-p\\log p = p\\log\\frac{1-p}{p}-\\log(1-p)\n$$\n\n沿着自信息的意义往下走，信息熵在信息论中的意义是，一个将众多信息/命题的集合作为备选答案的**问题**，**最少可以**用多少道 n 个选项的单选题的集合来等价替代。\n\n当这些最优的单选题确定之后，原问题的每一个选项，可以用单选题的答案序号来进行编码。指数的不同底数/信息量的不同单位就是数字的 n 进制，信息量就是相应进制下最大压缩编码后的位数。\n\n当然要讨论压缩的话，还需要另找地方记录各个单选题和选项，也就是压缩字典。\n\n## 比较两个概率分布的信息量\n\n而如何选择单选题，使之成为针对给定问题最优的问题集，会因为各个选项概率分布的不同而变化。即便是同一组信息/备选答案，两套不同的概率分布，各自会给出一套对自己最优的问题集，一套概率分布下的最优问题集不见得是另外一套概率分布下的最优问题集。\n\n\u003e 下面的表达式都只写出了离散变量的形式，连续随机变量需要将求和写成对应的积分。\n\u003e \n\n### 相对熵：Kullback–Leibler (K-L) Divergence\n\n英文里也叫 relative entropy 或者 I-divergence\n\n这里的两个概率分布映射自**同一个**随机变量空间。\n\n$$\nD_{KL}(p(X)|q(X))=\\sum_{x\\in X}p(x)\\log\\frac{p(x)}{q(x)}=-\\sum_{x\\in X}p(x)\\log\\frac{q(x)}{p(x)}\n$$\n\n这个量描述了当 *p*(*X*) 作为各选项的正确概率分布的情况下，用对 *q*(*X*) 最优的单选题去提问，**没问出来的信息**所需要的**额外的**单选题数目/编码数。\n\n在科学应用中，*p*(*X*) 一般是从实验中测量出来的概率分布，*q*(*X*) 是理论模型的预测。\n\n下面的例子计算了一个单选题，选 C、选 B、假想中一群学生的答案统计、胡猜四种概率分布 *p, q ,r , φ* 之间的 KL divergence。因为概率为零会出现发散问题，所以我们取 eps = 10^(-10) 把这些概率值截断：\n\n```python\nimport numpy as np\n\ndef kl_div(p,q,eps=1e-10):\n    p = np.clip(p,eps,1-eps)\n    q = np.clip(q,eps,1-eps)\n    return np.sum(p*np.log2(p/q))\n\np   = np.array([  0,   0,   1,   0])\nq   = np.array([  0,   1,   0,   0])\nr   = np.array([1/6, 1/6, 1/2, 1/6])\nphi = np.array([1/4, 1/4, 1/4, 1/4])\n\nresults = np.empty((4,4))\nfor i,v1 in enumerate([p,q,r,phi]):\n    for j,v2 in enumerate([p,q,r,phi]):\n        results[i,j] = kl_div(v1,v2)\n```\n\n| KL-div(行, 列)/bit | p | q | r | φ |\n| --- | --- | --- | --- | --- |\n| p = [0,0,1,0] | 0 | 33.219 | 1 | 2 |\n| q = [0,1,0,0] | 33.219 | 0 | 2.585 | 2 |\n| r = [1/6, 1/6, 1/2, 1/6] | 14.817 | 25.890 | 0 | 0.208 |\n| φ = [1/4,1/4,1/4,1/4] | 22.914 | 22.914 | 0.189 | 0 |\n\n从结果中我们可以看到：\n\n- 对角线为 0，符合其意义。\n- $$D_{KL}(p,q)$$ 和 $$D_{KL}(q,p)$$ 都应该是 +∞，这里的有限值是 eps 截断的结果\n- 除个别巧合，对称位置的值一般不相等。这个量不同于两点之间的距离。\n\n### 交叉熵：Cross Entropy\n\n这里的两个概率分布映射自**同一个**随机变量空间 X。\n\n概率分布 ***q* 相对于 *p*** 的交叉熵 cross entropy\n\n$$\nCE(p(X),q(X))=-\\sum_{x\\in X}p(x)\\log q(x)=S(p(X))+D_{KL}(p(X)|q(X))\n$$\n\n这个量描述了当 *p*(*X*) 作为各选项的正确概率分布的情况下，用对 *q*(*X*) 最优的单选题去提问，所需要的**总共的**单选题数目/编码数。\n\n类似于二项熵，*p* 和 *q* 之间的 binary cross entropy:\n\n$$\nBCE(p,q)=-p\\log q-(1-p)\\log(1-q)=p\\log\\frac{1-q}{q}-\\log(1-q)\n$$\n\n```python\ndef cross_entropy(p,q,eps=1e-10):\n    p = np.clip(p,eps,1-eps)\n    q = np.clip(q,eps,1-eps)\n    return -np.sum(p*np.log2(q))\n\nresults = np.empty((4,4))\nfor i,v1 in enumerate([p,q,r,phi]):\n    for j,v2 in enumerate([p,q,r,phi]):\n        results[i,j] = cross_entropy(v1,v2)\n```\n\n| Cross Entropy(行, 列)/bit | p      | q      | r   |   $$\\varphi$$   |\n| ---                      | ---    | ---    | ---   | --- |\n| p = [0,0,1,0]            | 0      | 33.219 | 1     | 2   |\n| q = [0,1,0,0]            | 33.219 | 0      | 2.585 |  2  |\n| r = [1/6, 1/6, 1/2, 1/6] | 16.610 | 27.683 | 1.792 | 2   |\n| $$\\varphi$$ = [1/4,1/4,1/4,1/4]    | 24.914 | 24.914 | 2.189 | 2   |\n\n- 对角线上不一定为零，而是自己的信息熵\n- 其他位置和 KL divergence 相差大约为第一个输入分布的信息熵，误差 eps 的截断\n\n### 互信息：Mutual Information\n\n这里的两个概率分布一般来说映射自**不同的**随机变量空间。\n\n$$\nMI(X,Y)=\\sum_{x,y}p(x,y)\\log\\frac{p(x,y)}{p(x)p(y)}=D_{KL}\\left(p(X,Y)|p(X)p(Y)\\right)\n$$\n\n从后一个等号可以看出，这一性质衡量的是 *X, Y* 两个随机变量的联合分布在多大程度上不同于“*X* 和 *Y* 相互独立”的零假设。两个随机变量相互独立时，互相不反映对方的信息，互信息 *MI* = 0。\n\n当从 *X* 所在的随机变量空间取样的难度比较大的时候，我们需要用容易取样的**另一个变量空间**的随机变量 *Y* 来推测 *X* 的情况，互信息就可以用来论证我们这种选择的合理性。\n\n## 扯点闲篇\n\n### PyTorch 中以此为基础的 loss functions\n\n`torch.nn` 中有如下几个和今天的文章相关的 loss functions：\n\n- `torch.[nn.KLDivLoss](https://pytorch.org/docs/stable/generated/torch.nn.KLDivLoss.html#torch.nn.KLDivLoss)`\n- `torch.[nn.CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss)`\n- `torch.[nn.BCELoss](https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html#torch.nn.BCELoss)`\n- `torch.[nn.BCEWithLogitsLoss](https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html#torch.nn.BCEWithLogitsLoss)`\n\n之所以没直接用这些函数计算上面的例子，是因为 `KLDivLoss` 是按元素计算，随后需要自己求和；`CrossEntropyLoss` 又是按类别的，还不需要归一化，而且文档的解释很复杂，我到现在也没看明白；而且还要注意这些函数的设计输入是不是 logit，这是机器学习里的概念，在此不展开了。\n\n### 玻尔兹曼的墓志铭\n\n$$\nS=k\\log W\n$$\n\n其中 *S* 是（微正则系综中的）热力学熵，*k* 是玻尔兹曼常数 $$k_B$$，*W* 是因为刻碑的师傅不会写 *Ω*。\n\nW 或 Ω 是处于相同能量的热力学状态的数量。因为你都需要统计物理了，显然是只知道能量，没办法知道所考虑的微观粒子究竟处于哪一个热力学状态。那此时的零假设就是处于所有状态的可能性相等，*p* = 1/Ω，信息熵 \n\n$$\nS =-\\sum_{m\\in M}p(m)\\log_n p(m)= -\\Omega\\cdot(\\frac{1}{\\Omega}\\log\\frac{1}{\\Omega})=\\log\\Omega\n$$\n\n和热力学熵只相差一个玻尔兹曼常数。这是因为信息熵是无量纲的，熵和温度的量纲相乘之后需要得到能量的量纲，只能由 $$k_B$$ 把量纲凑齐，而数值是自由能相关的实验里测出来的。\n\n好像这就是高中物理里熵的定义式是吧。\n\n上了大学以后，正则系综和巨正则系综中的熵也分别就是各自体系中各状态的概率分布的信息熵，乘上玻尔兹曼常数。~~（我也忘得差不多了，试图萌混过关）~~\n\n### 善卜者无先见之明\n\n公元 451 年，阿提拉 Attila 率领匈人攻入罗马领土，横扫有大量其他民族居住的高卢地区。西罗马帝国将军艾提乌斯 Aetius 联络了众多畏惧匈人的民族组成联军，其中包括西哥特人的王狄奥多里克 Theodoric，两军会战于卡塔隆 Catalaunian 平原。\n\n本来想用这个故事举例子来着，因为我记得阿提拉在战前找了个大师算了一卦，说是一位国王将战死，一个国家将崩塌。于是阿提拉很高兴，以为哥特人和狄奥多里克要玩完了，结果战斗打响，狄奥多里克确实死于乱军，但是罗马和哥特等族的联军击败了匈人，阿提拉的霸业雨打风吹去。\n\n于是试图说明算命的魅力就在于，用文字游戏表达一个自信息比较低的命题，同时误导对方相信一个自信息高得多的命题，在心理疏导之外，赚一个信息熵的差价。\n\n结果查证的时候发现好像不是这么回事，Barbarian Rising 故事片里的预言内容不一样；维基百科上没给出处，说算命的很准，于是阿提拉推迟到下午作战，方便晚上跑路；其他地方甚至压根没有算命的情节。但是写都写了，需要积累高考作文素材的小朋友们还是可以假装被我误导了~\n\n当然了，算命这个事还有一种情况，就是打着不确定的幌子，售卖确定但不方便承认自己确定的信息，那就是另一种生意，和另外的价格了~"},{"slug":"equivlance-between-diffusion-equation-and-random-walk","filename":"2024-04-25-equivlance-between-diffusion-equation-and-random-walk.md","date":"2024-04-25","title":".tex | 扩散方程和随机游走的等价","layout":"post","keywords":["tex","phy","m"],"excerpt":"之前 MCMC 讲错了","hasMath":true,"content":"\n\u003e 这些内容总结自美国研究生级别的《数学物理方法》两次课的笔记，大约两个小时。\n\u003cbr\u003e如果是中国大学本科的话，认真的老师半个小时庶几可以讲完;\n\u003cbr\u003e念 PPT 就算上课的话 15 分钟可以讲完，附赠一个段子;\n\u003cbr\u003e翻转课堂的话也就布置个作业，老师一句话可以讲完。\n\u003cbr\u003e以上数据除第一句外纯属揣测，没有黑任何人的意思，love and peace~\n\u003e \n\n### 扩散方程\n\n带有初值条件的扩散方程表述如下：\n\n$$\n\\begin{cases}\nu(x,t=0)=f(x) \\\\\n\\partial u(x,t)/\\partial t=\\sigma \\cdot \\partial^2 u(x,t)/ \\partial x^2\n\\end{cases}\n$$\n\n方程的解为：\n\n$$\nu(x,t) = \\frac{1}{\\sqrt{4\\pi \\sigma t}} \\int_{-\\infty}^{+\\infty} f(s)\\ e^{-\\frac{(x-s)^2}{4\\sigma t}}\\ ds\n$$\n\n解法是将 u(x,t) 对空间变量 x 作傅里叶变换为 U(k,t)，利用傅里叶变换的性质，变换后的方程将是关于时间 t 的一阶常微分方程。求解后作傅里叶逆变换 ~~即为上式。~~ ~~（完蛋，好久没做题了，那个 $$e^{-\\frac{(x-s)^2}{4\\sigma t}}$$ 是怎么凑出来的，为什么我直接给消掉了啊）~~ 凑出来了凑出来了，初值条件代入频域 k 空间里的通解来确定积分常数，可以看到结果 $$F(k) e^{-\\sigma t k^2}$$ 是两项之积，所以根据傅里叶变换的卷积定理，实空间 x 里的解是 f(x) 和 $$\\mathscr{F}_{k\\rightarrow x}^{-1}\\{e^{-\\sigma t k^2}\\}$$ 的卷积（所以上式的指数项以 (x-s) 为宗量），而计算后者的时候需要用到高斯积分～\n\n### 随机游走\n\n随机游走是一个离散过程，为了和连续时空中的扩散方程相对比，将空间变量 x 离散化为相隔 Δ 的格点 i，时间变量 t 离散化为相隔 δ 的 n。\n\n当一个粒子在 n 时刻位于格点 i 时，在下一个时刻 n+1, 它有 1/2 的概率移动到 i-1, 1/2 的概率移动到 i+1.\n\n所以，虽然每个进行随机游走的粒子在任意时刻都只有确定且唯一的位置，但是对于大量同样初始位置和运动规律的例子，n 时刻出现在 i 格点的概率 P(i,n) 有以下关系：\n\n$$\n\\begin{cases}\nP(i,0)= f_i \\\\\nP(i,n)=\\frac{1}{2}\\left[P(i-1,n-1)+P(i+1,n-1)\\right]\n\\end{cases}\n$$\n\n在初值条件为 $$f_i=\\delta_{i=0}$$ 时，递推结果如下：\n\n|  x 轴 — | i = -4 | i = -3 | i = -2 | i = -1 | O | i = 1 | i = 2 | i = 3 | i = 4 | → |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n| n = 0  | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 |  |\n| n = 1  | 0 | 0 | 0 | 1/2 | 0 | 1/2 | 0 | 0 | 0 |  |\n| n = 2  | 0 | 0 | 1/4 | 0 | 1/2 | 0 | 1/4 | 0 | 0 |  |\n| n = 3  | 0 | 1/8 | 0 | 3/8 | 0 | 3/8 | 0 | 1/8 | 0 |  |\n|**t 轴 ↓**|  |  |  |  |  |  |  |  |  |\n\n![](/photos/2024-04-25-random-walk-probabilities.png)\n\n离散的情况，很难对任意的初值条件写出解的表达式，但是对于上面的特殊情况，课上不加证明地给出了（可能是根据上图凑出来的）下面的解：\n\n$$\nP(i,n)=\\frac{1}{2^n}\\frac{n!}{\\left(\\frac{n+i}{2}\\right)!\\left(\\frac{n-i}{2}\\right)!}\n$$\n\n对的，以上关系只能表示 (n+i = 偶数) 的情况，但是康托尔告诉了我们，所有偶数和所有自然数的“数量”一样多，所以也没差太多～\n\n### 方程的等价\n\n概率的递推公式可以变换为：\n\n$$\n\\frac{1}{\\delta}\\left[P(i,n)-P(i,n-1)\\right]=\\frac{1}{2}\\left [\\frac{P(i-1,n-1)-2P(i,n-1)+P(i+1,n-1)}{\\Delta^2}\\right]\\frac{\\Delta^2}{\\delta}\n$$\n\n因为 $$x=i\\Delta,\\ t=n\\delta$$, 对两个变量的微分可以离散化成差分：\n\n$$\n\\frac{\\partial}{\\partial x}\\rightarrow \\frac{1}{\\Delta}\\left[()_i-()_{i-1}\\right],\\ \\frac{\\partial}{\\partial t}\\rightarrow \\frac{1}{\\delta}\\left[()_n-()_{n-1}\\right]\n$$\n\n直接就能看出扩散方程和随机游走的等价，且系数之间存在关系：$$\\sigma = \\frac{\\Delta^2}{2\\delta}$$\n\n### 解的等价\n\n只讨论一个 δ(x) 函数作为初值条件的情况，我们要证明此时扩散方程的解：\n\n$$\nu(x,t) = \\frac{1}{\\sqrt{4\\pi \\sigma t}} e^{-\\frac{x^2}{4\\sigma t}}\\ \\xleftarrow[{\\Delta,\\delta \\rightarrow 0;\\ i,n\\rightarrow \\infty}]{x=i\\Delta,\\ t=n\\delta} \\frac{1}{2^n}\\frac{n!}{\\left(\\frac{n+i}{2}\\right)!\\left(\\frac{n-i}{2}\\right)!} \\frac{1}{2\\Delta}\n$$\n\n只需讨论这一个情况，因为 δ(x-s) 函数可以看作将一个函数 f(x) 在自变量 x=s 时切片为 f(s)，而任何一个（性质比较“优美”的）函数都可以看作把它自己在定义域上的所有点切片后再重新叠加起来：\n\n$$\nf(x) = \\int_{-\\infty}^{+\\infty}f(s)\\delta(x-s)\\ ds\n$$\n\n过程需要用到 Sterling 公式对阶乘的近似：$$n! \\approx \\sqrt{2\\pi n}\\ n^n e^{-n}$$\n\n$$\n\\begin{array}{rcl}\n\\frac{P(i,n)}{2\\Delta} \u0026 \\approx \u0026 \\frac{1}{2\\Delta} \\frac{1}{2^n} \\frac{\\sqrt{2\\pi n}\\ n^n e^{-n}}{\\sqrt{\\frac{2\\pi (n-i)}{2}}\\ \\left(\\frac{n-i}{2}\\right)^{\\frac{n+i}{2}} e^{-\\frac{n+i}{2}}\\sqrt{\\frac{2\\pi (n+i)}{2}}\\ \\left(\\frac{n+i}{2}\\right)^{\\frac{n+i}{2}} e^{-\\frac{n+i}{2}}} \\\\\n\u0026 = \u0026 \\frac{1}{2\\Delta}\\frac{\\sqrt{2n}}{\\sqrt{\\pi(n^2-i^2)}}\\frac{n^n}{(n-i)^{\\frac{n}{2}-\\frac{i}{2}}(n+i)^{\\frac{n}{2}+\\frac{i}{2}}} \\\\\n\u0026 = \u0026 \\frac{1}{2\\Delta}\\frac{\\sqrt{2n}}{\\sqrt{\\pi(n^2-i^2)}}\\frac{n^n/n^n}{(n-i)^{\\frac{n}{2}}(n+i)^{\\frac{n}{2}}(n-i)^{-\\frac{i}{2}}(n+i)^{\\frac{i}{2}}/n^n} \\\\\n\u0026 = \u0026 \\frac{1}{2\\Delta}\\frac{\\sqrt{2n}}{\\sqrt{\\pi(n^2-i^2)}} \\frac{1}{\\left(1-\\frac{i}{n}\\right)^\\frac{n}{2}\\left(1+\\frac{i}{n}\\right)^\\frac{n}{2}\\left(1-\\frac{i}{n}\\right)^{-\\frac{i}{2}}\\left(1+\\frac{i}{n}\\right)^\\frac{i}{2}} \\\\\n\u0026 = \u0026 \\frac{1}{\\sqrt{2}\\Delta}\\frac{1}{\\sqrt{\\pi(n-\\frac{i^2}{n})}} \\frac{1}{\\left(1-\\frac{i^2}{n^2}\\right)^\\frac{n}{2}\\left(1-\\frac{i}{n}\\right)^{-\\frac{i}{2}}\\left(1+\\frac{i}{n}\\right)^\\frac{i}{2}} \\\\\n\u0026 \\xrightarrow[\\frac{i}{n}=\\frac{x\\Delta}{2\\sigma t},\\ \\frac{i^2}{n}=\\frac{x^2\\delta}{\\Delta^2 t}]{(1+a\\epsilon)^{1/\\epsilon}\\rightarrow e^a} \u0026 \\frac{1}{\\sqrt{4\\pi\\sigma t}}\\frac{1}{e^\\frac{x^2}{4\\sigma t} e^\\frac{x^2}{4\\sigma t} e^{-\\frac{x^2}{4\\sigma t}} } \\\\\n\u0026 = \u0026 \\frac{1}{\\sqrt{4\\pi\\sigma t}}e^{-\\frac{x^2}{4\\sigma t}}\n\\end{array}\n$$\n\n### 之前 MCMC 讲错了\n\n讲 Markov Chain Monte Carlo 模拟的时候举的例子是计算 $$\\int_{-\\infty}^{+\\infty}e^{-x^2}dx$$, 现在系数可以对上了：$$1=4\\sigma t=4 \\frac{\\Delta^2}{2\\delta} n\\delta = 2\\Delta^2n$$, 随机游走的步数和步长之间存在一个确定的关系，在步长确定的情况下，我们需要重复模拟大量粒子作相同步数的随机游走，然后统计这一确定步数走完之后的每个粒子的终末位置。\n\n所以，这并不是一个 Markov Chain Monte Carlo 模拟，只是一个普通的 Monte Carlo 模拟，我们拿到了想要知道的随机变量的原始概率分布，只不过取得符合这一概率分布的每一个样本的过程是一个 Markov 过程。\n\n正经的 MCMC，应该是只模拟一个粒子作随机行走，然后把它每一步的位置记录下来，统计到样本里去。这样的话时间 t 的信息就被抹去了，而且由于扩散方程描述的状态并不是热力学平衡态，并不能通过统计物理中的遍历性 (ergodicity) 来得到正确结果。\n\n采用了 Metropolis 算法的 MCMC, 一个粒子作随机行走只是其中的一个步骤，还要计算这一步之前和之后 $$e^{-x^2}$$ 的值，来决定这一步是否被加入样本，不成立的话要退回前一步继续走。\n"},{"slug":"mc-mcmc-markov-chain-monte-carlo-gibbs-sampling","filename":"2024-04-15-mc-mcmc-markov-chain-monte-carlo-gibbs-sampling.md","date":"2024-04-15","title":".tex | MC→MCMC 蒙特卡洛模拟，基于马尔科夫链采样","layout":"post","keywords":["tex","phy","m"],"excerpt":"蒙特卡洛模拟、马尔科夫链采样、Metropolis-Hastings 算法、吉布斯采样","hasMath":true,"content":"\nMonte Carlo 蒙特卡洛模拟，简称 MC. \n\nMarkov Chain Monte Carlo 是用马尔科夫链采样的蒙特卡洛模拟，简称 MCMC.\n\n## Monte Carlo 模拟\n\n这个比较简单了，举个例子，要计算 π 的近似值，可以在一块正方形板子里画一个内接圆，然后以均匀的概率往正方形里一粒一粒地扔沙子，每扔一粒，就判断并且记录这里沙子在圆内还是圆外，然后把沙子吹掉，如此往复。圆的面积是 πr²，正方形的面积是 4r²，所以落在圆内的概率（圆内沙子的数量和总数的比值）乘 4，就是所求。\n\n![](/photos/2024-04-15-monte-carlo-pi.png)\n\n归纳一下：当问题的解用一个随机变量的概率分布、期望值、二阶矩……等等来表示的时候，就生成一个符合该概率分布的随机样本，用样本的统计量去近似原概率分布。\n\n## Markov Chain Monte Carlo\n\n但是前述例子有一个步骤，就是我们往板子上扔完沙子要把沙子吹掉，每粒沙子，每次扔沙子之间也应该看不出区别，这是为了保证取样之间**相互独立且来自同一个概率分布**。\n\n但是很多取样过程无法满足这种条件，或者达成条件所需的成本很高。比如计算一个高斯积分 $$\\int_{-\\infty}^{+\\infty}e^{-x^2}dx$$，被积函数的取值范围涵盖整个实数集，想找一个在整个实数集上均匀分布的随机数发生器就比较难了。\n\n![](/photos/2024-04-15-monte-carlo-gaussian.png)\n\n但是学过物理的朋友应该知道，上面的被积函数是以狄拉克 δ(x) 函数为初值条件的一个扩散方程的解，在某一时刻的空间分布。（不想凑系数了，将就看吧）\n\n而扩散方程又是随机游走 (random walk) 在连续近似下的极限。\n\n所以我们直接模拟一堆粒子从原点出发作随机行走，向两个方向的概率相同，扩散系数以及积分里的常数对齐，统计粒子在整个过程中出现在不同 x 位置的频率，求和之后乘以步长就是积分结果。这个过程需要的随机数发生器容易获取得多，是一个以 0.5 为阈值的 [0,1) 的均匀分布，比如一个均匀硬币。\n\n而随机行走过程中走完每一步的位置，都只取决于前一步的位置，而与更久远的历史无关——这样的过程叫做马尔可夫过程。用这种方法取样获得随机样本的蒙特卡洛模拟，就是 MCMC.\n\n扩散方程和随机行走只是 MCMC 的一个很特殊很特殊的例子，而对于一般的 MCMC 模拟，有以下通用的 Markov Chain 采样的算法：\n\n### Metropolis-Hastings 算法\n\n已知一个随机变量 x, 和一个与目标概率分布 P(x) 成正比的函数 f(x)（不要求 f 归一化）\n\n1. 初始化\n    1. 选定初始采样点 $$x_0$$ \n    2. 选定一个采样函数 proposal function，也就是在已知当前 x 的取值时，下一个 x’ 取值的概率分布 $$g(x’\\vert x)$$；其中对于 Metropolis 算法，这个采样函数是对称的：$$g(x’\\vert x)=g(x\\vert x’)$$. 常用以两者之差为宗量的高斯函数。\n2. 在得出 t 时刻的 $$x_t$$ 之后：\n    1. 根据 $$g(x'\\vert x_t)$$ 抽样得到一个 x’\n    2. 计算 α = f(x’)/f(x) = P(x’)/P(x)\n    3. 决定是否将 x’ 加入样本\n        1. 如果 α ≥ 1, 直接加入\n        2. 如果 α \u003c 1, 以 α 为概率加入\n\n这种方法不保证采样的早期样本也符合目标概率分布，所以一般会抛弃最先加入的若干样本。\n\n### Gibbs 采样\n\n只是一种思路，不算是完整的算法。\n\n当被采样的随机变量是一个多维向量的情况，在不使用 Gibbs 采样的情况下，在迭代的某一步骤 t，每个分量都应该是前一步骤的函数：$$x_{i,t}=f(\\{x_{j,\\ t-1}\\})$$\n\n而 Gibbs 采样就是说，不必让每个维度 i 都根据前一个步骤的分量来取值，可以把当前 t 已经取样出来的分量直接带入到本回合后面的维度：$$x_{i,t}=f(\\{x_{j,\\ t}\\}_{j\u003ci}\\cup\\{x_{k,\\ t-1}\\}_{k\\ge i})$$"},{"slug":"note-consciousness-theories","filename":"2024-01-10-note-consciousness-theories.md","date":"2024-01-10","title":".tex | 意识理论笔记","layout":"post","keywords":["tex","bio"],"excerpt":"整合信息理论 (IIT) vs. 全局神经工作空间理论 (GNWH)","content":"\n\u003e 本文是《**[一场意识理论大混战，甚至“伪科学”帽子都飞出来了](https://mp.weixin.qq.com/s/S_nZFZD72Kq3sJmoXKplww)**》一文的读书笔记。\n\u003e \n\n## 名词解释\n\n- **发放**：对 spike 的翻译。发放率即 spike-count rate. [https://zh.wikipedia.org/wiki/神经编码](https://zh.wikipedia.org/wiki/%E7%A5%9E%E7%BB%8F%E7%BC%96%E7%A0%81)\n- **V1**：初级视皮层。[https://zh.wikipedia.org/wiki/视觉系统](https://zh.wikipedia.org/wiki/%E8%A7%86%E8%A7%89%E7%B3%BB%E7%BB%9F)\n\n## 事实陈述\n\n### 科赫 vs. 查默斯赌局：信息整合理论作者之一打的一个赌\n\n- 德裔美国神经科学家科赫 (Christof Koch) vs. 澳大利亚哲学家查默斯 (David Chalmers)\n- 内容：以下问题能否在25年内得到解决：\n    - 查默斯称为意识的“**困难问题**”（hard problem）：“主观的意识是怎样从客观的神经回路中涌现出来的”\n    - 等价于，科赫和其忘年交克里克所说的“**意识的神经相关集合**”（Neural correlates of consciousness）\n- 结果：没能解决，科赫认输，双方再约 25 年\n- 时间线\n    - 1990年，克里克和科赫《走向意识的神经生物学理论》（[Towards a Neurobiological Theory of Consciousness](https://profiles.nlm.nih.gov/spotlight/sc/catalog/nlm:nlmuid-101584582X469-doc)）\n        - 观点：研究意识也是一样，应该从研究脑中哪些神经活动和视知觉相关——意识的神经相关集合开始。\n        - 挑战：当主体受到视刺激后的脑活动变化，既可能是由于视知觉引起的，也可能是由于刺激变化本身引起的。\n    - 1996年，在德国工作的希腊神经科学家洛戈塞蒂斯（Nikos K. Logothetis）猴子双眼竞争实验\n        - **双眼竞争**：给主体的双眼分别看两个完全不同的景象时，主体看到的并非这两个景象的融合，而是轮流看到其中之一。\n        - 结果：\n            - 在初级视皮层和次级视皮层，绝大多数细胞的 **发放(?)** 率与知觉的反复变化无关。总体来说，只要一只眼睛有输入刺激，神经元的发放就会增强。这与猴子究竟看到了什么无关。\n            - 他们又发现在对猴子下颞叶（inferior temporal，IT）皮层及上颞叶沟（superior temporal sulcus，STS）的下侧（该区域与IT上部相邻）进行实验记录时，只有当猴子“看到”时才有发放。\n        - 推论：\n            - 一般认为 **V1(?)** 对意识贡献甚微。\n            - 克里克对此非常兴奋，他认为这一技术已使科学家找到了研究视知觉神经相关集合的钥匙，并宣称到20世纪末就能发现意识的神经相关集合。\n            - 像功能性核磁共振成像和光遗传学等新技术的出现，使科赫在当时认为，25年的时间去解决应该没问题。\n    - 科赫在1998年和查默斯打了这个赌。这个赌只与能否在2023年解决查默斯的“困难问题”有关，而与其他论点无关。\n\n### 意识的神经相关最小集合：信息整合理论的预备知识\n\n- 定义\n    - 意识的神经相关集合就是“神经元的某种机制或事件的集合。该集合是形成某个特定知觉或体验所需要的最小集合。”（[科赫2004](https://ajp.psychiatryonline.org/doi/10.1176/appi.ajp.162.2.407)）\n    - 他和托诺尼又引申出对所有可能意识内容的意识神经相关集合的总体，并称之为全意识神经相关集合（full neural correlates of consciousness）。（[2016](https://www.nature.com/articles/nrn.2016.22)）\n- 方法与相应结果\n    - 需要排除对脑涌现意识非必要的事件，比如报告信号这件事本身（通过检测眼动或瞳孔放大）\n        - 这种“无报告范式（no‑report paradigms）”所确定的有特定内容的神经相关集合比需要报告时得到的更局限于皮层后部。（[2016](https://www.nature.com/articles/nrn.2016.22)）\n    - 一种是“基于不同状态的方法”（state-based approaches）把清醒的健康受试者在不要求做任何任务而有意识时的脑活动和意识丧失时（如无梦睡眠、全身麻醉、昏迷或植物状态）的脑活动进行比较。\n        - 全意识神经相关集合往往包括额-顶叶网络，但是这里有些部分可能和受试者的警觉、注意等脑功能有关。\n    - 另一种“同样状态无任务范式”（within-state, no‑task paradigm）这主要是利用意识的自发波动，例如当受试者处于无快速眼动睡眠期时将其叫醒，有时受试者说是正在做梦，而有时则没有任何意识。把受试者在报告做梦或无意识前记录下来的脑电图进行比较，\n        - 全意识相关神经机制主要位于包括感觉区在内的后皮层热区（posterior cortical hot zone），也就是包括皮层后部颞-顶-枕叶交界处在内的脑区，这和根据有特定内容的意识神经相关集合所得的结果在总体上吻合得相当好，因此可以把后部皮层区看作意识神经相关集合的热门候选区。\n\n### 整合信息理论 vs. 全局神经工作空间理论\n\n- 整合信息理论 (IIT) 同一把大伞之下，有两个很不相同的内容\n    1. （根据“同样状态无任务范式”）全意识相关神经机制主要位于包括感觉区在内的后皮层热区（posterior cortical hot zone），也就是包括皮层后部颞-顶-枕叶交界处在内的脑区，这和根据有特定内容的意识神经相关集合所得的结果在总体上吻合得相当好，因此可以把后部皮层区看作意识神经相关集合的热门候选区。这一观点笔者称之为“后脑理论”。\n    2. 托诺尼早已因他制定了一个度量意识的指标Φ并冠名为“整合信息理论”而闻名，而科赫也曾称赞过这一理论是“有关意识的唯一有希望的基本理论”，因此他们的这一观点也被称为IIT\n        - 托诺尼、埃德尔曼（Gerald Edelman）曾经提出过“整体性”和“信息性”（或称“神经复杂性”）作为衡量意识程度的定量指标。托诺尼正是在这一基础上考虑了意识更多的基本性质（其核心依然是整体性和信息性，但是却略去了“主观性”或“私密性”这一意识的根本属性），并以此作为“公理”。\n        - 这些公理包括：內禀存在性（Intrinsic existence）、结构性（composition）、信息性、整体性（integration）、排他性（exclusion）。\n        - 在这些公理的基础之上，托诺尼认为如果一个物理系统要有意识的话，那么这个系统就必须有和上述公理相应的性质，它应该是一个有数量极大的可能状态的统一整体，为此在有关脑区之间必须有交互作用。意识的程度可用该系统超越其各组成部分所含信息量的总和的信息量来度量，他们把这称为“整合信息（integrated information）”，并用符号Φ来表示\n- 全局神经工作空间理论（global neuronal workspace hypothesis）\n    - 《[Consciousness and the Brain: Deciphering How the Brain Codes our Thoughts](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3971003/)》\n    - 法国认知神经科学家德阿纳（Stanislas Dehaene）提出。由于对意识仍然没有明确和普遍接受的定义，德阿纳将他的研究集中在他所谓的“进入意识（conscious access）”（受试者意识到了其所受刺激并可以向其他人报告的现象）上。\n    - 他们使用掩蔽、双眼竞争和其他方法表明，虽然刺激保持不变或几乎不变，但受试者的知觉却可能发生根本变化，例如从意识不到变成意识到，或正好相反，因此进入意识可以被视为唯一的变量，并可以通过实验对这一变量进行操控。\n    - 发现如下标记：\n        1. 刺激诱发的脑活动大大增强，扩大到多个脑区并突然引发前额叶皮层和顶叶皮层许多回路的活动；\n        2. 脑事件相关电位中的晚成分P3突然增强；\n        3. 在晚期突然爆发高频振荡；\n        4. 跨脑区域活动的同步化。\n    - 观点/结论\n        - 当有有意识的知觉时，神经元群以协调的方式开始发放，首先是在一些局部的特定区域，然后蔓延到皮层的广大范围。最终，它们侵入到许多前额叶和顶叶脑区，同时与前面的感觉区保持紧密同步。正是在这个时候，突然形成了一个协调一致的脑网络，有意识觉知也似乎由此产生。\n        - 意识是一种在全脑范围里的信息共享。人脑中有高效的长距离网络，特别是在前额叶皮层，以选择相关信息并将其扩播到整个脑。意识是一种演化装置，它使我们能够注意某个信息并在这一扩播系统中保持活跃。一旦这个信息被意识到了，根据我们当时的目标，它可以被灵活地传送到其他区域。\n\n### 邓普顿世界慈善基金会5年对抗合作\n\n- 权威杂志《科学》（*Science*）和《自然》（*Nature*）等载文称赞了这一对抗性合作\n- 有124位意识研究科学家联名发表公开信，把IIT谴责为“伪科学”\n- 六个独立实验室遵照双方预先商定的方案，并分别用功能性核磁共振、脑磁图和皮层电图技术对250名受试者测量其脑活动，以检验这两种理论对他们共同认同的两个实验方案中第一个方案的不同预测，双方自己并不参加实验。\n- 整合信息理论 IIT\n    - 有利：关于IIT，确实观察到，后皮层脑区持续有信息。\n    - 不利：并没有发现IIT所预测的脑区之间有持续的同步活动\n- 全局神经工作空间理论 GNWH\n    - 有利：意识的某些方面确实可以在前额叶皮层中表现出来\n    - 不利：\n        - 并非一切意识活动都可以在此有所反映\n        - 实验发现只有体验开始时才有信息扩布的证据，但未能发现在体验结束时也有扩布\n- 124位意识研究者，其中包括巴尔斯（Bernard J. Baars）、丹纳特（Daniel C. Dennett）和丘奇兰（Patricia S. Churchland）等著名学者，联名发布一封公开信，指责《科学》《自然》等媒体做了不实报道，并指责IIT是伪科学。抓住了和托诺尼的Φ值有关的问题全盘否定。\n\n## 作者观点\n\n- 解决意识的神经相关集合的问题呢，还是解决查默斯的困难问题，科赫似乎把这两个问题认为是同一个问题，作者不同意\n- 科赫/托诺尼和德阿纳的对抗性合作实际上是关公战秦琼。\n- 124人公开信故意回避了在对抗性合作中的IIT实际上是指后脑理论，而和Φ没有直接关系，因此他们对对抗性合作的批评就像是枪打稻草人，但是他们对Φ理论的批评却有其合理之处。\n    - 像意识这样复杂的对象能不能用公理化的方法来进行研究\n    - 在托诺尼的5条公理中也故意丢掉了对意识来说最关键的“主观性”，不完备的公理系统推导出来的Φ指标，只能度量意识作为神经系统超越其各组成部分所含信息量的总和的信息量这一个方面，而非意识本身。\n- 全局神经元工作空间假设对于进入意识标记的解释没问题，但对他的假设是否也能够解释意识或者即使只是进入意识本身持怀疑态度。\n    - “进入意识标记”并不是“进入意识”本身，就像某人的签名并不就是他自己一样。\n    - 除了用受试者的主观报告来判断他们是否意识到了什么之外，德阿纳的工作并没有触及意识的主观性问题。\n- 后脑理论的一个重要依据是采用“无报告”范式的研究方法，但是也有科学家根据一些报道称在采用这种方法时，也能在前额叶皮层检测到有活动，因此把这种方法贬之为“误导”。\n    - 不过被贬一方则坚称“从总体上来说，前额叶皮层对意识来说既非必要，也不充分，这和皮层后部完全不同。”\n- 动物行为学家戴维·培尼亚-古斯曼提出意识包括三个重要方面：主观意识、情感意识和元认知意识。\n    - 主观意识：主观存在感和具身的自我觉知感\n    - 元认知意识：自己知道自己是有认知能力"},{"slug":"probability-vs-likelihood","filename":"2023-11-21-probability-vs-likelihood.md","date":"2023-11-21","title":".tex | 概率 (probability) 和似然性 (likelihood)","layout":"post","keywords":["tex","m"],"hasMath":true,"excerpt":"如题","content":"\n一个随机变量 X 取值为 x 的概率 (probability)/概率密度，一般可以用一个有若干参数的函数来表示。这个函数的参数记作  $$\\theta$$：\n\n$$\nprob_X(x)=f(x|\\theta)\n$$\n\n而似然性 (likelihood) 就是把上式 f 看作以 $$\\theta$$ 为自变量，x 为参数的函数，从表达式上看不出区别：\n\n$$\nL(\\theta|x)=f(x|\\theta)=prob_X(x)\n$$\n\n最近处理一个数据集，整理完之后的直方图如下：\n\n![double-peak](/photos/2023-11-21-double-peak.png)\n\n比较明显，比起一个正态分布 $$f(x)=\\frac{1}{ \\sigma \\sqrt{2\\pi} } e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2}$$，这些数据更像是来自不同均值和方差的两个分布。那么对于每个数据点 $$x_0$$，它到底来自哪个分布呢？可以分别计算 $$L_1(\\mu_1,\\theta_1\\vert x_0) = f(x_0\\vert\\mu_1,\\theta_1)$$ 和 $$L_2(\\mu_2,\\theta_2\\vert x_0) = f(x_0\\vert\\mu_2,\\theta_2)$$，然后比较 $$L_1$$ 和 $$L_2$$ 的大小。\n"},{"slug":"what-is-intelligence-not-same-as-intelligence-is-what","filename":"2023-06-17-what-is-intelligence-not-same-as-intelligence-is-what.md","date":"2023-06-17","title":".tex | 什么是智能≠智能是什么","layout":"post","keywords":["tex","doc","md","ai"],"hasMath":true,"excerpt":"“什么是智能”的问题每每得不到回答，是因为它的逆问题“智能是什么”没有答案。","content":"\n## 0\n\n这是一篇酬和之作。\n\n徵文标题说的是：\n\n\u003e **機器會製造「內涵」嗎？**\n\u003e \n\n但是正文提出的问题是：\n\n\u003e AI透過程式組合出回答你問題的文字組合，有「涵義」嗎？\n\u003e \n\n可是，「內涵」和「涵義」两个词的——内涵/涵义——就不完全一样啊……\n\n“内涵”(connotation) 通常指词语或表达方式所隐含的情感、态度、暗示或附加的意义。它涉及到词语或表达方式所引起的情感、联想或隐含的观点。也就是弦外之音。\n\n而“涵义”(meaning) 一般指词语、表达方式或行为所传达的字面意义或字面上的定义。它强调的是直接的、明确的意义。\n\n看热闹不嫌事大，那我们再把问题搞复杂一点——在逻辑学里也有一个“内涵”(intension)，和“外延”(extension) 相对应。用**面向对象编程**的说法来理解，一个类里面定义的所有状态量和内部方法的集合，就构成这个类的“**内涵**”；所有（已经和将来能够）从这个类实例化出来的对象的集合，就构成这个类的“**外延**”。\n\n所以看起来，征文者想问的是日常“内涵”也就是言外之意，但是怕杠精（比如我）用有严格定义的逻辑“内涵”解构掉，所以换了“涵义”一词。\n\n这个问题很显然是因应最近大语言模型掀起的这一波 AI 浪潮。这个问题往前再问一句，就是“大语言模型是智能体/有智能吗？”\n\n- 前两年 DeepMind 的 AlphaGo/AlphaZero 系列 AI 在围棋中击败人类棋手时，人们也在问这个问题。\n- 上世纪四五十年代专家系统 (expert system) 刚刚开发的时候，人们也在问这个问题。\n- 从电子计算机往前追溯到机械计算机，甚至是巴比奇的差分机的时候，人们就已经开始问这样的问题了。\n\n这些问题求并集，然后在问题数量趋近于无穷下的极限，就是“什么是智能”。\n\n这样的问题每每得不到回答，是因为它的逆问题“智能是什么”没有答案。我们并没有智能的准确定义，只能一事一论。而之前的智能和非智能体的区别太明显，以至于作出判断也不能对智能的定义有所启发。\n\n## 1\n\n而对“智能是什么”的探究，哲学、逻辑学、计算机科学、生物学、管理学，不同领域的研究者有着不同的思路。\n\n### 古哲学·洞穴之壁与理念世界\n\n古希腊哲学家柏拉图在《理想国》里提到了“洞穴之壁”的寓言故事。\n\n有一群被囚禁在一个深洞的囚徒，从出生开始就被束缚在这个洞穴里，脖子和腿都被铁链锁住，没办法转身或离开。囚徒身后的洞穴入口处有一道火焰，火焰后有人持物体走过，物体的投射在洞穴内的墙壁上形成了影子。囚徒们就以为这些影子就是唯一的存在。\n\n这里的囚徒代表着人类，洞穴代表着世界，影子则代表着我们对于现象世界的感知和观念。人们的知识和信念往往受限于自己的经验和感知，就像囚徒们只看到了洞穴墙壁上的影子，而在影子之外还存在一个理想的理性世界。柏拉图用这个寓言故事表达了他对于人类认识和智慧的理解。所谓智慧，就是从洞穴的影子反过去推测火把前物体的能力。\n\n当然，这种思想被 Marx 主义定性为一种客观唯心主义、唯理论，是受其批判的。\n\n### 逻辑学·从命题到希尔伯特算符\n\n柏拉图的学生亚里士多德，今天在低年级的物理教科书里基本是个反面典型，但他对逻辑学进行了系统化和全面的研究，提出了许多逻辑学的基本概念和原理。这些成果后来成为了欧洲哲学和逻辑学的基石，对西方哲学和科学的发展产生了深远影响。\n\n所谓逻辑，就是研究命题的对错，以及如何判断命题对错的学问。而命题，就是能被判断对错的句子。但是句子显然可以再分成不同成分，于是就发明/发现了主体、客体、谓词、谓词的量词……等等概念，以及用这些概念构造命题的方法。\n\n但是要注意，虽然逻辑主要由语言来表达，但是逻辑还是和语言不同，主体、客体也不等于句子的主语、宾语。这两者的区别，基本可以类比于之前洞穴之壁寓言里的实体和影子。\n\n这种努力到目前为止的巅峰，基本上要数希尔伯特形式化逻辑系统了。感兴趣的朋友可以自行查阅戈得门特《代数学教程》的第一章，这玩意相当于思想界的引体向上，反正我是一个也拉不上去……\n\n### 计算机·从半导体到抽象语法树\n\n希尔伯特是德国的数学家，《代数学教程》也是数学而不是哲学教材。显而易见，逻辑虽然由哲学家奠基，但是主导权很快落到数学家，至少是哲学家兼数学家手里了。\n\n命题的“真”与“非真”同构于 {1, 0}，各种逻辑运算都可以分解成“或”与“非”两种基本逻辑运算的组合，这就是以数学家乔治·布尔 (George Boole) 命名的布尔代数。因为 {1, 0} 又可以同构于半导体电路的高低电位，和各种类似继电器的门电路组合，所以很容易用计算机在物理世界表示出这些逻辑运算。\n\n我们的电脑由上亿个这样的电位和逻辑门组成，一般的科普文章应该会去介绍芯片啊光刻机之类的东西，本文关注的是另一个方面：虽然生产电脑配件的厂商很多，不同的型号的元器件设计不同，组装出的成品应该千差万别，但是他们可以运行同样的程序，理想条件下（虽然实际工程中常常不理想）我们也可以期望他们跑出同样的结果。\n\n这说明所谓计算机科学，并不等同于研究计算机元件的电子科学和工程，这里电科和电子工程相当于洞穴岩壁上的影子，而计算机科学就相当于火光前的物体。这种超越物理的计算本质，一般用一种叫做“抽象语法树”的数据结构来表示。\n\n### 生物学·从神经元到神经网络\n\n人们发明计算机的时候，基本上还是把它当作工具，就没期望它有什么主体性和智慧。\n\n而随着生物学逐渐发现了神经系统及其作用，也随着物理学在二十世纪初的大发展之后的相对平静，很多物理学家开始插手其他学科。既然生命和非生命体的背后都服从同一套物理规律，既然物理学的众多成功经验说明，搞清楚构成系统的所有微观组成就可以理解宏观的系统，那么搞清楚人类的智力器官的基本单元以及相互作用，按理说也就能够理解什么是智慧。\n\n![a cartoon illustrating a neuron](/photos/2023-06-17-neuron.png)\n\n上图是一个神经细胞的结构示意图。从其他神经细胞释放出来的名为神经递质的化学物质，到达神经元左侧短且密集的树突之后，激活细胞膜表面的离子泵，主动运输离子跨过细胞膜，从而产生电信号。电信号沿细胞膜传导到右侧的树突，刺激凸触释放神经递质给下一个细胞。\n\n![a handdrawing style illustration of perceptron](/photos/2023-06-17-perceptron.png)\n\n上图就是根据神经元的工作原理抽象出的数学模型，名为 perceptron。一个 perceptron 就是一个函数，接受多个输入的自变量，加权求和之后套一个非线性的激活函数，得到一个输出。很多个这样的 perceptron 并连和串联，就构成下图，计算机算法中的神经网络。\n\n![a handdrawing style illustration of a neural network](/photos/2023-06-17-neural-network.png)\n\n而从实验方向研究神经系统，我们隔壁系就有，经常来我们系招人。基本上就是在小鼠的天灵盖上锯开一个天窗，然后给它带上个头盔，头盔上有能从天窗伸进去的电极，采集脑神经的电信号。以前头盔有网线伸到实验室天花板，实时传到数据中心的超算。现在好像进步了，改用 Wi-Fi 了。\n\n这实验怎么通过的伦理审查，咱也不知道，咱也不敢问……\n\n### 管理学·DIKW “数据-信息-知识-智慧”模型\n\n![a pyramid of DIKW model](/photos/2023-06-17-DIKW.png)\n\nDIKW 四个字母分别代表 data, information, knowledge, wisdom，即数据、信息、知识、智慧，是一种知识管理中的心智模型。\n\n四个层次，前一层都是后一层的基础，后一层都是对前一层的理解。\n\n如果是书面文字，数据就是笔画和字母；如果是语言，数据就是人声的响度、频率和音色。由笔画/字母/声音组成的有含义的字词就是信息。表示信息之间的关系的，可以判断对错的命题就是知识。包含和统摄各条知识的思想体系，就是智慧。\n\n反过来说，虽然智慧高于思想，但它仍需要通过把各条知识的表达汇总起来，才能被人感知。对知识的命题的理解依赖于构成名字的各个概念的涵义，属于信息水平的内容。而每个字都有不考虑其涵义的笔画字母构成。\n\n这层与层之间**看似**并没有插入额外的内容，智慧可以直接由笔画构成。但是我们一层层理解的深入，其实是不自觉地借用了我们当前社会约定俗成的解读方式。\n\n比如下面这个图片里的符号，对于现代人就只是数据，无法解读成信息。但是对于苏美尔人，这是用楔形文字表示的数字，是等腰直角三角形的腰和直角边的比值，也就是 $$\\sqrt{2}$$ 的近似值。\n\n![sumerian numerical approximation to square root of two](/photos/2023-06-17-ancient-root-2.png)\n\n约定俗成的数据解读方式，也就是关于**数据的数据**，根据西方的构词法，可以叫做“**元**数据”(meta-data)。\n\n数据和元数据一起构成信息，信息和元信息一起构成知识，知识和元知识一起构成智慧。俺坚持写博客的动机，就是用费曼学习法，把无意间使用的元知识显式地表达出来，而且记录下来，争取学而不退转。\n\n## 2\n\n回顾了这些，再来看大语言模型，就会发现它落在了各方努力的延长线的交点。\n\n大语言模型里有一个重要概念叫做“嵌入”(embedding)，就是把语言的基本字元 (token) 可逆地映射到一个超多维度的向量空间里。本来“国王”和“儿子”之间没办法加减乘除，但是嵌入后的向量空间里有加法和数乘，如果嵌入函数选得好，“国王”的向量 + “儿子”的向量，结果向量就约等于“王子”的向量。\n\n![illustration of vector addition from wikipedia](/photos/2023-06-17-vector-addition.png)\n\n生成式语言模型的核心就是一个超多元函数，接受前一个字嵌入后的向量作为输入，给出另一个向量作为输出，用嵌入函数的逆映射翻译成字元；再把旧的输出作为新的输入，直到输出结果是“语段结束”这样一个特殊字元为止。模型训练的过程，主要就是通过现成的语料，拟合这个超多元函数的参数。\n\n从 DIKW 模型来看，语言模型操作的是最基本的数据，它的输出究竟是什么信息，是不是正确的知识，体现了多少智慧，是人根据当下的社会文化来解读的。\n\n而实现 AI 的电子计算机，或是复杂生命的大脑，他们和智能之间的关系，应该就类似于具体的计算机电路和抽象语法树之间的关系。以此类比，未来的智能科学应该会成为一门独立的专业，它和计算机科学和神经生物学的区别，就像今天的电子科学与工程，和计算机科学之间的区别一样。当下神经生物学的热度，将来恐怕多半会被分流。\n\n这种对字符的计算不同于逻辑运算，语言模型不判断输出结果在逻辑上的正确与错误，这既给了他啥都能说几句的 feature，又给了它经常编假消息的 bug。\n\n想要改掉这种错误，引入对 AI 的纠错机制，治本之道恐怕还是诉诸于对世界的正确描述，与理论相关的还是要靠逻辑，与现实相关的还是要靠科学。\n\n只不过，大语言模型提供了一种数据结构，有希望把人类已知的真理储存在一起。对这种数据结构本身的研究，有可能反过来启发科学的发展。柏拉图的洞穴之壁可能不再是一个比喻，未来更大的语言模型的，亿万维度的参数空间有希望成为洞穴门口的那团火。\n\n只不过这一切都是“可能”，现在还只是 AI 的萌芽阶段，还没有足够的证据来证实或者证伪这种畅想。而且 AI 的参数量再大也是有限的，它所能表达的信息也就有限，而真理应当是无限的，就像科学一样，总要训练更新更大的模型，总要发现已知的未知，然后欣然接受更多未知的未知之存在。\n\n如果电子计算机实现的 AI 独立于人类产生了意识和超出人类的智慧，很难想象他们会继续用人类语言这种对他们来说很不方便的方式来交流。\n\n所以，哪怕是做个 AI 生成内容的质检员，科学家依然有事可做。这算是科学的堕落吗？当然不算，如果算的话，那从计算物理也被当作理论物理的那天起，人类就已经投降了（逃）\n\n## 3\n\n现在正面来回答问题：AI透過程式組合出回答你問題的文字組合，有「涵義」嗎？\n\n答：有。\n\n因为语言的「涵義」来自于语言的内容，和整个社会的文化，并不来自于这句话的作者的身份。即便是人与人之间的交流，诉诸身份也是一种非形式逻辑谬误，是理性不足的表现。只有在信息不足仍不得不下结论的时候才该使用，比如法律判决时的自由心证主义和/或法定证据主义。\n\n而鹿妈眼里真人鹿酱与 AI 鹿酱的区别，如果有的话，好像主要体现在动机的区别。动机这种东西，很多智慧不高的生物，比如小猫小狗都会有；而现在的 AI，似乎还没有展现出超出编程者设计的动机。编程写入的信息有限，现有 AI 的动机也就有限，鹿酱的赢面还是很大的。\n\n而动机是生物与非生物的区别吗？而什么是生物 ≠ 生物是什么，那就是另一个含混而复杂的问题了。\n\n## 4\n\n这篇博文发布的时候，高考应该已经结束了，马上该填报志愿了。\n\n那么，西元 2023 年，AI 来袭的当下，该选个啥专业在 AI 浪潮中幸存，或者选个啥专业给 AI 老爷带路呢？\n\n![a screenshot of a quotation from Three Body about attitudes towards aliens](/photos/2023-06-17-three-body-quotation.png)\n\n我的建议是，不要听别人的建议，按自己的兴趣来就好了。\n\n刚刚改开的时候，有一个超级热门的专业，叫科技英语。科技落下了好多年，对外开放需要语言交流，两者一结合应该是热门又稀缺了。结果呢，你现在还听说过这个专业吗？\n\n科技很重要是不错，语言很重要也不错，但是搞科技的人自己可以学英语，学英语的有几个搞得了科技？社会的进步主要靠创新，而创新的方向难以预测，不论这种预测分析听起来多有道理。\n\n如果真的找不到兴趣，那就在能力范围之内，找个难度最高的。如果想从事智力劳动，那数学含量是个不错的衡量标准；如果不排斥体力劳动，那训练时间越长越值得考虑。\n\n但这只是填志愿来不及时的权宜之计，发掘兴趣是人一生的课题。\n\n兴趣不是为了让你成功的时候更得意，毕竟成功的话不论做什么都很得意；\n\n兴趣是为了你不成功时也可以不失意，毕竟平凡才是人生的真谛。\n"},{"slug":"physics-based-neural-network-review-note","filename":"2023-03-20-physics-based-neural-network-review-note.md","date":"2023-03-20","title":".tex | 基于物理的神经网络 (PINN) 综述笔记","layout":"post","keywords":["tex","phy","md","ai"],"hasMath":true,"excerpt":"本文是《Scientific Machine Learning Through Physics–Informed Neural Networks: Where we are and What’s Next》这篇综述的读书笔记。","content":"\n\u003e 本文是《[Scientific Machine Learning Through Physics–Informed Neural Networks: Where we are and What’s Next](https://link.springer.com/article/10.1007/s10915-022-01939-z)》这篇综述的读书笔记。\n\u003e \n\n年前，今年新入职的天文学方面的一位老师给我们群发邮件，宣传某国家实验室超算的 GPU 编程马拉松活动，他可以担任指导老师。于是毫不意外地，我报了名。该编程马拉松项目还需要专门申请，申请材料里要写清楚打算干什么，于是报名的五六个人七嘴八舌地想创意。基于物理的神经网络 PINN 就是天文老师的点子。\n\n~~写到这里，我才意识到，老哥是不是想拿我们当免费劳动力啊~~~\n\n神经网络可以看作是一个复杂的非线性函数，接受一个（一般来说维度很高的）向量作为输入，一番计算后输出另一个向量。训练神经网络，就是找到这个函数的参数，绝大多数找参数的方法涉及计算网络输出对参数的偏导数，因此神经网络计算框架的核心功能就是自动微分 (auto-differentiation)。\n\n而很多物理问题，都可以用（偏）微分方程来描述，微分方程的解不是变量，而是函数，而且往往是复杂的非线性函数。所以基于物理的神经网络 (PINN) 就是以神经网络来表达这个函数，然后把这个函数带入到物理的微分方程中，把神经网络输出和真正的物理解之间的差距当作损失函数，反向传播回去来优化神经网络的参数。代入方程时的微分计算，正好可以利用现成框架的自动微分功能。\n\n在以 GPT 为代表的 transformer 类神经网络模型出现之前，自然语言处理类的机器学习项目，往往要在网络之外，利用人类的语法知识，对语段进行语义分割等等“中间任务”。Transformer 一出，算力出奇迹，中间任务逐渐变得没有必要了。\n\n在 GPT 崭露头角，并且越来越有迹象表明其将会涌现出通用人工智能的今天，这些基于物理的神经网络，会不会还未成熟就已过时？这种心情，就和《三体》第二卷开始，章北海和吴岳面对焊渍未漆的“唐”号航空母舰时差不多吧……\n\n\u003chr class=\"slender\"\u003e\n\n- Abstract\n    - PINNs are neural networks that encode model equations. a NN must fit observed data while reducing a PDE residual.\n\n1. Introduction\n    - The “curse of dimensionality” was first described by Bellman in the context of optimal control problems. (Bellman R.: Dynamic Programming. Sci. 153(3731), 34-37 (1966))\n    - Early work: MLP ([multilayer perceptron](https://en.wikipedia.org/wiki/Multilayer_perceptron)) with few hidden layers to solve PDEs. ([https://doi.org/10.1109/72.712178](https://doi.org/10.1109/72.712178))\n    - 感觉可能更全面的一篇综述：[https://doi.org/10.1007/s12206-021-0342-5](https://doi.org/10.1007/s12206-021-0342-5)。该文关注 what deep NN is used, how physical knowledge is represented, how physical information is integrated，本文只关于 PINN, a 2017 framework。\n\n    1. What the PINNs are\n        - PINNs solve problems involving PDEs:\n            - approximates PDE solutions by training a NN to minimize a loss function\n            - includes terms reflecting the initial and boundary conditions\n            - and PDE residual at selected points in the domain (called **collocation points**)\n            - given an input point in the integration domain, returns an estimated solution at that point.\n            - incorporates a [residual network](https://en.wikipedia.org/wiki/Residual_neural_network) that encodes the governing physical equations\n            - can be thought of as an **unsupervised strategy** when they are trained solely with physical equations in forward problems, but **supervised learning** when some properties are derived from data\n        - Advantages:\n            - [mesh-free](https://en.wikipedia.org/wiki/Meshfree_methods)? 但是我们给模型喂训练数据的时候往往已经暗含了 mesh 了吧\n            - on-demand computation after training\n            - forward and inverse problem using the same optimization, with minimal modification\n    2. What this Review is About\n        - 提到了一个做综述找文章的方法：本文涉及的文章可以在 Scopus 上进行高级搜索：`((physic* OR physical)) W/2 (informed OR constrained) W/2 “neural network”)`\n2. The Building Blocks of a PINN\n    - question:\n    \n    $$\n    F(u(z);\\gamma)=f(z),\\quad z\\ \\in\\ \\Omega \\\\ B(u(z))=g(z), \\quad z\\ \\in\\ \\partial \\Omega\n    $$\n    \n    - solution:\n    \n    $$\n    \\hat u_{\\theta}(z)\\approx u(z)\\\\ \\theta^* = \\arg\\min_{\\theta}\\left(\\omega_F L_F(\\theta)+\\omega_BL_B(\\theta)+\\omega_{data}L_{data}(\\theta)\\right)\n    $$\n    \n    1. Neural Network Architecture\n        - DNN (deep neural network) is an artificial neural network that is deeper than 2 layers.\n        \n        1. Feed-Forward Neural Network: \n            - $$u_{\\theta}(x) = C_{K} \\circ C_{k-1} ...\\alpha \\circ C_1(x),\\quad C_k(x) = W_k x_k + b_k$$\n            - Just change CNN from convolution to fully connected.\n            - Also known as multi-layer perceptrons (MLP)\n            \n            1. FFNN architectures \n                - Tartakovsky et al used 3 hidden layers, 50 units per layer,  and a hyperbolic tangent activation function. Other people use different numbers but of the same order of magnitude.\n                - A comparison paper: *Blechschmidt, J., Ernst, O.G.: Three ways to solve partial differential equations with neural networks –A review. GAMM-Mitteilungen 44(2), e202100,006 (2021).*\n            2. multiple FFNN: 2 phase [Stephan problem](https://en.wikipedia.org/wiki/Stefan_problem).\n            3. shallow networks: for training costs\n            4. activation function: the swish function in the paper has a learnable parameter, so — [how to add a learnable parameter in PyTorch](https://discuss.pytorch.org/t/how-could-i-create-a-module-with-learnable-parameters/28115)\n        2. Convolutional Neural Networks: \n            - I am most familiar with this one.\n            - $$f_i(x_i;W_i)=\\Phi_i(\\alpha_i(C_i(W_i,x_i)))$$\n            - performs well with multidimensional data such as images and speeches\n            \n            1. CNN architectures: \n                - `PhyGeoNet`: a physics-informed geometry-adaptive convolutional neural network. It uses a coordinate transformation to convert solution fields from irregular physical domains to rectangular reference domains.\n                - According to Fang ([https://doi.org/10.1109/TNNLS.2021.3070878](https://doi.org/10.1109/TNNLS.2021.3070878)), a Laplacian operator can be discretized using the finite volume approach, and the procedures are equivalent to convolution. Padding data can serve as boundary conditions.\n            2. convolutional encoder-decoder network\n        3. Recurrent Neural Network\n            - $$f_i(h_{i-1})=\\alpha\\left(W\\cdot h_{i-1}+U\\cdot x_i+b\\right)$$, where f is the layer-wise function, x is the input, h is the hidden vector state, W is a hidden-to-hidden weight matrix, U is an input-to-hidden matrix and b is a bias vector. 我认为等号左边的 $$h_{i-1}$$ 应当作为下标\n            - 感觉有点像 hidden Markov model，只不过 Markov 中间的 hidden layers 好像与序号无关（记不清了），~~RNN 看起来各个 W 和 H 似乎不同~~。**RNN cell is actually the exact same one and reused throughout.** (from [https://blog.floydhub.com/a-beginners-guide-on-recurrent-neural-networks-with-pytorch/](https://blog.floydhub.com/a-beginners-guide-on-recurrent-neural-networks-with-pytorch/)). Cartoon from Wikipedia:\n                \n                ![Untitled]({{ site.baseurl }}/assets/photos/2023-03-20-rnn-unit.png)\n                \n            - From [https://blog.floydhub.com/a-beginners-guide-on-recurrent-neural-networks-with-pytorch/](https://blog.floydhub.com/a-beginners-guide-on-recurrent-neural-networks-with-pytorch/):\n                \n                ![Untitled]({{ site.baseurl }}/assets/photos/2023-03-20-rnn-types.png)\n                \n            1. RNN architectures\n                - can be used to perform numerical Euler integration\n                - 基本上输出的第 i 项只与输入的第 i 和 i-1 项相关。\n            2. LSTM architectures\n                - 比 RNN 多更多中间隐变量，至于怎么做到整合长期记忆的，技术细节现在可以先略过\n        4. other architectures for PINN\n            1. Bayesian neural network: weights are distributions rather than deterministic values, and these distributions are learned using Bayesian inference. 只介绍了[一篇文章](https://doi.org/10.1016/j.jcp.2020.109913)\n            2. GAN architectures: \n                - two neural networks compete in a zero-sum game to deceive each other\n                - physics-informed GAN uses automatic differentiation to embed the governing physical laws in stochastic differential equations. The discriminator in PI–GAN is represented by a basic FFNN, while the generators are a combination of FFNNs and a NN induced by the SDE\n            3. multiple PINNs\n    2. Injection of Physical Laws\n        - 既然是要解常/偏微分方程，那么微分计算必不可少。四种方法：hand-coded, symbolic, numerical, auto-differentiation，最后一种显著胜出。所谓 auto-differentiation, 就是利用现成框架，框架自动给出原函数的导数的算法。\n        - Differential equation residual:\n            - $$r_F[\\hat u_\\theta](z)=r_\\theta(z):=F(\\hat u_\\theta(z);\\gamma)-f$$\n            - $$r_F[\\hat u_\\theta](z)=r_\\theta(x,t)=\\frac{\\partial}{\\partial t}\\hat u_\\theta(x,t)+F_x(\\hat u_\\theta(x,t))$$: 原文给出了来源，但是从字面上看不出来与前式的等价性\n        - Boundary condition residual: $$r_B[\\hat u_\\theta](z):=B(\\hat u_\\theta(z))-g(z)$$\n    3. Model Estimation by Learning Approaches\n        1. Observations about the Loss\n            - $$\\omega_F$$ accounts for the fidelity of the PDE model. Setting it to 0 trains the network without knowledge of underlying physics.\n            - In general, the number of $$\\theta$$ is more than the measurements, so regularization is needed.\n            - The number and position of residual points matter a lot.\n        2. Soft and Hard Constraints\n            - Soft: penalty terms. Bad:\n                - satisfying BC is not guaranteed\n                - assignment of the weight of BC affects learning efficiency, no theory for this.\n            - Hard: encoded into the network design. [Zhu et. al](https://doi.org/10.1007/s00466-020-01952-9)\n        3. Optimization methods\n            - minibatch sampling using the Adam algorithm\n            - increased sample size with L-BFGS (limited-memory Broyden-Fletcher-Goldfarb-Shanno)\n    4. Learning theory of PINN: roughly in DE, consistency + stability → convergence\n        1. convergence aspects: related to the number of parameters in NN\n        2. statistical learning error analysis: use *risk* to define *error*\n            - Empirical risk: $$\\hat R[u_\\theta]:=\\frac{1}{N}\\sum_{i=1}^N \\left\\|\\hat u_{\\theta}(z_i)-h_i\\right\\|^2$$\n            - Risk of using approximator: $$R[\\hat u_{\\theta}]:=\\int_{\\bar \\Omega}(\\hat u_{\\theta}(z)-u(z))^2dz$$\n            - Optimization error: the difference between the local and global minimum, is still an open question for PINN. $$E_O:=\\hat R[\\hat u_{\\theta}^*]-\\inf_{\\theta \\in \\Theta}\\hat R[u_\\theta]$$\n            - Generalization error: error when applied to unseen data. $$E_G:=\\sup_{\\theta \\in \\Theta}\\left\\|R[u_\\theta]-\\hat R[u_\\theta]\\right\\|$$\n            - Approximation error: $$E_A:=\\inf_{\\theta \\in \\Theta}R[u_\\theta]$$\n            - Global error between trained deep NN $$u^*_\\theta$$ and the correct solution is bounded: $$R[u^*_\\theta]\\le E_O+2E_G+E_A$$\n            - 有点乱，本来说 error 是误差，结果最后还是用 risk 作为误差\n        3. error analysis results for PINN\n3. Differential Problems Dealt with PINNs：读来感觉这一部分意义不大，将来遇到需要解决的问题时，回来看看之前有没有人做过就行了——另一方面看，一类方程就需要一类特殊构造的神经网络来解，那么说明神经网络解方程的通用性并不好~\n    1. Ordinary differential equations: \n        - Neural ODE as learners, a continuous representation of **ResNet**. [[Lai et al](https://doi.org/10.1016/j.jsv.2021.116196)], into 2 parts: a physics-informed term and an unknown discrepancy\n        - LSTM [[Zhang et al](https://doi.org/10.1016/j.cma.2020.113226)]\n        - [Directed graph models](https://doi.org/10.1016/j.compstruc.2020.106458) to implement ODE, and Euler RNN for numerical integration\n        - Symplectic Taylor neural networks in [Tong et al](https://doi.org/10.1016/j.jcp.2021.110325) use symplectic integrators\n    2. Partial differential equations: steady/unsteady的区别就是是否含时\n        1. steady-state PDEs\n        2. unsteady PDEs\n            1. Advection-diffusion-reaction problems\n                1. diffusion problems\n                2. advection problems\n            2. Flow problems\n                1. Navier-Stokes equations\n                2. hyperbolic equations\n            3. quantum problems\n    3. Other problems\n        1. Differential equations of fractional order\n            - automatic differentiation not applicable to fractional order → [L1 scheme](https://doi.org/10.1515/fca-2019-0086)\n            - [numerical discretization for fractional operators](https://doi.org/10.1137/18M1229845)\n            - [separate network to represent each fractional order](https://doi.org/10.1038/s43588-021-00158-0)\n        2. Uncertainty Estimation: [Bayesian](https://doi.org/10.1016/j.jcp.2020.109913)\n    4.  Solving a Differential Problem with PINN\n        - 1d non-linear Schrödinger equation\n        - dataset by simulation with MATLAB-based Chebfun open-source(?) software\n4. PINNs: Data, Applications, and Software\n    1. Data\n    2. Applications\n        1. Hemodynamics\n        2. Flows Problems\n        3. Optics and Electromagnetic Applications\n        4. Molecular Dynamics and Materials-Related Applications\n        5. Geoscience and Elastiostatic Problems\n        6. Industrial Application\n    3. Software\n        1. `DeepXDE`: initial library by one of the vanilla PINN authors\n        2. `NeuroDiffEq`: PyTorch based used at Harvard IACS\n        3. `Modulus`: previously known as Nvidia SimNet\n        4. `SciANN`: implementation of PINN as Keras wrapper\n        5. `PyDENs`: heat and wave equations\n        6. `NeuralPDE.jl`: part of SciML\n        7. `ADCME`: extending TensorFlow\n        8. `Nangs`: stopped updates, but faster than PyDENs\n        9. `TensorDiffEq`: TensorFlow for multi-worker distributed computing\n        10. `IDRLnet`: a python toolbox inspired by Nvidia SimNet\n        11. `Elvet`: coupled ODEs or PDEs, and variational problems about the minimization of a functional\n        12. Other Packages\n5. PINN Future Challenges and Directions\n    1. Overcoming Theoretical Difficulties in PINN\n    2. Improving Implementation Aspects in PINN\n    3. PINN in the SciML Framework\n    4. PINN in the AI Framework\n6. Conclusion\n"},{"slug":"logical-science-from-west","filename":"2022-08-22-logical-science-from-west.md","date":"2022-08-22","title":".doc | 也谈近代科学从西方起步","layout":"post","keywords":["doc","tex","phy","m","phi"],"excerpt":"为什么近代科学偏偏是在丢过一次古典传统的西方起步的呢？为什么那些成功继承了古典时代智慧的中古文明，比如伊斯兰文明或古中国文明，反而没有成功萌发近代科学思想呢？","content":"\n前不久在公众号转载过“海边的西塞罗”写的《**嗯！您关注的是一个早晚要“凉凉”的公众号**》，标题起得让人不知所云，但是文章内容讨论的是“近代科学为什么从西方起步”的问题，原文说：\n\n\u003e 既然你所讲述的，欧洲从古典时代到文艺复兴、科学曾经出现过一次“断层”，欧洲人是通过翻译阿拉伯人转译的古典时代文献才继承了希腊罗马先贤们的思想的。\n\u003e \n\u003e \n\u003e 那么**，为什么近代科学偏偏是在丢过一次古典传统的西方起步的呢？为什么那些成功继承了古典时代智慧的中古文明，比如伊斯兰文明或古中国文明，反而没有成功萌发近代科学思想呢？**\n\u003e \n\n作者立了一个靶子——\n\n\u003e 我之前听到的比较靠谱的解答，**是古希腊罗马有较好的数学思想，当定量的数学思想与定性的“自然哲学”发生结合，近代科学就诞生了。**\n但这种解释，其实也回答不了一个问题——你可以说古代东方离着希腊远，没有受到希腊某些思想的“药引”的启发。但特别奇怪的是，中世纪的中东却不是这样。\n伊斯兰文明的伍麦叶王朝在公元九世纪曾经掀起过一场声势浩大的“百年翻译运动”，……近代启发西方的那些古典思想典籍，阿拉伯人全有，且早获得了好几百年。\n\u003e \n\n给出的回答是所谓**“托勒密困境”**，即诸文明中的科学技术研究者因为要满足当权者/赞助者的功利性需要，将时间与精力耗费于附会科学（比如天文学）的非科学甚至伪科学（比如占星术）之上，而——\n\n\u003e 这种错误的职业拼接，锁死了天文学的进一步发展的通路，导致其无法实现向近代科学的飞跃——即便托勒密会数学、引入定量计算，也依然没用。\n\u003e \n\u003e **而这种“托勒密困境”，其实也是所有古典时代学者的困境——他们在研究学问时，必须回答“求用”的问题。**\n\u003e \n\u003e ……\n\u003e \n\u003e **于是从托勒密到哥白尼，我们会发现西方在这一轮对天文学的失而复得中，其实并没有增添什么，而是丢掉了一种东西——那就是“求用”的思维。**\n\u003e 欧洲知识分子们研究科学的正义性，来自于他们认定：自然作为一种上帝的造物，其本身就是美的。因此研究它、探索它本身，就是在赞美上帝，所以科学研究不必“求用”也有天然的正义性。\n\u003e \n\n作者写近代西方科学的不求用，是为了托物言志，检讨自己为了读者的关注不得不在历史写作之外“写时评、表达观点、带情绪”，预告自己将来可能会去写作崇高的钻研历史的题目。\n\n给蹭热点找理由这件事，我也做过嘛，感觉写的比这篇文章还简约隽永且立意高远呢～（文人相轻.jpg）\n\n但是科学革命发源于西方这个问题，我也很感兴趣，而且有自己的思考，而且思考的结果和上文不同。\n\n\u003chr class=\"slender\"\u003e\n\n学物理的孩子应该都听说过《费曼物理学讲义》的大名，没听说过的话建议听说一下，自主招生考试面试装逼的时候用的上。费曼先生在引言中也立了个靶子说——\n\n\u003e 你们可能会问，在讲述欧几里德几何时，先是陈述公理，然后作出各种各样的推论，那为什么在讲授物理学的时候不能先直截了当地列出基本规律，然后再就一切可能的情况说明定律的应用呢？\n\u003e \n\n然后上来就讲原子论，开篇问：\n\n\u003e 假如由于某种大灾难，所有的科学和知识都丢失了，只有一句话可传给下一代，那么怎样才能用最少的词汇来传达最多的信息呢？\n\u003e \n\n可惜这个问题仅仅是为了引出原子论，实在是大材小用。这说明费老先生浸淫于西方科学中，“不识庐山真面目，只缘身在此山中”。而我对近代科学起自西方的解释，正好就是这两句话串起来。下面就要兜一个大圈子，把两句话圆起来。\n\n\u003chr class=\"slender\"\u003e\n\n科学者，对世界之正确认知也。\n\n根据这个定义，把人们已知的，关于这个世界的所有知识罗列到一个集合里，这个集合就是科学。我们只谈到了一个集合，不涉及逻辑推演，也不涉及数学带来的定量优势，更不判断从事科学研究的人是否功利。\n\n但是，集合这种知识结构过于简单——\n\n- 集合里的各个元素都是平等的，要想表示出整个集合，除了全默写出来没别的办法；\n- 集合里的元素之间没有顺序，想取得其中的某一条科学命题，只能像抓阄一样，凭运气抽到为止。\n- 一旦由于天灾人祸，集合中的部分内容丢失，除了重新把当初发现它们时经历的艰难困苦重复一遍，也没有别的办法。（哦，也可以去隔壁文明的图书馆翻译。）\n\n所以，必须找到一种更复杂的结构，来组织这些信息，解决上述问题。\n\n\u003chr class=\"slender\"\u003e\n\n计算机专业有门基础课《数据结构与算法》，谈数据结构，最基础的两种就是数组和链表；谈算法，最基础的概念就是函数。注意，这里说的是数据结构，刚才说的是知识结构，两者可以类比，但并非同一概念。\n\n数组，和集合几乎一样，只不过给每个元素标记了一个序号。在计算机里，由于规定数组连续存放，每个元素占用内存长度相等，所以可以通过序号，从数组开头偏置指针，以 O(1) 的时间复杂度取得任意元素，快。\n\n类比到知识结构，语数外理化政史地生，一年级二年级三年级，第一章第二章第三章，第一第二第三个知识点，背吧。列表与列表之间井水不犯河水，你数学老师说你体育老师拉稀了不能上课，你体育老师说你数学老师放屁，两者完全可以在你的知识体系里共存。\n\n链表，和数组一样有顺序，但是并不给每个元素标号，而是在前一个元素的末尾，写上下一个元素的位置指针。找到一个元素需要从链表的开头一个一个往后捋，慢。好处是修改方便，在链表中间塞进去一个新元素，只需要把前面一个的指针指向新元素，新元素的指针指向后一个元素，删除一个旧元素也类似，只对增删点附近一个很小的区域进行改动，整个链表不会伤筋动骨。\n\n但是不论数组还是链表，都需要把所有的知识全写出来，随着时间的积累，科学的总量早晚要超越人脑的记忆力，超越笔记的厚度，对于个人，要皓首穷经，要韦编三绝，才有希望提出一点新内容；对于全人类，图书馆越造越大，一轮战乱，从头再来。\n\n于是函数登场。给定一个/一组输入，根据函数体描述的算法，返回确定的输出。那我们找到一种方法，写一个函数，接收链表的前一个元素作为输入，找到后一个元素输出。这样我们只需要存储第一个元素和这个函数，就可以恢复出整个链表，用计算换空间。\n\n\u003chr class=\"slender\"\u003e\n\n类比到知识结构，这个函数就是逻辑推演。\n\n科学内容中的每一条知识都是一个**命题**。\n\n从少数几条知识出发，这几条在逻辑上就称为**公理**，自然科学里也称之为**定律**。\n\n命题之间可以做**逻辑运算**，**或**、**且**、**非**、**蕴含**等等，运算的结果也是一条新的命题。命题的正确与否，取决于逻辑运算的规定。\n\n通过对公理和已经算出的真命题反复进行逻辑运算，产生的新的真命题，叫做**定理**。\n\n\u003chr class=\"slender\"\u003e\n\n欧几里德几何式的，也就是从有限多个命题出发，承认逻辑推演进行生成的新命题的正确性，这样的一种组织方式——\n\n- 对于学习，科学不再是一家之言，门户之见。一句话的正确性不再由说话者的身份决定，诉诸人身、诉诸权威成了谬误，“我爱吾师，但我更爱真理”一句话有了切实的落脚点。\n- 对于研究，降低了难度，后来者不必从头再来，而是站在前人的终点起跑。发现的新科学有办法整合进现有的科学，证伪的旧科学有办法剔除，而不会让科学整体伤筋动骨。愚弄黔首的矛盾和谬误，真理有办法与之势不两立。\n- 自带有容灾能力，科学得以在摧毁科学记录和科学家人身的重大灾难之后，在几百年的人才断档之后，依然有办法恢复。\n\n\u003chr class=\"slender\"\u003e\n\n刚才说数据结构和知识结构不同，知识管理界有个 DIKW 模型，也就是数据 (Data)、信息 (Information)、知识 (Knowledge)、智慧 (Wisdom)。\n\n纸张上的墨迹组成的字符只是数据，当这些单词按照语法理解为句段篇章之后才构成信息，这些篇章内容指代的概念、关系等等含义构成知识。如何理解知识与知识之间的关系需要智慧。\n\n“继承了古典时代智慧的中古文明，比如伊斯兰文明或古中国文明”——从各个文明没能演化出科学革命来看，**这些文明最多是有一部分学者继承了古典时代的知识，而没能认识到 *用逻辑组织知识* 这一智慧的价值**，而西方发掘出了这种智慧。至于这种发掘发生在西方，是偶然还是必然，由哪些条件促成，那是另一个很有趣的问题了。\n\n“我们会发现西方在这一轮对天文学的失而复得中，其实并没有增添什么，而是丢掉了一种东西——那就是‘求用’的思维。”——西方对天文学的失去，对应的是罗马统治下的和平结束时的战乱与社会崩溃，不论之后的文艺复兴如何光辉灿烂，**这种失落都是对科学乃至整个文明的威胁**，如果没有这种失落，科学革命想必会更早更容易发生。况且这种失落到复兴的整个过程中，对科学有影响的因素实在是太多了，既有正面又有负面，实在是难以分析归因。\n\n至于不求用的思维，有了逻辑推演，科学工作者的产出提高，高到了让社会愿意供养其全职研究的地步，那么不求用的思维，自然会建立起来；不求用对科研效率的提升，良性反哺科学的发展，自然会蔚然成风。反过来，**只有不求用的态度，研究者没有逻辑推演发展科学的能力，资助者没有逻辑推演评价成果的本事，不求用的态度只会鼓励灌水，产出真没用的水货。**\n\n\u003chr class=\"slender\"\u003e\n\n数学对自然科学的作用，定量化只是一个副产品。更重要的是作为逻辑科学的集大成者，发明/发现逻辑推演的规则，探索逻辑推演作为方法论的能力边界。一言以蔽之，欧几里德之后，数学已不只是“数字的学问”。\n\n至于费曼先生，他怎么可能不知道四大力学确实就是按照欧几里德式的，从基本定律出发的方式讲授的呢？面对一伙学普通物理的本科新生，说这种话实在有点骗小孩儿的嫌疑，怪不得那门课上到后来，本科生全都跑了。物理和数学的区别，在于理论和实验两条腿走路，但是理论的这条腿，实实在在地来自于超越了“数字的学问”的数学。\n"},{"slug":"essential-cell-biology-index","filename":"2022-04-29-essential-cell-biology-index.md","date":"2022-04-29","title":".pdf | 《细胞生物学精要 Essential Cell Biology》读书笔记","layout":"post","keywords":["pdf","tex","bio"],"excerpt":"捋一遍目录真的是读一次读不完的书很有效率的做法。","hasMath":true,"content":"\n\n\u003e 2024年1月29日更新：读“我们如何得知”第 3, 13, 15, 16, 17 节, “图版”第 3-1 节\n\n自从在个人博客上开启了[那本书](%7B%%20post_url%202022-03-05-great-cultural-revolution-ten-years-0%20%%7D)的读书笔记系列之后，发现捋一遍目录真的是读一次读不完的书很有效率的做法。\n\n上次发了一篇[《生物学知识提纲》](%7B%%20post_url%202022-02-25-barron-360-biology-contents%20%%7D)，其实我家里是有好几本大部头的本科程度的生物学教材的，这本《细胞生物学精要》就是其中之一，是爸妈来看我的时候用行李箱带过来的中译本。快要找工作了，如果决定更进一步地进入生物领域的话，是时候看点书了。\n\n这本书除了正常的章节之外，每一章都会对于一个科学命题，回答“我们如何得知这一命题”的问题，这一节插在正文中间，这次整理的时候单独摘了出来。另外部分章节末尾有图解部分概念的“图版”，也单独摘了出来。章节繁多，所以只展开了自己当下比较感兴趣的部分，并且将之标红加粗。\n\n## 篇章结构\n\n1. 介绍细胞\n2. 细胞的化学成分\n3. **能量、催化作用、生物合成**\n    1. 细胞中能量的应用\n    2. **自由能和催化作用**\n    3. 活化的载体分子与生物合成\n4. 蛋白质的结构和功能\n5. DNA 和染色体\n6. DNA 复制、修复和重组\n7. 从 DNA 到蛋白质：细胞如何阅读基因组\n8. **基因表达调控**\n    1. 基因表达概述\n    2. **转录是如何开启的**\n    3. 造成特异细胞类型的分子机制\n    4. **转录后调控**\n9. 基因和基因组如何进化\n10. 基因及基因组分析\n11. 膜的结构\n12. **膜转运**\n13. 细胞如何从食物中获得能量\n14. 线粒体和叶绿体中的能量生产\n15. 胞内区室及转运\n16. **细胞通讯**\n    1. 细胞信号传导的一般原理\n    2. G 蛋白偶联受体\n    3. 酶联受体\n17. 细胞骨架\n18. **细胞分裂周期概述**\n    1. 细胞分裂周期\n    2. **细胞周期控制系统**\n    3. S 期\n    4. M 期\n    5. 有丝分裂\n    6. 胞质分裂\n    7. 细胞数量和细胞大小的控制\n19. 性与遗传\n20. 细胞群落：组织、干细胞、癌\n\n## 我们如何得知\n\n1. 生命的共同机制\n2. 什么是大分子\n3. **使用动力学模拟和操作代谢途径**：基本都在 Machaelis-Menton 模型框架之内\n    - 一个酶对其催化反应的最大速率 $$V_{max}$$:\n        - 测量试管中不同底物浓度下的速率，找到其在底物浓度趋于无穷时的渐进行为。将浓度和速率都取倒数后作图，纵轴截距就是最大速率(!)\n        - 更快的反应（几微秒内），需要“停/流仪”([stopped flow](https://en.wikipedia.org/wiki/Stopped-flow))\n    - 调控：竞争性抑制剂不改变 $$V_{max}$$\n    - 设计：通过模拟，但是完全没提模拟的细节。可能需要看正文 10.3.3\n4. 探究蛋白质的结构\n5. 基因由 DNA 组成\n6. 复制的性质\n7. 破译遗传密码\n8. 基因调控——Eve 的故事\n9. 基因数目\n10. 人类基因组测序\n11. 测量膜流\n12. 乌贼为我们展示膜兴奋性的奥秘\n13. **发现柠檬酸循环**\n    - 当时的发现者 Krebs 等人：\n        - 肌肉切碎后的悬浮液中，特定的一群分子被快速氧化\n        - 这些分子形成两条通路：柠檬酸 → α-酮戊二酸 → 琥珀酸；琥珀酸 → 延胡索酸 → 苹果酸 → 草酰乙酸。（没有质谱技术，如何辨别这些小分子？）\n        - 小剂量的以上分子加入后可以导致大量氧气消耗\n        - 丙二酸实验\n            - 丙二酸可以特异性抑制琥珀酸脱氢酶的活性（如何知道？丙二酸结构上类似琥珀酸即丁二酸）\n            - 琥珀酸脱氢酶催化琥珀酸变为延胡索酸\n            - 将柠檬酸、异柠檬酸、α-酮戊二酸（之前通路的**上游**分子）加入含有丙二酸的肌肉细胞悬浮液后，琥珀酸会累积\n            - 将延胡索酸、苹果酸、草酰乙酸（之前通路的**下游**分子）加入含有丙二酸的肌肉细胞悬浮液后，琥珀酸**也**会累积\n        - 结论/假设：下游存在一个反应，使得最下游反应物变成最上游反应物\n            - 肌肉悬液和丙酮酸和草酰乙酸共培养时，形成了柠檬酸\n            - 假设：丙酮酸 + 草酰乙酸 → 柠檬酸或丙酮酸 + 草酰乙酸 → 柠檬酸\n            - 实际：乙酰 CoA 作为丙酮酸和草酰乙酸的中间体，十年之后才被发现\n    - 现有技术：放射性标记物 + 质谱法\n        - 放射性标记物\n        - 质谱法\n14. 化学渗透偶联如何驱动 ATP 合成\n15. **追踪蛋白质和囊泡运输**\n    - 定位到特定细胞器的蛋白含有特定“信号序列”，信号序列交换实验\n    1. in vitro, 将细胞器从细胞中分离出来，和携带信号序列的蛋白质置于同一试管中，放射性氨基酸标记和追踪；图15-29 (B) 没看懂汉语\n    2. 高温时分泌缺陷的酵母中发现了 25 种和胞吐有关的基因。25°C 转运正常，35°C 异常积累在内质网、高尔基体、囊泡中\n    3. 荧光蛋白视频\n16. **解析细胞信号通路**\n    1. 检测磷酸化：当信号通路接收到胞外信号分子时，多种蛋白被磷酸化\n        1. 破碎细胞、通过凝胶按大小分开、使用抗体检测磷酸化的蛋白\n        2. 在细胞暴露于信号分子时，提供放射性的 ATP；破碎细胞并通过凝胶；将凝胶在 X 光底片上曝光。\n    2. 鉴定相互作用蛋白：\n        1. 免疫共沉淀：用抗体抓住特定的蛋白，如果其正与其他蛋白结合，则被结合蛋白也会沉淀\n        2. DNA 重组技术：构建一系列突变蛋白，可以鉴定出用来结合的氨基酸位点。使用此种方法时，细胞内不能有对信号分子相应的正常受体（如何做到？）\n    3. 开关通路\n        1. 开：DNA 重组技术，编码不需外源信号也能持续激活的受体蛋白，例如人类癌症中的 Ras\n        2. 关：\n            1. DNA 重组技术，构建“显性失活”(dominant negative) 突变体\n            2. 小干扰 RNA (siRNA)，降解编码相应蛋白的 mRNA 或阻止 mRNA 翻译\n    4. 通路排序\n        1. 动物筛选：成千上万的实验动物用一种突变源处理，寻找突变所在的信号通路中哪一个没有正常工作，由此找到信号通路中编码蛋白的基因\n        2. 确定顺序：已于研究目标 X，选定一个基准蛋白 C，引入一种失活的 X 突变，再引入持续激活的 C，看通路是否仍能工作，能则 X 在 C 上游，反之则在下游。\n17. **寻找马达蛋白**\n    - 难点：在细胞外分离和研究相关蛋白质\n    - 方法：\n        - 显微镜技术发展，从特定种类的细胞中将胞内运输系统挤压出来。\n            - ~~光学显微镜时代：拥有巨大轴突的乌贼神经细胞，从轴突中挤出细胞质（轴浆），然后研究粒子跨细胞膜运动，轴浆被丢弃。~~\n            - 视频增强显微镜时代：空间分辨率 200nm，看到囊泡和被膜细胞器沿着细胞骨架移动，从 30~50nm 的颗粒到 5000nm 的线粒体。\n            - 这种运动需要 ATP，AMP-PNP 竞争性抑制细胞器转运\n        - 用纯化的缆绳、马达、货物从零开始装配一套正常运转的运输系统\n            - 抗体实验表明蛋白丝缆绳是 α-微管蛋白\n            - Ron Vale, Thomas Reese \u0026 Michael Sheetz 将纯化的缆绳（乌贼的视叶中纯化的微管）和货物（乌贼轴突中分离的细胞器）放在一起，寻找诱发运动的分子。乌贼轴突细胞质的提取物可以诱发运动\n            - AMP-PNP 虽抑制其运动，但仍能使组件结合。结合后提取微管，希望马达蛋白仍附着在微管上。加入 ATP 释放附着的蛋白，得到 110kDa 的多肽\n        - 细胞内观测：Steven Blcok et al, 1990\n            - 微小的硅珠包裹上低浓度的动力蛋白，能观察到硅珠沿微管行走\n            - 马达蛋白和荧光蛋白偶联，可以观察到单个动力蛋白分子\n            - 发现：每个分子从微管上掉落之前走大约 100 步，每步 8nm，约等于微管蛋白单体的长度。ATP 水解实验表明每步消耗一个 ATP\n            - 动力蛋白有两个头部，被认为以左右交替的方式前进。\n18. 细胞周期蛋白和 Cdk 的发现\n19. 利用 SNP 解开人类疾病的面纱\n20. 搞清楚那些对癌症有关键性影响的基因\n\n## 图版\n\n- 1-1. 显微镜\n- 1-2. 细胞结构\n- 2-1. 化学键和化学基团\n- 2-2. 水的化学性质\n- 2-3. 几种糖的类型概述\n- 2-4. 脂肪酸和其他脂类\n- 2-5. 蛋白质里的20种氨基酸\n- 2-6. 核苷酸概述\n- 2-7. 非共价键的主要类型\n- 3-1. **自由能与生物学反应**：写得不好，太多不加解释的事实陈述\n    - 反应的自由能变化和平衡常数之间存在函数关系，推导在 3.2.6 节\n    - 生物系统中，吉布斯自由焓为正的反应的发生，经常依靠反应耦合（多个反应共享一个或多个中间物）。总的自由能变化是各个反应自由能之和。\n- 4-1. 蛋白质功能的几个例子\n- **4-2. 描述小型 SH2 蛋白结构域的四种不同方式**\n- 4-3. 抗体的制备和使用\n- **4-4. 细胞的裂解和细胞抽提物的初步分离**\n    - “我们如何得知17”提到\n- **4-5. 用层析法分离蛋白质**\n    - “我们如何得知17”提到\n- 4-6. 用电泳法分离蛋白质\n- 13-1. 详解糖酵解的10个步骤\n- 13-2. 完整的柠檬酸循环\n- **14-1. 氧化还原电位**\n- 17-1. 三种蛋白丝的主要类型\n- 18-1. 动物细胞 M 期的主要阶段\n- 19-1. 经典遗传学的要义\n"},{"slug":"barron-360-biology-contents","filename":"2022-02-25-barron-360-biology-contents.md","date":"2022-02-25","title":".tex | 生物学知识提纲","layout":"post","keywords":["tex","bio"],"excerpt":"最近在学校书店看到一本书，是 BARRON’S 360 系列的《A Complete Study Guide to Biology with Online Practice》。把目录抄录下来，对照内容自己找资料自学。","content":"\n最近在学校书店看到一本书，是 BARRON’S 360 系列的《A Complete Study Guide to Biology with Online Practice》。（原本）觉得对半路出家的我来说很有用，但是美国的书实在是太TM贵了，于是把目录抄录下来，对照内容自己找资料自学。\n\n~~算是开了一个新坑？~~\n\n抄完之后才发现这基本上就是国内高中的生物学水平，对生物分类方面的介绍还是立足于向下一代传递信息，有趣但是对我的研究帮助不大；对分子生物学和细胞生物学方面的介绍少了点。“世界上最好的高中教育在美国，只不过是在美国大学。”段子诚不我欺。\n\n1. 总论/元信息 \u003cbr\u003e\nBIOLOGY: THE SCIENCE OF LIFE\n    1. 生物的科学\u003cbr\u003e\n    The Science of Biology\n    2. 生物学诸分支\u003cbr\u003e\n    Branches of Biology\n    3. 当代生物学家的工作\u003cbr\u003e\n    The Work of the Modern Biologist\n2. 生命的特征\u003cbr\u003e\n CHARACTERISTICS OF LIFE\n    1. 主要生命活动\u003cbr\u003e\n    Major Life Functions\n    2. 生物如何命名\u003cbr\u003e\n    How Living Things Are Named\n    3. 五界分类系统\u003cbr\u003e\n    Five-Kingdom System of Classifications\n    4. 六界分类系统\u003cbr\u003e\n    Six-Kingdom System of Classifications\n    5. 三域分类系统\u003cbr\u003e\n    Three-Domain System of Classifications\n3. 细胞：生命的基本单位 \u003cbr\u003e\nTHE CELL: BASIC UNIT OF LIFE\n    1. 作为基本单位的细胞\u003cbr\u003e\n    The Cell as a Basic Unit\n    2. 细胞的各部分\u003cbr\u003e\n    Parts of a Cell\n    3. 动植物细胞比较\u003cbr\u003e\n    Comparison of Plant and Animal Cells\n    4. 细胞和组织\u003cbr\u003e\n    Organization of Cells and Tissues\n    5. 细胞增殖\u003cbr\u003e\n    Cell Reproduction\n4. 生命的化学 \u003cbr\u003e\nTHE CHEMISTRY OF LIFE\n    1. 一些基础化学原则\u003cbr\u003e\n    Some Basic Principles of Chemistry\n    2. 化学键\u003cbr\u003e\n    Chemical Bonding\n    3. 化学反应\u003cbr\u003e\n    Chemical Reactions\n5. 细胞的基础化学 \u003cbr\u003e\nTHE BASIC CHEMISTRY OF CELLS\n    1. 作为化学工厂的细胞\u003cbr\u003e\n    The Cell as a Chemical Factory\n    2. 活体细胞中酶的作用\u003cbr\u003e\n    The Role of Enzyme in Living Cells\n    3. 细胞呼吸\u003cbr\u003e\n    Cellular Respiration\n6. 细菌和病毒 \u003cbr\u003e\nBACTERIA AND VIRUSES\n    1. 原核生物\u003cbr\u003e\n    Prokaryotes\n    2. 细菌的分类\u003cbr\u003e\n    Classification of Bacteria\n        1. 古菌界\u003cbr\u003e\n        Kingdom Archaeobacteria\n        2. 细菌界\u003cbr\u003e\n        Kingdom Eubacteria\n    3. 细菌的重要性\u003cbr\u003e\n    Importance of Bacteria\n    4. 病毒\u003cbr\u003e\n    Virus\n7. 原生生物界：原生动物、类真菌原生生物、类植物原生生物\u003cbr\u003e\nTHE PROTIST KINGDOM: PROTOZOA, FUNGUS-LIKE PROTISTS, PLANT-LIKE PROTISTS\n    1. 主要原生生物\u003cbr\u003e\n    Major Groups of Protists\n        1. 原生动物\u003cbr\u003e\n        Protozoa, the animal-like protists\n        2. 类真菌原生生物\u003cbr\u003e\n        Fungus-like protists\n        3. 类植物原生生物\u003cbr\u003e\n        plant-like protists\n    2. 原生生物对人类的重要性\u003cbr\u003e\n    Importance of Protists to Humans\n8. 真菌\u003cbr\u003e\nTHE FUNGI\n    1. 真菌的一般特性\u003cbr\u003e\n    General Features of Fungi\n    2. 主要真菌门\u003cbr\u003e\n    Major Divisions of Fungi\n    3. 特殊营养关系\u003cbr\u003e\n    Special Nutritional Relationships\n    4. 真菌对人类的重要性\u003cbr\u003e\n    Importance of Fungi to Humans\n9. 绿色植物\u003cbr\u003e\nTHE GREEN PLANTS\n    1. 植物的一般特点\u003cbr\u003e\n    General Characteristics of Plants\n    2. 主要植物门\u003cbr\u003e\n    Major Divisions of Plants\n    3. 光合作用\u003cbr\u003e\n    Photosynthesis\n    4. 植物激素\u003cbr\u003e\n    Plant Hermones\n    5. 光周期律\u003cbr\u003e\n    Photoperiodicity\n10. 无脊椎动物：从海绵到软体动物\u003cbr\u003e\nINVERTEBRATES: SPONGES TO MOLLUSKS\n    1. 动物体的基本组织结构\u003cbr\u003e\n    Basic Organization of the Animal Body\n    2. 无脊椎动物的一般特点\u003cbr\u003e\n    General Characteristics of Invertebrates\n    3. 海绵\u003cbr\u003e\n    Phylum Porifera — Sponges\n    4. 腔肠动物——水螅、水母、珊瑚、海葵及其近亲\u003cbr\u003e\n    Phylum Cnidaria (Coelenterates) — Hydrozoa, Jellyfish, Corals, Sea Anemones, and Their Relatives\n    5. 扁形动物门\u003cbr\u003e\n    Phylum Platyhelminthes—Flatworms\n    6. 线虫动物门\u003cbr\u003e\n    Phylum Nematoda—Roundworms\n    7. 环节动物门\u003cbr\u003e\n    Phylum Annelida—Segmented Worms\n    8. 软体动物门—— ~~蛤与蛤丝~~ \u003cbr\u003e\n    Phylum Mollusks—Clams and Their Relatives\n11. 无脊椎动物：节肢动物\u003cbr\u003e\nINVERTEBRATES: THE ANTHROPODA\n    1. 主要代表纲\u003cbr\u003e\n    Major Representative Classes\n    2. 蛛形钢\u003cbr\u003e\n    Class Arachnida—Spiders and Ticks\n    3. 软甲纲\u003cbr\u003e\n    Class Malacostraca—Lobsters and Their Relatives\n    4. 昆虫纲\u003cbr\u003e\n    Class Insecta\n    5. 节肢动物对人类的重要性\u003cbr\u003e\n    Importance of Arthropoda to Humans\n12. 后口动物、脊索动物、哺乳动物\u003cbr\u003e\nDEUTEROSTOMES, CHORDATES, AND MAMMALS\n    1. 后口动物总门\u003cbr\u003e\n    The Deuterostomes\n    2. 脊索动物门\u003cbr\u003e\n    Phylum Chordata—The Chordates\n    3. 脊椎动物亚门\u003cbr\u003e\n    Subphylum Vertebrata—The Vertebrates\n13. 人：一种特殊的有脊椎动物\u003cbr\u003e\nHOMO SAPIENS: A SPECIAL VERTEBRATE\n    1. 灵长类的特点\u003cbr\u003e\n    Characteristics of Primates\n    2. 骨骼系统\u003cbr\u003e\n    The Skeletal System\n    3. 肌肉系统\u003cbr\u003e\n    The Muscular System\n    4. 神经系统\u003cbr\u003e\n    The Nervous System\n    5. 内分泌系统\u003cbr\u003e\n    The Endocrine System\n    6. 呼吸系统\u003cbr\u003e\n    The Respiratory System\n    7. 循环系统\u003cbr\u003e\n    The Circulatory System\n    8. 淋巴系统\u003cbr\u003e\n    The Lymphatic System\n    9. 消化系统\u003cbr\u003e\n    The Digestive System\n    10. 排泄系统\u003cbr\u003e\n    The Excretory System\n    11. 感觉器官\u003cbr\u003e\n    Sense Organs\n    12. 生殖\u003cbr\u003e\n    Reproduction\n14. 营养：吃出健康\u003cbr\u003e\nNUTRITION: EATING FOR HEALTH\n    1. 大量营养物质\u003cbr\u003e\n    Macronutrients: Carbohydrates, Proteins, and Lipids\n    2. 微量营养物质\u003cbr\u003e\n    Micronutrients: Vitamins and Minerals\n    3. 进食障碍\u003cbr\u003e\n    Eating Disorders\n15. 人类疾病和免疫系统\u003cbr\u003e\nDISEASES OF HOMO SAPIENS AND THE IMMUNE SYS传染病TEM\n    1. 传染病\u003cbr\u003e\n    Infectious Diseases\n    2. 免疫系统\u003cbr\u003e\n    The Immune System\n    3. 非传染病\u003cbr\u003e\n    Noninfectious Diseases\n16. 遗传和基因学\u003cbr\u003e\nHEREDITY AND GENETICS\n    1. 经典遗传原理\u003cbr\u003e\n    Classical Principles of Heredity\n    2. 现代基因学\u003cbr\u003e\n    Modern Genetics\n    3. 基因技术\u003cbr\u003e\n    Genetic Technologies\n    4. 逆转录病毒——RNA中的基因编码\u003cbr\u003e\n    The Retrovirus of AIDS—Genetic Coding in RNA\n    5. 遗传对人类的重要性\u003cbr\u003e\n    Importance of Heredity to Humans\n17. 演化论\u003cbr\u003e\nPRINCIPLES OF EVOLUTION\n    1. 演化的证据\u003cbr\u003e\n    Evidence of Evolution\n    2. 演化的思想\u003cbr\u003e\n    Ideas About Evolution\n    3. 生命的起源\u003cbr\u003e\n    The Origin of Life\n    4. 人类的演化\u003cbr\u003e\n    Evolution of Humans\n18. 生态学\u003cbr\u003e\nECOLOGY\n    1. 生态系统的概念\u003cbr\u003e\n    The Concept of the Ecosystem\n    2. 生态系统中的能量流动\u003cbr\u003e\n    Energy Flow in an Ecosystem\n    3. 生物地球化学循环\u003cbr\u003e\n    Biogeochemical Cycles\n    4. 限制因素的概念\u003cbr\u003e\n    The Limiting Factor Concept\n    5. 温室效应\u003cbr\u003e\n    Greenhouse Effect\n    6. 全球变暖\u003cbr\u003e\n    Global Warming\n    7. 生态演化\u003cbr\u003e\n    Ecological Succession\n    8. 世界生物群系\u003cbr\u003e\n    World Biomes\n    9. 人类和生物圈\u003cbr\u003e\n    Humans and the Biosphere\n    10. 生态保护\u003cbr\u003e\n    Conservation"},{"slug":"lab-note","filename":"2020-09-21-lab-note.md","date":"2020-09-21","title":".tex | 整理一些关于实验记录的文章","layout":"post","keywords":["tex","phy","bio"],"excerpt":"摸鱼的方式有很多种，琢磨如何完美地进行实验记录就是个挺不错的由头。","content":"\n1. [微信公众号“BioArt植物”，原作者 Elisabeth Pain ，《实验记录到底怎么记？》](https://www.sciencemag.org/careers/2019/09/how-keep-lab-notebook)\n1. [Howard Kannare, 《Writing the Laboratory Notebook》](https://files.eric.ed.gov/fulltext/ED344734.pdf)\n1. [MIT Department of Mechanical Engineering, 《Instructions for Using Your Laboratory Notebook》](http://web.mit.edu/me-ugoffice/communication/labnotebooks.pdf)\n1. [微信公众号“生物学霸”，《颜宁：讲讲如何记实验记录》](https://xw.qq.com/partner/vivoscreen/20200820A00HZI00?vivoRcdMark=1)\n\n微信平台不允许添加指向微信之外的超链接，资源的获取方式见正文。作为报复，以上四条资源中有两条最早是在公众号里看到的，但是博文给出的链接都来自微信之外 :-)\n\n\u003chr class=\"slender\"\u003e\n\n## 0\n\n疫情依旧，摸鱼依旧。摸鱼的方式有很多种，其中比较高级的一种是打着完美主义的旗号，对着一个还没完成，或者根本不存在完成时的东西，疯狂输出时间和精力。琢磨如何完美地进行实验记录就是个挺不错的由头。\n\n说实话，学界对研究记录的要求实际上并不算严格，这一点在《Writing the Laboratory Notebook》里也有佐证。商业机构研发部门的研究记录会成为将来知识产权争端的主要证据，稍有不慎就是真金白银的经济损失，甚至关乎企业的生死存亡。而学界的工作在“科技”中偏重于”科“（即便是工程学科），在”研发“中偏重于”研“。（四者有什么联系和区别？科学认识世界，技术改造世界，研究把钱变成知识，开发把知识变成钱。）自由比起规范显然更有利于在未知领域的探索。\n\n所以，我们实验室对于实验记录并没有成文的规则，大家自己找本子自己记，格式和内容都跟随自己的喜好来，同实验室的同学也很少交流这个问题，仿佛说了就是承认自己的科研能力有问题。\n\n## 1\n\n越不谈越是心虚，于是在看到了《实验记录到底怎么记？》这篇文章之后，下决心要处理掉这个问题。这篇文章的作者访谈了几个科研人员，然后将他们的对话打碎，分到四个问题之下：\n\n- 为什么还要花时间精力去做实验记录？\n- 用传统纸质的记录本，还是电子版，还是都有？\n- 采取什么策略来保证实验记录有条理、完整并且实用？\n- 其他……\n\n并不推荐这篇文章，原因从这四个问题就能看出来：第一条属于幸存者偏见，一个觉得记录不重要的人压根不会认真记录，从而很难成为访谈对象；第二条属于典型的”有的人……有的人……“英式废话文套路；第三个问题太笼统，本应该细分为更明确的子问题；最后一个“其他”说明作者都不知道该怎么总结这些对话。明明是一篇文章，硬生生写出了微博一般的碎片感。\n\n## 2\n\n于是在网上继续找资源，机缘巧合之下，在一个知乎问题之下看到了一个还不错的答案，里面提到了 Howard Kannare 的《Writing the Laboratory Notebook》这本书。真的是“机缘巧合”，因为现在的我已经找不到原来的那个问题和答案了，哪怕专门为了这篇文章搜索了半天……这说明了网络资源的收藏和管理也是一个技术活（又可以水一篇文章了）。\n\n这本书在网上有英文的完整影印版，很容易就能搜到，实在不行的话在微信后台留言\"HowardKannare\"可以收到下载网址（注意回复的关键词没有空格）。\n\n本书各章的标题如下：\n\n1. The Reasons for Note keeping - An Overview\n2. The Hardware of Note keeping - Books, Pens, and Paper\n3. Legal and Ethical Aspects - Ownership, Rights, and Obligations\n4. Management of Notekeeping - Practices for Issuance, Use, and Storage of Notebooks\n5. Organizing and Writing the Notebook - Be Flexible\n6. Examples of Notebook Entries\n7. Patents and Invention Protection\n8. The Electronic Notebook\n9. Appendix A: Some Suggestions for Teaching Laboratory Notekeeping\n10. Appendix B: Photographs from the Historical Laboratory Notebooks of Famous Scientists\n\n书很长，有 150 多页，这导致内容涉及方方面面，包括了那些我们可能不是那么急需的方面；还有很多我们今天可能并不十分需要的冷知识，比如几十年前美国出产的纸张由于某种工艺导致保存期限比较短等等。好在多数章节最后都有总结，可以帮人省下不少时间。\n\n另一方面，这本书出版于1985年，那是一个什么年代呢？苹果在前一年才刚刚推出了 Macintosh 电脑，C++ 在当年才刚刚出版。所以对于电子实验记录，书中只在第8章和纸本笔记进行了一个简单的对比，而且有比较明显的时代局限性。\n\n总之，如果真要读这本书的话，抱着练习英语阅读的目标，要远比学记笔记要少些失望。\n\n## 3\n\n干了半天之后开始怀疑这件事从一开始是否有必要，这可是 PhD 的保留节目了。\n\n尤其是在读过《Writing the Laboratory Notebook》的第5章之后，读到单篇实验报告应该包括 introduction, experimental plan, observations and data, discussion of results 的时候，恍然发现，这不就是本科实验课要交的实验记录的写法吗，之所以没有老师教过我们怎么记实验记录，是不是因为他们觉得这个事情已经教过了？\n\n既然如此，那么第三份材料就是 MIT 机械系给本科生的实验报告要求，不长，只有 6 页，还包括了超过两页的模板笔记，可以当作《Writing the Laboratory Notebook》关于笔记内容部分的精华集锦来看。微信后台回复 \"MITlab\" 可以收到 PDF 下载地址。\n\n但这反而说明了，PI们散养研究生，不对实验记录进行更进一步的要求和培训是不对的。因为“实验记录本”≠“实验记录们”，研究生的工作不同于本科实验课，本科生做实验就只有在固定时间和固定时长的实验课上，一切超出规定范围内的动作大概率都是无用功甚至错误，每个实验要做什么，有哪些步骤，会观察到什么现象，都是事先设计好的，实验记录的各个部分会有哪些内容，大体上没跑。研究生的工作则不然，大到整个博士期间的所有工作都可以算作是一个项目（毕竟会写成一篇毕业论文），小到显微镜从开机到观测到关机的几个小时也可以整出一篇报告来，如何划分研究的基本单元？一个人可能同时在做相对独立的多项工作，是连续记录还是分开平行记录？以纸本为主，电子版主要用于备份，还是主要用电子设备，随手记在纸上的拍照作为附件？\n\n## 4\n\n对于这篇文章没有太多可说的，覆盖范围和《实验记录到底怎么记？》类似，感觉就相当于颜宁女士自己一个人对那篇文章中的问题的回答，由于是一个人的回答，所以不会有前一篇文章中不同观点混在一起的分裂感。看过了《Writing the Laboratory Notebook》之后，会发现文中的每一个点都在书中可以找到。\n"},{"slug":"yeast-cloth-food-house-travel","filename":"2019-12-29-yeast-cloth-food-house-travel.md","date":"2019-12-29","title":".tex | 实验室中酵母菌的衣食住行","layout":"post","keywords":["tex","bio"],"excerpt":"实验室里的酵母，吃得怎样，住在哪里，过得好吗？","content":"\n“酵母”其实是1500多种微生物的一个统称，但是在科学研究中最常用的一种是 _Saccharomyces cerevisiae_，下文中的酵母一词指的就是它们。作为一种单细胞生物，酵母在指数生长期，通过出芽生殖的方式，平均约90分钟就可以繁殖一代。同时作为真核生物，核膜以及各种具膜细胞器应有尽有。而且酵母对脂质的代谢路径和人类的代谢路径在进化上是同源的，也就是说有些治疗人类疾病的药物，也可以在酵母细胞中产生效果。结合酵母菌快速繁殖的特性，可以方便相关药物的筛选。再加上人们熟知的发酵、酿酒等应用，酵母就成了生物实验室中非常常见的一种模式生物。\n\n## “衣”\n\n~~全裸，下一题。~~\n\n这个问题其实还是可以再多说两句的。\n\n这里的“衣”显然是一种比喻，既然是比喻，那就要看说话的人想强调的是衣服的哪种性质。酵母的细胞膜外面还有一层细胞壁，对于一个细胞的定义来说，细胞壁并不是必需的。从这个角度讲，完全可以把细胞壁比作酵母的外套。这层外套的主要成分是糖类，既包括起到结构支撑作用的多糖，也包括传递信息，可以用来对酵母进行特异性识别的糖蛋白。\n\n即便有了这么一层外套，单个的酵母细胞在显微镜下依然是透明的，在普通的亮场 (bright field) 显微镜下，可以隐约看到酵母细胞内部比较巨大且明显的结构，比如液泡。要想看得更清楚，就得用上 DIC（微分干涉相差，differential interference contrast）显微技术了。要说清楚 DIC，需要一篇独立的文章，而文章的很大一部分都会是数学。简而言之，光在不同的介质中的传播速度不同，表征介质的这一性质的物理量叫做“折射率”，折射率 × 光传播的距离 = 光程。当我们让两束相同的光线，通过给定的距离，其中一束光线通过我们的酵母细胞，另一束经过一段折射率已知的均匀介质（比如说空气），两束光通过的光程不同，再次见面时就会有差别，用衍射的方法比较一下两束光，就能得出酵母细胞内不同位置的折射率信息，从而看出不同的结构。\n\nDIC 设备比普通的亮场显微镜要贵不少，而且有些不同的细胞器其实长得很像，即便用上 DIC 也不容易分清楚，这时候我们就要对酵母细胞本身动点手脚。有些荧光染色剂可以和某种细胞器特异性地结合，用特定波长的光照射细胞，被染色的细胞器就会发出另外一种波长的荧光，我们用滤光片把入射光过滤掉，就可以得到这种细胞器在细胞中的位置、形状和大小信息。除了用染色剂，还可以把荧光蛋白基因插入到某些细胞器蛋白质的基因旁边，这样就免去了每次观察细胞时染色的麻烦。~~（“所以你们实验室做转基因喽？那你告诉我，转基因食品能不能吃？”）~~\n\n之前两段说的是单个细胞，而在固态的培养基（“住”的部分会介绍）上面，酵母细胞不能大范围移动，单个酵母细胞作为祖先不断繁殖，“子又有孙，孙又有子”，久而久之（其实也就两三天）就可以形成一个菌落，就是培养基上一个肉眼可见的斑点。不论是黄色的 YPD 培养基还是白色的 SD 培养基，菌落的颜色肉眼观察不出区别，都是乳白色。\n\n## “食”\n\n酵母是一种异养生物，也就是说需要摄入营养物质，既要利用其中的化学能维持自己的生命活动，也要利用这些物质的原子和分子构建自己身体的组成成分。能够满足微生物细胞生长繁殖需求的营养物质，叫做培养基 (medium)。我们实验室中常用的培养基主要就两种，一是 YPD 培养基，二是 SD 选择培养基。\n\n### YPD\n\n这个名字直接来源于其成分——Yeast extract, Peptone, Dextrose——酵母提取物、胨、葡萄糖。用酵母提取物去喂酵母……不能多想，想多了就有点怪怪的……好处是真的好用，生长速度比下面要讲的 SD 培养基要快一些。而且由于我们实验室都是 SD 选择培养基，所以野生型酵母只能用 YPD。\n\n那么缺点呢？一个问题就是酵母提取物里面，究竟有什么物质，每种物质占比多少，都是一笔糊涂账，实验结果就不好分析。\n\n另外，YPD 培养基在很多波长的光照射下都会发出微弱的荧光，在之前说过的荧光显微观察中，这种荧光就会成为背景噪声。所以 YPD 培养的细胞不能直接进行荧光显微观察，要换到 SD 培养基养一段时间（一般半个小时以上），或者干脆就直接养在 SD 培养基里。\n\n### SD 选择培养基\n\nSD 的全称是 synthesis defined，意思是人工合成，而且成分是明确且固定的，配方在一般的科研网站上都能找到。而选择培养基的意思是，相比于普通的 SD 培养基，某种营养物质（一般是某种氨基酸）被去掉了，比如说 SD-His 培养基中就不含有组氨酸 (Histidine)。这些营养物质都是野生酵母没办法自己合成的，但是在用同源重组的方法进行基因编辑的时候，我们除了会导入目标基因之外，还会一起导入能表达合成这一营养物质的酶的基因。把被编辑过的细胞接种在选择培养基上，那些没能成功编辑的细胞将无法合成相关营养物质，只有编辑成功的细胞能够幸存，并形成菌落，这就是“选择”一词的由来。\n\n## “住”\n\n由于酵母既没有嘴，也不像草履虫一样拥有可以游动的鞭毛，所以只能和营养物质生活在一起。（当然也是和代谢废物生活在一起，不能多想……）所以说，培养基不仅是食物，还很类似于酵母菌的“家具”。\n\n前面说到了培养基，定义里说是营养物质，但是并没有强调其物质状态。培养基既可以按照配方配置成溶液，这叫做液态培养基。在溶液中按比例（一般是 2% 的质量分数）加入琼脂 (agar)，先加热溶解，然后稍微冷却之后倒入培养皿 (petri dish)，继续冷却就形成了凝胶，这样就制成了一个半固态培养基。\n\n在培养基中加入了细胞之后，此时的混合物应该叫做培养液 (culture)，如果把 medium 和 culture 弄混了的话，有些人的脸上会浮现出微妙的笑容。屠格涅夫说过：“有教养不是吃饭的时候不洒汤，而是别人洒汤的时候你不去看他。”为了避免别人没有教养，我们还是应该勉为其难地区分一下这两个概念。:-)\n\n说完家具来说房子，也就是 culture 的容器。\n\n在液态培养液中，酵母的密度要略微大于培养基的密度，所以长时间的静置会使得细胞在容器的底部沉积，影响生长速度。所以我们需要把容器固定在恒温的机械平台上不断摇晃，为了让离心力尽可能地大，容器要选用底面积比较大的锥形瓶，用普通试管的话需要将其斜放。~~（我看看有哪个高中生蹦出来说不存在离心力这回事，哼）~~\n\n对于琼脂培养基，培养皿直接放在恒温箱里静置。前面说过，单个细胞会繁殖，然后无法移动，形成一个菌落。没有基因突变的话，这一个菌落中细胞的基因型都是相同的。如果两个空间上相邻的菌落不断长大，就会连在一起，进而长成一个大菌落（突然想到了光学课本里瑞利判据那张连环画），此时就不好确定基因型的纯洁了。在长到这种情况之前，就要把培养皿用 3M 胶带封起来，然后放到低温下暂存，等待日后实验时取出单个菌落继续培养，或是冷冻长期保存。\n\n说到了温度，酵母菌的正常生长需要 30 摄氏度。刚才说到要保存菌落已经足够大的培养皿，这时的温度一般是 4 摄氏度，也就是冰箱冷藏室的温度。而长期保存细胞的话需要放入 -70 ~ -80 摄氏度的低温冷柜中。\n\n## “行”\n\n我们实验室还并没有厉害到需要给其他实验室寄送我们培养的菌株的地步，我们实验室里的菌株是怎么来的我也忘了……所以这一节所说的“行”，指的是酵母菌在不同的培养基之间的转移。\n\n网上的教程里比较讲究，啥高科技工具都有，就跟吃螃蟹的蟹八件似的。我们实验室比较莽，不论是冷冻的细胞，还是液态或者琼脂培养基中的细胞，一律都是用消毒杀菌过的细木棍来取细胞，如果不是要转移到琼脂培养皿的话，甚至还可以用一次性移液枪头。琼脂培养基的话要仔细蘸一蘸，确保取到的细胞来自同一个菌落，冷冻细胞的话就刮一刮容器里的冰沙，液态培养基的话就随便蘸一蘸。\n\n如果目的地是液态培养基的话就很简单，木棍伸进去，搅合搅合，完事儿。\n\n如果是琼脂培养基的话，要轻轻地在琼脂表面来回涂擦，要小心不能刮坏琼脂层。来回涂擦几次之后，把木棍换到之前没接触细胞的一面，在刚才涂擦的区域边缘，往没涂过的地方来回涂擦几次，如是反复者二三。这样做是因为刚开始的细胞数量可能比较多，长出来的菌落从一开始就连成一片，后来几次涂擦时细胞的密度就会越来越低，某个区域就会出现足够数量又相距足够距离的一群菌落。总之是个技术活还是个经验活。\n\n如果是新制备长期保存的细胞的话，需要先在液态培养基中将细胞养到合适的浓度，然后在专用的容器中按照 1:1 的比例混合培养液和甘油，然后就可以放进冷柜了。"}]],["ai",[{"slug":"afaik-generative-ai","filename":"2023-09-11-afaik-generative-ai.md","date":"2023-09-11","title":"·如是我闻 | 生成式人工智能","layout":"post","keywords":["md","ai","rss"],"excerpt":"","content":"\n## 生成式语言模型\n\n### 模型\n\n- OpenAI/GPT\n- Claude\n- `bloomchat`, 可以商用 [[GitHub](https://github.com/sambanova/bloomchat)]\n- `falcon40B`\n    - apache 2.0 许可证，可商用[[huggingface](http://huggingface.co/tiiuae)]\n    - gpt3 的性能，更少的运算资源，其中Falcon 7B可以跑在苹果Mac上 [[推特](https://twitter.com/rickawsb/status/1666148546285043714)]\n- `TigerBot`: 一款国产自研的多语言任务大模型，70亿参数和1800亿参数两个版本 [[GitHub](https://github.com/TigerResearch/TigerBot)]\n- `QLoRA`: 单个GPU，ChatGPT 99%的能力，消费级GPU微调12个小时就可以达到97%的ChatGPT水平，4B就可以保持16B精度的效果 [[论文](https://www.notion.so/Endocytic-trafficking-promotes-vacuolar-enlargements-for-fast-cell-expansion-rates-in-plants-6b8f0a313c184ccba9fb5a035bb04a0e?pvs=21)] [[GitHub](https://www.notion.so/pdf-14a94950d61c42d3b03bb132f7655589?pvs=21)]\n- `MBT 30B`: 开源商用模型为数不多的选择里出现了一个比Falcon 40B更好的模型 [[Twitter](https://twitter.com/fi56622380/status/1672137540281974784)]\n- `GLM-6B` \u0026 `GLM2-6B`: 智谱AI发布，对学术研究完全开放，并且在完成企业登记获得授权后，允许免费商业使用。[[Twitter](https://twitter.com/GanymedeNil/status/1679892021807550465)][[微信公众号@GLM大模型](https://mp.weixin.qq.com/s?__biz=MzkxNjMzMjM3NA==\u0026mid=2247484214\u0026idx=1\u0026sn=e42153f987a74d1ffc7882f7cc09670d)]\n- `Llama 2`: Meta开源大语言模型Llama 2，可免费商用. [[微信](https://mp.weixin.qq.com/s/9pcmrCEyp2AQsL3MbPYx-Q?utm_source=pocket_saves)介绍]\n    - Jim Fan 评论 [[推特，翻译](https://twitter.com/dotey/status/1681553916373135362?utm_source=pocket_saves)]\n    - 很多团队几乎都达成共识， RLHF 不重要，SFT 就够了。现在 Llama2 的论文说 RLHF 非常非常重要。[[推特](https://twitter.com/oran_ge/status/1681793774685659136?utm_source=pocket_saves)]\n    - `LLaMA-2-7B-32K`, context为32K的模型 [[推特](https://twitter.com/JefferyTatsuya/status/1685423475979325440)][[Twitter](https://twitter.com/togethercompute/status/1685048832168714240)]\n\n### 基于模型，直接可用的产品\n\n- OpenAI/GPT\n    - ChatGPT\n        - 2023年6月13日，GPT提供了函数调用，让ChatGPT来自己调用函数。[[Twitter](https://twitter.com/cryptonerdcn/status/1668733300070924288)][[OpenAI](https://openai.com/blog/function-calling-and-other-api-updates)][[用法 Twitter@宝玉](https://twitter.com/dotey/status/1668728109376450566)]\n    - ChatGPT - Code Interpreter\n        - 介绍 [[推特](https://twitter.com/fuyufjh/status/1684191835210809344)][[YouTube](https://www.youtube.com/watch?v=4wGlRrir_u4)]\n        - 《ChatGPT 探索：Code Interpreter 高级指南》[[微信@浮之静](https://mp.weixin.qq.com/s/K_csi1oWDv5tEaeeKSlvwA?utm_source=pocket_saves)]\n        - 源码可能被套出。[[Twitter](https://twitter.com/fuergaosi/status/1679457847237820416)]\n        - 对 code interpreter 的逆向工程 [[Twitter](https://twitter.com/Yampeleg/status/1678045605527003136)][[Mem](https://mem.ai/p/xyy8ULiAce1BecTxnU0M)]\n    - OpenAI API\n        - 2023年8月23日，OpenAI 开放了 GPT-3.5 的微调的API [[推特](https://twitter.com/dotey/status/1694207797351616703)]\n    - OpenAI on Azure 内置了一个内容过滤器 [[推特1](https://twitter.com/jw1dev/status/1666613728106938368)][[推特2](https://twitter.com/jw1dev/status/1666622878962548740)]\n    - `forefront`: 完全免费 GPT-4 的工具 [[登录](https://accounts.forefront.ai/)]，大概基于 `gptfree-ts` [[GitHub](https://github.com/xiangsx/gpt4free-ts)]\n    - `BratGPT`: ChatGPT的激进版本。[[官网](https://bratgpt.com/)]\n    - `SmartStudy`: 提供文本文档，创建10个问题的小测验。[[官网](https://smartstudy.streamlit.app/)]\n    - `XrayGPT`: 通过给定的 X 光片来促进围绕胸部 X 光片的自动化分析的研究。[[GitHub](https://twitter.com/CarsonYangk8s/status/1661588037892198401)]\n    - `FinGPT`: 类似BloomBerg的开源方案，RLHF 和 Lora 的低秩技术 [[Twitter](https://twitter.com/JefferyTatsuya/status/1668433680887615488)]\n- 微软\n    - BingAI\n        - 本地部署方案 [[推特](https://twitter.com/geekbb/status/1665692703055552513)][[GitHub](https://github.com/adams549659584/go-proxy-bingai)]\n    - VsCode Copilot\n    - Office 365 Copilot: 每月每名用户30美元. [[verge](https://www.theverge.com/2023/7/18/23798627/microsoft-365-copilot-price-commercial-enterprise)][[微信](https://mp.weixin.qq.com/s/9pcmrCEyp2AQsL3MbPYx-Q?utm_source=pocket_saves)]\n- Claude+\n    - 例子：阅读多份行业报告 [[推特](https://twitter.com/iamshaynez/status/1684398211958730753)]\n- `Llama`\n    - llama2.ai: 一个基于 llama 2 的聊天机器人，非官方。[[网站](https://llama2.ai/)]\n    - WizardCoder 34B based on Code Llama 写代码 [[推特](https://twitter.com/dotey/status/1696202647269785875)]\n- WebGLM: 清华开源的带网络搜索功能的 GLM 实现 [[GitHub](https://www.notion.so/pdf-14a94950d61c42d3b03bb132f7655589?pvs=21)]\n- mendable: 根据开发文档进行问答 [[官网](https://www.mendable.ai/usecases/documentation)]\n- 阅读 PDF 文档\n    - Humata.ai\n    - explainpaper\n    - ChatPDF\n    - [[对比](https://twitter.com/oran_ge/status/1683432444169711616?utm_source=pocket_saves)] Claude2支持超长上下文，摘要信息量更大，更适合长文提炼。ChatDOC 具有页码溯源、表格解析、原文定位功能，数据找得准，也方便二次验证，能够限制大语言的幻觉问题。\n- Obsidian-copliot: 快速获取文字的核心观点\n- 视频内容梗概\n    - Glarity: 浏览器插件，基于ChatGPT和字幕生成Youtube摘要，20秒看完梗概 [[Twitter](https://twitter.com/starzqeth/status/1640867876109422595)]\n    - summarize-tech: 5分钟了解长视频的要点. [[Twitter](https://twitter.com/starzqeth/status/1640867876109422595)]\n- webpilot: 可联网可读网页链接的插件 Webpilot 推出的 Chrome 版插件 [[chrome](https://chrome.google.com/webstore/detail/webpilot-copilot-for-all/biaggnjibplcfekllonekbonhfgchopo?utm_source=link)]\n\n### 模型教程、评论、二次开发\n\n- 一般性原理\n    - 《Prompt 编写模式》[[phodal](https://prompt-patterns.phodal.com)]\n    - 《LLM+Embedding构建问答系统的局限性及优化方案》[[知乎](https://zhuanlan.zhihu.com/p/641132245)]\n    - 基于检索的 LM，外挂一个数据库用来检索。[[推特](https://twitter.com/cosmtrek/status/1678077835418955781)][[GitHub.io](https://acl2023-retrieval-lm.github.io/)]\n    - 一篇泼冷水的论文 [[ACL Anthology](https://aclanthology.org/2023.findings-acl.426/)]\n    - 即刻出的Prompt调试工具。[[Twitter](https://twitter.com/vista8/status/1678784460786135040)][[官网](https://promptknit.com/)]\n- GPT\n    - GPT best practice [[OpenAI](https://platform.openai.com/docs/guides/gpt-best-practices?utm_source=pocket_saves)]\n    - Andrew Ng 吴恩达 \u0026 Isa Fulford from OpenAI 《Build system with [#ChatGPT](https://twitter.com/hashtag/ChatGPT?src=hashtag_click) API》[推特@**[金田達也](https://twitter.com/JefferyTatsuya)**]\n        - 借助 CoT 的思路，翻译字幕，返回正确的 JSON 格式 [[推特](https://twitter.com/dotey/status/1665476562219573249)]\n        - 同样的加入了CoT（Chain of Though）的Prompt，如果让GPT打印出来步骤，效果非常好，但是如果不让GPT打印（省点token，以及更容易解析），那么GPT就会偷懒 [[Twitter](https://twitter.com/dotey/status/1668736426286915590)1][[Twitter2](https://twitter.com/dotey/status/1664335473500626946)]\n    - 熊猫吃短信是 Twitter@威力狈 开发的垃圾短信过滤工具。将其与 GPT 结合的一些讨论\n        - [Twitter@威力狈](https://twitter.com/waylybaye/status/1664253928970788864)：尝试了下用 ChatGPT 自动标注数据，效果太差了。\n        - [Twitter@宝玉](https://twitter.com/dotey/status/1669028955842650139)：通常如果我写的话，会做一些小调整\n        - [Twitter@IIInoki](https://twitter.com/IIInoki)：是的，感觉八爷用 API 用得有点糙……就只是很简单的 prompt 达到的效果都还不错\n    - 《ChatGPT 越过山丘之后，再来谈谈 LLM 应用方向》[[橘子汽水铺](https://quail.ink/orange/p/chatgpt-cross-over-the-hills-and-discuss-llm-application-directions)]\n- `LangChain`:\n    - 官方教程 [[推特](https://twitter.com/LangChainAI/status/1665009694627250176)][[streamlit](https://blog.streamlit.io/langchain-tutorial-1-build-an-llm-powered-app-in-18-lines-of-code/)]\n    - 一个使用 LangChain 和 GPT Index 的教程 [[leanpub, 收费](https://leanpub.com/langchain)][[Pocket](https://getpocket.com/read/3839490971)]\n    - LangChain for LLM Application Development 基于LangChain的大语言模型应用开发 [[YouTube](https://t.co/JXV1SBI2OA)]\n        - 基于Embedding的文档问答。stuff, map reduce, refine, map rerank [[Twitter@宝玉](https://twitter.com/dotey/status/1667790801420558342)]\n    - Chanin Nantasenamat: LangChain tutorial #1: Build an LLM-powered app in 18 lines of code [[streamlit](https://blog.streamlit.io/langchain-tutorial-1-build-an-llm-powered-app-in-18-lines-of-code/?utm_source=pocket_saves)]\n    - 把一篇很长的 PDF 内容喂给 ChatGPT，然后向他提问\n        - 纯 JS 开源工具推荐 [[推特](https://twitter.com/Barret_China/status/1638119945749037056)]\n        - 用 `LangChain` 六七行代码就可以搞定了 [[LangChain](https://js.langchain.com/docs/get_started/introduction)]\n- `AutoChain`\n    - 介绍 [[推特](https://twitter.com/zhangjintao9020/status/1683996172980199429)][[GitHub](https://github.com/Forethought-Technologies/AutoChain)]\n    - 《我为什么放弃了 LangChain》[[推特](https://twitter.com/Barret_China/status/1683135367862718465)][[微信](https://mp.weixin.qq.com/s/jIbz9JYc8-_ua-QLENX__A)] 推友提出的 AutoChain 替代方案 [[推特](https://twitter.com/Barret_China/status/1684211570186887170?utm_source=pocket_saves)]\n- `OpenDAN`: 为各类 AI 模块提供运行环境，并提供它们之间的互操作性协议。可创建诸如律师、医生、教师，甚至男女朋友等角色 [[GitHub](https://twitter.com/Barret_China/status/1666455683758161920)]\n- “视频语音↔文字”任务相关\n    - 指定视频URL，识别文字，翻译 [[GitHub](https://github.com/lewangdev/autotranslate)]\n    - `WhisperX`: 按照单词对齐时间戳，生成的字幕都是完整的句子 [[GitHub](https://github.com/m-bain/whisperX)]。[[Twitter@宝玉](https://twitter.com/dotey/status/1667394662628204546)] 写了一个可以根据 YouTube Url 识别 YouTube 字幕的 [Jupyter Notebook](https://github.com/JimLiu/whisper-subtitles/blob/main/whisperx_youtube_subtitle.ipynb)\n    - `audiocraft`: audio processing and generation with deep learning. [[GitHub](https://github.com/facebookresearch/audiocraft)]\n    - [[推特]](https://twitter.com/Barret_China/status/1684218981639413760) 小作文\n    - `yt-dlp` 一行命令下载视频字幕的工具，不需 puppeteer 无头浏览器 [[推特](https://twitter.com/Barret_China/status/1684228477644570624)][[GitHub](https://github.com/yt-dlp/yt-dlp)]\n- ChatGPT + AI agent + ScholarAI + Noteable 写的小综述 [[链接失效](https://t.co/eqVc2LIfSz)]\n- `MusicGen`: 将文本和旋律转化为完整乐曲 [[Twitter](https://twitter.com/Fenng/status/1668141100610248705)][[ReadHub](https://www.notion.so/3753e42dc4204a99ab83a725b655a632?pvs=21)]\n- `MMS`: 一个声音模型 [[HuggingFace](https://huggingface.co/docs/transformers/main/en/model_doc/mms)]\n- `FRVR Forge`: AI-Powered End-to-End Game Creation [[Twitter](https://twitter.com/FRVRGames/status/1669758477789540365)][[官网](https://www.notion.so/ai-University-Cloud-8078b4682e454a5fba982f67e4530498?pvs=21)]\n\n### 开发平台\n\nRunpod: 租用 GPU 跑模型并创建 Serverless API 一站式服务，最低只要0.2刀/hr。[[官网](http://runpod.io)]\n\n### 杂项\n\n- 2023年5月27日、28日，OpenAI 使用 Sentry 审计工具封禁来自中国的用户，解决方案：\n    - 路由器 Clash 规则 [[推特](https://twitter.com/wey_gu/status/1663003950214438912?utm_source=pocket_saves)]\n    - 改用 Azure OpenAI service [[推特](https://twitter.com/zhangjintao9020/status/1662865819041402880)]\n    - Cloudflare WARP [[左耳朵](https://haoel.github.io/)]\n\n## 生成式图像模型\n\n2023年5月31日，Adobe 添加人工智能相关功能 generative fill。[[推特](https://twitter.com/CodeByPoonam/status/1663824055164887040)]\n\n- 配置要求极低，连Win掌机都能跑，但是不能断网。[[推特](https://twitter.com/OfflineHelper/status/1666042746866663424)]\n- 填充将横屏的视频转换为竖屏。[[推特](https://twitter.com/Alex_Cerrato/status/1681677307843432449)]\n\n`MidJourney`\n\n- 在提示词中添加相机镜头信息。[[推特](https://twitter.com/4rtofficial/status/1663310457854099458)]\n- zoom [[Twitter](https://twitter.com/jesselaunz/status/1674210886695923712)]\n\n`StableDiffusion`\n\n- Eric Fu: 训练指南. [[Coding Husky](https://ericfu.me/stable-diffusion-finetune-guide/?utm_source=pocket_reader)]\n- 文字或者符号融合生成图片 [[Twitter](https://twitter.com/op7418/status/1680223090138316800)][[微信](https://mp.weixin.qq.com/s/rvpU4XhToldoec_bABeXJw)]\n\n`StyleDrop`: Google 基于 MUSE 的样式迁移 transformer [[推特](https://twitter.com/recatm/status/1665056017107886080)][[GitHub.io](https://styledrop.github.io/)]\n\n`Redream`: 从视频到二次元动画 [[推特](https://twitter.com/heyBarsee/status/1665034805384290307)][[GitHub](https://github.com/Fictiverse/Redream)]\n\n`Runway` Gen-2: 文本生成视频和图片生成视频, 4 秒钟 [[推特](https://twitter.com/op7418/status/1666461595818504192)][[需注册](https://app.runwayml.com/login)]\n\n一个 AI 视频解决方案，来自南洋理工，代码尚未开源 [[Twitter](https://twitter.com/op7418/status/1669026494885285888)] [[GitHub.io](https://anonymous-31415926.github.io/)][[Twitter2](https://twitter.com/rickawsb/status/1672310994390126593)][[arxiv](https://arxiv.org/abs/2306.07954)]\n\n`AWPortrait1.1`: 图像生成 [[Twitter](https://twitter.com/dynamicwangs/status/1673730591462928385)][[LibLibai](https://www.liblibai.com/modelinfo/721fa2d298b262d7c08f0337ebfe58f8)] \n\n`Anything AI`: 可以取代照片中的任何物体。免费，不需要注册. [[官网](https://www.anything-ai.com/)]\n\n`PixelLab`: 草图创建2D图像. [[官网](https://www.pixellab.ai/)]\n"},{"slug":"what-is-intelligence-not-same-as-intelligence-is-what","filename":"2023-06-17-what-is-intelligence-not-same-as-intelligence-is-what.md","date":"2023-06-17","title":".tex | 什么是智能≠智能是什么","layout":"post","keywords":["tex","doc","md","ai"],"hasMath":true,"excerpt":"“什么是智能”的问题每每得不到回答，是因为它的逆问题“智能是什么”没有答案。","content":"\n## 0\n\n这是一篇酬和之作。\n\n徵文标题说的是：\n\n\u003e **機器會製造「內涵」嗎？**\n\u003e \n\n但是正文提出的问题是：\n\n\u003e AI透過程式組合出回答你問題的文字組合，有「涵義」嗎？\n\u003e \n\n可是，「內涵」和「涵義」两个词的——内涵/涵义——就不完全一样啊……\n\n“内涵”(connotation) 通常指词语或表达方式所隐含的情感、态度、暗示或附加的意义。它涉及到词语或表达方式所引起的情感、联想或隐含的观点。也就是弦外之音。\n\n而“涵义”(meaning) 一般指词语、表达方式或行为所传达的字面意义或字面上的定义。它强调的是直接的、明确的意义。\n\n看热闹不嫌事大，那我们再把问题搞复杂一点——在逻辑学里也有一个“内涵”(intension)，和“外延”(extension) 相对应。用**面向对象编程**的说法来理解，一个类里面定义的所有状态量和内部方法的集合，就构成这个类的“**内涵**”；所有（已经和将来能够）从这个类实例化出来的对象的集合，就构成这个类的“**外延**”。\n\n所以看起来，征文者想问的是日常“内涵”也就是言外之意，但是怕杠精（比如我）用有严格定义的逻辑“内涵”解构掉，所以换了“涵义”一词。\n\n这个问题很显然是因应最近大语言模型掀起的这一波 AI 浪潮。这个问题往前再问一句，就是“大语言模型是智能体/有智能吗？”\n\n- 前两年 DeepMind 的 AlphaGo/AlphaZero 系列 AI 在围棋中击败人类棋手时，人们也在问这个问题。\n- 上世纪四五十年代专家系统 (expert system) 刚刚开发的时候，人们也在问这个问题。\n- 从电子计算机往前追溯到机械计算机，甚至是巴比奇的差分机的时候，人们就已经开始问这样的问题了。\n\n这些问题求并集，然后在问题数量趋近于无穷下的极限，就是“什么是智能”。\n\n这样的问题每每得不到回答，是因为它的逆问题“智能是什么”没有答案。我们并没有智能的准确定义，只能一事一论。而之前的智能和非智能体的区别太明显，以至于作出判断也不能对智能的定义有所启发。\n\n## 1\n\n而对“智能是什么”的探究，哲学、逻辑学、计算机科学、生物学、管理学，不同领域的研究者有着不同的思路。\n\n### 古哲学·洞穴之壁与理念世界\n\n古希腊哲学家柏拉图在《理想国》里提到了“洞穴之壁”的寓言故事。\n\n有一群被囚禁在一个深洞的囚徒，从出生开始就被束缚在这个洞穴里，脖子和腿都被铁链锁住，没办法转身或离开。囚徒身后的洞穴入口处有一道火焰，火焰后有人持物体走过，物体的投射在洞穴内的墙壁上形成了影子。囚徒们就以为这些影子就是唯一的存在。\n\n这里的囚徒代表着人类，洞穴代表着世界，影子则代表着我们对于现象世界的感知和观念。人们的知识和信念往往受限于自己的经验和感知，就像囚徒们只看到了洞穴墙壁上的影子，而在影子之外还存在一个理想的理性世界。柏拉图用这个寓言故事表达了他对于人类认识和智慧的理解。所谓智慧，就是从洞穴的影子反过去推测火把前物体的能力。\n\n当然，这种思想被 Marx 主义定性为一种客观唯心主义、唯理论，是受其批判的。\n\n### 逻辑学·从命题到希尔伯特算符\n\n柏拉图的学生亚里士多德，今天在低年级的物理教科书里基本是个反面典型，但他对逻辑学进行了系统化和全面的研究，提出了许多逻辑学的基本概念和原理。这些成果后来成为了欧洲哲学和逻辑学的基石，对西方哲学和科学的发展产生了深远影响。\n\n所谓逻辑，就是研究命题的对错，以及如何判断命题对错的学问。而命题，就是能被判断对错的句子。但是句子显然可以再分成不同成分，于是就发明/发现了主体、客体、谓词、谓词的量词……等等概念，以及用这些概念构造命题的方法。\n\n但是要注意，虽然逻辑主要由语言来表达，但是逻辑还是和语言不同，主体、客体也不等于句子的主语、宾语。这两者的区别，基本可以类比于之前洞穴之壁寓言里的实体和影子。\n\n这种努力到目前为止的巅峰，基本上要数希尔伯特形式化逻辑系统了。感兴趣的朋友可以自行查阅戈得门特《代数学教程》的第一章，这玩意相当于思想界的引体向上，反正我是一个也拉不上去……\n\n### 计算机·从半导体到抽象语法树\n\n希尔伯特是德国的数学家，《代数学教程》也是数学而不是哲学教材。显而易见，逻辑虽然由哲学家奠基，但是主导权很快落到数学家，至少是哲学家兼数学家手里了。\n\n命题的“真”与“非真”同构于 {1, 0}，各种逻辑运算都可以分解成“或”与“非”两种基本逻辑运算的组合，这就是以数学家乔治·布尔 (George Boole) 命名的布尔代数。因为 {1, 0} 又可以同构于半导体电路的高低电位，和各种类似继电器的门电路组合，所以很容易用计算机在物理世界表示出这些逻辑运算。\n\n我们的电脑由上亿个这样的电位和逻辑门组成，一般的科普文章应该会去介绍芯片啊光刻机之类的东西，本文关注的是另一个方面：虽然生产电脑配件的厂商很多，不同的型号的元器件设计不同，组装出的成品应该千差万别，但是他们可以运行同样的程序，理想条件下（虽然实际工程中常常不理想）我们也可以期望他们跑出同样的结果。\n\n这说明所谓计算机科学，并不等同于研究计算机元件的电子科学和工程，这里电科和电子工程相当于洞穴岩壁上的影子，而计算机科学就相当于火光前的物体。这种超越物理的计算本质，一般用一种叫做“抽象语法树”的数据结构来表示。\n\n### 生物学·从神经元到神经网络\n\n人们发明计算机的时候，基本上还是把它当作工具，就没期望它有什么主体性和智慧。\n\n而随着生物学逐渐发现了神经系统及其作用，也随着物理学在二十世纪初的大发展之后的相对平静，很多物理学家开始插手其他学科。既然生命和非生命体的背后都服从同一套物理规律，既然物理学的众多成功经验说明，搞清楚构成系统的所有微观组成就可以理解宏观的系统，那么搞清楚人类的智力器官的基本单元以及相互作用，按理说也就能够理解什么是智慧。\n\n![a cartoon illustrating a neuron](/photos/2023-06-17-neuron.png)\n\n上图是一个神经细胞的结构示意图。从其他神经细胞释放出来的名为神经递质的化学物质，到达神经元左侧短且密集的树突之后，激活细胞膜表面的离子泵，主动运输离子跨过细胞膜，从而产生电信号。电信号沿细胞膜传导到右侧的树突，刺激凸触释放神经递质给下一个细胞。\n\n![a handdrawing style illustration of perceptron](/photos/2023-06-17-perceptron.png)\n\n上图就是根据神经元的工作原理抽象出的数学模型，名为 perceptron。一个 perceptron 就是一个函数，接受多个输入的自变量，加权求和之后套一个非线性的激活函数，得到一个输出。很多个这样的 perceptron 并连和串联，就构成下图，计算机算法中的神经网络。\n\n![a handdrawing style illustration of a neural network](/photos/2023-06-17-neural-network.png)\n\n而从实验方向研究神经系统，我们隔壁系就有，经常来我们系招人。基本上就是在小鼠的天灵盖上锯开一个天窗，然后给它带上个头盔，头盔上有能从天窗伸进去的电极，采集脑神经的电信号。以前头盔有网线伸到实验室天花板，实时传到数据中心的超算。现在好像进步了，改用 Wi-Fi 了。\n\n这实验怎么通过的伦理审查，咱也不知道，咱也不敢问……\n\n### 管理学·DIKW “数据-信息-知识-智慧”模型\n\n![a pyramid of DIKW model](/photos/2023-06-17-DIKW.png)\n\nDIKW 四个字母分别代表 data, information, knowledge, wisdom，即数据、信息、知识、智慧，是一种知识管理中的心智模型。\n\n四个层次，前一层都是后一层的基础，后一层都是对前一层的理解。\n\n如果是书面文字，数据就是笔画和字母；如果是语言，数据就是人声的响度、频率和音色。由笔画/字母/声音组成的有含义的字词就是信息。表示信息之间的关系的，可以判断对错的命题就是知识。包含和统摄各条知识的思想体系，就是智慧。\n\n反过来说，虽然智慧高于思想，但它仍需要通过把各条知识的表达汇总起来，才能被人感知。对知识的命题的理解依赖于构成名字的各个概念的涵义，属于信息水平的内容。而每个字都有不考虑其涵义的笔画字母构成。\n\n这层与层之间**看似**并没有插入额外的内容，智慧可以直接由笔画构成。但是我们一层层理解的深入，其实是不自觉地借用了我们当前社会约定俗成的解读方式。\n\n比如下面这个图片里的符号，对于现代人就只是数据，无法解读成信息。但是对于苏美尔人，这是用楔形文字表示的数字，是等腰直角三角形的腰和直角边的比值，也就是 $$\\sqrt{2}$$ 的近似值。\n\n![sumerian numerical approximation to square root of two](/photos/2023-06-17-ancient-root-2.png)\n\n约定俗成的数据解读方式，也就是关于**数据的数据**，根据西方的构词法，可以叫做“**元**数据”(meta-data)。\n\n数据和元数据一起构成信息，信息和元信息一起构成知识，知识和元知识一起构成智慧。俺坚持写博客的动机，就是用费曼学习法，把无意间使用的元知识显式地表达出来，而且记录下来，争取学而不退转。\n\n## 2\n\n回顾了这些，再来看大语言模型，就会发现它落在了各方努力的延长线的交点。\n\n大语言模型里有一个重要概念叫做“嵌入”(embedding)，就是把语言的基本字元 (token) 可逆地映射到一个超多维度的向量空间里。本来“国王”和“儿子”之间没办法加减乘除，但是嵌入后的向量空间里有加法和数乘，如果嵌入函数选得好，“国王”的向量 + “儿子”的向量，结果向量就约等于“王子”的向量。\n\n![illustration of vector addition from wikipedia](/photos/2023-06-17-vector-addition.png)\n\n生成式语言模型的核心就是一个超多元函数，接受前一个字嵌入后的向量作为输入，给出另一个向量作为输出，用嵌入函数的逆映射翻译成字元；再把旧的输出作为新的输入，直到输出结果是“语段结束”这样一个特殊字元为止。模型训练的过程，主要就是通过现成的语料，拟合这个超多元函数的参数。\n\n从 DIKW 模型来看，语言模型操作的是最基本的数据，它的输出究竟是什么信息，是不是正确的知识，体现了多少智慧，是人根据当下的社会文化来解读的。\n\n而实现 AI 的电子计算机，或是复杂生命的大脑，他们和智能之间的关系，应该就类似于具体的计算机电路和抽象语法树之间的关系。以此类比，未来的智能科学应该会成为一门独立的专业，它和计算机科学和神经生物学的区别，就像今天的电子科学与工程，和计算机科学之间的区别一样。当下神经生物学的热度，将来恐怕多半会被分流。\n\n这种对字符的计算不同于逻辑运算，语言模型不判断输出结果在逻辑上的正确与错误，这既给了他啥都能说几句的 feature，又给了它经常编假消息的 bug。\n\n想要改掉这种错误，引入对 AI 的纠错机制，治本之道恐怕还是诉诸于对世界的正确描述，与理论相关的还是要靠逻辑，与现实相关的还是要靠科学。\n\n只不过，大语言模型提供了一种数据结构，有希望把人类已知的真理储存在一起。对这种数据结构本身的研究，有可能反过来启发科学的发展。柏拉图的洞穴之壁可能不再是一个比喻，未来更大的语言模型的，亿万维度的参数空间有希望成为洞穴门口的那团火。\n\n只不过这一切都是“可能”，现在还只是 AI 的萌芽阶段，还没有足够的证据来证实或者证伪这种畅想。而且 AI 的参数量再大也是有限的，它所能表达的信息也就有限，而真理应当是无限的，就像科学一样，总要训练更新更大的模型，总要发现已知的未知，然后欣然接受更多未知的未知之存在。\n\n如果电子计算机实现的 AI 独立于人类产生了意识和超出人类的智慧，很难想象他们会继续用人类语言这种对他们来说很不方便的方式来交流。\n\n所以，哪怕是做个 AI 生成内容的质检员，科学家依然有事可做。这算是科学的堕落吗？当然不算，如果算的话，那从计算物理也被当作理论物理的那天起，人类就已经投降了（逃）\n\n## 3\n\n现在正面来回答问题：AI透過程式組合出回答你問題的文字組合，有「涵義」嗎？\n\n答：有。\n\n因为语言的「涵義」来自于语言的内容，和整个社会的文化，并不来自于这句话的作者的身份。即便是人与人之间的交流，诉诸身份也是一种非形式逻辑谬误，是理性不足的表现。只有在信息不足仍不得不下结论的时候才该使用，比如法律判决时的自由心证主义和/或法定证据主义。\n\n而鹿妈眼里真人鹿酱与 AI 鹿酱的区别，如果有的话，好像主要体现在动机的区别。动机这种东西，很多智慧不高的生物，比如小猫小狗都会有；而现在的 AI，似乎还没有展现出超出编程者设计的动机。编程写入的信息有限，现有 AI 的动机也就有限，鹿酱的赢面还是很大的。\n\n而动机是生物与非生物的区别吗？而什么是生物 ≠ 生物是什么，那就是另一个含混而复杂的问题了。\n\n## 4\n\n这篇博文发布的时候，高考应该已经结束了，马上该填报志愿了。\n\n那么，西元 2023 年，AI 来袭的当下，该选个啥专业在 AI 浪潮中幸存，或者选个啥专业给 AI 老爷带路呢？\n\n![a screenshot of a quotation from Three Body about attitudes towards aliens](/photos/2023-06-17-three-body-quotation.png)\n\n我的建议是，不要听别人的建议，按自己的兴趣来就好了。\n\n刚刚改开的时候，有一个超级热门的专业，叫科技英语。科技落下了好多年，对外开放需要语言交流，两者一结合应该是热门又稀缺了。结果呢，你现在还听说过这个专业吗？\n\n科技很重要是不错，语言很重要也不错，但是搞科技的人自己可以学英语，学英语的有几个搞得了科技？社会的进步主要靠创新，而创新的方向难以预测，不论这种预测分析听起来多有道理。\n\n如果真的找不到兴趣，那就在能力范围之内，找个难度最高的。如果想从事智力劳动，那数学含量是个不错的衡量标准；如果不排斥体力劳动，那训练时间越长越值得考虑。\n\n但这只是填志愿来不及时的权宜之计，发掘兴趣是人一生的课题。\n\n兴趣不是为了让你成功的时候更得意，毕竟成功的话不论做什么都很得意；\n\n兴趣是为了你不成功时也可以不失意，毕竟平凡才是人生的真谛。\n"},{"slug":"physics-based-neural-network-review-note","filename":"2023-03-20-physics-based-neural-network-review-note.md","date":"2023-03-20","title":".tex | 基于物理的神经网络 (PINN) 综述笔记","layout":"post","keywords":["tex","phy","md","ai"],"hasMath":true,"excerpt":"本文是《Scientific Machine Learning Through Physics–Informed Neural Networks: Where we are and What’s Next》这篇综述的读书笔记。","content":"\n\u003e 本文是《[Scientific Machine Learning Through Physics–Informed Neural Networks: Where we are and What’s Next](https://link.springer.com/article/10.1007/s10915-022-01939-z)》这篇综述的读书笔记。\n\u003e \n\n年前，今年新入职的天文学方面的一位老师给我们群发邮件，宣传某国家实验室超算的 GPU 编程马拉松活动，他可以担任指导老师。于是毫不意外地，我报了名。该编程马拉松项目还需要专门申请，申请材料里要写清楚打算干什么，于是报名的五六个人七嘴八舌地想创意。基于物理的神经网络 PINN 就是天文老师的点子。\n\n~~写到这里，我才意识到，老哥是不是想拿我们当免费劳动力啊~~~\n\n神经网络可以看作是一个复杂的非线性函数，接受一个（一般来说维度很高的）向量作为输入，一番计算后输出另一个向量。训练神经网络，就是找到这个函数的参数，绝大多数找参数的方法涉及计算网络输出对参数的偏导数，因此神经网络计算框架的核心功能就是自动微分 (auto-differentiation)。\n\n而很多物理问题，都可以用（偏）微分方程来描述，微分方程的解不是变量，而是函数，而且往往是复杂的非线性函数。所以基于物理的神经网络 (PINN) 就是以神经网络来表达这个函数，然后把这个函数带入到物理的微分方程中，把神经网络输出和真正的物理解之间的差距当作损失函数，反向传播回去来优化神经网络的参数。代入方程时的微分计算，正好可以利用现成框架的自动微分功能。\n\n在以 GPT 为代表的 transformer 类神经网络模型出现之前，自然语言处理类的机器学习项目，往往要在网络之外，利用人类的语法知识，对语段进行语义分割等等“中间任务”。Transformer 一出，算力出奇迹，中间任务逐渐变得没有必要了。\n\n在 GPT 崭露头角，并且越来越有迹象表明其将会涌现出通用人工智能的今天，这些基于物理的神经网络，会不会还未成熟就已过时？这种心情，就和《三体》第二卷开始，章北海和吴岳面对焊渍未漆的“唐”号航空母舰时差不多吧……\n\n\u003chr class=\"slender\"\u003e\n\n- Abstract\n    - PINNs are neural networks that encode model equations. a NN must fit observed data while reducing a PDE residual.\n\n1. Introduction\n    - The “curse of dimensionality” was first described by Bellman in the context of optimal control problems. (Bellman R.: Dynamic Programming. Sci. 153(3731), 34-37 (1966))\n    - Early work: MLP ([multilayer perceptron](https://en.wikipedia.org/wiki/Multilayer_perceptron)) with few hidden layers to solve PDEs. ([https://doi.org/10.1109/72.712178](https://doi.org/10.1109/72.712178))\n    - 感觉可能更全面的一篇综述：[https://doi.org/10.1007/s12206-021-0342-5](https://doi.org/10.1007/s12206-021-0342-5)。该文关注 what deep NN is used, how physical knowledge is represented, how physical information is integrated，本文只关于 PINN, a 2017 framework。\n\n    1. What the PINNs are\n        - PINNs solve problems involving PDEs:\n            - approximates PDE solutions by training a NN to minimize a loss function\n            - includes terms reflecting the initial and boundary conditions\n            - and PDE residual at selected points in the domain (called **collocation points**)\n            - given an input point in the integration domain, returns an estimated solution at that point.\n            - incorporates a [residual network](https://en.wikipedia.org/wiki/Residual_neural_network) that encodes the governing physical equations\n            - can be thought of as an **unsupervised strategy** when they are trained solely with physical equations in forward problems, but **supervised learning** when some properties are derived from data\n        - Advantages:\n            - [mesh-free](https://en.wikipedia.org/wiki/Meshfree_methods)? 但是我们给模型喂训练数据的时候往往已经暗含了 mesh 了吧\n            - on-demand computation after training\n            - forward and inverse problem using the same optimization, with minimal modification\n    2. What this Review is About\n        - 提到了一个做综述找文章的方法：本文涉及的文章可以在 Scopus 上进行高级搜索：`((physic* OR physical)) W/2 (informed OR constrained) W/2 “neural network”)`\n2. The Building Blocks of a PINN\n    - question:\n    \n    $$\n    F(u(z);\\gamma)=f(z),\\quad z\\ \\in\\ \\Omega \\\\ B(u(z))=g(z), \\quad z\\ \\in\\ \\partial \\Omega\n    $$\n    \n    - solution:\n    \n    $$\n    \\hat u_{\\theta}(z)\\approx u(z)\\\\ \\theta^* = \\arg\\min_{\\theta}\\left(\\omega_F L_F(\\theta)+\\omega_BL_B(\\theta)+\\omega_{data}L_{data}(\\theta)\\right)\n    $$\n    \n    1. Neural Network Architecture\n        - DNN (deep neural network) is an artificial neural network that is deeper than 2 layers.\n        \n        1. Feed-Forward Neural Network: \n            - $$u_{\\theta}(x) = C_{K} \\circ C_{k-1} ...\\alpha \\circ C_1(x),\\quad C_k(x) = W_k x_k + b_k$$\n            - Just change CNN from convolution to fully connected.\n            - Also known as multi-layer perceptrons (MLP)\n            \n            1. FFNN architectures \n                - Tartakovsky et al used 3 hidden layers, 50 units per layer,  and a hyperbolic tangent activation function. Other people use different numbers but of the same order of magnitude.\n                - A comparison paper: *Blechschmidt, J., Ernst, O.G.: Three ways to solve partial differential equations with neural networks –A review. GAMM-Mitteilungen 44(2), e202100,006 (2021).*\n            2. multiple FFNN: 2 phase [Stephan problem](https://en.wikipedia.org/wiki/Stefan_problem).\n            3. shallow networks: for training costs\n            4. activation function: the swish function in the paper has a learnable parameter, so — [how to add a learnable parameter in PyTorch](https://discuss.pytorch.org/t/how-could-i-create-a-module-with-learnable-parameters/28115)\n        2. Convolutional Neural Networks: \n            - I am most familiar with this one.\n            - $$f_i(x_i;W_i)=\\Phi_i(\\alpha_i(C_i(W_i,x_i)))$$\n            - performs well with multidimensional data such as images and speeches\n            \n            1. CNN architectures: \n                - `PhyGeoNet`: a physics-informed geometry-adaptive convolutional neural network. It uses a coordinate transformation to convert solution fields from irregular physical domains to rectangular reference domains.\n                - According to Fang ([https://doi.org/10.1109/TNNLS.2021.3070878](https://doi.org/10.1109/TNNLS.2021.3070878)), a Laplacian operator can be discretized using the finite volume approach, and the procedures are equivalent to convolution. Padding data can serve as boundary conditions.\n            2. convolutional encoder-decoder network\n        3. Recurrent Neural Network\n            - $$f_i(h_{i-1})=\\alpha\\left(W\\cdot h_{i-1}+U\\cdot x_i+b\\right)$$, where f is the layer-wise function, x is the input, h is the hidden vector state, W is a hidden-to-hidden weight matrix, U is an input-to-hidden matrix and b is a bias vector. 我认为等号左边的 $$h_{i-1}$$ 应当作为下标\n            - 感觉有点像 hidden Markov model，只不过 Markov 中间的 hidden layers 好像与序号无关（记不清了），~~RNN 看起来各个 W 和 H 似乎不同~~。**RNN cell is actually the exact same one and reused throughout.** (from [https://blog.floydhub.com/a-beginners-guide-on-recurrent-neural-networks-with-pytorch/](https://blog.floydhub.com/a-beginners-guide-on-recurrent-neural-networks-with-pytorch/)). Cartoon from Wikipedia:\n                \n                ![Untitled]({{ site.baseurl }}/assets/photos/2023-03-20-rnn-unit.png)\n                \n            - From [https://blog.floydhub.com/a-beginners-guide-on-recurrent-neural-networks-with-pytorch/](https://blog.floydhub.com/a-beginners-guide-on-recurrent-neural-networks-with-pytorch/):\n                \n                ![Untitled]({{ site.baseurl }}/assets/photos/2023-03-20-rnn-types.png)\n                \n            1. RNN architectures\n                - can be used to perform numerical Euler integration\n                - 基本上输出的第 i 项只与输入的第 i 和 i-1 项相关。\n            2. LSTM architectures\n                - 比 RNN 多更多中间隐变量，至于怎么做到整合长期记忆的，技术细节现在可以先略过\n        4. other architectures for PINN\n            1. Bayesian neural network: weights are distributions rather than deterministic values, and these distributions are learned using Bayesian inference. 只介绍了[一篇文章](https://doi.org/10.1016/j.jcp.2020.109913)\n            2. GAN architectures: \n                - two neural networks compete in a zero-sum game to deceive each other\n                - physics-informed GAN uses automatic differentiation to embed the governing physical laws in stochastic differential equations. The discriminator in PI–GAN is represented by a basic FFNN, while the generators are a combination of FFNNs and a NN induced by the SDE\n            3. multiple PINNs\n    2. Injection of Physical Laws\n        - 既然是要解常/偏微分方程，那么微分计算必不可少。四种方法：hand-coded, symbolic, numerical, auto-differentiation，最后一种显著胜出。所谓 auto-differentiation, 就是利用现成框架，框架自动给出原函数的导数的算法。\n        - Differential equation residual:\n            - $$r_F[\\hat u_\\theta](z)=r_\\theta(z):=F(\\hat u_\\theta(z);\\gamma)-f$$\n            - $$r_F[\\hat u_\\theta](z)=r_\\theta(x,t)=\\frac{\\partial}{\\partial t}\\hat u_\\theta(x,t)+F_x(\\hat u_\\theta(x,t))$$: 原文给出了来源，但是从字面上看不出来与前式的等价性\n        - Boundary condition residual: $$r_B[\\hat u_\\theta](z):=B(\\hat u_\\theta(z))-g(z)$$\n    3. Model Estimation by Learning Approaches\n        1. Observations about the Loss\n            - $$\\omega_F$$ accounts for the fidelity of the PDE model. Setting it to 0 trains the network without knowledge of underlying physics.\n            - In general, the number of $$\\theta$$ is more than the measurements, so regularization is needed.\n            - The number and position of residual points matter a lot.\n        2. Soft and Hard Constraints\n            - Soft: penalty terms. Bad:\n                - satisfying BC is not guaranteed\n                - assignment of the weight of BC affects learning efficiency, no theory for this.\n            - Hard: encoded into the network design. [Zhu et. al](https://doi.org/10.1007/s00466-020-01952-9)\n        3. Optimization methods\n            - minibatch sampling using the Adam algorithm\n            - increased sample size with L-BFGS (limited-memory Broyden-Fletcher-Goldfarb-Shanno)\n    4. Learning theory of PINN: roughly in DE, consistency + stability → convergence\n        1. convergence aspects: related to the number of parameters in NN\n        2. statistical learning error analysis: use *risk* to define *error*\n            - Empirical risk: $$\\hat R[u_\\theta]:=\\frac{1}{N}\\sum_{i=1}^N \\left\\|\\hat u_{\\theta}(z_i)-h_i\\right\\|^2$$\n            - Risk of using approximator: $$R[\\hat u_{\\theta}]:=\\int_{\\bar \\Omega}(\\hat u_{\\theta}(z)-u(z))^2dz$$\n            - Optimization error: the difference between the local and global minimum, is still an open question for PINN. $$E_O:=\\hat R[\\hat u_{\\theta}^*]-\\inf_{\\theta \\in \\Theta}\\hat R[u_\\theta]$$\n            - Generalization error: error when applied to unseen data. $$E_G:=\\sup_{\\theta \\in \\Theta}\\left\\|R[u_\\theta]-\\hat R[u_\\theta]\\right\\|$$\n            - Approximation error: $$E_A:=\\inf_{\\theta \\in \\Theta}R[u_\\theta]$$\n            - Global error between trained deep NN $$u^*_\\theta$$ and the correct solution is bounded: $$R[u^*_\\theta]\\le E_O+2E_G+E_A$$\n            - 有点乱，本来说 error 是误差，结果最后还是用 risk 作为误差\n        3. error analysis results for PINN\n3. Differential Problems Dealt with PINNs：读来感觉这一部分意义不大，将来遇到需要解决的问题时，回来看看之前有没有人做过就行了——另一方面看，一类方程就需要一类特殊构造的神经网络来解，那么说明神经网络解方程的通用性并不好~\n    1. Ordinary differential equations: \n        - Neural ODE as learners, a continuous representation of **ResNet**. [[Lai et al](https://doi.org/10.1016/j.jsv.2021.116196)], into 2 parts: a physics-informed term and an unknown discrepancy\n        - LSTM [[Zhang et al](https://doi.org/10.1016/j.cma.2020.113226)]\n        - [Directed graph models](https://doi.org/10.1016/j.compstruc.2020.106458) to implement ODE, and Euler RNN for numerical integration\n        - Symplectic Taylor neural networks in [Tong et al](https://doi.org/10.1016/j.jcp.2021.110325) use symplectic integrators\n    2. Partial differential equations: steady/unsteady的区别就是是否含时\n        1. steady-state PDEs\n        2. unsteady PDEs\n            1. Advection-diffusion-reaction problems\n                1. diffusion problems\n                2. advection problems\n            2. Flow problems\n                1. Navier-Stokes equations\n                2. hyperbolic equations\n            3. quantum problems\n    3. Other problems\n        1. Differential equations of fractional order\n            - automatic differentiation not applicable to fractional order → [L1 scheme](https://doi.org/10.1515/fca-2019-0086)\n            - [numerical discretization for fractional operators](https://doi.org/10.1137/18M1229845)\n            - [separate network to represent each fractional order](https://doi.org/10.1038/s43588-021-00158-0)\n        2. Uncertainty Estimation: [Bayesian](https://doi.org/10.1016/j.jcp.2020.109913)\n    4.  Solving a Differential Problem with PINN\n        - 1d non-linear Schrödinger equation\n        - dataset by simulation with MATLAB-based Chebfun open-source(?) software\n4. PINNs: Data, Applications, and Software\n    1. Data\n    2. Applications\n        1. Hemodynamics\n        2. Flows Problems\n        3. Optics and Electromagnetic Applications\n        4. Molecular Dynamics and Materials-Related Applications\n        5. Geoscience and Elastiostatic Problems\n        6. Industrial Application\n    3. Software\n        1. `DeepXDE`: initial library by one of the vanilla PINN authors\n        2. `NeuroDiffEq`: PyTorch based used at Harvard IACS\n        3. `Modulus`: previously known as Nvidia SimNet\n        4. `SciANN`: implementation of PINN as Keras wrapper\n        5. `PyDENs`: heat and wave equations\n        6. `NeuralPDE.jl`: part of SciML\n        7. `ADCME`: extending TensorFlow\n        8. `Nangs`: stopped updates, but faster than PyDENs\n        9. `TensorDiffEq`: TensorFlow for multi-worker distributed computing\n        10. `IDRLnet`: a python toolbox inspired by Nvidia SimNet\n        11. `Elvet`: coupled ODEs or PDEs, and variational problems about the minimization of a functional\n        12. Other Packages\n5. PINN Future Challenges and Directions\n    1. Overcoming Theoretical Difficulties in PINN\n    2. Improving Implementation Aspects in PINN\n    3. PINN in the SciML Framework\n    4. PINN in the AI Framework\n6. Conclusion\n"},{"slug":"parameters-in-convolution-in-neural-network-and-transposeconv","filename":"2022-12-29-parameters-in-convolution-in-neural-network-and-transposeconv.md","date":"2022-12-29","title":".ai | 神经网络中的卷积及其参数","layout":"post","keywords":["ai","py","md","m"],"hasMath":true,"excerpt":"在读 PyTorch 的文档和源码的时候，发现写文档的人也不怎么解释啥是卷积，卷积的各个参数是什么意思，只在文档里扔了个链接就完事了……","content":"\n在读 PyTorch 的文档和源码的时候，发现写文档的人也不怎么解释啥是卷积，卷积的各个参数是什么意思，只在文档里扔了个链接就完事了，链接那头是一个 GitHub 上的动图演示仓库，是一篇论文《A guide to convolution arithmetic for deep learning》（链接在文末）的附件。于是这篇文章，基本上就是论文的读书笔记了。\n\n## 数学的卷积：连续 vs. 离散\n\n### 定义\n\n连续的情况，两个单变量函数 $$f(\\cdot)$$ 和 $$g(\\cdot)$$ 的卷积，定义为：\n\n$$\n\\left(f*g\\right)(x):=\\int_{-\\infty}^{\\infty}f(\\tau)g(x-\\tau)d\\tau\n$$\n\n离散的情况，两个向量（也就是一阶张量） $$\\vec f$$ 和 $$\\vec g$$ 的卷积，定义为：\n\n$$\n\\left(\\vec f * \\vec g\\right)_i := \\sum_{j=-\\infty}^{\\infty} f_j g_{i-j}\n$$\n\n多变量函数/高阶张量的情况，只需要多加几重积分/求和号就可以类推了。\n\n看这两个定义——\n\n只看等号左边的话，可以把卷积看作是一种特殊的乘法，也就是一种**运算。**f 和 g 的地位是平等的，卷积甚至还满足交换律，你甚至可以把两者的顺序变一变；\n\n但是看等号右边的话，卷积就应该被看作是一种**变换**。f 和 g 的地位不再平等，f 是被变换的函数/向量，g 是变换的核 (kernel)。函数的情况里，g 把定义在 $$\\tau$$ 空间里的函数 f 变换成了 x 空间里的另一个函数；向量的情况里，g 把一个 J (j 所有可能取值的数量) 维向量 f 变换成了一个 I (i 所有可能取值的数量) 维向量。\n\n神经网络中的卷积，**借用**的主要是第二种**理解**。\n\n### 手算一个例子\n\n例如 $$\\vec f = (1,2,3,4)$$, $$\\vec g = (1,2,3)$$，而且约定下标从 0 开始的话——\n\n\u0026nbsp; $$(\\vec f*\\vec g)_0 = f_0g_0 = 1$$\n\n\u0026nbsp; $$(\\vec f*\\vec g)_1 = f_0g_1 + f_1g_0  = 4$$\n\n\u0026nbsp; $$(\\vec f*\\vec g)_2 = f_0g_2 + f_1g_1 + f_2g_0 = 10$$\n\n\u0026nbsp; $$(\\vec f*\\vec g)_3 = f_1g_2 + f_2g_1 + f_3g_0 = 16$$\n\n\u0026nbsp; $$(\\vec f*\\vec g)_4 = f_2g_2 + f_3g_1 = 17$$\n\n\u0026nbsp; $$(\\vec f*\\vec g)_5 = f_3g_2 = 12$$\n\n不想手算？\n\n```python\nimport numpy as np\nfrom scipy import signal\nsignal.convolve(np.array([1,2,3,4]),np.array([1,2,3]))\n```\n\n### 形象化表示\n\n上面的计算过程，可以看作是——\n\n1. 把 g 向量的**顺序反过来；**\n2. 把 g 的最右一个元素和 f 的最左元素对齐，\n3. 上下两行都有数字的列相乘（也就是把没有数字的地方看作 0），然后把所有乘积相加，得到 f*g 的第一项；\n4. 把 g 向右移动一格\n5. 重复第3、4步\n6. 直到 g 的最左项移动到 f 的最右一个元素。\n\n形如下列各表：\n\n| f |  |  | 1 | 2 | 3 | 4 |\n| g | 3 | 2 | 1 |  |  |  |\n| --- | --- | --- | --- | --- | --- | --- |\n| (f*g)(0) = 1 |  |  | 1 |  |  |  |\n\n\u003chr class=\"slender\"\u003e\n\n| f |  | 1 | 2 | 3 | 4 |\n| g | 3 | 2 | 1 |  |  |\n| --- | --- | --- | --- | --- | --- |\n| (f*g)(1) = 4 |  | 2 | 2 |  |  |\n\n\u003chr class=\"slender\"\u003e\n\n| f | 1 | 2 | 3 | 4 |\n| --- | --- | --- | --- | --- |\n| g | 3 | 2 | 1 |  |\n| (f*g)(2) = 10 | 3 | 4 | 3 |  |\n\n\u003chr class=\"slender\"\u003e\n\n| f | 1 | 2 | 3 | 4 |\n| g |  | 3 | 2 | 1 |\n| --- | --- | --- | --- | --- |\n| (f*g)(3) = 16 |  | 6 | 6 | 4 |\n\n\u003chr class=\"slender\"\u003e\n\n| f | 1 | 2 | 3 | 4 |  |\n| g |  |  | 3 | 2 | 1 |\n| --- | --- | --- | --- | --- | --- |\n| (f*g)(4) = 17 |  |  | 9 | 8 |  |\n\n\u003chr class=\"slender\"\u003e\n\n| f | 1 | 2 | 3 | 4 |  |  |\n| g |  |  |  | 3 | 2 | 1 |\n| --- | --- | --- | --- | --- | --- | --- |\n| (f*g)(5) = 12 |  |  |  | 12 |  |  |\n\n## 机器学习的卷积，是卷积吗？\n\n看论文给出的图 Figure 1.1，在卷积核是灰色 3\\*3 矩阵的情况下，对蓝色 5\\*5 矩阵的卷积就是直接把核对齐到蓝色矩阵上，**并没有把核的元素顺序颠倒过来**。\n\n这玩意能叫卷积吗？\n\n![convolution]({{ site.baseurl }}/assets/photos/2022-12-29-convolution.png)\n\n有人强行挽尊，说我们画图示的时候已经把核给颠倒过来了，想知道卷积核就把灰色小矩阵再颠倒回去——\n\n但是，不颠倒就对齐相乘的运算也是有名字的，叫 cross correlation。核有没有颠倒，convolution 还是 cross correlation 一组合，可以带来升维打击般的混乱，堪比高中化学的“还原剂被氧化，氧化剂被还原”……所以，对于计算机专业的数学水平，不予置评～\n\n（你这样纠缠有意思吗？.jpg）\n\n## 卷积`torch.nn.Conv` 及其各个参数\n\n### `in_channels` \u0026 `out_channels`\n\n“卷积”的意义在于用一种比较省内存的方式，考虑输入张量中各个元素，和空间上相近的邻居元素之间的关系。所以只需要在真的存在空间关系的维度做卷积，其他维度可以留着不动。\n\n比如一张彩色图片，是一个 (颜色*高度*宽度) 的 3 阶张量，我们只需要对高度和宽度两个维度做卷积，颜色就是不参与“卷积”的 channel。\n\n`in_channel` 就是被“卷积”的张量的 channel 数，`out_channel` 是“卷积”结果的 channel 数。比如我们想从一张 RGB 三色图片中分辨出前景和背景两种不同区域，`in_channel=3`, `out_channel=2`。\n\n而 `in_channel` 如何能够与 `out_channel` 取值不同，原理见 Figure 1.3。我们使用 `out_channel` 个不含 channel 维度的“卷积”核，每一个核都与每一个 in channel 做卷积，得到图中的蓝、紫色小矩阵，然后直接把不同的 in channel 暴力求和，得到的结果分别作为卷积结果的 out channel。（这个暴力求和与我以前想得不一样，我以为是什么每一元素都做了一个`in_channel`*`out_channel` 的全联通层）\n\n![channels]({{ site.baseurl }}/assets/photos/2022-12-29-channels.png)\n\n PyTorch 的习惯，对于一个 N 阶“卷积”，参与卷积的是张量的最后 N 阶，`in_channel` 和 `out_channel` 也就是被卷张量和卷积结果的 `Tensor.shape[-(N+1)]`\n\n后面图示的例子都没有考虑 `in_channel` 和 `out_channel` 的数量，也就是都当作 1 了。\n\n### `kernel_size`\n\n就是灰色矩阵“卷积”核，每边有几个数字。如果不同方向的边长不一，该参数就需要用一个 tuple 来表示。Figure 1.1 的灰色卷积核，`kernel_size=(3,3)`\n\n![kernel]({{ site.baseurl }}/assets/photos/2022-12-29-kernel.png)\n\n### `padding` \u0026 `padding_mode`\n\n前面手算例子的时候很鸡贼地把 0 作为向量下标的起点。如果采用日常 1 开头的下标来算，第 1 项结果为零，整个卷积结果的长度会长很多，而且多出来的后面几项也都是零。\n\n而且在这个过程中，我们实际上是把一个有限长度的向量，看作了一个以所有整数 $$\\Z$$ 为定义域的函数，除了那有限的几项之外，其余地方都定义函数值为 0。\n\n用计算机计算的话显然没法如此奢侈地谈“无限多个”，例子中实际用到的，在 $$\\vec f$$ 左右两边各需要 2 个 0，也就是说 `padding=2`, `padding_mode='zeros'`\n\nFigure 1.2 表示的就是 `padding=(1,1)` 的情况（蓝色是被卷张量，白色是 padding，灰色是卷积核，绿色是卷积结果）：\n\n![padding]({{ site.baseurl }}/assets/photos/2022-12-29-padding.png)\n\n既然神经网络中的卷积并不是真正的卷积，所以他们索性不装了——\n\n正常卷积的结果往往比被卷张量大一圈（具体大多少取决于  `kernel_size`, `padding`, `stride` 多个参数），但是图像处理的时候经常希望输出图片和输入图片一样大，此时可以用字符串 `“same”` 作为 `padding` 的参数，自动计算 padding 的大小。`“strict”` 则表示 `padding=0`, 这样输出图片尺寸会变小，但是没有 padding，也就没有往图片里掺杂研究者对图片边缘以外信息的臆测。\n\n同时 `padding_mode` 参数表示往被卷张量四周填充的数字也不一定是 0。比如对于图片，0 往往表示纯黑，而绝大多数图片的视野之外，往往是和图片边缘像素值相差不大的值。所以 `padding_mode` 除了 `zeros` 之外，还接受以下取值：\n\n- `reflect`: 以图片边缘为镜面，把边缘附近的像素值对陈反射出去；\n- `replicate`: 只取边缘的像素值作为常数，直接向外延拓；\n- `circular`: 类似于物理中的周期性边界条件，取对边附近的像素值作为 padding 内容。\n\n### `stride`\n\n前面手算卷积的第4步，把卷积核向右移动了1格，如果每次移动超过1格，就需要这个参数指定移动步长。如果不同方向的步长不同，也是用 tuple 来表示。\n\nFigure 1.4 表示的就是 `stride=(2,2)` 的情况（蓝色是被卷张量，蓝色中的深色块是卷积核，绿色是卷积结果）：\n\n![stride]({{ site.baseurl }}/assets/photos/2022-12-29-stride.png)\n\n### `dilation`\n\n这个参数把“卷积”核撑开，也就相当于在“卷积”核的相邻元素之间加 0。Figure 1.5 表示的就是 `dilation=(1,1)` 的情况（蓝色是被卷张量，蓝色中的深色块是卷积核，绿色是卷积结果）：\n\n![dilation]({{ site.baseurl }}/assets/photos/2022-12-29-dilation.png)\n\n比如 `dilation=1` 时，(1,2,3) 的卷积核就相当于 (1,0,2,0,3)\n\n比如 `dilation=2` 时，(1,2,3) 的卷积核就相当于 (1,0,0,2,0,0,3)\n\n这样可以让卷积核在尺寸比较小的情况下，覆盖到更大面积的被卷张量。当然具体实现时，不可能直接补 0 这么浪费内存。\n\n### `groups`\n\n该参数必须是 `in_channel` 和 `out_channel` 的公约数，当其不为 1 时，就相当于同时做 `groups` 个卷积，其中每个卷积的 `in_channel=in_channel/groups`, `out_channel=out_channel/groups`\n\n### `bias`\n\n该参数是一个布尔值，卷积类似于一种高维空间里的乘法，这个参数就决定是否要拟合 `y=kx+b` 中的 `b`\n\n## “卷积”的“逆运算”： `TransposeConv`\n\n卷积的结果比 padding 之后的被卷张量要小。尤其当“卷积”的 `stride` 约等于 `kernel_size` 时，卷积的就变成了某些池化 (pooling)（求最大值不是一种线性算子，所以最大值池化不能用卷积表示，但是平均值池化可以）。\n\n那么在类似 U-net 这样的模型里，右半边的数据升维（下图中的绿箭头），就需要一种“卷积”的“逆运算”。有人把这种运算叫做 deconvolution，有人叫做 transposed convolution，还有人叫做 convolution with fractional strides。\n\n![Unet]({{ site.baseurl }}/assets/photos/2022-12-29-unet.png)\n\nPyTorch 取的是第二种名字。论文解释了为什么这么取名字，笔记以后有时间再补上把……\n\n因为这个与运算本身就是作为“卷积”的逆运算出现的，所以 PyTorch 的文档里这么说：\n\n\u003e This is set so that when a `Conv2d` and a `ConvTranspose2d` are initialized with same parameters, they are inverses of each other in regard to the input and output shapes.\n\u003e \n\n也就是说，把 `ConvTranspose` 的输入和输出反过来，然后按照 `Conv` 的规则确定各个参数，填入 `ConvTranspose` 的括号里就可以了，除了 `output_padding`\n\n### `output_padding`\n\n`ConvTranspose` 的输出就是对应 `Conv` 的输入。看 Figure 2.7：\n\n![padding_output]({{ site.baseurl }}/assets/photos/2022-12-29-output-padding.png)\n\n当 $$(input+2*padding)/stride$$ 不能整除的时候，最右的几列最下的几行就被卷积核忽略掉了。那么在逆运算 `TransposeConv` 中，这就意味着同一个输入可能对应着 $$stride-1$$ 种可能的输出。`output_padding`参数就可以消除这种歧义，调整 `TransposeConv` 输出张量的尺寸。\n\n## 参考链接\n\n- 给卷积正名: [https://www.kaggle.com/general/225375](https://www.kaggle.com/general/225375)\n- PyTorch Conv2d 源码: [https://pytorch.org/docs/stable/_modules/torch/nn/modules/conv.html#_ConvNd](https://pytorch.org/docs/stable/_modules/torch/nn/modules/conv.html#_ConvNd)\n- 论文: [https://arxiv.org/abs/1603.07285](https://arxiv.org/abs/1603.07285)\n- 动图演示: [https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md](https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md)\n- U-net: [https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/](https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/)\n- PyTorch TransposeConv 文档: [https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html](https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html)\n"},{"slug":"pytorch-dataset-dataloader-sampler","filename":"2022-09-01-pytorch-dataset-dataloader-sampler.md","date":"2022-09-01","title":".py | PyTorch 数据处理方面的封装","layout":"post","keywords":["md","py","ai"],"excerpt":"一般监督学习的数据结构和处理过程，以及 PyTorch 对上述结构和处理过程的封装","content":"\n## 一般监督学习的数据结构和处理过程\n\n### 训练集、验证集、测试集\n\n所有数据整体构成一个大集合，这个集合的每一个元素都包含一个输入和一个目标，分别记作 x 和 y。\n\n把这个大集合分成互相没有交集的三个子集，分别是训练集 (training set)、验证集 (validation set)、测试集 (test set)。\n\n- 训练集和验证集在训练过程中使用。\n    - 训练集的数据带入模型时，模型处于训练模式，模型输出对参数的导数被记录。通过比较把“模型输出”和“训练目标 y”代入**损失函数**的损失，更新模型的参数。同时记录“模型输出”和“训练目标 y”带入验证函数的结果，和验证集比较。\n    - 验证集的数据代入模型时，模型处于求值模式，模型只根据输入计算输出，对参数的导数不记录。通过观察“模型输出”和“训练目标 y”带入**验证函数**的结果，观察训练是否陷入“过拟合”。当训练集的验证函数结果不断下降，但是验证集的验证函数结果几乎不变时，可以认为模型过拟合。\n- 测试集在训练完成之后使用，代入模型时，模型处于求值模式。用于评价训练结果的好坏。\n\n### epoch vs. batch\n\n如果把所有数据同时进行训练，所需要的空间一般都大于电脑内存。所以一般会将训练集随机分成若干批次 (batch)，一个批次的数据同时塞入模型进行训练，在一个 batch 里每一个模型输出对参数的导数累加在一起，整个 batch 结束后更新模型参数，同时导数清零。因为 batch 这个概念和内存有关，所以数值一般选择为 2 的指数。\n\n将训练集所有的 batches 跑完一次称为而一个 epoch。一次训练一般需要很多 epochs，直到损失函数结果足够低，或验证集显示出现过拟合。\n\n## PyTorch 对上述结构和处理过程的封装\n\n### `Dataset`\n\n前面已经说了，数据集包括输入和目标两部分，`Dateset` 及其子类的作用就是\n\n如果在把数据装入 `Dataset` 之前就已经是规整的两个张量了的话——\n\n```python\nimport torch\nfrom torch.utils import data\n\n# ...\n\nfor x,y in zip(train_x,train_y):\n    # do something with x and y\n\ntrainset = TensorDataset(train_x,train_y)\nfor x,y in trainset:\n    # do something with x and y\n```\n\n——这一步确实没什么意思。\n\n有意思的地方在于可以自己写一个数据集类，继承 `Dataset`，然后重载 `__getitem__()` 和 `__len__()` 方法，这样可以把一些不适合用张量表示的数据塞进 `Dataset` 里面，对图像进行学习的话可以在此处加入图像增强的步骤，并进一步用于 `DataLoader`\n\n### `DataLoader`\n\n`DataLoader` = `Dataset` + `Sampler`，因为一般的教程里只需要讲数据集进行简单随机划分，也就只用到了 `batch_size` 等等参数，用到 Sampler 的地方很少。\n\n最常见的用例就是 `WeightedRandomSampler` 。训练分类器的时候，有时其中一个类别的数据远少于其他，那么训练器就更难判断出这一分类（因为只要无脑排除这个类别就能获得不错的正确率），所以需要平衡不同组别之间的权重。\n\n```python\nlist(WeightedRandomSampler(weights=[0.1, 0.9, 0.4, 0.7, 3.0, 0.6], num_samples=5, replacement=True))\n# [4, 4, 1, 4, 5]\nlist(WeightedRandomSampler(weights=[0.9, 0.4, 0.5, 0.2, 0.3, 0.1], num_samples=5, replacement=False))\n# [0, 1, 4, 3, 2]\n```\n\n平衡完之后转化为 batch，搭配 `BatchSampler`：\n\n```python\nlist(BatchSampler(WeightedRandomSampler(weights=[0.1, 0.9, 0.4, 0.7, 3.0, 0.6], num_samples=5, replacement=True), batch_size=2, drop_last=False))\n# [[4, 4], [1, 4], 5]\nlist(BatchSampler(WeightedRandomSampler(weights=[0.1, 0.9, 0.4, 0.7, 3.0, 0.6], num_samples=5, replacement=True), batch_size=2, drop_last=True))\n# [[0, 1], [4, 3]]\n```\n\n### 汇总一下\n\n```python\nimport torch\nfrom torch.utils import data\n\ntrain_x = torch.rand((100,5))\ntrain_y = torch.rand((100,2))\ntrainset = data.TensorDataset(train_x,train_y)\n\n# either:\ntrainloader = data.DataLoader(\n    trainset,\n    batch_size=2,\n    drop_last=True,\n    sampler=data.WeightedRandomSampler(\n        weights=[0.1, 0.9, 0.4, 0.7, 3.0, 0.6], \n        num_samples=5, \n        replacement=True))\n# or:\ntrainloader = data.DataLoader(\n    trainset,\n    batch_sampler=data.BatchSampler(\n        data.WeightedRandomSampler(\n            weights=[0.1, 0.9, 0.4, 0.7, 3.0, 0.6], \n            num_samples=5, \n            replacement=True), \n        batch_size=2, \n        drop_last=True))\n\nfor epoch in range(100):\n    for x,y in trainloader:\n        train(model,x,y,loss_function)\n```\n\n需要注意的是，`for x,y in trainset` 的 x 和 y 的维度是单个数据的维度，最简单的情况就是是 P 和 Q 维向量，而此时如果把 batch_size 记作 B，`for x,y in trainloader` 中的 x 和 y 是维度分别为 (B,P) 和 (B,Q) 的矩阵。`train()` 函数里面的计算要考虑到多出的这一个维度。\n\n## 参考链接：\n\n- [https://pytorch.org/tutorials/beginner/basics/data_tutorial.html](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html)\n- [https://pytorch.org/tutorials/beginner/nn_tutorial.html](https://pytorch.org/tutorials/beginner/nn_tutorial.html)\n- [https://pytorch.org/tutorials/beginner/ptcheat.html](https://pytorch.org/tutorials/beginner/ptcheat.html)\n- [https://pytorch.org/docs/stable/data.html?highlight=dataset#torch.utils.data.Dataset](https://pytorch.org/docs/stable/data.html?highlight=dataset#torch.utils.data.Dataset)\n- [https://pytorch.org/docs/stable/data.html?highlight=dataloader#torch.utils.data.DataLoader](https://pytorch.org/docs/stable/data.html?highlight=dataloader#torch.utils.data.DataLoader)\n- [https://pytorch.org/docs/stable/data.html?highlight=dataloader#torch.utils.data.Sampler](https://pytorch.org/docs/stable/data.html?highlight=dataloader#torch.utils.data.Sampler)\n- [https://pytorch.org/docs/stable/_modules/torch/utils/data/dataset.html#Dataset](https://pytorch.org/docs/stable/_modules/torch/utils/data/dataset.html#Dataset)\n"},{"slug":"knowledge-structure-machine-learning-image-processing-summer-school","filename":"2022-08-17-knowledge-structure-machine-learning-image-processing-summer-school.md","date":"2022-08-17","title":".py | 深度学习暑期学校知识点","layout":"post","keywords":["md","py","ai"],"excerpt":"机器学习在图像处理当中的应用，知识结构树","content":"\n- 电脑设置、python 入门\n    - NoMachine, ssh, python, conda, jupyter\n    - 文件夹操作 `pathlib`, 图片操作 `skimage`\n    - 数据增强 (data augmentation): `imgaug`\n    - TensorBoard\n- 机器学习简介\n    - Linear Classifier\n    - `sklearn**.**model_selection**.**train_test_split()`\n    - Stochastic gradient descent\n    - TensorFlow:\n        - `tensorflow_addons as tfa`\n        - `tfa.image.rotate()`, `tf.image.random_flip_left_right()`\n        - **`from** tensorflow.keras **import** Model`, **`from** tensorflow.keras.models **import** Sequential`, **`from** tensorflow.keras.layers **import** Input, Flatten, Dense, Activation, BatchNormalization, Conv2D, MaxPool2D, Softmax`\n        - `tf**.**keras**.**losses**.**CategoricalCrossentropy()`, `tf**.**keras**.**optimizers**.**Adam(lr**=**1e-3, clipnorm**=**0.001)`\n        - `linear_classifier **=**``Model(...)`, `linear_classifier.compile()`, `linear_classifier.fit()`, `linear_classifier.predict()`\n- 深度学习简介\n    - Perceptron\n    - Perceptron-based XOR gate\n    - **decision boundary** of your model: `np.meshgrid`\n- 图像恢复 (image restoration)\n    - CARE network\n    - Noise2Nosie, Noise2Void\n- 图像翻译 (image translation)\n    - micro-DL: a tool to generate and train U-net from config files.\n- 图像语义分割 (image semantic segmentation): 比较详细，前两节有点水了。\n    - **`from** PIL **import** Image`, **`import** imageio`, **`from** torchvision **import** transforms`\n    - **`from** torch.utils.data **import** Dataset, DataLoader`, **`import** torch.nn **as** nn`, **`from** torch.nn **import** functional **as** F`,  **`from** torch.utils.tensorboard **import** SummaryWriter`,\n    - U-net on PyTorch\n- 图像实例分割 (instance segmentation)\n    1. Foreground segmentation: \n        - **Receptive Field of View**: The term is borrowed from biology where it describes the \"portion of sensory space that can elicit neuronal responses when stimulated\" (wikipedia). Each output pixel can look at/depends on an input patch with that diameter centered at its position. Based on this patch, the network has to be able to make a decision about the prediction for the respective pixel.\n        - **Early Stopping** to avoid overfitting: define an `EarlyStopping` class\n    2. Instance Segmentation:\n        - Ideas:\n            - Three-class model (foreground, background, boundary),\n            - Distance transform (label for each pixel is the distance to the closest boundary),\n            - Edge affinity (consider not just the pixel but also its direct neighbors, predicts the probability that there is an edge, this is called affinity.) 听的时候懂了，回来看的时候没太看懂\n            - Metric learning (learns to predict an embedding vector for each pixel.)\n    3. **Tile and Stitch：**\n        - 当需要处理的图片过大时，将图片切分成多个小图，分别预测之后拼接在一起。\n        - 文中说图片尺寸不是 某个参数的整数倍的时候拼贴结果会不连续，但是代码注释中说等于这个整数倍的时候会不连续，晕。\n        - [https://arxiv.org/pdf/2101.05846.pdf](https://arxiv.org/pdf/2101.05846.pdf)\n    4. 一个实例，epithelia cells\n- 失败模式：极其之水，就是科普了一下训练参数错误的后果，以及一点对抗学习的内容\n- 追踪：比较水，因为机器学习追踪的运算量极大，且主讲人感觉就是来做广告的，所以就直接用 CoLab 体验了一下就完事了。（就这还加州理工呢~）\n- 知识提取：\n    - 前面的基本上是从像素到像素的映射，这里的知识提取是从图片到标签的映射。\n    - CycleGAN\n    - **Create a balanced Dataloader**\n    "},{"slug":"what-a-PyTorch-project-looks-like","filename":"2022-08-17-what-a-PyTorch-project-looks-like.md","date":"2022-08-17","title":".py | 一个 PyTorch 机器学习项目长什么样","layout":"post","keywords":["md","py","ai"],"excerpt":"官网的一个pytorch教程的笔记，原文先按照第一性原理，尽量用原生 python 写了一遍，然后一步一步重构成接近生产环境的代码。这里我把顺序反过来，先放出重构之后的最终结果","content":"\n自学，或者说一切学习和教学，本质就是在已经掌握的知识和未知的目标知识之间修路。路有两种修法，一是理论或者说是第一性原理路线，从不证自明的公理或者已经掌握的知识出发，通过逻辑推理一步步得到新的知识；另一种是实践或者说工程师路线，拿到一个已经可以工作的产品，划分成各个子系统，通过输入的改变来观察输出的不同，直到子系统简化到自己可以理解的地步，不再是黑箱，借此了解整个系统的功能。\n\n但是当学习的对象复杂到一定程度之后，凭借一个人的自学能力，只用其中一种方法往往难以钻透。又或者两种方法学到的路线并非同一条路。对于机器学习，理论路线就是“让输入数据通过一个带有超多参数的函数，根据函数返回值和输出数据之间的差别修正参数，直到函数能够近似输入数据和输出数据之间的关系”；实践中代码往往会使用很多库作者封装好的函数，只读源码往往一头雾水。\n\n所以，看到 PyTorch 官网的这篇教程 **WHAT IS TORCH.NN *REALLY*?:** [https://pytorch.org/tutorials/beginner/nn_tutorial.html](https://pytorch.org/tutorials/beginner/nn_tutorial.html) 可以说是喜出望外，把两种路线写出的代码都给了出来，对于自学者来说，就像罗塞塔石碑一样可以互相对照。这里我把 CNN 相关的部分抽掉了，毕竟 CNN 只是深度学习的一个子集，深度学习只是机器学习的一个子集，和这篇文章的主题关系不大。\n\n原文先按照第一性原理，尽量用原生 python 写了一遍，然后一步一步重构成接近生产环境的代码。这里我把顺序反过来，先放出重构之后的最终结果：\n\n```python\nfrom pathlib import Path\nimport requests\nimport pickle\nimport gzip\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\nfrom torch import optim\nfrom torch.utils.data import TensorDataset,DataLoader\n\n# Using GPU\n\nprint(torch.cuda.is_available())\ndev = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n\n# Wrapping DataLoader\n# https://pytorch.org/tutorials/beginner/basics/data_tutorial.html?highlight=dataloader\n# https://pytorch.org/tutorials/beginner/data_loading_tutorial.html?highlight=dataloader\n\ndef preprocess(x, y):\n    return x.view(-1, 1, 28, 28).to(dev), y.to(dev)\n\ndef get_data(train_ds, valid_ds, bs):\n    return (\n        DataLoader(train_ds, batch_size=bs, shuffle=True),\n        DataLoader(valid_ds, batch_size=bs * 2),\n    )\n\nclass WrappedDataLoader:\n    def __init__(self, dl, func):\n        self.dl = dl\n        self.func = func\n\n    def __len__(self):\n        return len(self.dl)\n\n    def __iter__(self):\n        batches = iter(self.dl)\n        for b in batches:\n            yield (self.func(*b))\n\n# Define the neural network model to be trained\n\n# # If the model is simple:\n# model = nn.Sequential(nn.Linear(784, 10))\n\n# generally the model is a class that inherites nn.Module and implements forward()\nclass Mnist_Logistic(nn.Module):\n    def __init__(self):\n        super().__init__()\n        # self.weights = nn.Parameter(torch.randn(784, 10) / math.sqrt(784))\n        # self.bias = nn.Parameter(torch.zeros(10))\n        self.lin = nn.Linear(784, 10)\n\n    def forward(self, xb):\n        # return xb @ self.weights + self.bias\n        return self.lin(xb)\n\n# Define the training pipeline in fit()\n\ndef loss_batch(model, loss_func, xb, yb, opt=None):\n    loss = loss_func(model(xb), yb)\n\n    if opt is not None:\n        loss.backward()\n        opt.step()\n        opt.zero_grad()\n\n    return loss.item(), len(xb)\n\ndef fit(epochs, model, loss_func, opt, train_dl, valid_dl):\n    for epoch in range(epochs):\n        model.train()\n        for xb, yb in train_dl:\n            loss_batch(model, loss_func, xb, yb, opt)\n\n        model.eval()\n        with torch.no_grad():\n            losses, nums = zip(\n                *[loss_batch(model, loss_func, xb, yb) for xb, yb in valid_dl]\n            )\n        val_loss = np.sum(np.multiply(losses, nums)) / np.sum(nums)\n\n        print(epoch, val_loss)\n    return None\n\n# __main()__:\n\n# data\nDATA_PATH = Path(\"data\")\nPATH = DATA_PATH / \"mnist\"\n\nPATH.mkdir(parents=True, exist_ok=True)\n\nURL = \"https://github.com/pytorch/tutorials/raw/master/_static/\"\nFILENAME = \"mnist.pkl.gz\"\n\nif not (PATH / FILENAME).exists():\n        content = requests.get(URL + FILENAME).content\n        (PATH / FILENAME).open(\"wb\").write(content)\nwith gzip.open((PATH / FILENAME).as_posix(), \"rb\") as f:\n        ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding=\"latin-1\")\n\nx_train, y_train, x_valid, y_valid = map(\n    torch.tensor, (x_train, y_train, x_valid, y_valid)\n)\n\ntrain_dataset = TensorDataset(x_train, y_train)\nvalid_dataset = TensorDataset(x_valid, y_valid)\ntrain_dataloader, valid_dataloader = get_data(train_ds, valid_ds, bs)\ntrain_dataloader = WrappedDataLoader(train_dataloader, preprocess)\nvalid_dataloader = WrappedDataLoader(valid_dataloader, preprocess)\n\n# hyperparameters/model\nlearning_rate = 0.1\nepochs = 2\nloss_function = F.cross_entropy # loss function\nmodel = Mnist_CNN()\nmodel.to(dev)\noptimizer = optim.SGD(model.parameters(), lr=learning_rate , momentum=0.9)\n\n# training\nfit(epochs, model, loss_function, optimizer, train_dataloader, valid_dataloader)\n```\n\n可以看到，一个项目主干可以分成4部分：\n\n1. 准备数据\n2. 定义模型\n3. 描述流程\n4. 实际运行\n\n下面把各部分拆分开来，把两种思路的代码进行对比。\n\n## 1. 准备数据\n\n### 重构之前\n\n```python\nDATA_PATH = Path(\"data\")\nPATH = DATA_PATH / \"mnist\"\n\nPATH.mkdir(parents=True, exist_ok=True)\n\nURL = \"https://github.com/pytorch/tutorials/raw/master/_static/\"\nFILENAME = \"mnist.pkl.gz\"\n\nif not (PATH / FILENAME).exists():\n        content = requests.get(URL + FILENAME).content\n        (PATH / FILENAME).open(\"wb\").write(content)\nwith gzip.open((PATH / FILENAME).as_posix(), \"rb\") as f:\n        ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding=\"latin-1\")\n\nx_train, y_train, x_valid, y_valid = map(\n    torch.tensor, (x_train, y_train, x_valid, y_valid)\n)\nn, c = x_train.shape\n```\n\n### 重构以后：\n\n```python\n# Wrapping DataLoader\n# https://pytorch.org/tutorials/beginner/basics/data_tutorial.html?highlight=dataloader\n# https://pytorch.org/tutorials/beginner/data_loading_tutorial.html?highlight=dataloader\n\ndef preprocess(x, y):\n    return x.view(-1, 1, 28, 28).to(dev), y.to(dev)\n\ndef get_data(train_ds, valid_ds, bs):\n    return (\n        DataLoader(train_ds, batch_size=bs, shuffle=True),\n        DataLoader(valid_ds, batch_size=bs * 2),\n    )\n\nclass WrappedDataLoader:\n    def __init__(self, dl, func):\n        self.dl = dl\n        self.func = func\n\n    def __len__(self):\n        return len(self.dl)\n\n    def __iter__(self):\n        batches = iter(self.dl)\n        for b in batches:\n            yield (self.func(*b))\n```\n\n## 2. 定义模型\n\n### 重构之前\n\n```python\nweights = torch.randn(784, 10) / math.sqrt(784)\nweights.requires_grad_()\nbias = torch.zeros(10, requires_grad=True)\n\ndef log_softmax(x):\n    return x - x.exp().sum(-1).log().unsqueeze(-1)\n\ndef model(xb):\n    return log_softmax(xb @ weights + bias)\n\ndef nll(input, target):\n    return -input[range(target.shape[0]), target].mean()\nloss_func = nll\n\ndef accuracy(out, yb):\n    preds = torch.argmax(out, dim=1)\n    return (preds == yb).float().mean()\n```\n\n### 重构以后\n\n```python\n# If the model is simple:\nmodel = nn.Sequential(nn.Linear(784, 10))\n\n# generally the model is a class that inherites nn.Module and implements forward()\nclass Mnist_Logistic(nn.Module):\n    def __init__(self):\n        super().__init__()\n        # self.weights = nn.Parameter(torch.randn(784, 10) / math.sqrt(784))\n        # self.bias = nn.Parameter(torch.zeros(10))\n        self.lin = nn.Linear(784, 10)\n\n    def forward(self, xb):\n        # return xb @ self.weights + self.bias\n        return self.lin(xb)\n\n```\n\n## 3. 描述流程\n\n### 重构之前\n\n```python\nlr = 0.5  # learning rate\nepochs = 2  # how many epochs to train for\n\nfor epoch in range(epochs):\n    for i in range((n - 1) // bs + 1):\n        #         set_trace()\n        start_i = i * bs\n        end_i = start_i + bs\n        xb = x_train[start_i:end_i]\n        yb = y_train[start_i:end_i]\n        pred = model(xb)\n        loss = loss_func(pred, yb)\n\n        loss.backward()\n        with torch.no_grad():\n            weights -= weights.grad * lr\n            bias -= bias.grad * lr\n            weights.grad.zero_()\n            bias.grad.zero_()\n```\n\n### 重构以后\n\n```python\n\ndef loss_batch(model, loss_func, xb, yb, opt=None):\n    loss = loss_func(model(xb), yb)\n\n    if opt is not None:\n        loss.backward()\n        opt.step()\n        opt.zero_grad()\n\n    return loss.item(), len(xb)\n\ndef fit(epochs, model, loss_func, opt, train_dl, valid_dl):\n    for epoch in range(epochs):\n        model.train()\n        for xb, yb in train_dl:\n            loss_batch(model, loss_func, xb, yb, opt)\n\n        model.eval()\n        with torch.no_grad():\n            losses, nums = zip(\n                *[loss_batch(model, loss_func, xb, yb) for xb, yb in valid_dl]\n            )\n        val_loss = np.sum(np.multiply(losses, nums)) / np.sum(nums)\n\n        print(epoch, val_loss)\n    return None\n```\n\n## 4. 实际运行\n\n### 重构之前\n\n```python\n# __main()__:\nprint(loss_func(model(xb), yb), accuracy(model(xb), yb))\n```\n\n### 重构以后\n\n```python\n# __main()__:\n\n# data\nDATA_PATH = Path(\"data\")\nPATH = DATA_PATH / \"mnist\"\n\nPATH.mkdir(parents=True, exist_ok=True)\n\nURL = \"https://github.com/pytorch/tutorials/raw/master/_static/\"\nFILENAME = \"mnist.pkl.gz\"\n\nif not (PATH / FILENAME).exists():\n        content = requests.get(URL + FILENAME).content\n        (PATH / FILENAME).open(\"wb\").write(content)\nwith gzip.open((PATH / FILENAME).as_posix(), \"rb\") as f:\n        ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding=\"latin-1\")\n\nx_train, y_train, x_valid, y_valid = map(\n    torch.tensor, (x_train, y_train, x_valid, y_valid)\n)\n\ntrain_dataset = TensorDataset(x_train, y_train)\nvalid_dataset = TensorDataset(x_valid, y_valid)\ntrain_dataloader, valid_dataloader = get_data(train_ds, valid_ds, bs)\ntrain_dataloader = WrappedDataLoader(train_dataloader, preprocess)\nvalid_dataloader = WrappedDataLoader(valid_dataloader, preprocess)\n\n# hyperparameters/model\nlearning_rate = 0.1\nepochs = 2\nloss_function = F.cross_entropy # loss function\nmodel = Mnist_CNN()\nmodel.to(dev)\noptimizer = optim.SGD(model.parameters(), lr=learning_rate , momentum=0.9)\n\n# training\nfit(epochs, model, loss_function, optimizer, train_dataloader, valid_dataloader)\n```"},{"slug":"tensorboard-on-pytorch","filename":"2022-04-08-tensorboard-on-pytorch.md","date":"2022-04-08","title":".py | TensorBoard 笔记（PyTorch 版）","layout":"post","keywords":["md","py","ai"],"excerpt":"TensorBoard 是 TensorFlow 团队开发的一款可视化工具，方便观察和调整机器学习的数据集、模型、超参数和训练结果等等。但是不知道为什么，PyTorch 调用 TensorBoard，要比 TensorFlow 方便简单得多，\u003cdel\u003e这何尝不是一种 NTR\u003c/del\u003e……","content":"\n- 官网教程：[https://pytorch.org/tutorials/intermediate/tensorboard_tutorial.html](https://pytorch.org/tutorials/intermediate/tensorboard_tutorial.html)\n- 官方文档：[https://pytorch.org/docs/stable/tensorboard.html](https://pytorch.org/docs/stable/tensorboard.html)\n\nTensorBoard 是 TensorFlow 团队开发的一款可视化工具，方便观察和调整机器学习的数据集、模型、超参数和训练结果等等。但是不知道为什么，PyTorch 调用 TensorBoard，要比 TensorFlow 方便简单得多，~~这何尝不是一种 NTR~~……\n\n---\n\n## 用法和效果\n\n一个使用了 TensorBoard 的 torch 项目的主文件一般是这样的（把和 TensorBoard 无关的部分都注释掉了）：\n\n```python\n# imports\n# import matplotlib.pyplot as plt\n# import numpy as np\n\n# import torch\n# import torchvision\n# import torchvision.transforms as transforms\n\n# import torch.nn as nn\n# import torch.nn.functional as F\n# import torch.optim as optim\n\nfrom torch.utils.tensorboard import SummaryWriter\n\n# # transforms\n# transform = transforms.Compose(\n#     [transforms.ToTensor(),\n#     transforms.Normalize((0.5,), (0.5,))])\n\n# # datasets\n# trainset = torchvision.datasets.FashionMNIST('./data',\n#     download=True,\n#     train=True,\n#     transform=transform)\n# testset = torchvision.datasets.FashionMNIST('./data',\n#     download=True,\n#     train=False,\n#     transform=transform)\n\n# # dataloaders\n# trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n#                                         shuffle=True, num_workers=2)\n# testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n#                                         shuffle=False, num_workers=2)\n\n# # constant for classes\n# classes = ('T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n#         'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle Boot')\n\n# class Net(nn.Module):\n#     def __init__(self):\n#         super(Net, self).__init__()\n#         self.conv1 = nn.Conv2d(1, 6, 5)\n#         self.pool = nn.MaxPool2d(2, 2)\n#         self.conv2 = nn.Conv2d(6, 16, 5)\n#         self.fc1 = nn.Linear(16 * 4 * 4, 120)\n#         self.fc2 = nn.Linear(120, 84)\n#         self.fc3 = nn.Linear(84, 10)\n#     def forward(self, x):\n#         x = self.pool(F.relu(self.conv1(x)))\n#         x = self.pool(F.relu(self.conv2(x)))\n#         x = x.view(-1, 16 * 4 * 4)\n#         x = F.relu(self.fc1(x))\n#         x = F.relu(self.fc2(x))\n#         x = self.fc3(x)\n#         return x\n# net = Net()\n\n# criterion = nn.CrossEntropyLoss()\n# optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n\n# default `log_dir` is \"runs\" - we'll be more specific here\nwriter = SummaryWriter('runs/fashion_mnist_experiment_1')\n\nrunning_loss = 0.0\nfor epoch in range(1):  # loop over the dataset multiple times\n    for i, data in enumerate(trainloader, 0):\n        # # get the inputs; data is a list of [inputs, labels]\n        # inputs, labels = data\n\n        # # zero the parameter gradients\n        # optimizer.zero_grad()\n\n        # # forward + backward + optimize\n        # outputs = net(inputs)\n        # loss = criterion(outputs, labels)\n        # loss.backward()\n        # optimizer.step()\n\n        # running_loss += loss.item()\n        if i % 1000 == 999:    # every 1000 mini-batches...\n            # ...log the running loss\n            writer.add_scalar('training loss',\n                            running_loss / 1000,\n                            epoch * len(trainloader) + i) # 注意这一行！\n# print('Finished Training')\n```\n\n正常情况下，使用了tensorboard 的项目在训练的过程中，可以用网页浏览器打开网址 `localhost:6006`，应该可以看到和下图类似但不同的画面：\n\n![tensorboard](/photos/2022-04-08-tensorboard.png)\n\n下面来仔细分解。\n\n## 代码分解\n\n[引入](python-import-script-module-package) TensorBoard 需要下面一行代码：\n\n```python\nfrom torch.utils.tensorboard import SummaryWriter\n```\n\n从名字就能看出来，`SummaryWriter` 是一个 class。粗略用了一下文档页面的业内搜索，好像整个 `torch.utils.tensorboard` 就只有这一个 class。\n\n新建一个这个类的实例：\n\n```python\nwriter = SummaryWriter('runs/fashion_mnist_experiment_1')\n```\n\n这一步会在当前工作环境下新建一个 `/runs` 文件夹，\n\n要想显示导航栏上的“SCALARS”、“IMAGES”等等选项卡，并且让自己想观察的数据显示在各自类别的选项卡里，需要调用 `SummaryWriter` 下面的各种方法，比如 `add_scalar()`, `add_image()`.\n\n### 各种方法\n\n用法和效果举例如下：\n\n- `add_scalar()`: 在一张图中画出**一个标量**指标随学习迭代的**变化曲线**。\n    - `tag`: 图片的标题。\n    - `scalar_value`: 指标的值，也就是纵坐标。\n    - `global_step`: 全局迭代次数，也就是横坐标。\n- `add_scalars()`: 在一张图中同时画出**多个指标**随学习迭代的**变化曲线**。\n    - `main_tag`: 图片的标题\n    - `tag_scalar_dict`: 一个字典，字典的键是变量的名字，值是各个变量的纵坐标。\n    - `global_step`: 全局迭代次数，也就是横坐标。\n- `add_custome_scalars()`: 没有用过，也没见到例子，不太明白。根据描述像是把之前 `add_scalar()` 和 `add_scalars()` 的结果重新组合，对 SCALARS 选项卡重新排版。每个 `SummaryWriter` 只能运行一次，可以在训练开始前运行，也可以在之后。\n    - `layout`: 只有一个这参数，是一个字典，字典的键像是新图片/章节的名字，值是下一级字典或者是 `add_scalar()` 出现过/将要出现的 `tag` 参数。\n- `add_figure()`: 显示 `matplotlib` 画出的**图表**。\n    - `tag`: 标题\n    - `figure`: 图表，要求类型为 `matplotlib.pyplot.figure`\n    - `global_step`:迭代次数，效果如何 没试过。\n- `add_histogram()`: 在一张图中画出一个样本的**直方图**，以及这个直方图随迭代变化的规律。这是个三维图片，x 轴是直方图的取值范围，y 轴是迭代次数，z 轴是直方图的频率值。\n    - `tag`: 图片标题。\n    - `values`: 一个 `torch.Tensor` 或者 `numpy.array` ，用于绘制直方图的样本.\n    - `global_step`: 迭代次数，y 轴分量。\n    - `bins`: 取样间隔参数，`numpy.histogram()` 中用到的。 ****\n- `add_graph()`: 一般用于在训练前画出神经网络的**图状结构**。\n    - `model`: 要画的模型，类型是 `torch.nn.Module`\n    - `input_to_model=None`: （可选）输入模型的变量。\n- `add_mesh()`: **三维点云**。\n    - `tag`: 表格标题。\n    - `vertices`: 顶点三维坐标的列表。\n    - `colors`: 顶点的颜色。\n    - `faces`: （可选）没看懂 (Indices of vertices within each triangle.)\n    - `config_dict`: 用于画图的 ThreeJS 的参数。\n    - `global_step`: 迭代次数。\n- `add_embeddding()`: 神经网络的输入一般是高维向量，此工具将高维数据**投影到三维**空间，然后画出图像，方便我们感知训练集内样本之间的关系。\n    - `mat`: 一个矩阵，每一行都是一个要处理的向量。\n    - `metadata`: 标记文字，一般是列表。\n    - `label_img`: 标记图片，显示在每个数据点旁边的\n    - `global_step`: 迭代次数，一般没人用。\n    - `tag`: 图片标题。\n- `add_pr_curve()`: 准确率 (precision) -召回率 (call back) 曲线。准确率=真阳性/(真阳性+假阳性)，召回率=真阳性/(真阳性+假阴性)\n    - `tag`: 图片标题。\n    - `labels`: Ground truth 数据，每个数据点对应一个布尔值。\n    - `predictions`: 模型的输出，每个数据点对应一个 [0,1] 之间的实数。\n    - `global_step`: 迭代次数。\n    - `num_thresholds`:用于画出 PR 曲线的阈值的数目。\n- `add_hparams()`: 比较不同次运行之间的超参数。没太看懂。\n    - `hparam_dict`: 超参数的名称和取值\n    - `metric_dict`: metric （不知道怎么翻译）的名称和取值\n    - `hparam_domain_discrete`: （可选）字典，超参数的名称和有限个可能的取值。\n    - `run_name`: 运行的名称，将会成为 `logdir` 的一部分。\n- `add_image()`: 在 IMAGES 选项卡中显示**一张图片**。\n    - `tag`: 图片名称。\n    - `img_tensor`: 一个 `torch.Tensor` 或者 `numpy.array`，被显示的图片。对应于下面的 `dataformats` 参数。\n    - `global_step`: 迭代次数，没见有人在显示图片的时候用过。\n    - `dataformats=’CHW’`: 图片各维度的顺序。默认是“颜色-高度-宽度”。\n- `add_images()`: 并列显示**多张图片**。\n    - `tag`: 图片组的标题。\n    - `img_tensor`: 一个 `torch.Tensor` 或者 `numpy.array`，被显示的图片。图片个维度的含义由 `datadormats` 给出。\n    - `global_step`: 迭代次数，没见有人在显示图片的时候用过。\n    - `datadormats=’NCHW’`: 图片各维度的顺序。默认为“图片序号-颜色-高度-宽度”。\n- `add_video()`: 略\n- `add_audio()`: 略\n- `add_text()`: 略"},{"slug":"install-pytorch-cpu-on-fedora","filename":"2021-10-13-install-pytorch-cpu-on-fedora.md","date":"2021-10-13","title":".py | Fedora 上安装 CPU 版 pytorch","layout":"post","keywords":["md","py","ai"],"excerpt":"import torch (as tf)","content":"\n马上要参加一个暑期学校，关于深度学习在显微图像处理当中的应用。\n\n深度学习是机器学习的一个分支，机器学习中的绝大多数数据都可以抽象为向量（一阶张量），绝大多数的算法都可以分解为向量之间的运算，或者对向量的变换，表示为矩阵（二阶张量）。这就对张量计算相关算法的库函数产生了很大的需求。PyTorch 和 TensorFlow，还有其他的一些库，比如 Keras，Caffe 等等等等，都是为此而生。早期版本的 pytorch 和 tensorflow 有很大的区别，但是随着版本的迭代，两者逐渐兼并和挤掉了其他的竞争者，两者的相似之处也越来越多，“变成了自己曾经最讨厌的样子”。lol\n\n不知道课上究竟要使用哪种机器学习的框架，所以决定把 PyTorch 和 TensorFlow 全都安装了（摊手）。正好可以接着上一篇的 [python 教程](python-interpreter-editor-virtualenv) 往下写。\n\n先说一下自己的软硬件环境：Intel 家的 CPU 和集成显卡（玩不了《文明6》）。虽然不在官方支持 Linux 的名单上，但是自己安装了 Fedora，内核更新了几次之后已经没有了兼容性问题。python 版本 3.9.6，包管理器是 pip，编辑器是 vscode。\n\n## 建立虚拟环境\n\n为什么要建立虚拟环境的问题本系列的前一篇已经回答过了，这次直接开干。我给两个虚拟环境分别取名为 `torch` 和`tf` 。关于命令行部分的代码，为了表示各个虚拟环境，特别加上了命令提示符`(env)[me@mycomputer]$`，抄代码的时候注意去掉。\n\n```bash\n\n[me@mycomputer]$ mkvirtualenv torch\n(torch)[me@mycomputer]$\n```\n\n## 安装\n\n在 PyTorch 官网，找到自己的硬件配制对应的安装命令：[https://pytorch.org/get-started/locally/](https://pytorch.org/get-started/locally/)。比如我的就是 `Stable`\u003e`Linux`\u003e`Pip`\u003e`Python`\u003e`CPU`。把生成的命令复制到命令行：\n\n```bash\n\n(torch)[me@mycomputer]$ pip3 install torch==1.9.1+cpu torchvision==0.10.1+cpu torchaudio==0.9.1 -f https://download.pytorch.org/whl/torch_stable.html\n```\n\n等待各种提示信息显示安装完成。\n\n## 验证和退出\n\n按照 [官网给出的方法](https://pytorch.org/get-started/locally/#linux-verification)，验证安装是否成功：\n\n```python\n\nimport torch\nx = torch.rand(5, 3)\nprint(x)\n\n# tensor([[0.3799, 0.4494, 0.4296],\n#       [0.5800, 0.0180, 0.3110],\n#       [0.9847, 0.0125, 0.2648],\n#       [0.0296, 0.3142, 0.9266],\n#       [0.3192, 0.9645, 0.5545]])\n```\n\n为了下一步安装 tensorflow，先要退出到默认的虚拟环境：\n\n```bash\n\n(torch)[me@mycomputer]$ deactivate\n[me@mycomputer]$\n```\n\n## 说好的 TensorFlow 呢\n\n本来这篇文章是打算把  pytorch 和 tensorflow 一起写了，结果 tensorflow 实在是不给力。\n\n- 直接安装\n\n在 [TensorFlow 的官网](https://www.tensorflow.org/install)上方导航栏找到 install 按钮，然后在页面左侧找到 package/pip，[安装命令](https://www.tensorflow.org/install/pip#3.-install-the-tensorflow-pip-package)也是只有一句话\n\n```python\n\npip install --upgrade tensorflow\n```\n\n然而不行，虽然安装过程中没有报错，但是验证安装的时候报出一堆错误。\n\n报错信息里有一句 `Could not load dynamic library 'libcudart.so.11.0'`，怀疑上面命令安装的是 GPU 版本。\n\n- 安装 CPU 版本\n\n在网页正文的“Package Location”一节找到了 CPU 版本的安装文件：`https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow_cpu-2.6.0-cp39-cp39-manylinux2010_x86_64.whl`，于是删除虚拟环境、重建虚拟环境、重新安装。\n\n```bash\n\n(tf)[me@mycomputer]$ deactivate\n[me@mycomputer]$ rmvirtualenv tf\n[me@mycomputer]$ mkvirtualenv tf\n(tf)[me@mycomputer]$ pip install --upgrade pip\n(tf)[me@mycomputer]$ pip install https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow_cpu-2.6.0-cp39-cp39-manylinux2010_x86_64.whl\n```\n\n运行官方提供的测试之后依然会有警告信息：\n\n```bash\n\n(tf) [shixing@yoga-laptop ~]$ python -c \"import tensorflow as tf;print(tf.reduce_sum(tf.random.normal([1000, 1000])))\"\n20XX-XX-XX XX:XX:XX.XXXXXX: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\ntf.Tensor(-1338.4773, shape=(), dtype=float32)\n```\n\n[stackoverflow 的这个回答](https://stackoverflow.com/questions/47068709/your-cpu-supports-instructions-that-this-tensorflow-binary-was-not-compiled-to-u) 说，需要从源码编译 tensorflow，具体的方法在[官网也有](https://www.tensorflow.org/install/source#linux)，但是实在是太麻烦了，~~（还是鸽了）~~ 下次单独写成一篇吧。"}]]]},"__N_SSG":true},"page":"/articles/[id]","query":{"id":"what-is-intelligence-not-same-as-intelligence-is-what"},"buildId":"buildID","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>